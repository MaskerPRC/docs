<!DOCTYPE html>
<html lang="zh_CN">
<head>
  <meta charset="UTF-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Index — PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/genindex.html">
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css">
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css">
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css">
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css">
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css">
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css">
    <link rel="index" title="Index" href="#">
    <link rel="search" title="Search" href="search.html">

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head><body class="pytorch-body"><div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">学习</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class="dropdown-title">开始使用</span>
                  <p>在本地运行 PyTorch 或快速开始使用支持的云平台之一</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">教程</span><p></p>
                  <p>PyTorch 教程中的新内容</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">学习基础知识</span><p></p>
                  <p>熟悉 PyTorch 的概念和模块</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch 食谱</span><p></p>
                  <p>精简版、可直接部署的 PyTorch 代码示例</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">PyTorch 入门 - YouTube 系列</span><p></p>
                  <p>通过我们引人入胜的 YouTube 教程系列掌握 PyTorch 基础知识</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">生态系统</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">工具</span><p></p>
                  <p>了解 PyTorch 生态系统中的工具和框架</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">社区</span>
                  <p>加入 PyTorch 开发者社区，贡献、学习并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">论坛</span>
                  <p>讨论 PyTorch 代码、问题、安装、研究的地方</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">开发者资源</span>
                  <p>查找资源并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">贡献者奖项 - 2024</span><p></p>
                  <p>本届 PyTorch 会议揭晓获奖者</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">关于 PyTorch Edge</span><p></p>
                  <p>为边缘设备构建创新和隐私感知的 AI 体验</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span><p></p>
                  <p>基于移动和边缘设备的端到端推理能力解决方案</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch 文档</span><p></p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">文档</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span><p></p>
                  <p>探索文档以获取全面指导，了解如何使用 PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch 领域</span><p></p>
                  <p>阅读 PyTorch 领域的文档，了解更多关于特定领域库的信息</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">博客与新闻</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch 博客</span><p></p>
                  <p>捕捉最新的技术新闻和事件</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">社区博客</span><p></p>
                  <p>PyTorch 生态系统故事</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">视频</span><p></p>
                  <p>了解最新的 PyTorch 教程、新内容等</p>
                </a><a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">社区故事</span><p></p>
                  <p>学习如何我们的社区使用 PyTorch 解决真实、日常的机器学习问题</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">活动</span><p></p>
                  <p>查找活动、网络研讨会和播客</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">通讯</span><p></p>
                  <p>跟踪最新更新</p>
                </a>
            </div>
          </div></li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">关于</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch 基金会</span><p></p>
                  <p>了解更多关于 PyTorch 基金会的信息</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">管理委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">云信用计划</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">技术顾问委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">员工</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">联系我们</span><p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">成为会员</a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>



   

    

    <div class="table-of-contents-link-wrapper">
      <span>目录</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="https://pytorch.org/docs/versions.html">主程序 (2.7.0+cpu ) ▼</a>
    </div>
    <div id="searchBox">
    <div class="searchbox" id="googleSearchBox">
      <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
      <div class="gcse-search"></div>
    </div>
    <div id="sphinxSearchBox" style="display: none;">
      <div role="search">
        <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
          <input type="text" name="q" placeholder="Search Docs">
          <input type="hidden" name="check_keywords" value="yes">
          <input type="hidden" name="area" value="default">
        </form>
      </div>
    </div>
  </div>
  <form id="searchForm">
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="google" checked="">谷歌搜索</label>
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="sphinx">经典搜索</label>
  </form>

  <script>
     document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
  </script>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/genindex.html">您正在查看不稳定开发者预览文档。请点击此处查看最新稳定版本的文档。</a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">社区</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch 治理 | 构建 + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch 贡献指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch 设计哲学</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch 治理 | 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch 治理 | 维护者</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者笔记</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">自动混合精度示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">广播语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU 多线程和 TorchScript 推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA 语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/custom_operators.html">PyTorch 自定义算子页面</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">分布式数据并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">扩展 PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">使用 autograd.Function 扩展 torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">常见问题解答</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/fsdp.html">FSDP 笔记</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/get_start_xpu.html">在 Intel GPU 上入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm)语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">大规模部署功能</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/libtorch_stable_abi.html">LibTorch 稳定 ABI</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">模块</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS 后端</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">多进程最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">数值精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">可重现性</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">序列化语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows 常见问题解答</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语言绑定</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">张量属性</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">张量视图</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="accelerator.html">torch.accelerator</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">理解 CUDA 内存使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">生成快照</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">使用可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">快照 API 参考</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.memory.html">torch.mtia.memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">元设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.html">torch.distributed.tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.fsdp.fully_shard.html">torch.distributed.fsdp.fully_shard</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">复数</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP 通信钩子</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">分布式 RPC 框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">类型信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">命名张量</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">命名张量操作覆盖率</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_environment_variables.html">火炬环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">库</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">火炬推荐</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch 在 XLA 设备上</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/ao">torchao</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        文档 &gt;</li>

        
      <li>索引</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">快捷键</div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              

<h1 id="index">索引</h1>

<div class="genindex-jumpbox">
 <a href="#_"><strong>_</strong></a>
 | <a href="#A"><strong>A</strong></a>
 | <a href="#B"><strong>B</strong></a>
 | <a href="#C"><strong>C</strong></a>
 | <a href="#D"><strong>D</strong></a>
 | <a href="#E"><strong>E</strong></a>
 | <a href="#F"><strong>F</strong></a>
 | <a href="#G"><strong>G</strong></a>
 | <a href="#H"><strong>H</strong></a>
 | <a href="#I"><strong>I</strong></a>
 | <a href="#J"><strong>J</strong></a>
 | <a href="#K"><strong>K</strong></a>
 | <a href="#L"><strong>L</strong></a>
 | <a href="#M"><strong>M</strong></a>
 | <a href="#N"><strong>N</strong></a>
 | <a href="#O"><strong>O</strong></a>
 | <a href="#P"><strong>P</strong></a>
 | <a href="#Q"><strong>Q</strong></a>
 | <a href="#R"><strong>R</strong></a>
 | <a href="#S"><strong>S</strong></a>
 | <a href="#T"><strong>T</strong></a>
 | <a href="#U"><strong>U</strong></a>
 | <a href="#V"><strong>V</strong></a>
 | <a href="#W"><strong>W</strong></a>
 | <a href="#X"><strong>X</strong></a>
 | <a href="#Z"><strong>Z</strong></a>
 
</div>
<h2 id="_">_</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.DTensor.__create_chunk_list__">__create_chunk_list__() (torch.distributed.tensor.DTensor 方法)</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__getstate__">__getstate__() (torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState 方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.FileStore.__init__">__init__() (torch.distributed.FileStore 方法)</a>

      <ul>
        <li><a href="distributed.html#torch.distributed.HashStore.__init__">(torch.distributed.HashStore 方法)</a>
</li>
        <li><a href="distributed.html#torch.distributed.PrefixStore.__init__">(torch.distributed.PrefixStore 方法)</a>
</li>
        <li><a href="distributed.html#torch.distributed.Store.__init__">(torch.distributed.Store 方法)</a>
</li>
        <li><a href="distributed.html#torch.distributed.TCPStore.__init__">(torch.distributed.TCPStore 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Graph.__init__">(torch.fx.Graph 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.GraphModule.__init__">(torch.fx.GraphModule 方法)</a>
</li>
        <li><a href="monitor.html#torch.monitor.Event.__init__">(torch.monitor.Event 方法)</a>
</li>
        <li><a href="monitor.html#torch.monitor.Stat.__init__">(torch.monitor.Stat 方法)</a>
</li>
        <li><a href="monitor.html#torch.monitor.TensorboardEventHandler.__init__">(torch.monitor.TensorboardEventHandler 方法)</a>
</li>
        <li><a href="package.html#torch.package.PackageExporter.__init__">(torch.package.PackageExporter 方法)</a>
</li>
        <li><a href="package.html#torch.package.PackageImporter.__init__">(torch.package.PackageImporter 方法)</a>
</li>
        <li><a href="tensors.html#torch.Tensor.__init__">torch.Tensor 方法</a>
</li>
        <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.__init__">(torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      </ul></li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__setstate__">__setstate__() (torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState 方法)</a>
</li>
      <li><a href="generated/torch._assert.html#torch._assert">_assert() (在模块 torch 中)</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._assign_worker_ranks">_assign_worker_ranks() (torch.distributed.elastic.agent.server.SimpleElasticAgent 方法)</a>
</li>
      <li><a href="generated/torch.cuda.jiterator._create_jit_fn.html#torch.cuda.jiterator._create_jit_fn">torch.cuda.jiterator 模块中的_create_jit_fn()函数</a>
</li>
      <li><a href="generated/torch.cuda.jiterator._create_multi_output_jit_fn.html#torch.cuda.jiterator._create_multi_output_jit_fn">torch.cuda.jiterator 模块中的_create_multi_output_jit_fn()函数</a>
</li>
      <li><a href="torch_cuda_memory.html#torch.cuda.memory._dump_snapshot">torch.cuda.memory 模块中的_dump_snapshot()函数</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._exit_barrier">torch.distributed.elastic.agent.server.SimpleElasticAgent 方法中的_exit_barrier()</a>
</li>
      <li><a href="generated/torch._foreach_abs.html#torch._foreach_abs">torch 模块中的_foreach_abs()</a>
</li>
      <li><a href="generated/torch._foreach_abs_.html#torch._foreach_abs_">torch 模块中的_foreach_abs_()</a>
</li>
      <li><a href="generated/torch._foreach_acos.html#torch._foreach_acos">torch 模块中的_foreach_acos()</a>
</li>
      <li><a href="generated/torch._foreach_acos_.html#torch._foreach_acos_">torch 模块中的_foreach_acos_()</a>
</li>
      <li><a href="generated/torch._foreach_asin.html#torch._foreach_asin">torch 模块中的_foreach_asin()</a>
</li>
      <li><a href="generated/torch._foreach_asin_.html#torch._foreach_asin_">torch 模块中的_foreach_asin_()</a>
</li>
      <li><a href="generated/torch._foreach_atan.html#torch._foreach_atan">torch 模块中的_foreach_atan()</a>
</li>
      <li><a href="generated/torch._foreach_atan_.html#torch._foreach_atan_">torch 模块中的_foreach_atan_()</a>
</li>
      <li><a href="generated/torch._foreach_ceil.html#torch._foreach_ceil">torch 模块中的_foreach_ceil()</a>
</li>
      <li><a href="generated/torch._foreach_ceil_.html#torch._foreach_ceil_">torch 模块中的_foreach_ceil_()</a>
</li>
      <li><a href="generated/torch._foreach_cos.html#torch._foreach_cos">torch 模块中的_foreach_cos()</a>
</li>
      <li><a href="generated/torch._foreach_cos_.html#torch._foreach_cos_">torch 模块中的_foreach_cos_()</a>
</li>
      <li><a href="generated/torch._foreach_cosh.html#torch._foreach_cosh">torch 模块中的_foreach_cosh()</a>
</li>
      <li><a href="generated/torch._foreach_cosh_.html#torch._foreach_cosh_">torch 模块中的_foreach_cosh_()</a>
</li>
      <li><a href="generated/torch._foreach_erf.html#torch._foreach_erf">torch 模块中的_foreach_erf()</a>
</li>
      <li><a href="generated/torch._foreach_erf_.html#torch._foreach_erf_">torch 模块中的_foreach_erf_()</a>
</li>
      <li><a href="generated/torch._foreach_erfc.html#torch._foreach_erfc">torch 模块中的_foreach_erfc()</a>
</li>
      <li><a href="generated/torch._foreach_erfc_.html#torch._foreach_erfc_">torch 模块中的_foreach_erfc_()</a>
</li>
      <li><a href="generated/torch._foreach_exp.html#torch._foreach_exp">torch 模块中的_foreach_exp()</a>
</li>
      <li><a href="generated/torch._foreach_exp_.html#torch._foreach_exp_">torch 模块中的_foreach_exp_()</a>
</li>
      <li><a href="generated/torch._foreach_expm1.html#torch._foreach_expm1">torch 模块中的_foreach_expm1()</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch._foreach_expm1_.html#torch._foreach_expm1_">torch 模块中的_foreach_expm1_()</a>
</li>
      <li><a href="generated/torch._foreach_floor.html#torch._foreach_floor">torch 模块中的_foreach_floor()</a>
</li>
      <li><a href="generated/torch._foreach_floor_.html#torch._foreach_floor_">torch 模块中的_foreach_floor_()</a>
</li>
      <li><a href="generated/torch._foreach_frac.html#torch._foreach_frac">torch 模块中的_foreach_frac()</a>
</li>
      <li><a href="generated/torch._foreach_frac_.html#torch._foreach_frac_">torch 模块中的_foreach_frac_()</a>
</li>
      <li><a href="generated/torch._foreach_lgamma.html#torch._foreach_lgamma">torch 模块中的_foreach_lgamma()</a>
</li>
      <li><a href="generated/torch._foreach_lgamma_.html#torch._foreach_lgamma_">torch 模块中的_foreach_lgamma_()</a>
</li>
      <li><a href="generated/torch._foreach_log.html#torch._foreach_log">torch 模块中的_foreach_log()</a>
</li>
      <li><a href="generated/torch._foreach_log10.html#torch._foreach_log10">torch 模块中的_foreach_log10()</a>
</li>
      <li><a href="generated/torch._foreach_log10_.html#torch._foreach_log10_">torch 模块中的_foreach_log10_()</a>
</li>
      <li><a href="generated/torch._foreach_log1p.html#torch._foreach_log1p">torch 模块中的_foreach_log1p()</a>
</li>
      <li><a href="generated/torch._foreach_log1p_.html#torch._foreach_log1p_">torch 模块中的_foreach_log1p_()</a>
</li>
      <li><a href="generated/torch._foreach_log2.html#torch._foreach_log2">torch 模块中的_foreach_log2_()</a>
</li>
      <li><a href="generated/torch._foreach_log2_.html#torch._foreach_log2_">torch 模块中的_foreach_log2_()</a>
</li>
      <li><a href="generated/torch._foreach_log_.html#torch._foreach_log_">torch 模块中的_foreach_()</a>
</li>
      <li><a href="generated/torch._foreach_neg.html#torch._foreach_neg">torch 模块中的_foreach_neg()</a>
</li>
      <li><a href="generated/torch._foreach_neg_.html#torch._foreach_neg_">torch 模块中的_foreach_neg_()</a>
</li>
      <li><a href="generated/torch._foreach_reciprocal.html#torch._foreach_reciprocal">torch 模块中的_foreach_reciprocal()</a>
</li>
      <li><a href="generated/torch._foreach_reciprocal_.html#torch._foreach_reciprocal_">torch 模块中的_foreach_reciprocal_()</a>
</li>
      <li><a href="generated/torch._foreach_round.html#torch._foreach_round">torch 模块中的_foreach_round()</a>
</li>
      <li><a href="generated/torch._foreach_round_.html#torch._foreach_round_">torch 模块中的_foreach_round_()</a>
</li>
      <li><a href="generated/torch._foreach_sigmoid.html#torch._foreach_sigmoid">torch 模块中的_foreach_sigmoid()</a>
</li>
      <li><a href="generated/torch._foreach_sigmoid_.html#torch._foreach_sigmoid_">torch 模块中的_foreach_sigmoid_()</a>
</li>
      <li><a href="generated/torch._foreach_sin.html#torch._foreach_sin">torch 模块中的_foreach_sin()</a>
</li>
      <li><a href="generated/torch._foreach_sin_.html#torch._foreach_sin_">torch 模块中的_foreach_sin_()</a>
</li>
      <li><a href="generated/torch._foreach_sinh.html#torch._foreach_sinh">torch 模块中的_foreach_sinh()</a>
</li>
      <li><a href="generated/torch._foreach_sinh_.html#torch._foreach_sinh_">torch 模块中的_foreach_sinh_()</a>
</li>
      <li><a href="generated/torch._foreach_sqrt.html#torch._foreach_sqrt">torch 模块中的_foreach_sqrt()</a>
</li>
      <li><a href="generated/torch._foreach_sqrt_.html#torch._foreach_sqrt_">torch 模块中的_foreach_sqrt_()</a>
</li>
      <li><a href="generated/torch._foreach_tan.html#torch._foreach_tan">torch 模块中的_foreach_tan()</a>
</li>
      <li><a href="generated/torch._foreach_tan_.html#torch._foreach_tan_">torch 模块中的_foreach_tan_()</a>
</li>
      <li><a href="generated/torch._foreach_trunc.html#torch._foreach_trunc">torch 模块中的_foreach_trunc()函数</a>
</li>
      <li><a href="generated/torch._foreach_trunc_.html#torch._foreach_trunc_">torch 模块中的_foreach_trunc_()函数</a>
</li>
      <li><a href="generated/torch._foreach_zero_.html#torch._foreach_zero_">torch 模块中的_foreach_zero_()函数</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._initialize_workers">torch.distributed.elastic.agent.server.SimpleElasticAgent 方法中的_initialize_workers()函数</a>
</li>
      <li><a href="profiler.html#torch.profiler._KinetoProfile">torch.profiler 中的 KinetoProfile（类）</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._monitor_workers">torch.distributed.elastic.agent.server.SimpleElasticAgent 方法中的_monitor_workers()</a>
</li>
      <li><a href="torch_cuda_memory.html#torch.cuda.memory._record_memory_history">torch.cuda.memory 模块中的_record_memory_history()</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._rendezvous">torch.distributed.elastic.agent.server.SimpleElasticAgent 方法中的_rendezvous()</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._restart_workers">_restart_workers() (torch.distributed.elastic.agent.server.SimpleElasticAgent 方法)</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._shutdown">_shutdown() (torch.distributed.elastic.agent.server.SimpleElasticAgent 方法)</a>
</li>
      <li><a href="torch_cuda_memory.html#torch.cuda.memory._snapshot">_snapshot() (在模块 torch.cuda.memory 中)</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._start_workers">_start_workers() (torch.distributed.elastic.agent.server.SimpleElasticAgent 方法)</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent._stop_workers">`_stop_workers() (torch.distributed.elastic.agent.server.SimpleElasticAgent 方法)`</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="A">A</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.abs.html#torch.abs">`abs() (模块 torch 中)`</a>

      <ul>
        <li><a href="generated/torch.Tensor.abs.html#torch.Tensor.abs">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.abs_.html#torch.Tensor.abs_">`abs_() (torch.Tensor 方法)`</a>
</li>
      <li><a href="generated/torch.absolute.html#torch.absolute">torch 模块中的 absolute()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.absolute.html#torch.Tensor.absolute">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.absolute_.html#torch.Tensor.absolute_">torch.Tensor 方法中的 absolute_()函数</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.AbsTransform">torch.distributions.transforms 中的 AbsTransform 类</a>
</li>
      <li><a href="generated/torch.acos.html#torch.acos">torch 模块中的 acos()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.acos.html#torch.Tensor.acos">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.acos_.html#torch.Tensor.acos_">torch.Tensor 方法中的 acos_()</a>
</li>
      <li><a href="generated/torch.acosh.html#torch.acosh">torch 模块中的 acosh()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.acosh.html#torch.Tensor.acosh">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.acosh_.html#torch.Tensor.acosh_">acosh_() (torch.Tensor 方法)</a>
</li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.TimerClient.acquire">acquire() (torch.distributed.elastic.timer.TimerClient 方法)</a>
</li>
      <li><a href="generated/torch.cuda.MemPoolContext.html#torch.cuda.MemPoolContext.active_pool">active_pool() (torch.cuda.MemPoolContext 静态方法)</a>
</li>
      <li><a href="generated/torch.optim.Adadelta.html#torch.optim.Adadelta">Adadelta（torch.optim 类）</a>
</li>
      <li><a href="generated/torch.optim.Adafactor.html#torch.optim.Adafactor">Adafactor（torch.optim 类）</a>
</li>
      <li><a href="generated/torch.optim.Adagrad.html#torch.optim.Adagrad">Adagrad（torch.optim 类）</a>
</li>
      <li><a href="generated/torch.optim.Adam.html#torch.optim.Adam">Adam（torch.optim 类）</a>
</li>
      <li><a href="generated/torch.optim.Adamax.html#torch.optim.Adamax">Adamax（torch.optim 中的类）</a>
</li>
      <li><a href="generated/torch.optim.AdamW.html#torch.optim.AdamW">AdamW（torch.optim 中的类）</a>
</li>
      <li><a href="export.html#torch.export.unflatten.FlatArgsAdapter.adapt">adapt()（torch.export.unflatten.FlatArgsAdapter 方法）</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Timer.adaptive_autorange">adaptive_autorange()（torch.utils.benchmark.Timer 方法）</a>
</li>
      <li><a href="generated/torch.nn.functional.adaptive_avg_pool1d.html#torch.nn.functional.adaptive_avg_pool1d">adaptive_avg_pool1d()（在模块 torch.nn.functional 中）</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool2d.html#torch.ao.nn.quantized.functional.adaptive_avg_pool2d">adaptive_avg_pool2d（torch.ao.nn.quantized.functional 类中）</a>
</li>
      <li><a href="generated/torch.nn.functional.adaptive_avg_pool2d.html#torch.nn.functional.adaptive_avg_pool2d">adaptive_avg_pool2d()（在模块 torch.nn.functional 中）</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool3d.html#torch.ao.nn.quantized.functional.adaptive_avg_pool3d">adaptive_avg_pool3d（torch.ao.nn.quantized.functional 类中）</a>
</li>
      <li><a href="generated/torch.nn.functional.adaptive_avg_pool3d.html#torch.nn.functional.adaptive_avg_pool3d">adaptive_avg_pool3d()（在模块 torch.nn.functional 中）</a>
</li>
      <li><a href="generated/torch.nn.functional.adaptive_max_pool1d.html#torch.nn.functional.adaptive_max_pool1d">adaptive_max_pool1d()（在模块 torch.nn.functional 中）</a>
</li>
      <li><a href="generated/torch.nn.functional.adaptive_max_pool2d.html#torch.nn.functional.adaptive_max_pool2d">adaptive_max_pool2d()（在模块 torch.nn.functional 中）</a>
</li>
      <li><a href="generated/torch.nn.functional.adaptive_max_pool3d.html#torch.nn.functional.adaptive_max_pool3d">adaptive_max_pool3d()（在模块 torch.nn.functional 中）</a>
</li>
      <li><a href="generated/torch.nn.AdaptiveAvgPool1d.html#torch.nn.AdaptiveAvgPool1d">torch.nn 中的 AdaptiveAvgPool1d（类）</a>
</li>
      <li><a href="generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d">torch.nn 中的 AdaptiveAvgPool2d（类）</a>
</li>
      <li><a href="generated/torch.nn.AdaptiveAvgPool3d.html#torch.nn.AdaptiveAvgPool3d">torch.nn 中的 AdaptiveAvgPool3d（类）</a>
</li>
      <li><a href="generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss">torch.nn 中的 AdaptiveLogSoftmaxWithLoss（类）</a>
</li>
      <li><a href="generated/torch.nn.AdaptiveMaxPool1d.html#torch.nn.AdaptiveMaxPool1d">AdaptiveMaxPool1d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d">AdaptiveMaxPool2d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.AdaptiveMaxPool3d.html#torch.nn.AdaptiveMaxPool3d">AdaptiveMaxPool3d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.add.html#torch.add">add()（模块 torch 中的函数）</a>

      <ul>
        <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.add">(torch.ao.ns._numeric_suite.Shadow 方法)</a>
</li>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.add">(torch.distributed.elastic.rendezvous.etcd_store.EtcdStore 方法)</a>
</li>
        <li><a href="distributed.html#torch.distributed.Store.add">(torch.distributed.Store 方法)</a>
</li>
        <li><a href="generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints.add">(torch.fx.experimental.symbolic_shapes.DimConstraints 方法)</a>
</li>
        <li><a href="monitor.html#torch.monitor.Stat.add">(torch.monitor.Stat 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.add.html#torch.Tensor.add">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.add_.html#torch.Tensor.add_">add_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_audio">add_audio() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars">add_custom_scalars() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.add_dependency">add_dependency() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="futures.html#torch.futures.Future.add_done_callback">add_done_callback() (torch.futures.Future 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.add_dtype_config">add_dtype_config() (torch.ao.quantization.backend_config.BackendPatternConfig 方法)</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_embedding">add_embedding() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints.add_equality">add_equality() (torch.fx.experimental.symbolic_shapes.DimConstraints 方法)</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_figure">add_figure() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_graph">add_graph() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_histogram">add_histogram() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_hparams">add_hparams() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_image">add_image() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_images">add_images() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.add_loggers">torch.ao.ns._numeric_suite_fx 模块中的 add_loggers() 函数</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_mesh">torch.utils.tensorboard.writer.SummaryWriter 方法中的 add_mesh()</a>
</li>
      <li><a href="profiler.html#torch.profiler._KinetoProfile.add_metadata">torch.profiler._KinetoProfile 方法中的 add_metadata()</a>
</li>
      <li><a href="profiler.html#torch.profiler._KinetoProfile.add_metadata_json">torch.profiler._KinetoProfile 方法中的 add_metadata_json()</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.add_module">add_module() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.add_module">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.add_module">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.add_param_group">add_param_group() (torch.distributed.optim.ZeroRedundancyOptimizer 方法)</a>

      <ul>
        <li><a href="generated/torch.optim.Adadelta.html#torch.optim.Adadelta.add_param_group">(torch.optim.Adadelta 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adafactor.html#torch.optim.Adafactor.add_param_group">(torch.optim.Adafactor 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adagrad.html#torch.optim.Adagrad.add_param_group">(torch.optim.Adagrad 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adam.html#torch.optim.Adam.add_param_group">(torch.optim.Adam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adamax.html#torch.optim.Adamax.add_param_group">(torch.optim.Adamax 方法)</a>
</li>
        <li><a href="generated/torch.optim.AdamW.html#torch.optim.AdamW.add_param_group">(torch.optim.AdamW 方法)</a>
</li>
        <li><a href="generated/torch.optim.ASGD.html#torch.optim.ASGD.add_param_group">(torch.optim.ASGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.LBFGS.html#torch.optim.LBFGS.add_param_group">(torch.optim.LBFGS 方法)</a>
</li>
        <li><a href="generated/torch.optim.NAdam.html#torch.optim.NAdam.add_param_group">(torch.optim.NAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Optimizer.add_param_group.html#torch.optim.Optimizer.add_param_group">(torch.optim.Optimizer 方法)</a>
</li>
        <li><a href="generated/torch.optim.RAdam.html#torch.optim.RAdam.add_param_group">(torch.optim.RAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.RMSprop.html#torch.optim.RMSprop.add_param_group">(torch.optim.RMSprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.Rprop.html#torch.optim.Rprop.add_param_group">(torch.optim.Rprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.SGD.html#torch.optim.SGD.add_param_group">(torch.optim.SGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.add_param_group">(torch.optim.SparseAdam 方法)</a>
</li>
      </ul></li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve">add_pr_curve() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.add_pruning_method">add_pruning_method() (torch.nn.utils.prune.PruningContainer 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.add_quant_dequant.html#torch.ao.quantization.add_quant_dequant">add_quant_dequant (torch.ao.quantization 类)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.add_relu">add_relu() (torch.ao.ns._numeric_suite.Shadow 方法)</a>
</li>
      <li><a href="notes/serialization.html#torch.serialization.add_safe_globals">add_safe_globals() (模块 torch.serialization 中)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.add_scalar">add_scalar() (torch.ao.ns._numeric_suite.Shadow 方法)</a>

      <ul>
        <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalar">(torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      </ul></li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalars">add_scalars() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.add_shadow_loggers">add_shadow_loggers() (在模块 torch.ao.ns._numeric_suite_fx 中)</a>
</li>
      <li><a href="fx.html#torch.fx.GraphModule.add_submodule">add_submodule() (torch.fx.GraphModule 方法)</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_text">add_text() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.add_var_to_val">add_var_to_val() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_video">add_video() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="generated/torch.addbmm.html#torch.addbmm">torch 模块中的 addbmm()</a>

      <ul>
        <li><a href="generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.addbmm_.html#torch.Tensor.addbmm_">torch.Tensor 方法中的 addbmm_()</a>
</li>
      <li><a href="generated/torch.addcdiv.html#torch.addcdiv">torch 模块中的 addcdiv()</a>

      <ul>
        <li><a href="generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.addcdiv_.html#torch.Tensor.addcdiv_">addcdiv_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.addcmul.html#torch.addcmul">addcmul() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.addcmul_.html#torch.Tensor.addcmul_">addcmul_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.addmm.html#torch.addmm">addmm() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.sparse.addmm.html#torch.sparse.addmm">（在 torch.sparse 模块中）</a>
</li>
        <li><a href="generated/torch.Tensor.addmm.html#torch.Tensor.addmm">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_">addmm_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.addmv.html#torch.addmv">addmv() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.addmv.html#torch.Tensor.addmv">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.addmv_.html#torch.Tensor.addmv_">addmv_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.addr.html#torch.addr">torch 模块中的 addr()</a>

      <ul>
        <li><a href="generated/torch.Tensor.addr.html#torch.Tensor.addr">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.addr_.html#torch.Tensor.addr_">torch.Tensor 方法中的 addr_()</a>
</li>
      <li><a href="generated/torch.adjoint.html#torch.adjoint">torch 模块中的 adjoint()</a>

      <ul>
        <li><a href="generated/torch.Tensor.adjoint.html#torch.Tensor.adjoint">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.functional.affine_grid.html#torch.nn.functional.affine_grid">affine_grid() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.AffineQuantizedObserverBase.html#torch.ao.quantization.observer.AffineQuantizedObserverBase">AffineQuantizedObserverBase (torch.ao.quantization.observer 中的类)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.AffineTransform">AffineTransform (torch.distributions.transforms 中的类)</a>
</li>
      <li><a href="monitor.html#torch.monitor.Aggregation">torch.monitor 中的聚合（类）</a>
</li>
      <li><a href="special.html#torch.special.airy_ai">torch.special 模块中的 airy_ai()</a>
</li>
      <li><a href="named_tensor.html#torch.Tensor.align_as">torch.Tensor 方法中的 align_as()</a>
</li>
      <li><a href="named_tensor.html#torch.Tensor.align_to">torch.Tensor 方法中的 align_to()</a>
</li>
      <li><a href="generated/torch.all.html#torch.all">torch 模块中的 all()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.all.html#torch.Tensor.all">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.all_gather">torch.distributed 模块中的 all_gather()函数</a>
</li>
      <li><a href="distributed.html#torch.distributed.all_gather_into_tensor">torch.distributed 模块中的 all_gather_into_tensor()函数</a>
</li>
      <li><a href="distributed.html#torch.distributed.all_gather_object">torch.distributed 模块中的 all_gather_object()函数</a>
</li>
      <li><a href="fx.html#torch.fx.Node.all_input_nodes">torch.fx.Node 属性中的 all_input_nodes</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.all_paths">torch.package.PackageExporter 方法中的 all_paths()</a>
</li>
      <li><a href="distributed.html#torch.distributed.all_reduce">torch.distributed 模块中的 all_reduce()</a>
</li>
      <li><a href="distributed.html#torch.distributed.all_to_all">torch.distributed 模块中的 all_to_all()</a>
</li>
      <li><a href="distributed.html#torch.distributed.all_to_all_single">torch.distributed 模块中的 all_to_all_single()</a>
</li>
      <li><a href="generated/torch.allclose.html#torch.allclose">torch 模块中的 allclose()</a>

      <ul>
        <li><a href="generated/torch.Tensor.allclose.html#torch.Tensor.allclose">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.MemPool.html#torch.cuda.MemPool.allocator">allocator (torch.cuda.MemPool 属性)</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction">allow_bf16_reduced_precision_reduction (在模块 torch.backends.cuda.matmul 中)</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp">allow_fp16_bf16_reduction_math_sdp() (在模块 torch.backends.cuda 中)</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction">torch.backends.cuda.matmul 模块中的 allow_fp16_reduced_precision_reduction</a>
</li>
      <li><a href="generated/torch.compiler.allow_in_graph.html#torch.compiler.allow_in_graph">torch.compiler 模块中的 allow_in_graph()</a>
</li>
      <li><a href="autograd.html#torch.autograd.graph.allow_mutation_on_saved_tensors">torch.autograd.graph 类中的 allow_mutation_on_saved_tensors</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.matmul.allow_tf32">torch.backends.cuda.matmul 模块中的 allow_tf32</a>

      <ul>
        <li><a href="backends.html#torch.backends.cudnn.allow_tf32">(在 torch.backends.cudnn 模块中)</a>
</li>
      </ul></li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook">allreduce_hook() (在 torch.distributed.algorithms.ddp_comm_hooks.default_hooks 模块中)</a>
</li>
      <li><a href="generated/torch.nn.functional.alpha_dropout.html#torch.nn.functional.alpha_dropout">alpha_dropout() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="generated/torch.nn.AlphaDropout.html#torch.nn.AlphaDropout">AlphaDropout (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.amax.html#torch.amax">torch 模块中的 amax()</a>

      <ul>
        <li><a href="generated/torch.Tensor.amax.html#torch.Tensor.amax">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.amin.html#torch.amin">torch 模块中的 amin()</a>

      <ul>
        <li><a href="generated/torch.Tensor.amin.html#torch.Tensor.amin">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.aminmax.html#torch.aminmax">torch 模块中的 aminmax()</a>

      <ul>
        <li><a href="generated/torch.Tensor.aminmax.html#torch.Tensor.aminmax">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.and_masks">torch.nn.attention.flex_attention 模块中的 and_masks()</a>
</li>
      <li><a href="generated/torch.angle.html#torch.angle">torch 模块中的 angle()</a>

      <ul>
        <li><a href="generated/torch.Tensor.angle.html#torch.Tensor.angle">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.annotate.html#torch.jit.annotate">annotate()（在 torch.jit 模块中）</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.any.html#torch.any">any()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.any.html#torch.Tensor.any">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="torch.compiler_aot_inductor.html#torch._inductor.aoti_compile_and_package">aoti_compile_and_package() (在模块 torch._inductor 中)</a>
</li>
      <li><a href="torch.compiler_aot_inductor.html#torch._inductor.aoti_load_package">aoti_load_package() (在模块 torch._inductor 中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.Store.append">append() (torch.distributed.Store 方法)</a>

      <ul>
        <li><a href="fx.html#torch.fx.Node.append">(torch.fx.Node 方法)</a>
</li>
        <li><a href="generated/torch.nn.ModuleList.html#torch.nn.ModuleList.append">(torch.nn.ModuleList 方法)</a>
</li>
        <li><a href="generated/torch.nn.ParameterList.html#torch.nn.ParameterList.append">torch.nn.ParameterList 方法</a>
</li>
        <li><a href="generated/torch.nn.Sequential.html#torch.nn.Sequential.append">(torch.nn.Sequential 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.apply">apply() (torch.autograd.function.BackwardCFunction 方法)</a>

      <ul>
        <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.apply">(torch.distributed.fsdp.FullyShardedDataParallel 方法)</a>
</li>
        <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.apply">(torch.jit.ScriptModule 方法)</a>
</li>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.apply">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.apply">(torch.nn.utils.prune.BasePruningMethod 类方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.apply">(torch.nn.utils.prune.CustomFromMask 类方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.apply">(torch.nn.utils.prune.Identity 类方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.apply">(torch.nn.utils.prune.L1Unstructured 类方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.apply">(torch.nn.utils.prune.LnStructured 类方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.apply">(torch.nn.utils.prune.PruningContainer 类方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.apply">(torch.nn.utils.prune.RandomStructured 类方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.apply">(torch.nn.utils.prune.RandomUnstructured 类方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.apply">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.apply_.html#torch.Tensor.apply_">apply_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.apply_jvp">apply_jvp() (torch.autograd.function.BackwardCFunction 方法)</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.apply_mask">apply_mask() (torch.nn.utils.prune.BasePruningMethod 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.apply_mask">(torch.nn.utils.prune.CustomFromMask 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.apply_mask">(torch.nn.utils.prune.Identity 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.apply_mask">(torch.nn.utils.prune.L1Unstructured 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.apply_mask">(torch.nn.utils.prune.LnStructured 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.apply_mask">(torch.nn.utils.prune.PruningContainer 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.apply_mask">(torch.nn.utils.prune.RandomStructured 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.apply_mask">(torch.nn.utils.prune.RandomUnstructured 方法)</a>
</li>
      </ul></li>
      <li><a href="onnx_dynamo.html#torch.onnx.ONNXProgram.apply_weights">apply_weights() (torch.onnx.ONNXProgram 方法)</a>
</li>
      <li><a href="generated/torch.arange.html#torch.arange">arange() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.arccos.html#torch.arccos">torch 模块中的 arccos()</a>

      <ul>
        <li><a href="generated/torch.Tensor.arccos.html#torch.Tensor.arccos">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.arccos_.html#torch.Tensor.arccos_">torch.Tensor 方法中的 arccos_()</a>
</li>
      <li><a href="generated/torch.arccosh.html#torch.arccosh">torch 模块中的 arccosh()</a>

      <ul>
        <li><a href="generated/torch.Tensor.arccosh.html#torch.Tensor.arccosh">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.arccosh_.html#torch.Tensor.arccosh_">arccosh_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.arcsin.html#torch.arcsin">arcsin() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_">arcsin_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.arcsinh.html#torch.arcsinh">arcsinh() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.arcsinh_.html#torch.Tensor.arcsinh_">arcsinh_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.arctan.html#torch.arctan">torch 模块中的 arctan()</a>

      <ul>
        <li><a href="generated/torch.Tensor.arctan.html#torch.Tensor.arctan">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.arctan2.html#torch.arctan2">torch 模块中的 arctan2()</a>

      <ul>
        <li><a href="generated/torch.Tensor.arctan2.html#torch.Tensor.arctan2">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.arctan2_.html#torch.Tensor.arctan2_">arctan2_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.arctan_.html#torch.Tensor.arctan_">arctan_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.arctanh.html#torch.arctanh">arctanh() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.arctanh_.html#torch.Tensor.arctanh_">arctanh_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled">are_deterministic_algorithms_enabled() (在 torch 模块中)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.arg_constraints">arg_constraints (torch.distributions.bernoulli.Bernoulli 属性)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.arg_constraints">（torch.distributions.beta.Beta 属性）</a>
</li>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.arg_constraints">(torch.distributions.binomial.Binomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.arg_constraints">(torch.distributions.categorical.Categorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.arg_constraints">(torch.distributions.cauchy.Cauchy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.chi2.Chi2.arg_constraints">(torch.distributions.chi2.Chi2 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.arg_constraints">(torch.distributions.dirichlet.Dirichlet 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.arg_constraints">(torch.distributions.distribution.Distribution 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.arg_constraints">(torch.distributions.exponential.Exponential 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints">(torch.distributions.fishersnedecor.FisherSnedecor 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.arg_constraints">(torch.distributions.gamma.Gamma 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.arg_constraints">(torch.distributions.geometric.Geometric 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.arg_constraints">(torch.distributions.gumbel.Gumbel 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_cauchy.HalfCauchy.arg_constraints">(torch.distributions.half_cauchy.HalfCauchy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_normal.HalfNormal.arg_constraints">(torch.distributions.half_normal.HalfNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.arg_constraints">(torch.distributions.independent.Independent 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.inverse_gamma.InverseGamma.arg_constraints">(torch.distributions.inverse_gamma.InverseGamma 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints">(torch.distributions.kumaraswamy.Kumaraswamy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.arg_constraints">(torch.distributions.laplace.Laplace 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints">(torch.distributions.lkj_cholesky.LKJCholesky 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.arg_constraints">(torch.distributions.log_normal.LogNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints">(torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints">(torch.distributions.mixture_same_family.MixtureSameFamily 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.arg_constraints">(torch.distributions.multinomial.Multinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints">(torch.distributions.multivariate_normal.MultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.negative_binomial.NegativeBinomial.arg_constraints">(torch.distributions.negative_binomial.NegativeBinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.arg_constraints">(torch.distributions.normal.Normal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints">(torch.distributions.one_hot_categorical.OneHotCategorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.pareto.Pareto.arg_constraints">(torch.distributions.pareto.Pareto 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.arg_constraints">(torch.distributions.poisson.Poisson 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints">(torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints">(torch.distributions.relaxed_bernoulli.RelaxedBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints">(torch.distributions.relaxed_categorical.RelaxedOneHotCategorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.arg_constraints">(torch.distributions.studentT.StudentT 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints">(torch.distributions.transformed_distribution.TransformedDistribution 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.arg_constraints">(torch.distributions.uniform.Uniform 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.von_mises.VonMises.arg_constraints">(torch.distributions.von_mises.VonMises 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.weibull.Weibull.arg_constraints">(torch.distributions.weibull.Weibull 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.wishart.Wishart.arg_constraints">(torch.distributions.wishart.Wishart 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.argmax.html#torch.argmax">argmax() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.argmax.html#torch.Tensor.argmax">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.argmin.html#torch.argmin">torch 模块中的 argmin()</a>

      <ul>
        <li><a href="generated/torch.Tensor.argmin.html#torch.Tensor.argmin">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="fx.html#torch.fx.Node.args">torch.fx.Node 属性 args</a>
</li>
      <li><a href="generated/torch.argsort.html#torch.argsort">torch 模块中的 argsort()</a>

      <ul>
        <li><a href="generated/torch.Tensor.argsort.html#torch.Tensor.argsort">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.argwhere.html#torch.argwhere">argwhere()（在模块 torch 中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.argwhere.html#torch.Tensor.argwhere">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.attention.bias.CausalVariant.html#torch.nn.attention.bias.CausalVariant.as_integer_ratio">as_integer_ratio()（torch.nn.attention.bias.CausalVariant 方法）</a>

      <ul>
        <li><a href="torch.html#torch.SymFloat.as_integer_ratio">(torch.SymFloat 方法)</a>
</li>
        <li><a href="torch.html#torch.SymInt.as_integer_ratio">(torch.SymInt 方法)</a>
</li>
      </ul></li>
      <li><a href="nested.html#torch.nested.as_nested_tensor">as_nested_tensor()（在 torch.nested 模块中）</a>
</li>
      <li><a href="generated/torch.sparse.as_sparse_gradcheck.html#torch.sparse.as_sparse_gradcheck">as_sparse_gradcheck()（在 torch.sparse 模块中）</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.CallgrindStats.as_standardized">as_standardized() (torch.utils.benchmark.CallgrindStats 方法)</a>
</li>
      <li><a href="generated/torch.as_strided.html#torch.as_strided">as_strided() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.as_subclass.html#torch.Tensor.as_subclass">as_subclass() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.as_tensor.html#torch.as_tensor">as_tensor() (在模块 torch 中)</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.as_tuple">as_tuple() (torch.nn.attention.flex_attention.BlockMask 方法)</a>
</li>
      <li><a href="generated/torch.asarray.html#torch.asarray">asarray() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.optim.ASGD.html#torch.optim.ASGD">ASGD (torch.optim 中的类)</a>
</li>
      <li><a href="generated/torch.asin.html#torch.asin">torch 模块中的 asin()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.asin.html#torch.Tensor.asin">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.asin_.html#torch.Tensor.asin_">torch.Tensor 方法中的 asin_()函数</a>
</li>
      <li><a href="generated/torch.asinh.html#torch.asinh">torch 模块中的 asinh()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.asinh.html#torch.Tensor.asinh">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.asinh_.html#torch.Tensor.asinh_">asinh_() (torch.Tensor 方法)</a>
</li>
      <li><a href="testing.html#torch.testing.assert_allclose">assert_allclose() (在 torch.testing 模块中)</a>
</li>
      <li><a href="testing.html#torch.testing.assert_close">assert_close() (在 torch.testing 模块中)</a>
</li>
      <li><a href="generated/torch.compiler.assume_constant_result.html#torch.compiler.assume_constant_result">assume_constant_result()（在 torch.compiler 模块中）</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.functions.async_execution">async_execution()（在 torch.distributed.rpc.functions 模块中）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.state_dict_saver.async_save">async_save()（在 torch.distributed.checkpoint.state_dict_saver 模块中）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.state_dict_saver.AsyncCheckpointerType">AsyncCheckpointerType（torch.distributed.checkpoint.state_dict_saver 模块中的类）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.staging.AsyncStager">torch.distributed.checkpoint.staging 中的 AsyncStager（类）</a>
</li>
      <li><a href="generated/torch.atan.html#torch.atan">torch 模块中的 atan()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.atan.html#torch.Tensor.atan">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.atan2.html#torch.atan2">torch 模块中的 atan2()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.atan2.html#torch.Tensor.atan2">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.atan2_.html#torch.Tensor.atan2_">atan2_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.atan_.html#torch.Tensor.atan_">atan_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.atanh.html#torch.atanh">atanh() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.atanh.html#torch.Tensor.atanh">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.atanh_.html#torch.Tensor.atanh_">atanh_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.atleast_1d.html#torch.atleast_1d">atleast_1d() (在 torch 模块中)</a>
</li>
      <li><a href="generated/torch.atleast_2d.html#torch.atleast_2d">atleast_2d() (在 torch 模块中)</a>
</li>
      <li><a href="generated/torch.atleast_3d.html#torch.atleast_3d">torch 模块中的 atleast_3d()</a>
</li>
      <li><a href="generated/torch.jit.Attribute.html#torch.jit.Attribute">torch.jit 类中的属性</a>
</li>
      <li><a href="amp.html#torch.autocast">torch 类中的 autocast</a>

      <ul>
        <li><a href="amp.html#torch.cpu.amp.autocast">torch.cpu.amp 类中</a>
</li>
        <li><a href="amp.html#torch.cuda.amp.autocast">(torch.cuda.amp 中的类)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel">AveragedModel (torch.optim.swa_utils 中的类)</a>
</li>
      <li><a href="generated/torch.nn.functional.avg_pool1d.html#torch.nn.functional.avg_pool1d">avg_pool1d() (torch.nn.functional 模块中的函数)</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.avg_pool2d.html#torch.ao.nn.quantized.functional.avg_pool2d">avg_pool2d (torch.ao.nn.quantized.functional 中的类)</a>
</li>
      <li><a href="generated/torch.nn.functional.avg_pool2d.html#torch.nn.functional.avg_pool2d">avg_pool2d()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.avg_pool3d.html#torch.ao.nn.quantized.functional.avg_pool3d">avg_pool3d（torch.ao.nn.quantized.functional 类）</a>
</li>
      <li><a href="generated/torch.nn.functional.avg_pool3d.html#torch.nn.functional.avg_pool3d">avg_pool3d()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.nn.AvgPool1d.html#torch.nn.AvgPool1d">AvgPool1d（torch.nn 类）</a>
</li>
      <li><a href="generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d">AvgPool2d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.AvgPool3d.html#torch.nn.AvgPool3d">AvgPool3d（torch.nn 中的类）</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="B">B</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributed.html#torch.distributed.Backend">Backend（torch.distributed 中的类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig">BackendConfig（torch.ao.quantization.backend_config 中的类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig">torch.ao.quantization.backend_config 中的 BackendPatternConfig（类）</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.BackendType">torch.distributed.rpc 中的 BackendType（类）</a>
</li>
      <li><a href="generated/torch.autograd.backward.html#torch.autograd.backward">torch.autograd 模块中的 backward() 函数</a>

      <ul>
        <li><a href="rpc.html#torch.distributed.autograd.backward">torch.distributed.autograd 模块中</a>
</li>
        <li><a href="generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward">(torch.autograd.函数静态方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.backward">(torch.autograd.function.内联函数静态方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.backward">(torch.autograd.function.NestedIOFunction 方法)</a>
</li>
        <li><a href="rpc.html#torch.distributed.rpc.PyRRef.backward">(torch.distributed.rpc.PyRRef 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.backward.html#torch.Tensor.backward">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.backward_extended">backward_extended() (torch.autograd.function.NestedIOFunction 方法)</a>
</li>
      <li><a href="generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction">torch.autograd.function 中的 BackwardCFunction（类）</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.BackwardPrefetch">BackwardPrefetch (torch.distributed.fsdp 中的类)</a>
</li>
      <li><a href="generated/torch.baddbmm.html#torch.baddbmm">torch 模块中的 baddbmm()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.baddbmm_.html#torch.Tensor.baddbmm_">torch.Tensor 方法中的 baddbmm_()函数</a>
</li>
      <li><a href="distributed.html#torch.distributed.barrier">torch.distributed 模块中的 barrier()函数</a>
</li>
      <li><a href="generated/torch.signal.windows.bartlett.html#torch.signal.windows.bartlett">torch.signal.windows 模块中的 bartlett()函数</a>
</li>
      <li><a href="generated/torch.bartlett_window.html#torch.bartlett_window">torch 模块中的 bartlett_window()函数</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod">torch.nn.utils.prune 中的 BasePruningMethod（类）</a>
</li>
      <li><a href="distributed.html#torch.distributed.batch_isend_irecv">torch.distributed 模块中的 batch_isend_irecv()</a>
</li>
      <li><a href="generated/torch.nn.functional.batch_norm.html#torch.nn.functional.batch_norm">torch.nn.functional 模块中的 batch_norm()</a>
</li>
      <li><a href="distributions.html#torch.distributions.distribution.Distribution.batch_shape">torch.distributions.distribution.Distribution 属性中的 batch_shape</a>
</li>
      <li><a href="generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.batch_sizes">torch.nn.utils.rnn.PackedSequence 属性 batch_sizes</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook">torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook 中的 batched_powerSGD_hook()</a>
</li>
      <li><a href="generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d">torch.nn 中的 BatchNorm1d 类</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.BatchNorm2d.html#torch.ao.nn.quantized.BatchNorm2d">torch.ao.nn.quantized 中的 BatchNorm2d 类</a>

      <ul>
        <li><a href="generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.BatchNorm3d.html#torch.ao.nn.quantized.BatchNorm3d">BatchNorm3d (torch.ao.nn.quantized 中的类)</a>

      <ul>
        <li><a href="generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="data.html#torch.utils.data.BatchSampler">BatchSampler (torch.utils.data 中的类)</a>
</li>
      <li><a href="generated/torch.nn.BCELoss.html#torch.nn.BCELoss">BCELoss（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss">BCEWithLogitsLoss（torch.nn 中的类）</a>
</li>
      <li><a href="backends.html#torch.backends.cudnn.benchmark">benchmark（torch.backends.cudnn 模块中的）</a>
</li>
      <li><a href="backends.html#torch.backends.cudnn.benchmark_limit">benchmark_limit（torch.backends.cudnn 模块中的）</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli">torch.distributions.bernoulli 中的 Bernoulli（类）</a>
</li>
      <li><a href="generated/torch.bernoulli.html#torch.bernoulli">torch 模块中的 bernoulli()</a>

      <ul>
        <li><a href="generated/torch.Tensor.bernoulli.html#torch.Tensor.bernoulli">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_">bernoulli_(torch.Tensor 方法)</a>
</li>
      <li><a href="special.html#torch.special.bessel_j0">torch.special 模块中的 bessel_j0()</a>
</li>
      <li><a href="special.html#torch.special.bessel_j1">torch.special 模块中的 bessel_j1()</a>
</li>
      <li><a href="distributions.html#torch.distributions.beta.Beta">torch.distributions.beta 中的 Beta 类</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook">torch.distributed.algorithms.ddp_comm_hooks.default_hooks 模块中的 bf16_compress_hook()</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper">torch.distributed.algorithms.ddp_comm_hooks.default_hooks 模块中的 bf16_compress_wrapper()函数</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.bfloat16">torch.jit.ScriptModule 方法中的 bfloat16()</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.bfloat16">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.bfloat16">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.bfloat16.html#torch.Tensor.bfloat16">torch.Tensor 方法</a>
</li>
        <li><a href="storage.html#torch.TypedStorage.bfloat16">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.bfloat16">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.BFloat16Storage">BFloat16Storage (torch 中的类)</a>
</li>
      <li><a href="generated/torch.nn.Bilinear.html#torch.nn.Bilinear">torch.nn 中的双线性（类）</a>
</li>
      <li><a href="generated/torch.nn.functional.bilinear.html#torch.nn.functional.bilinear">torch.nn.functional 模块中的 bilinear()</a>
</li>
      <li><a href="generated/torch.nn.functional.binary_cross_entropy.html#torch.nn.functional.binary_cross_entropy">torch.nn.functional 模块中的 binary_cross_entropy()</a>
</li>
      <li><a href="generated/torch.nn.functional.binary_cross_entropy_with_logits.html#torch.nn.functional.binary_cross_entropy_with_logits">torch.nn.functional 模块中的 binary_cross_entropy_with_logits()</a>
</li>
      <li><a href="generated/torch.bincount.html#torch.bincount">torch 模块中的 bincount()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.bincount.html#torch.Tensor.bincount">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.bind_symbols">torch.fx.experimental.symbolic_shapes.ShapeEnv 方法中的 bind_symbols()</a>
</li>
      <li><a href="distributions.html#torch.distributions.binomial.Binomial">torch.distributions.binomial 中的 Binomial 类</a>
</li>
      <li><a href="generated/torch.nn.attention.bias.CausalVariant.html#torch.nn.attention.bias.CausalVariant.bit_count">bit_count() (torch.nn.attention.bias.CausalVariant 方法)</a>
</li>
      <li><a href="generated/torch.nn.attention.bias.CausalVariant.html#torch.nn.attention.bias.CausalVariant.bit_length">bit_length() (torch.nn.attention.bias.CausalVariant 方法)</a>
</li>
      <li><a href="generated/torch.bitwise_and.html#torch.bitwise_and">bitwise_and() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and">torch.Tensor 方法</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.Tensor.bitwise_and_.html#torch.Tensor.bitwise_and_">bitwise_and_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift">bitwise_left_shift() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.bitwise_left_shift_.html#torch.Tensor.bitwise_left_shift_">bitwise_left_shift_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.bitwise_not.html#torch.bitwise_not">torch 模块中的 bitwise_not()</a>

      <ul>
        <li><a href="generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.bitwise_not_.html#torch.Tensor.bitwise_not_">torch.Tensor 方法 bitwise_not_()</a>
</li>
      <li><a href="generated/torch.bitwise_or.html#torch.bitwise_or">torch 模块中的 bitwise_or()</a>

      <ul>
        <li><a href="generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.bitwise_or_.html#torch.Tensor.bitwise_or_">bitwise_or_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift">bitwise_right_shift() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.bitwise_right_shift_.html#torch.Tensor.bitwise_right_shift_">torch.Tensor 方法：位右移_()</a>
</li>
      <li><a href="generated/torch.bitwise_xor.html#torch.bitwise_xor">模块 torch 中的 bitwise_xor()</a>

      <ul>
        <li><a href="generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.bitwise_xor_.html#torch.Tensor.bitwise_xor_">torch.Tensor 方法：位异或_()</a>
</li>
      <li><a href="generated/torch.signal.windows.blackman.html#torch.signal.windows.blackman">torch.signal.windows 模块中的 blackman()函数</a>
</li>
      <li><a href="generated/torch.blackman_window.html#torch.blackman_window">torch 模块中的 blackman_window()函数</a>
</li>
      <li><a href="generated/torch.block_diag.html#torch.block_diag">torch 模块中的 block_diag()函数</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.BLOCK_SIZE">torch.nn.attention.flex_attention.BlockMask 属性中的 BLOCK_SIZE</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Timer.blocked_autorange">blocked_autorange() (torch.utils.benchmark.Timer 方法)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.staging.BlockingAsyncStager">BlockingAsyncStager (torch.distributed.checkpoint.staging 中的类)</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask">BlockMask (torch.nn.attention.flex_attention 中的类)</a>
</li>
      <li><a href="generated/torch.bmm.html#torch.bmm">bmm() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.bmm.html#torch.Tensor.bmm">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.intrinsic.BNReLU2d.html#torch.ao.nn.intrinsic.BNReLU2d">BNReLU2d (torch.ao.nn.intrinsic 中的类)</a>

      <ul>
        <li><a href="generated/torch.ao.nn.intrinsic.quantized.BNReLU2d.html#torch.ao.nn.intrinsic.quantized.BNReLU2d">torch.ao.nn.intrinsic.quantized 中的（类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.intrinsic.BNReLU3d.html#torch.ao.nn.intrinsic.BNReLU3d">BNReLU3d (torch.ao.nn.intrinsic 中的类)</a>

      <ul>
        <li><a href="generated/torch.ao.nn.intrinsic.quantized.BNReLU3d.html#torch.ao.nn.intrinsic.quantized.BNReLU3d">torch.ao.nn.intrinsic.quantized 中的（类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.bool.html#torch.Tensor.bool">bool() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.bool">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.bool">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.BoolStorage">torch 中的 BoolStorage（类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.bound_sympy">torch.fx.experimental.symbolic_shapes.ShapeEnv 方法的 bound_sympy()</a>
</li>
      <li><a href="distributed.html#torch.distributed.Work.boxed">torch.distributed.Work 方法的 boxed()</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html#torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.boxed_run">torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts 方法的 boxed_run()</a>

      <ul>
        <li><a href="fx.html#torch.fx.Interpreter.boxed_run">(torch.fx.Interpreter 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.breakpoint">breakpoint()（在 torch.distributed 模块中）</a>
</li>
      <li><a href="generated/torch.cuda.comm.broadcast.html#torch.cuda.comm.broadcast">broadcast()（在 torch.cuda.comm 模块中）</a>

      <ul>
        <li><a href="distributed.html#torch.distributed.broadcast">（在 torch.distributed 模块中）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.comm.broadcast_coalesced.html#torch.cuda.comm.broadcast_coalesced">torch.cuda.comm 模块中的 broadcast_coalesced()</a>
</li>
      <li><a href="distributed.html#torch.distributed.broadcast_object_list">torch.distributed 模块中的 broadcast_object_list()</a>
</li>
      <li><a href="generated/torch.broadcast_shapes.html#torch.broadcast_shapes">torch 模块中的 broadcast_shapes()</a>
</li>
      <li><a href="generated/torch.broadcast_tensors.html#torch.broadcast_tensors">torch 模块中的 broadcast_tensors()</a>
</li>
      <li><a href="generated/torch.broadcast_to.html#torch.broadcast_to">torch 模块中的 broadcast_to()</a>

      <ul>
        <li><a href="generated/torch.Tensor.broadcast_to.html#torch.Tensor.broadcast_to">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader">torch.distributed.checkpoint.format_utils 中的 BroadcastingTorchSaveReader 类</a>
</li>
      <li><a href="generated/torch.bucketize.html#torch.bucketize">torch 模块中的 bucketize()</a>
</li>
      <li><a href="generated/torch.nn.parameter.Buffer.html#torch.nn.parameter.Buffer">torch.nn.parameter 中的 Buffer（类）</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.GradBucket.buffer">torch.distributed.GradBucket 中的 buffer()函数</a>
</li>
      <li><a href="export.html#torch.export.ExportedProgram.buffers">torch.export.ExportedProgram 方法中的 buffers()</a>

      <ul>
        <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.buffers">(torch.jit.ScriptModule 方法)</a>
</li>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.buffers">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.buffers">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.api.RendezvousStoreInfo.build">build() (torch.distributed.elastic.rendezvous.api.RendezvousStoreInfo 静态方法)</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.stage.build_stage">build_stage() (在模块 torch.distributed.pipelining.stage 中)</a>
</li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.BuildExtension">torch.utils.cpp_extension 模块中的 BuildExtension()</a>
</li>
      <li><a href="generated/torch.Tensor.byte.html#torch.Tensor.byte">torch.Tensor 方法中的 byte()</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.byte">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.byte">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.ByteStorage">torch 中的 ByteStorage（类）</a>
</li>
      <li><a href="storage.html#torch.UntypedStorage.byteswap">torch.UntypedStorage 方法中的 byteswap()</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="C">C</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend">torch.distributed.elastic.rendezvous.c10d_rendezvous_backend 中的 C10dRendezvousBackend（类）</a>
</li>
      <li><a href="generated/torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached">torch.nn.utils.parametrize 模块中的 cached()</a>
</li>
      <li><a href="generated/torch.cuda.caching_allocator_alloc.html#torch.cuda.caching_allocator_alloc">caching_allocator_alloc() (在模块 torch.cuda 中)</a>
</li>
      <li><a href="generated/torch.cuda.caching_allocator_delete.html#torch.cuda.caching_allocator_delete">caching_allocator_delete() (在模块 torch.cuda 中)</a>
</li>
      <li><a href="generated/torch.cuda.memory.caching_allocator_enable.html#torch.cuda.memory.caching_allocator_enable">caching_allocator_enable() (在模块 torch.cuda.memory 中)</a>
</li>
      <li><a href="nn.init.html#torch.nn.init.calculate_gain">calculate_gain() (在模块 torch.nn.init 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.AffineQuantizedObserverBase.html#torch.ao.quantization.observer.AffineQuantizedObserverBase.calculate_qparams">calculate_qparams() (torch.ao.quantization.observer.AffineQuantizedObserverBase 方法)</a>

      <ul>
        <li><a href="generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver.calculate_qparams">(torch.ao.quantization.observer.最小-最大观察者方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html#torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.call_function">call_function() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts 方法)</a>

      <ul>
        <li><a href="fx.html#torch.fx.Graph.call_function">(torch.fx.Graph 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Interpreter.call_function">(torch.fx.Interpreter 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Transformer.call_function">(torch.fx.Transformer 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html#torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.call_method">call_method() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts 方法)</a>

      <ul>
        <li><a href="fx.html#torch.fx.Graph.call_method">(torch.fx.Graph 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Interpreter.call_method">(torch.fx.Interpreter 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html#torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.call_module">call_module() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts 方法)</a>

      <ul>
        <li><a href="fx.html#torch.fx.Graph.call_module">(torch.fx.Graph 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Interpreter.call_module">(torch.fx.Interpreter 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Tracer.call_module">(torch.fx.Tracer 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Transformer.call_module">(torch.fx.Transformer 方法)</a>
</li>
      </ul></li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.CallgrindStats">CallgrindStats (torch.utils.benchmark 中的类)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.CallMethodKey.html#torch.fx.experimental.symbolic_shapes.CallMethodKey">CallMethodKey (torch.fx.experimental.symbolic_shapes 中的类)</a>
</li>
      <li><a href="generated/torch.can_cast.html#torch.can_cast">torch 模块中的 can_cast()</a>
</li>
      <li><a href="generated/torch.cuda.can_device_access_peer.html#torch.cuda.can_device_access_peer">torch.cuda 模块中的 can_device_access_peer()</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.can_use_cudnn_attention">torch.backends.cuda 模块中的 can_use_cudnn_attention()</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.can_use_efficient_attention">torch.backends.cuda 模块中的 can_use_efficient_attention()</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.can_use_flash_attention">can_use_flash_attention() (在模块 torch.backends.cuda 中)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.canonicalize_bool_expr.html#torch.fx.experimental.symbolic_shapes.canonicalize_bool_expr">canonicalize_bool_expr() (在模块 torch.fx.experimental.symbolic_shapes 中)</a>
</li>
      <li><a href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.capture_begin">capture_begin() (torch.cuda.CUDAGraph 方法)</a>
</li>
      <li><a href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.capture_end">capture_end() (torch.cuda.CUDAGraph 方法)</a>
</li>
      <li><a href="generated/torch.cartesian_prod.html#torch.cartesian_prod">torch 模块中的 cartesian_prod()</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.cat">torch.distributions.constraints 模块中的 cat()</a>
</li>
      <li><a href="generated/torch.cat.html#torch.cat">torch 模块中的 cat()</a>

      <ul>
        <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.cat">(torch.ao.ns._numeric_suite.Shadow 方法)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.categorical.Categorical">灵态翻译（torch.distributions.categorical 中的类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.CatTransform">CatTransform（torch.distributions.transforms 中的类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.cauchy.Cauchy">高斯（torch.distributions.cauchy 中的类）</a>
</li>
      <li><a href="generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_">cauchy_()（torch.Tensor 方法）</a>
</li>
      <li><a href="generated/torch.nn.attention.bias.causal_lower_right.html#torch.nn.attention.bias.causal_lower_right">causal_lower_right()（在 torch.nn.attention.bias 模块中）</a>
</li>
      <li><a href="generated/torch.nn.attention.bias.causal_upper_left.html#torch.nn.attention.bias.causal_upper_left">causal_upper_left()（在 torch.nn.attention.bias 模块中）</a>
</li>
      <li><a href="generated/torch.nn.attention.bias.CausalBias.html#torch.nn.attention.bias.CausalBias">CausalBias（torch.nn.attention.bias 模块中的类）</a>
</li>
      <li><a href="generated/torch.nn.attention.bias.CausalVariant.html#torch.nn.attention.bias.CausalVariant">CausalVariant（torch.nn.attention.bias 模块中的类）</a>
</li>
      <li><a href="generated/torch.Tensor.ccol_indices.html#torch.Tensor.ccol_indices">ccol_indices() (torch.Tensor 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.cdf">cdf() (torch.distributions.cauchy.Cauchy 方法)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.cdf">(torch.distributions.distribution.Distribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.cdf">(torch.distributions.exponential.Exponential 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.cdf">(torch.distributions.gamma.Gamma 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_cauchy.HalfCauchy.cdf">(torch.distributions.half_cauchy.HalfCauchy 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_normal.HalfNormal.cdf">(torch.distributions.half_normal.HalfNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.cdf">(torch.distributions.laplace.Laplace 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.cdf">(torch.distributions.mixture_same_family.MixtureSameFamily 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.cdf">(torch.distributions.normal.Normal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.cdf">(torch.distributions.transformed_distribution.TransformedDistribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.cdf">(torch.distributions.uniform.Uniform 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cdist.html#torch.cdist">cdist() (在 torch 模块中)</a>
</li>
      <li><a href="generated/torch.Tensor.cdouble.html#torch.Tensor.cdouble">cdouble() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.ceil.html#torch.ceil">ceil() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.ceil.html#torch.Tensor.ceil">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.ceil_.html#torch.Tensor.ceil_">ceil_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.celu.html#torch.ao.nn.quantized.functional.celu">celu (torch.ao.nn.quantized.functional 中的类)</a>
</li>
      <li><a href="generated/torch.nn.CELU.html#torch.nn.CELU">CELU (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.functional.celu.html#torch.nn.functional.celu">celu()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.Tensor.cfloat.html#torch.Tensor.cfloat">cfloat()（torch.Tensor 方法）</a>
</li>
      <li><a href="generated/torch.chain_matmul.html#torch.chain_matmul">chain_matmul()（在 torch 模块中）</a>
</li>
      <li><a href="data.html#torch.utils.data.ChainDataset">ChainDataset（torch.utils.data 中的类）</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler">ChainedScheduler（torch.optim.lr_scheduler 中的类）</a>
</li>
      <li><a href="generated/torch.Tensor.chalf.html#torch.Tensor.chalf">chalf()（torch.Tensor 方法）</a>
</li>
      <li><a href="generated/torch.cuda.change_current_allocator.html#torch.cuda.change_current_allocator">change_current_allocator()（在 torch.cuda 模块中）</a>
</li>
      <li><a href="generated/torch.nn.ChannelShuffle.html#torch.nn.ChannelShuffle">ChannelShuffle（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.Tensor.char.html#torch.Tensor.char">char() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.char">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.char">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.CharStorage">CharStorage (torch 中的类)</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.check">check() (torch.distributed.elastic.rendezvous.etcd_store.EtcdStore 方法)</a>

      <ul>
        <li><a href="distributed.html#torch.distributed.Store.check">(torch.distributed.Store 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.constraints.Constraint.check">(torch.distributions.constraints.Constraint 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.check_consistent.html#torch.fx.experimental.symbolic_shapes.check_consistent">check_consistent() (在模块 torch.fx.experimental.symbolic_shapes 中)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.check_equal">check_equal() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="onnx_verification.html#torch.onnx.verification.check_export_model_diff">check_export_model_diff (torch.onnx.verification 类)</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.check_is_root">check_is_root() (torch.distributed.fsdp.FullyShardedDataParallel 方法)</a>
</li>
      <li><a href="generated/torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants">check_sparse_tensor_invariants (torch.sparse 类)</a>
</li>
      <li><a href="checkpoint.html#torch.utils.checkpoint.checkpoint">torch.utils.checkpoint 模块中的 checkpoint()函数</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.FileSystemReader.checkpoint_id">torch.distributed.checkpoint.FileSystemReader 属性的 checkpoint_id</a>
</li>
      <li><a href="checkpoint.html#torch.utils.checkpoint.checkpoint_sequential">torch.utils.checkpoint 模块中的 checkpoint_sequential()函数</a>
</li>
      <li><a href="checkpoint.html#torch.utils.checkpoint.CheckpointPolicy">torch.utils.checkpoint 模块中的 CheckpointPolicy 类</a>
</li>
      <li><a href="distributions.html#torch.distributions.chi2.Chi2">Chi2（torch.distributions.chi2 中的类）</a>
</li>
      <li><a href="elastic/errors.html#torch.distributed.elastic.multiprocessing.errors.ChildFailedError">ChildFailedError（torch.distributed.elastic.multiprocessing.errors 中的类）</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.children">children()（torch.jit.ScriptModule 方法）</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.children">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.children">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cholesky.html#torch.cholesky">cholesky() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.linalg.cholesky.html#torch.linalg.cholesky">(在 torch.linalg 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.cholesky.html#torch.Tensor.cholesky">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.linalg.cholesky_ex.html#torch.linalg.cholesky_ex">torch.linalg 模块中的 cholesky_ex()函数</a>
</li>
      <li><a href="generated/torch.cholesky_inverse.html#torch.cholesky_inverse">torch 模块中的 cholesky_inverse()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.cholesky_inverse.html#torch.Tensor.cholesky_inverse">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cholesky_solve.html#torch.cholesky_solve">torch 模块中的 cholesky_solve()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.cholesky_solve.html#torch.Tensor.cholesky_solve">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.chunk.html#torch.chunk">chunk() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.chunk.html#torch.Tensor.chunk">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.CircularPad1d.html#torch.nn.CircularPad1d">CircularPad1d (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.CircularPad2d.html#torch.nn.CircularPad2d">CircularPad2d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.CircularPad3d.html#torch.nn.CircularPad3d">CircularPad3d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.clamp.html#torch.ao.nn.quantized.functional.clamp">clamp（torch.ao.nn.quantized.functional 中的类）</a>
</li>
      <li><a href="generated/torch.clamp.html#torch.clamp">clamp()（torch 模块中的函数）</a>

      <ul>
        <li><a href="generated/torch.Tensor.clamp.html#torch.Tensor.clamp">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_">clamp_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.cleanup">cleanup() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.cufft_plan_cache.clear">clear() (在模块 torch.backends.cuda.cufft_plan_cache 中)</a>

      <ul>
        <li><a href="generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.clear">(torch.autograd.profiler_util.StringTable 方法)</a>
</li>
        <li><a href="generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.clear">(torch.nn.ModuleDict 方法)</a>
</li>
        <li><a href="generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.clear">(torch.nn.ParameterDict 方法)</a>
</li>
      </ul></li>
      <li><a href="notes/serialization.html#torch.serialization.clear_safe_globals">clear_safe_globals() (在模块 torch.serialization 中)</a>
</li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.TimerServer.clear_timers">clear_timers() (torch.distributed.elastic.timer.TimerServer 方法)</a>
</li>
      <li><a href="generated/torch.clip.html#torch.clip">clip() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.clip.html#torch.Tensor.clip">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.clip_.html#torch.Tensor.clip_">clip_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.nn.utils.clip_grad_norm.html#torch.nn.utils.clip_grad_norm">clip_grad_norm()（在 torch.nn.utils 模块中）</a>
</li>
      <li><a href="generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_">clip_grad_norm_()（在 torch.nn.utils 模块中）</a>

      <ul>
        <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_">(torch.distributed.fsdp.FullyShardedDataParallel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_">clip_grad_value_()（在 torch.nn.utils 模块中）</a>
</li>
      <li><a href="generated/torch.nn.utils.clip_grads_with_norm_.html#torch.nn.utils.clip_grads_with_norm_">clip_grads_with_norm_() (在模块 torch.nn.utils 中)</a>
</li>
      <li><a href="generated/torch.cuda.clock_rate.html#torch.cuda.clock_rate">clock_rate() (在模块 torch.cuda 中)</a>
</li>
      <li><a href="generated/torch.clone.html#torch.clone">clone() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.autograd.grad_mode.inference_mode.html#torch.autograd.grad_mode.inference_mode.clone">(torch.autograd.grad_mode.inference_mode 方法)</a>
</li>
        <li><a href="generated/torch.autograd.grad_mode.set_grad_enabled.html#torch.autograd.grad_mode.set_grad_enabled.clone">(torch.autograd.grad_mode.set_grad_enabled 方法)</a>
</li>
        <li><a href="generated/torch.autograd.grad_mode.set_multithreading_enabled.html#torch.autograd.grad_mode.set_multithreading_enabled.clone">(torch.autograd.grad_mode.set_multithreading_enabled 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.clone.html#torch.Tensor.clone">torch.Tensor 方法</a>
</li>
        <li><a href="storage.html#torch.TypedStorage.clone">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.clone">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Generator.html#torch.Generator.clone_state">clone_state() (torch.Generator 方法)</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.close">close (torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout 属性)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.close">close() (torch.package.PackageExporter 方法)</a>

      <ul>
        <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.close">(torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d.cls_to_become">cls_to_become (torch.nn.LazyBatchNorm1d 属性)</a>

      <ul>
        <li><a href="generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d.cls_to_become">(torch.nn.LazyBatchNorm2d 属性)</a>
</li>
        <li><a href="generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d.cls_to_become">(torch.nn.LazyBatchNorm3d 属性)</a>
</li>
        <li><a href="generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d.cls_to_become">(torch.nn.LazyConv1d 属性)</a>
</li>
        <li><a href="generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d.cls_to_become">(torch.nn.LazyConv2d 属性)</a>
</li>
        <li><a href="generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d.cls_to_become">(torch.nn.LazyConv3d 属性)</a>
</li>
        <li><a href="generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d.cls_to_become">(torch.nn.LazyConvTranspose1d 属性)</a>
</li>
        <li><a href="generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d.cls_to_become">(torch.nn.LazyConvTranspose2d 属性)</a>
</li>
        <li><a href="generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d.cls_to_become">(torch.nn.LazyConvTranspose3d 属性)</a>
</li>
        <li><a href="generated/torch.nn.LazyInstanceNorm1d.html#torch.nn.LazyInstanceNorm1d.cls_to_become">(torch.nn.LazyInstanceNorm1d 属性)</a>
</li>
        <li><a href="generated/torch.nn.LazyInstanceNorm2d.html#torch.nn.LazyInstanceNorm2d.cls_to_become">(torch.nn.LazyInstanceNorm2d 属性)</a>
</li>
        <li><a href="generated/torch.nn.LazyInstanceNorm3d.html#torch.nn.LazyInstanceNorm3d.cls_to_become">(torch.nn.LazyInstanceNorm3d 属性)</a>
</li>
        <li><a href="generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear.cls_to_become">(torch.nn.LazyLinear 属性)</a>
</li>
        <li><a href="generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter.cls_to_become">(torch.nn.parameter.UninitializedParameter 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.coalesce.html#torch.Tensor.coalesce">coalesce() (torch.Tensor 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.GraphModule.code">torch.fx.GraphModule 属性</a>

      <ul>
        <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.code">torch.jit.ScriptModule 属性</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.code_with_constants">code_with_constants (torch.jit.ScriptModule 属性)</a>
</li>
      <li><a href="generated/torch.Tensor.col_indices.html#torch.Tensor.col_indices">col_indices() (torch.Tensor 方法)</a>
</li>
      <li><a href="data.html#torch.utils.data._utils.collate.collate">collate() (位于模块 torch.utils.data._utils.collate)</a>
</li>
      <li><a href="futures.html#torch.futures.collect_all">collect_all() (位于模块 torch.futures)</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Timer.collect_callgrind">collect_callgrind() (torch.utils.benchmark.Timer 方法)</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Compare.colorize">colorize() (torch.utils.benchmark.Compare 方法)</a>
</li>
      <li><a href="generated/torch.column_stack.html#torch.column_stack">torch 模块中的 column_stack()</a>
</li>
      <li><a href="distributed.tensor.parallel.html#torch.distributed.tensor.parallel.ColwiseParallel">torch.distributed.tensor.parallel 中的 ColwiseParallel（类）</a>
</li>
      <li><a href="generated/torch.combinations.html#torch.combinations">torch 模块中的 combinations()</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.debug.CommDebugMode">CommDebugMode（torch.distributed.tensor.debug 中的类）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.commit_tensor">commit_tensor()（torch.distributed.checkpoint.LoadPlanner 方法）</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Compare">Compare（torch.utils.benchmark 中的类）</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.compare_model_outputs">compare_model_outputs()（在 torch.ao.ns._numeric_suite 模块中）</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.compare_model_stub">compare_model_stub()（在模块 torch.ao.ns._numeric_suite 中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.compare_results.html#torch.ao.quantization.compare_results">compare_results（torch.ao.quantization 中的类）</a>
</li>
      <li><a href="distributed.html#torch.distributed.Store.compare_set">compare_set()（torch.distributed.Store 方法）</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.compare_weights">compare_weights()（在模块 torch.ao.ns._numeric_suite 中）</a>
</li>
      <li><a href="generated/torch.compile.html#torch.compile">torch 模块中的 compile()函数</a>

      <ul>
        <li><a href="generated/torch.compiler.compile.html#torch.compiler.compile">torch.compiler 模块中</a>
</li>
        <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.compile">(torch.jit.ScriptModule 方法)</a>
</li>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.compile">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.compile">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.mps.compile_shader.html#torch.mps.compile_shader">torch.mps 模块中的 compile_shader()函数</a>
</li>
      <li><a href="generated/torch.compiled_with_cxx11_abi.html#torch.compiled_with_cxx11_abi">torch 模块中的 compiled_with_cxx11_abi()函数</a>
</li>
      <li><a href="generated/torch.complex.html#torch.complex">torch 模块中的 complex()函数</a>
</li>
      <li><a href="storage.html#torch.TypedStorage.complex_double">torch.TypedStorage 方法中的 complex_double()</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.complex_double">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.TypedStorage.complex_float">torch.TypedStorage 方法中的 complex_float()</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.complex_float">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.ComplexDoubleStorage">torch 中的 ComplexDoubleStorage 类</a>
</li>
      <li><a href="storage.html#torch.ComplexFloatStorage">torch 中的 ComplexFloatStorage（类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution">torch.distributions.mixture_same_family.MixtureSameFamily 属性 component_distribution</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.ComposeTransform">torch.distributions.transforms 中的 ComposeTransform（类）</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns.fx.utils.compute_cosine_similarity">torch.ao.ns.fx.utils 模块中的 compute_cosine_similarity()函数</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.compute_mask">compute_mask() (torch.nn.utils.prune.BasePruningMethod 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.compute_mask">(torch.nn.utils.prune.LnStructured 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.compute_mask">(torch.nn.utils.prune.PruningContainer 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.compute_mask">(torch.nn.utils.prune.RandomStructured 方法)</a>
</li>
      </ul></li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns.fx.utils.compute_normalized_l2_error">compute_normalized_l2_error() (在模块 torch.ao.ns.fx.utils 中)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns.fx.utils.compute_sqnr">compute_sqnr() (在模块 torch.ao.ns.fx.utils 中)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.compute_unbacked_bindings.html#torch.fx.experimental.symbolic_shapes.compute_unbacked_bindings">compute_unbacked_bindings() (在模块 torch.fx.experimental.symbolic_shapes 中)</a>
</li>
      <li><a href="onnx_dynamo.html#torch.onnx.ONNXProgram.compute_values">compute_values() (torch.onnx.ONNXProgram 方法)</a>
</li>
      <li><a href="generated/torch.concat.html#torch.concat">concat() (在 torch 模块中)</a>
</li>
      <li><a href="data.html#torch.utils.data.ConcatDataset">ConcatDataset (torch.utils.data 中的类)</a>
</li>
      <li><a href="generated/torch.concatenate.html#torch.concatenate">concatenate() (在 torch 模块中)</a>
</li>
      <li><a href="distributions.html#torch.distributions.inverse_gamma.InverseGamma.concentration">浓度（torch.distributions.inverse_gamma.InverseGamma 属性）</a>
</li>
      <li><a href="distributions.html#torch.distributions.beta.Beta.concentration0">浓度 0（torch.distributions.beta.Beta 属性）</a>
</li>
      <li><a href="distributions.html#torch.distributions.beta.Beta.concentration1">浓度 1（torch.distributions.beta.Beta 属性）</a>
</li>
      <li><a href="generated/torch.cond.html#torch.cond">cond()（在模块 torch 中）</a>

      <ul>
        <li><a href="cond.html#torch._higher_order_ops.cond.cond">(在模块 torch._higher_order_ops.cond 中)</a>
</li>
        <li><a href="generated/torch.linalg.cond.html#torch.linalg.cond">(在 torch.linalg 模块中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig.configs">configs (torch.ao.quantization.backend_config.BackendConfig 属性)</a>
</li>
      <li><a href="elastic/metrics.html#torch.distributed.elastic.metrics.configure">configure() (在模块 torch.distributed.elastic.metrics 中)</a>

      <ul>
        <li><a href="elastic/timer.html#torch.distributed.elastic.timer.configure">(在模块 torch.distributed.elastic.timer 中)</a>
</li>
      </ul></li>
      <li><a href="rpc.html#torch.distributed.rpc.PyRRef.confirmed_by_owner">confirmed_by_owner() (torch.distributed.rpc.PyRRef 方法)</a>
</li>
      <li><a href="generated/torch.conj.html#torch.conj">conj() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.conj.html#torch.Tensor.conj">torch.Tensor 方法</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.conj_physical.html#torch.conj_physical">conj_physical() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.conj_physical_.html#torch.Tensor.conj_physical_">conj_physical_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.nn.attention.bias.CausalVariant.html#torch.nn.attention.bias.CausalVariant.conjugate">conjugate() (torch.nn.attention.bias.CausalVariant 方法)</a>

      <ul>
        <li><a href="torch.html#torch.SymFloat.conjugate">(torch.SymFloat 方法)</a>
</li>
      </ul></li>
      <li><a href="elastic/metrics.html#torch.distributed.elastic.metrics.api.ConsoleMetricHandler">ConsoleMetricHandler (torch.distributed.elastic.metrics.api 中的类)</a>
</li>
      <li><a href="distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict">consolidate_state_dict() (torch.distributed.optim.ZeroRedundancyOptimizer 方法)</a>
</li>
      <li><a href="nn.init.html#torch.nn.init.constant_">constant_() (模块 torch.nn.init 中的)</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR">torch.optim.lr_scheduler 中的 ConstantLR（类）</a>
</li>
      <li><a href="generated/torch.nn.ConstantPad1d.html#torch.nn.ConstantPad1d">torch.nn 中的 ConstantPad1d（类）</a>
</li>
      <li><a href="generated/torch.nn.ConstantPad2d.html#torch.nn.ConstantPad2d">torch.nn 中的 ConstantPad2d（类）</a>
</li>
      <li><a href="generated/torch.nn.ConstantPad3d.html#torch.nn.ConstantPad3d">torch.nn 中的 ConstantPad3d（类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.constrain_range.html#torch.fx.experimental.symbolic_shapes.constrain_range">constrain_range()（在模块 torch.fx.experimental.symbolic_shapes 中）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.constrain_unify.html#torch.fx.experimental.symbolic_shapes.constrain_unify">constrain_unify()（在模块 torch.fx.experimental.symbolic_shapes 中）</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.Constraint">Constraint（类，在 torch.distributions.constraints 中）</a>

      <ul>
        <li><a href="export.html#torch.export.Constraint">torch 模块中的 export 函数</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.constraint_registry.ConstraintRegistry">ConstraintRegistry（类，在 torch.distributions.constraint_registry 中）</a>
</li>
      <li><a href="elastic/events.html#torch.distributed.elastic.events.construct_and_record_rdzv_event">torch.distributed.elastic.events 模块中的 construct_and_record_rdzv_event() 函数</a>
</li>
      <li><a href="rpc.html#torch.distributed.autograd.context">torch.distributed.autograd 中的 context 类</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.experimental.context_parallel">torch.distributed.tensor.experimental 模块中的 context_parallel() 函数</a>
</li>
      <li><a href="generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous">torch.Tensor 方法 contiguous()</a>
</li>
      <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli">torch.distributions.continuous_bernoulli 中的 ContinuousBernoulli 类</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.Conv1d.html#torch.ao.nn.quantized.Conv1d">torch.ao.nn.quantized 中的 Conv1d 类</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.conv1d.html#torch.ao.nn.quantized.functional.conv1d">torch.ao.nn.quantized.functional 中的 conv1d 类</a>
</li>
      <li><a href="generated/torch.nn.Conv1d.html#torch.nn.Conv1d">torch.nn 中的 Conv1d 类</a>
</li>
      <li><a href="generated/torch.nn.functional.conv1d.html#torch.nn.functional.conv1d">conv1d() (在模块 torch.nn.functional 中)</a>
</li>
      <li><a href="generated/torch.ao.nn.qat.Conv2d.html#torch.ao.nn.qat.Conv2d">Conv2d (torch.ao.nn.qat 中的类)</a>

      <ul>
        <li><a href="generated/torch.ao.nn.quantized.Conv2d.html#torch.ao.nn.quantized.Conv2d">(torch.ao.nn.quantized 类)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.functional.conv2d.html#torch.ao.nn.quantized.functional.conv2d">conv2d (在 torch.ao.nn.quantized.functional 中的类)</a>
</li>
      <li><a href="generated/torch.nn.Conv2d.html#torch.nn.Conv2d">Conv2d (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d">torch.nn.functional 中的 conv2d()函数</a>
</li>
      <li><a href="generated/torch.ao.nn.qat.Conv3d.html#torch.ao.nn.qat.Conv3d">torch.ao.nn.qat 中的 Conv3d 类</a>

      <ul>
        <li><a href="generated/torch.ao.nn.quantized.Conv3d.html#torch.ao.nn.quantized.Conv3d">(torch.ao.nn.quantized 类)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.functional.conv3d.html#torch.ao.nn.quantized.functional.conv3d">torch.ao.nn.quantized.functional 中的 conv3d 类</a>
</li>
      <li><a href="generated/torch.nn.Conv3d.html#torch.nn.Conv3d">torch.nn 中的 Conv3d 类</a>
</li>
      <li><a href="generated/torch.nn.functional.conv3d.html#torch.nn.functional.conv3d">conv3d()（在模块 torch.nn.functional 中）</a>
</li>
      <li><a href="generated/torch.nn.functional.conv_transpose1d.html#torch.nn.functional.conv_transpose1d">conv_transpose1d()（在模块 torch.nn.functional 中）</a>
</li>
      <li><a href="generated/torch.nn.functional.conv_transpose2d.html#torch.nn.functional.conv_transpose2d">conv_transpose2d()（在模块 torch.nn.functional 中）</a>
</li>
      <li><a href="generated/torch.nn.functional.conv_transpose3d.html#torch.nn.functional.conv_transpose3d">conv_transpose3d()（在模块 torch.nn.functional 中）</a>
</li>
      <li><a href="generated/torch.ao.nn.intrinsic.ConvBn1d.html#torch.ao.nn.intrinsic.ConvBn1d">ConvBn1d（torch.ao.nn.intrinsic 中的类）</a>

      <ul>
        <li><a href="generated/torch.ao.nn.intrinsic.qat.ConvBn1d.html#torch.ao.nn.intrinsic.qat.ConvBn1d">torch.ao.nn.intrinsic.qat 中的（类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.intrinsic.ConvBn2d.html#torch.ao.nn.intrinsic.ConvBn2d">ConvBn2d（torch.ao.nn.intrinsic 中的类）</a>

      <ul>
        <li><a href="generated/torch.ao.nn.intrinsic.qat.ConvBn2d.html#torch.ao.nn.intrinsic.qat.ConvBn2d">torch.ao.nn.intrinsic.qat 中的（类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.intrinsic.ConvBn3d.html#torch.ao.nn.intrinsic.ConvBn3d">ConvBn3d（torch.ao.nn.intrinsic 中的类）</a>

      <ul>
        <li><a href="generated/torch.ao.nn.intrinsic.qat.ConvBn3d.html#torch.ao.nn.intrinsic.qat.ConvBn3d">torch.ao.nn.intrinsic.qat 中的（类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.intrinsic.ConvBnReLU1d.html#torch.ao.nn.intrinsic.ConvBnReLU1d">ConvBnReLU1d（torch.ao.nn.intrinsic 中的类）</a>

      <ul>
        <li><a href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU1d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU1d">torch.ao.nn.intrinsic.qat 中的（类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.intrinsic.ConvBnReLU2d.html#torch.ao.nn.intrinsic.ConvBnReLU2d">ConvBnReLU2d（torch.ao.nn.intrinsic 中的类）</a>

      <ul>
        <li><a href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU2d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU2d">torch.ao.nn.intrinsic.qat 中的（类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.intrinsic.ConvBnReLU3d.html#torch.ao.nn.intrinsic.ConvBnReLU3d">ConvBnReLU3d（torch.ao.nn.intrinsic 中的类）</a>

      <ul>
        <li><a href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU3d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU3d">torch.ao.nn.intrinsic.qat 中的（类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.convert.html#torch.ao.quantization.convert">convert（torch.ao.quantization 中的类）</a>
</li>
      <li><a href="generated/torch.nn.utils.convert_conv2d_weight_memory_format.html#torch.nn.utils.convert_conv2d_weight_memory_format">convert_conv2d_weight_memory_format()（torch.nn.utils 模块中）</a>
</li>
      <li><a href="generated/torch.nn.utils.convert_conv3d_weight_memory_format.html#torch.nn.utils.convert_conv3d_weight_memory_format">torch.nn.utils 模块中的 convert_conv3d_weight_memory_format()函数</a>
</li>
      <li><a href="generated/torch.ao.quantization.quantize_fx.convert_fx.html#torch.ao.quantization.quantize_fx.convert_fx">torch.ao.quantization.quantize_fx 模块中的 convert_fx 类</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.convert_n_shadows_model">torch.ao.ns._numeric_suite_fx 模块中的 convert_n_shadows_model()函数</a>
</li>
      <li><a href="generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm.convert_sync_batchnorm">torch.nn.SyncBatchNorm 类中的 convert_sync_batchnorm()方法</a>
</li>
      <li><a href="generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig">torch.ao.quantization.fx.custom_config 中的 ConvertCustomConfig（类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ConvertIntKey.html#torch.fx.experimental.symbolic_shapes.ConvertIntKey">torch.fx.experimental.symbolic_shapes 中的 ConvertIntKey（类）</a>
</li>
      <li><a href="generated/torch.ao.nn.intrinsic.ConvReLU1d.html#torch.ao.nn.intrinsic.ConvReLU1d">torch.ao.nn.intrinsic 中的 ConvReLU1d（类）</a>

      <ul>
        <li><a href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU1d.html#torch.ao.nn.intrinsic.quantized.ConvReLU1d">torch.ao.nn.intrinsic.quantized 中的（类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.intrinsic.ConvReLU2d.html#torch.ao.nn.intrinsic.ConvReLU2d">torch.ao.nn.intrinsic 中的 ConvReLU2d（类）</a>

      <ul>
        <li><a href="generated/torch.ao.nn.intrinsic.qat.ConvReLU2d.html#torch.ao.nn.intrinsic.qat.ConvReLU2d">torch.ao.nn.intrinsic.qat 中的（类）</a>
</li>
        <li><a href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU2d.html#torch.ao.nn.intrinsic.quantized.ConvReLU2d">torch.ao.nn.intrinsic.quantized 中的（类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.intrinsic.ConvReLU3d.html#torch.ao.nn.intrinsic.ConvReLU3d">ConvReLU3d（torch.ao.nn.intrinsic 中的类）</a>

      <ul>
        <li><a href="generated/torch.ao.nn.intrinsic.qat.ConvReLU3d.html#torch.ao.nn.intrinsic.qat.ConvReLU3d">torch.ao.nn.intrinsic.qat 中的（类）</a>
</li>
        <li><a href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU3d.html#torch.ao.nn.intrinsic.quantized.ConvReLU3d">torch.ao.nn.intrinsic.quantized 中的（类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.ConvTranspose1d.html#torch.ao.nn.quantized.ConvTranspose1d">ConvTranspose1d（torch.ao.nn.quantized 中的类）</a>

      <ul>
        <li><a href="generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.ConvTranspose2d.html#torch.ao.nn.quantized.ConvTranspose2d">ConvTranspose2d（torch.ao.nn.quantized 中的类）</a>

      <ul>
        <li><a href="generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.ConvTranspose3d.html#torch.ao.nn.quantized.ConvTranspose3d">ConvTranspose3d（torch.ao.nn.quantized 中的类）</a>

      <ul>
        <li><a href="generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.copy">copy() (torch.autograd.profiler_util.StringTable 方法)</a>

      <ul>
        <li><a href="export.html#torch.export.decomp_utils.CustomDecompTable.copy">(torch.export.decomp_utils.CustomDecompTable 方法)</a>
</li>
        <li><a href="generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.copy">(torch.nn.ParameterDict 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.copy_.html#torch.Tensor.copy_">copy_() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.copy_">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.copy_">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.copysign.html#torch.copysign">copysign() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.copysign.html#torch.Tensor.copysign">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.copysign_.html#torch.Tensor.copysign_">copysign_() (torch.Tensor 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.CorrCholeskyTransform">torch.distributions.transforms 中的 CorrCholeskyTransform (类)</a>
</li>
      <li><a href="generated/torch.corrcoef.html#torch.corrcoef">torch 模块中的 corrcoef()</a>

      <ul>
        <li><a href="generated/torch.Tensor.corrcoef.html#torch.Tensor.corrcoef">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cos.html#torch.cos">torch 模块中的 cos()</a>

      <ul>
        <li><a href="generated/torch.Tensor.cos.html#torch.Tensor.cos">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.cos_.html#torch.Tensor.cos_">torch.Tensor 方法中的 cos_()</a>
</li>
      <li><a href="generated/torch.cosh.html#torch.cosh">cosh() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.cosh.html#torch.Tensor.cosh">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.cosh_.html#torch.Tensor.cosh_">cosh_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.signal.windows.cosine.html#torch.signal.windows.cosine">cosine() (在模块 torch.signal.windows 中)</a>
</li>
      <li><a href="generated/torch.nn.functional.cosine_embedding_loss.html#torch.nn.functional.cosine_embedding_loss">cosine_embedding_loss() (在模块 torch.nn.functional 中)</a>
</li>
      <li><a href="generated/torch.nn.functional.cosine_similarity.html#torch.nn.functional.cosine_similarity">torch.nn.functional 模块中的 cosine_similarity()函数</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR">torch.optim.lr_scheduler 模块中的 CosineAnnealingLR 类</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts">torch.optim.lr_scheduler 模块中的 CosineAnnealingWarmRestarts 类</a>
</li>
      <li><a href="generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss">torch.nn 模块中的 CosineEmbeddingLoss 类</a>
</li>
      <li><a href="generated/torch.nn.CosineSimilarity.html#torch.nn.CosineSimilarity">余弦相似度（torch.nn 中的类）</a>
</li>
      <li><a href="monitor.html#torch.monitor.Stat.count">count（torch.monitor.Stat 属性）</a>
</li>
      <li><a href="generated/torch.autograd.forward_ad.UnpackedDualTensor.html#torch.autograd.forward_ad.UnpackedDualTensor.count">count()（torch.autograd.forward_ad.UnpackedDualTensor 方法）</a>

      <ul>
        <li><a href="generated/torch.autograd.profiler_util.Kernel.html#torch.autograd.profiler_util.Kernel.count">torch.autograd.profiler_util.Kernel 方法</a>
</li>
        <li><a href="generated/torch.jit.Attribute.html#torch.jit.Attribute.count">torch.jit.Attribute 方法</a>
</li>
        <li><a href="generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.count">(torch.nn.utils.rnn.PackedSequence 方法)</a>
</li>
        <li><a href="size.html#torch.Size.count">(torch.Size 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.count_nonzero.html#torch.count_nonzero">count_nonzero()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.count_nonzero.html#torch.Tensor.count_nonzero">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.CallgrindStats.counts">counts() (torch.utils.benchmark.CallgrindStats 方法)</a>
</li>
      <li><a href="generated/torch.cov.html#torch.cov">cov() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.cov.html#torch.Tensor.cov">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix">covariance_matrix (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal 属性)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix">(torch.distributions.multivariate_normal.MultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.wishart.Wishart.covariance_matrix">(torch.distributions.wishart.Wishart 属性)</a>
</li>
      </ul></li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.CppExtension">CppExtension() (在模块 torch.utils.cpp_extension 中)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.cpu">cpu() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.cpu">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.cpu">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.cpu.html#torch.Tensor.cpu">torch.Tensor 方法</a>
</li>
        <li><a href="storage.html#torch.TypedStorage.cpu">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.cpu">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="fsdp.html#torch.distributed.fsdp.CPUOffload">CPUOffload (torch.distributed.fsdp 中的类)</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.CPUOffloadPolicy">CPUOffloadPolicy (torch.distributed.fsdp 中的类)</a>
</li>
      <li><a href="fx.html#torch.fx.Tracer.create_arg">create_arg() (torch.fx.Tracer 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.Tracer.create_args_for_root">create_args_for_root() (torch.fx.Tracer 方法)</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.create_backend">create_backend() (在模块 torch.distributed.elastic.rendezvous.c10d_rendezvous_backend 中)</a>

      <ul>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.create_backend">(在模块 torch.distributed.elastic.rendezvous.etcd_rendezvous_backend 中)</a>
</li>
      </ul></li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.create_block_mask">create_block_mask() (在模块 torch.nn.attention.flex_attention 中)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.create_global_plan">create_global_plan() (torch.distributed.checkpoint.LoadPlanner 方法)</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.SavePlanner.create_global_plan">(torch.distributed.checkpoint.SavePlanner 方法)</a>
</li>
      </ul></li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.create_handler">create_handler() (在模块 torch.distributed.elastic.rendezvous.dynamic_rendezvous 中)</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.health_check_server.create_healthcheck_server">create_healthcheck_server() (在模块 torch.distributed.elastic.agent.server.health_check_server 中)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.create_local_plan">create_local_plan() (torch.distributed.checkpoint.LoadPlanner 方法)</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.SavePlanner.create_local_plan">(torch.distributed.checkpoint.SavePlanner 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.create_mask">create_mask() (在模块 torch.nn.attention.flex_attention 中)</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.create_nested_block_mask">create_nested_block_mask() (在模块 torch.nn.attention.flex_attention 中)</a>
</li>
      <li><a href="fx.html#torch.fx.Graph.create_node">create_node() (torch.fx.Graph 方法)</a>

      <ul>
        <li><a href="fx.html#torch.fx.Tracer.create_node">(torch.fx.Tracer 方法)</a>
</li>
      </ul></li>
      <li><a href="fx.html#torch.fx.Tracer.create_proxy">create_proxy() (torch.fx.Tracer 方法)</a>
</li>
      <li><a href="checkpoint.html#torch.utils.checkpoint.create_selective_checkpoint_contexts">torch.utils.checkpoint 模块中的 create_selective_checkpoint_contexts() 函数</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_symbol">torch.fx.experimental.symbolic_shapes.ShapeEnv 方法中的 create_symbol() 函数</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_symbolic_sizes_strides_storage_offset">torch.fx.experimental.symbolic_shapes.ShapeEnv 方法中的 create_symbolic_sizes_strides_storage_offset() 函数</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_symboolnode">torch.fx.experimental.symbolic_shapes.ShapeEnv 方法中的 create_symboolnode() 函数</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_symfloatnode">create_symfloatnode() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_symintnode">create_symintnode() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unbacked_symbool">create_unbacked_symbool() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unbacked_symfloat">create_unbacked_symfloat() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unbacked_symint">create_unbacked_symint() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unspecified_symbol">create_unspecified_symbol() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.create_unspecified_symint_and_symbol">create_unspecified_symint_and_symbol() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.cross.html#torch.cross">cross() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.linalg.cross.html#torch.linalg.cross">(在 torch.linalg 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.cross.html#torch.Tensor.cross">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy">torch.nn.functional 模块中的 cross_entropy()函数</a>
</li>
      <li><a href="generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss">torch.nn 模块中的 CrossEntropyLoss 类</a>
</li>
      <li><a href="generated/torch.Tensor.crow_indices.html#torch.Tensor.crow_indices">torch.Tensor 方法中的 crow_indices()</a>
</li>
      <li><a href="generated/torch.nn.functional.ctc_loss.html#torch.nn.functional.ctc_loss">torch.nn.functional 模块中的 ctc_loss()函数</a>
</li>
      <li><a href="generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss">CTCLoss（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.cuda">cuda()（torch.jit.ScriptModule 方法）</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.cuda">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.cuda">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.cuda.html#torch.Tensor.cuda">torch.Tensor 方法</a>
</li>
        <li><a href="storage.html#torch.TypedStorage.cuda">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.cuda">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.CUDAExtension">CUDAExtension()（在 torch.utils.cpp_extension 模块中）</a>
</li>
      <li><a href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph">CUDAGraph（torch.cuda 中的类）</a>
</li>
      <li><a href="generated/torch.compiler.cudagraph_mark_step_begin.html#torch.compiler.cudagraph_mark_step_begin">cudagraph_mark_step_begin()（在 torch.compiler 模块中）</a>
</li>
      <li><a href="generated/torch.cuda.CUDAPluggableAllocator.html#torch.cuda.CUDAPluggableAllocator">CUDAPluggableAllocator（torch.cuda 模块中的类）</a>
</li>
      <li><a href="generated/torch.cuda.cudart.html#torch.cuda.cudart">cudart()（在 torch.cuda 模块中）</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.cudnn_sdp_enabled">cudnn_sdp_enabled()（在 torch.backends.cuda 模块中）</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.cufft_plan_cache">cufft_plan_cache（在 torch.backends.cuda 模块中）</a>
</li>
      <li><a href="generated/torch.cummax.html#torch.cummax">cummax()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.cummax.html#torch.Tensor.cummax">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cummin.html#torch.cummin">cummin()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.cummin.html#torch.Tensor.cummin">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cumprod.html#torch.cumprod">cumprod() (模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.cumprod_.html#torch.Tensor.cumprod_">cumprod_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.cumsum.html#torch.cumsum">cumsum()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.cumsum_.html#torch.Tensor.cumsum_">cumsum_()（torch.Tensor 方法）</a>
</li>
      <li><a href="generated/torch.cumulative_trapezoid.html#torch.cumulative_trapezoid">cumulative_trapezoid()（在 torch 模块中）</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.CumulativeDistributionTransform">torch.distributions.transforms 中的 CumulativeDistributionTransform (类)</a>
</li>
      <li><a href="generated/torch.accelerator.current_accelerator.html#torch.accelerator.current_accelerator">torch.accelerator 模块中的 current_accelerator()</a>
</li>
      <li><a href="generated/torch.mps.current_allocated_memory.html#torch.mps.current_allocated_memory">torch.mps 模块中的 current_allocated_memory()</a>
</li>
      <li><a href="generated/torch.cuda.current_blas_handle.html#torch.cuda.current_blas_handle">torch.cuda 模块中的 current_blas_handle()</a>
</li>
      <li><a href="generated/torch.cpu.current_device.html#torch.cpu.current_device">current_device() (在模块 torch.cpu 中)</a>

      <ul>
        <li><a href="generated/torch.cuda.current_device.html#torch.cuda.current_device">(在模块 torch.cuda 中)</a>
</li>
        <li><a href="generated/torch.mtia.current_device.html#torch.mtia.current_device">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="generated/torch.xpu.current_device.html#torch.xpu.current_device">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.accelerator.current_device_idx.html#torch.accelerator.current_device_idx">current_device_idx() (在模块 torch.accelerator 中)</a>
</li>
      <li><a href="generated/torch.accelerator.current_device_index.html#torch.accelerator.current_device_index">current_device_index() (在模块 torch.accelerator 中)</a>
</li>
      <li><a href="generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker.current_step">current_step() (torch.autograd.profiler.KinetoStepTracker 类方法)</a>
</li>
      <li><a href="generated/torch.accelerator.current_stream.html#torch.accelerator.current_stream">current_stream() (在模块 torch.accelerator 中)</a>

      <ul>
        <li><a href="generated/torch.cpu.current_stream.html#torch.cpu.current_stream">(在模块 torch.cpu 中)</a>
</li>
        <li><a href="generated/torch.cuda.current_stream.html#torch.cuda.current_stream">(在模块 torch.cuda 中)</a>
</li>
        <li><a href="generated/torch.mtia.current_stream.html#torch.mtia.current_stream">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="generated/torch.xpu.current_stream.html#torch.xpu.current_stream">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="amp.html#torch.amp.custom_bwd">custom_bwd()（在 torch.amp 模块中）</a>

      <ul>
        <li><a href="amp.html#torch.cuda.amp.custom_bwd">(在 torch.cuda.amp 模块中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.utils.prune.custom_from_mask.html#torch.nn.utils.prune.custom_from_mask">custom_from_mask()（在 torch.nn.utils.prune 模块中）</a>
</li>
      <li><a href="amp.html#torch.amp.custom_fwd">custom_fwd()（在 torch.amp 模块中）</a>

      <ul>
        <li><a href="amp.html#torch.cuda.amp.custom_fwd">(在 torch.cuda.amp 模块中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.CUSTOM_KEY.html#torch.ao.quantization.CUSTOM_KEY">CUSTOM_KEY (在 torch.ao.quantization 模块中)</a>
</li>
      <li><a href="library.html#torch.library.custom_op">custom_op() (在 torch.library 模块中)</a>
</li>
      <li><a href="export.html#torch.export.decomp_utils.CustomDecompTable">CustomDecompTable (类，在 torch.export.decomp_utils 中)</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask">torch.nn.utils.prune 中的 CustomFromMask（类）</a>
</li>
      <li><a href="export.html#torch.export.graph_signature.CustomObjArgument">torch.export.graph_signature 中的 CustomObjArgument（类）</a>
</li>
      <li><a href="library.html#torch._library.custom_ops.CustomOpDef">torch._library.custom_ops 中的 CustomOpDef（类）</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR">torch.optim.lr_scheduler 中的 CyclicLR（类）</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="D">D</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="monitor.html#torch.monitor.Event.data">data (torch.monitor.Event 属性)</a>

      <ul>
        <li><a href="generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.data">(torch.nn.utils.rnn.PackedSequence 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.functional.torch.nn.parallel.data_parallel.html#torch.nn.parallel.data_parallel">data_parallel() (在模块 torch.nn.parallel 中)</a>
</li>
      <li><a href="generated/torch.Tensor.data_ptr.html#torch.Tensor.data_ptr">data_ptr() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.data_ptr">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.data_ptr">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="monitor.html#torch.monitor.data_value_t">data_value_t (torch.monitor 中的类)</a>
</li>
      <li><a href="data.html#torch.utils.data.DataLoader">DataLoader (torch.utils.data 中的类)</a>
</li>
      <li><a href="generated/torch.nn.DataParallel.html#torch.nn.DataParallel">torch.nn 中的 DataParallel（类）</a>
</li>
      <li><a href="data.html#torch.utils.data.Dataset">torch.utils.data 中的 Dataset（类）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.dcp_to_torch_save">torch.distributed.checkpoint.format_utils 模块中的 dcp_to_torch_save()函数</a>
</li>
      <li><a href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.debug_dump">torch.cuda.CUDAGraph 方法中的 debug_dump()</a>
</li>
      <li><a href="generated/torch.func.debug_unwrap.html#torch.func.debug_unwrap">torch.func 模块中的 debug_unwrap()</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig.default_activation_only_qconfig.html#torch.ao.quantization.qconfig.default_activation_only_qconfig">torch.ao.quantization.qconfig 模块中的 default_activation_only_qconfig</a>
</li>
      <li><a href="data.html#torch.utils.data.default_collate">torch.utils.data 模块中的 default_collate()</a>
</li>
      <li><a href="data.html#torch.utils.data.default_convert">torch.utils.data 模块中的 default_convert()</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.default_debug_observer.html#torch.ao.quantization.observer.default_debug_observer">default_debug_observer（在 torch.ao.quantization.observer 模块中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig.default_debug_qconfig.html#torch.ao.quantization.qconfig.default_debug_qconfig">default_debug_qconfig（在 torch.ao.quantization.qconfig 模块中）</a>
</li>
      <li><a href="export.html#torch.export.exported_program.default_decompositions">default_decompositions()（在 torch.export.exported_program 模块中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig.default_dynamic_qconfig.html#torch.ao.quantization.qconfig.default_dynamic_qconfig">default_dynamic_qconfig（在 torch.ao.quantization.qconfig 模块中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.default_dynamic_quant_observer.html#torch.ao.quantization.observer.default_dynamic_quant_observer">torch.ao.quantization.observer 模块中的 default_dynamic_quant_observer</a>
</li>
      <li><a href="generated/torch.ao.quantization.default_eval_fn.html#torch.ao.quantization.default_eval_fn">torch.ao.quantization 中的 default_eval_fn 类</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.default_factory">torch.autograd.profiler_util.StringTable 属性中的 default_factory</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.default_fake_quant.html#torch.ao.quantization.fake_quantize.default_fake_quant">torch.ao.quantization.fake_quantize 模块中的 default_fake_quant</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.default_float_qparams_observer.html#torch.ao.quantization.observer.default_float_qparams_observer">default_float_qparams_observer（在模块 torch.ao.quantization.observer 中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.default_fused_act_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_act_fake_quant">default_fused_act_fake_quant（在模块 torch.ao.quantization.fake_quantize 中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant">default_fused_per_channel_wt_fake_quant（在模块 torch.ao.quantization.fake_quantize 中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant">default_fused_wt_fake_quant（在模块 torch.ao.quantization.fake_quantize 中）</a>
</li>
      <li><a href="torch.html#torch.torch.default_generator">default_generator (torch.torch 属性)</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.default_histogram_fake_quant.html#torch.ao.quantization.fake_quantize.default_histogram_fake_quant">default_histogram_fake_quant (在模块 torch.ao.quantization.fake_quantize 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.default_histogram_observer.html#torch.ao.quantization.observer.default_histogram_observer">default_histogram_observer (在模块 torch.ao.quantization.observer 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.default_observer.html#torch.ao.quantization.observer.default_observer">default_observer (在模块 torch.ao.quantization.observer 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig.default_per_channel_qconfig.html#torch.ao.quantization.qconfig.default_per_channel_qconfig">torch.ao.quantization.qconfig 模块中的 default_per_channel_qconfig</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant.html#torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant">torch.ao.quantization.fake_quantize 模块中的 default_per_channel_weight_fake_quant</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.default_per_channel_weight_observer.html#torch.ao.quantization.observer.default_per_channel_weight_observer">torch.ao.quantization.observer 模块中的 default_per_channel_weight_observer</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.default_placeholder_observer.html#torch.ao.quantization.observer.default_placeholder_observer">torch.ao.quantization.observer 模块中的 default_placeholder_observer</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig.default_qat_qconfig.html#torch.ao.quantization.qconfig.default_qat_qconfig">default_qat_qconfig（在模块 torch.ao.quantization.qconfig 中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig.default_qat_qconfig_v2.html#torch.ao.quantization.qconfig.default_qat_qconfig_v2">default_qat_qconfig_v2（在模块 torch.ao.quantization.qconfig 中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig.default_qconfig.html#torch.ao.quantization.qconfig.default_qconfig">default_qconfig（在模块 torch.ao.quantization.qconfig 中）</a>
</li>
      <li><a href="generated/torch.cuda.default_stream.html#torch.cuda.default_stream">default_stream()（在模块 torch.cuda 中）</a>

      <ul>
        <li><a href="generated/torch.mtia.default_stream.html#torch.mtia.default_stream">（在 torch.mtia 模块中）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.default_weight_fake_quant.html#torch.ao.quantization.fake_quantize.default_weight_fake_quant">default_weight_fake_quant（在 torch.ao.quantization.fake_quantize 模块中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.default_weight_observer.html#torch.ao.quantization.observer.default_weight_observer">default_weight_observer（在 torch.ao.quantization.observer 模块中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig.default_weight_only_qconfig.html#torch.ao.quantization.qconfig.default_weight_only_qconfig">default_weight_only_qconfig（在 torch.ao.quantization.qconfig 模块中）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.DefaultLoadPlanner">torch.distributed.checkpoint 中的 DefaultLoadPlanner (类)</a>
</li>
      <li><a href="elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.DefaultLogsSpecs">torch.distributed.elastic.multiprocessing.api 中的 DefaultLogsSpecs (类)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.DefaultSavePlanner">torch.distributed.checkpoint 中的 DefaultSavePlanner (类)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.defer_runtime_assert">torch.fx.experimental.symbolic_shapes.ShapeEnv 方法中的 defer_runtime_assert()</a>
</li>
      <li><a href="generated/torch.mtia.DeferredMtiaCallError.html#torch.mtia.DeferredMtiaCallError">延迟调用 Mtia 错误</a>
</li>
      <li><a href="library.html#torch.library.define">define()（在 torch.library 模块中）</a>

      <ul>
        <li><a href="library.html#torch.library.Library.define">torch.library 的 Library 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.definitely_false.html#torch.fx.experimental.symbolic_shapes.definitely_false">definitely_false()（在 torch.fx.experimental.symbolic_shapes 模块中）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.definitely_true.html#torch.fx.experimental.symbolic_shapes.definitely_true">definitely_true()（在 torch.fx.experimental.symbolic_shapes 模块中）</a>
</li>
      <li><a href="generated/torch.deg2rad.html#torch.deg2rad">deg2rad()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="fx.html#torch.fx.GraphModule.delete_all_unused_submodules">delete_all_unused_submodules()（torch.fx.GraphModule 方法）</a>
</li>
      <li><a href="distributed.html#torch.distributed.Store.delete_key">delete_key() (torch.distributed.Store 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.GraphModule.delete_submodule">delete_submodule() (torch.fx.GraphModule 方法)</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.CallgrindStats.delta">delta() (torch.utils.benchmark.CallgrindStats 方法)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.denied_modules">denied_modules() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.FunctionCounts.denoise">denoise() (torch.utils.benchmark.FunctionCounts 方法)</a>
</li>
      <li><a href="generated/torch.nn.attention.bias.CausalVariant.html#torch.nn.attention.bias.CausalVariant.denominator">分母 (torch.nn.attention.bias.CausalVariant 属性)</a>
</li>
      <li><a href="generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim">dense_dim() (torch.Tensor 方法)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.deny">deny() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.dependency_graph_string">torch.package.PackageExporter 方法 dependency_graph_string()</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.dependent_property">torch.distributions.constraints 模块中的 dependent_property</a>
</li>
      <li><a href="generated/torch.dequantize.html#torch.dequantize">torch 模块中的 dequantize()</a>

      <ul>
        <li><a href="generated/torch.ao.nn.quantizable.MultiheadAttention.html#torch.ao.nn.quantizable.MultiheadAttention.dequantize">torch.ao.nn.quantizable.MultiheadAttention 方法()</a>
</li>
        <li><a href="generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.DeQuantStub.html#torch.ao.quantization.DeQuantStub">DeQuantStub (torch.ao.quantization 中的类)</a>
</li>
      <li><a href="generated/torch.cuda.gds.GdsFile.html#torch.cuda.gds.GdsFile.deregister_handle">deregister_handle() (torch.cuda.gds.GdsFile 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.deserialize_symexpr">deserialize_symexpr() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.det.html#torch.det">torch 模块中的 det()函数</a>

      <ul>
        <li><a href="generated/torch.linalg.det.html#torch.linalg.det">(在 torch.linalg 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.det.html#torch.Tensor.det">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.detach.html#torch.Tensor.detach">detach()方法（torch.Tensor）</a>
</li>
      <li><a href="generated/torch.Tensor.detach_.html#torch.Tensor.detach_">detach_() (torch.Tensor 方法)</a>
</li>
      <li><a href="autograd.html#torch.autograd.detect_anomaly">detect_anomaly (torch.autograd 中的类)</a>
</li>
      <li><a href="backends.html#torch.backends.cudnn.deterministic">deterministic (模块 torch.backends.cudnn 中的)</a>
</li>
      <li><a href="tensor_attributes.html#torch.device">device (torch 中的类)</a>

      <ul>
        <li><a href="generated/torch.cuda.device.html#torch.cuda.device">(torch.cuda 中的类)</a>
</li>
        <li><a href="generated/torch.mtia.device.html#torch.mtia.device">(torch.mtia 中的类)</a>
</li>
        <li><a href="generated/torch.xpu.device.html#torch.xpu.device">（torch.xpu 中的类）</a>
</li>
        <li><a href="generated/torch.autograd.profiler_util.Kernel.html#torch.autograd.profiler_util.Kernel.device">(torch.autograd.profiler_util.Kernel 属性)</a>
</li>
        <li><a href="generated/torch.Generator.html#torch.Generator.device">(torch.Generator 属性)</a>
</li>
        <li><a href="generated/torch.Tensor.device.html#torch.Tensor.device">（torch.Tensor 属性）</a>
</li>
        <li><a href="storage.html#torch.TypedStorage.device">(torch.TypedStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.device">(torch.UntypedStorage 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.accelerator.device_count.html#torch.accelerator.device_count">device_count() (在模块 torch.accelerator 中)</a>

      <ul>
        <li><a href="generated/torch.cpu.device_count.html#torch.cpu.device_count">(在模块 torch.cpu 中)</a>
</li>
        <li><a href="generated/torch.cuda.device_count.html#torch.cuda.device_count">(在模块 torch.cuda 中)</a>
</li>
        <li><a href="generated/torch.mps.device_count.html#torch.mps.device_count">（在模块 torch.mps 中）</a>
</li>
        <li><a href="generated/torch.mtia.device_count.html#torch.mtia.device_count">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="generated/torch.xpu.device_count.html#torch.xpu.device_count">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps">device_maps (torch.distributed.rpc.TensorPipeRpcBackendOptions 属性)</a>
</li>
      <li><a href="generated/torch.cuda.device_memory_used.html#torch.cuda.device_memory_used">device_memory_used() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.DTensor.device_mesh">设备网格（torch.distributed.tensor.DTensor 属性）</a>
</li>
      <li><a href="generated/torch.cuda.device_of.html#torch.cuda.device_of">device_of（torch.cuda 中的类）</a>

      <ul>
        <li><a href="generated/torch.xpu.device_of.html#torch.xpu.device_of">（torch.xpu 中的类）</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.device_mesh.DeviceMesh">DeviceMesh（torch.distributed.device_mesh 中的类）</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.devices">设备（torch.distributed.rpc.TensorPipeRpcBackendOptions 属性）</a>
</li>
      <li><a href="distributions.html#torch.distributions.chi2.Chi2.df">df（torch.distributions.chi2.Chi2 属性）</a>
</li>
      <li><a href="generated/torch.diag.html#torch.diag">diag()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.diag.html#torch.Tensor.diag">torch.Tensor 方法</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.diag_embed.html#torch.diag_embed">diag_embed()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.diag_embed.html#torch.Tensor.diag_embed">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.diagflat.html#torch.diagflat">diagflat()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.diagflat.html#torch.Tensor.diagflat">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="onnx_dynamo.html#torch.onnx.DiagnosticOptions">torch.onnx 中的 DiagnosticOptions（类）</a>
</li>
      <li><a href="generated/torch.diagonal.html#torch.diagonal">torch 模块中的 diagonal()函数</a>

      <ul>
        <li><a href="generated/torch.linalg.diagonal.html#torch.linalg.diagonal">(在 torch.linalg 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.diagonal_scatter.html#torch.diagonal_scatter">torch 模块中的 diagonal_scatter()</a>

      <ul>
        <li><a href="generated/torch.Tensor.diagonal_scatter.html#torch.Tensor.diagonal_scatter">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.diff.html#torch.diff">torch 模块中的 diff()</a>

      <ul>
        <li><a href="generated/torch.Tensor.diff.html#torch.Tensor.diff">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.digamma.html#torch.digamma">torch 模块中的 digamma()函数</a>

      <ul>
        <li><a href="special.html#torch.special.digamma">（在 torch.special 模块中）</a>
</li>
        <li><a href="generated/torch.Tensor.digamma.html#torch.Tensor.digamma">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.digamma_.html#torch.Tensor.digamma_">torch.Tensor 方法中的 digamma_()</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.placement_types.Shard.dim">dim (torch.distributed.tensor.placement_types.Shard 属性)</a>
</li>
      <li><a href="export.html#torch.export.dynamic_shapes.Dim">Dim() (在模块 torch.export.dynamic_shapes 中)</a>
</li>
      <li><a href="generated/torch.Tensor.dim.html#torch.Tensor.dim">dim() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.dim_order.html#torch.Tensor.dim_order">dim_order() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints">torch.fx.experimental.symbolic_shapes 中的 DimConstraints（类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.DimDynamic.html#torch.fx.experimental.symbolic_shapes.DimDynamic">torch.fx.experimental.symbolic_shapes 中的 DimDynamic（类）</a>
</li>
      <li><a href="export.html#torch.export.dims">torch.export 模块中的 dims()</a>
</li>
      <li><a href="nn.init.html#torch.nn.init.dirac_">torch.nn.init 模块中的 dirac_()</a>
</li>
      <li><a href="package.html#torch.package.Directory">torch.package 中的 Directory（类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet">torch.distributions.dirichlet 中的 Dirichlet（类）</a>
</li>
      <li><a href="generated/torch.compiler.disable.html#torch.compiler.disable">torch.compiler 模块中的 disable()函数</a>

      <ul>
        <li><a href="generated/torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants.disable">torch.sparse.check_sparse_tensor_invariants 静态方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.disable_fake_quant.html#torch.ao.quantization.fake_quantize.disable_fake_quant">torch.ao.quantization.fake_quantize 中的 disable_fake_quant (类)</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.disable_observer.html#torch.ao.quantization.fake_quantize.disable_observer">torch.ao.quantization.fake_quantize 中的 disable_observer (类)</a>
</li>
      <li><a href="autograd.html#torch.autograd.graph.disable_saved_tensors_hooks">torch.autograd.graph 中的 disable_saved_tensors_hooks (类)</a>
</li>
      <li><a href="generated/torch.dist.html#torch.dist">torch 模块中的 dist() 函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.dist.html#torch.Tensor.dist">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.DistBackendError">DistBackendError (torch.distributed 中的类)</a>
</li>
      <li><a href="distributed.html#torch.distributed.DistError">DistError (torch.distributed 中的类)</a>
</li>
      <li><a href="distributed.html#torch.distributed.DistNetworkError">DistNetworkError (torch.distributed 中的类)</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.distribute_module">distribute_module()（在 torch.distributed.tensor 模块中）</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.distribute_tensor">distribute_tensor()（在 torch.distributed.tensor 模块中）</a>
</li>
      <li><a href="generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">DistributedDataParallel（torch.nn.parallel 中的类）</a>
</li>
      <li><a href="distributed.optim.html#torch.distributed.optim.DistributedOptimizer">DistributedOptimizer（torch.distributed.optim 中的类）</a>
</li>
      <li><a href="data.html#torch.utils.data.distributed.DistributedSampler">torch.utils.data.distributed 中的 DistributedSampler（类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.distribution.Distribution">torch.distributions.distribution 中的 Distribution（类）</a>
</li>
      <li><a href="distributed.html#torch.distributed.DistStoreError">torch.distributed 中的 DistStoreError（类）</a>
</li>
      <li><a href="generated/torch.div.html#torch.div">torch 模块中的 div()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.div.html#torch.Tensor.div">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.div_.html#torch.Tensor.div_">div_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.divide.html#torch.divide">divide() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.divide.html#torch.Tensor.divide">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.divide_.html#torch.Tensor.divide_">divide_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.DivideByKey.html#torch.fx.experimental.symbolic_shapes.DivideByKey">DivideByKey (torch.fx.experimental.symbolic_shapes 中的类)</a>
</li>
      <li><a href="futures.html#torch.futures.Future.done">done() (torch.futures.Future 方法)</a>
</li>
      <li><a href="generated/torch.dot.html#torch.dot">dot() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.dot.html#torch.Tensor.dot">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.double">double() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.double">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.double">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.double.html#torch.Tensor.double">torch.Tensor 方法</a>
</li>
        <li><a href="storage.html#torch.TypedStorage.double">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.double">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.DoubleStorage">DoubleStorage (torch 中的类)</a>
</li>
      <li><a href="hub.html#torch.hub.download_url_to_file">torch.hub 模块中的 download_url_to_file()函数</a>
</li>
      <li><a href="generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine.draw">torch.quasirandom.SobolEngine 方法中的 draw()函数</a>
</li>
      <li><a href="generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine.draw_base2">torch.quasirandom.SobolEngine 方法中的 draw_base2()函数</a>
</li>
      <li><a href="generated/torch.mps.driver_allocated_memory.html#torch.mps.driver_allocated_memory">torch.mps 模块中的 driver_allocated_memory()函数</a>
</li>
      <li><a href="generated/torch.nn.Dropout.html#torch.nn.Dropout">torch.nn 中的 Dropout（类）</a>
</li>
      <li><a href="generated/torch.nn.functional.dropout.html#torch.nn.functional.dropout">torch.nn.functional 模块中的 dropout()函数</a>
</li>
      <li><a href="generated/torch.nn.Dropout1d.html#torch.nn.Dropout1d">torch.nn 中的 Dropout1d（类）</a>
</li>
      <li><a href="generated/torch.nn.functional.dropout1d.html#torch.nn.functional.dropout1d">torch.nn.functional 模块中的 dropout1d()函数</a>
</li>
      <li><a href="generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d">Dropout2d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.functional.dropout2d.html#torch.nn.functional.dropout2d">dropout2d()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.nn.Dropout3d.html#torch.nn.Dropout3d">Dropout3d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.functional.dropout3d.html#torch.nn.functional.dropout3d">dropout3d()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.dsplit.html#torch.dsplit">torch 模块中的 dsplit()</a>

      <ul>
        <li><a href="generated/torch.Tensor.dsplit.html#torch.Tensor.dsplit">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.dstack.html#torch.dstack">torch 模块中的 dstack()</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.DTensor">torch.distributed.tensor 中的 DTensor 类</a>
</li>
      <li><a href="tensor_attributes.html#torch.dtype">torch 中的 dtype 类</a>

      <ul>
        <li><a href="storage.html#torch.BFloat16Storage.dtype">(torch.BFloat16Storage 属性)</a>
</li>
        <li><a href="storage.html#torch.BoolStorage.dtype">(torch.BoolStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.ByteStorage.dtype">(torch.ByteStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.CharStorage.dtype">(torch.CharStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.ComplexDoubleStorage.dtype">(torch.ComplexDoubleStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.ComplexFloatStorage.dtype">(torch.ComplexFloatStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.DoubleStorage.dtype">(torch.DoubleStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.FloatStorage.dtype">(torch.FloatStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.HalfStorage.dtype">(torch.HalfStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.IntStorage.dtype">(torch.IntStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.LongStorage.dtype">(torch.LongStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.QInt32Storage.dtype">(torch.QInt32Storage 属性)</a>
</li>
        <li><a href="storage.html#torch.QInt8Storage.dtype">(torch.QInt8Storage 属性)</a>
</li>
        <li><a href="storage.html#torch.QUInt2x4Storage.dtype">(torch.QUInt2x4Storage 属性)</a>
</li>
        <li><a href="storage.html#torch.QUInt4x2Storage.dtype">(torch.QUInt4x2Storage 属性)</a>
</li>
        <li><a href="storage.html#torch.QUInt8Storage.dtype">(torch.QUInt8Storage 属性)</a>
</li>
        <li><a href="storage.html#torch.ShortStorage.dtype">(torch.ShortStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.TypedStorage.dtype">(torch.TypedStorage 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.dtype">dtype() (torch.onnx.JitScalarType 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig">torch.ao.quantization.backend_config 中的 DTypeConfig（类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.DTypeWithConstraints.html#torch.ao.quantization.backend_config.DTypeWithConstraints">torch.ao.quantization.backend_config 中的 DTypeWithConstraints（类）</a>
</li>
      <li><a href="generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level">torch.autograd.forward_ad 中的 dual_level（类）</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.Kernel.html#torch.autograd.profiler_util.Kernel.duration">torch.autograd.profiler_util.Kernel 属性中的 duration</a>
</li>
      <li><a href="export.html#torch.export.dynamic_shapes.ShapesCollection.dynamic_shapes">dynamic_shapes() (torch.export.dynamic_shapes.ShapesCollection 方法)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner">DynamicMetaLoadPlanner (torch.distributed.checkpoint.format_utils 中的类)</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler">DynamicRendezvousHandler (torch.distributed.elastic.rendezvous.dynamic_rendezvous 中的类)</a>
</li>
      <li><a href="onnx_dynamo.html#torch.onnx.dynamo_export">dynamo_export() (模块 torch.onnx 中)</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="E">E</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.linalg.eig.html#torch.linalg.eig">torch.linalg 模块中的 eig()</a>
</li>
      <li><a href="generated/torch.linalg.eigh.html#torch.linalg.eigh">torch.linalg 模块中的 eigh()</a>
</li>
      <li><a href="generated/torch.linalg.eigvals.html#torch.linalg.eigvals">torch.linalg 模块中的 eigvals()</a>
</li>
      <li><a href="generated/torch.linalg.eigvalsh.html#torch.linalg.eigvalsh">torch.linalg 模块中的 eigvalsh()</a>
</li>
      <li><a href="generated/torch.einsum.html#torch.einsum">torch 模块中的 einsum()函数</a>
</li>
      <li><a href="generated/torch.cuda.Event.html#torch.cuda.Event.elapsed_time">torch.cuda.Event 方法中的 elapsed_time()</a>

      <ul>
        <li><a href="generated/torch.Event.html#torch.Event.elapsed_time">torch.Event 方法</a>
</li>
        <li><a href="generated/torch.mps.event.Event.html#torch.mps.event.Event.elapsed_time">torch.mps.event.Event 方法</a>
</li>
        <li><a href="generated/torch.mtia.Event.html#torch.mtia.Event.elapsed_time">(torch.mtia.Event 方法)</a>
</li>
        <li><a href="generated/torch.xpu.Event.html#torch.xpu.Event.elapsed_time">(torch.xpu.Event 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.profiler_util.Interval.html#torch.autograd.profiler_util.Interval.elapsed_us">elapsed_us() (torch.autograd.profiler_util.Interval 方法)</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.ElasticAgent">ElasticAgent (torch.distributed.elastic.agent.server 中的类)</a>
</li>
      <li><a href="generated/torch.Tensor.element_size.html#torch.Tensor.element_size">element_size() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.element_size">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.element_size">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="fx.html#torch.fx.Graph.eliminate_dead_code">eliminate_dead_code() (torch.fx.Graph 方法)</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.ELU.html#torch.ao.nn.quantized.ELU">ELU (torch.ao.nn.quantized 中的类)</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.elu.html#torch.ao.nn.quantized.functional.elu">elu (torch.ao.nn.quantized.functional 中的类)</a>
</li>
      <li><a href="generated/torch.nn.ELU.html#torch.nn.ELU">ELU (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.functional.elu.html#torch.nn.functional.elu">torch.nn.functional 模块中的 elu()函数</a>
</li>
      <li><a href="generated/torch.nn.functional.elu_.html#torch.nn.functional.elu_">torch.nn.functional 模块中的 elu_()函数</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.Embedding.html#torch.ao.nn.quantized.Embedding">torch.ao.nn.quantized 模块中的 Embedding 类</a>

      <ul>
        <li><a href="generated/torch.nn.Embedding.html#torch.nn.Embedding">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.functional.embedding.html#torch.nn.functional.embedding">torch.nn.functional 模块中的 embedding()函数</a>
</li>
      <li><a href="generated/torch.nn.functional.embedding_bag.html#torch.nn.functional.embedding_bag">torch.nn.functional 模块中的 embedding_bag()</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.EmbeddingBag.html#torch.ao.nn.quantized.EmbeddingBag">torch.ao.nn.quantized 模块中的 EmbeddingBag 类</a>

      <ul>
        <li><a href="generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="autograd.html#torch.autograd.profiler.emit_itt">torch.autograd.profiler 模块中的 emit_itt 类</a>
</li>
      <li><a href="autograd.html#torch.autograd.profiler.emit_nvtx">torch.autograd.profiler 模块中的 emit_nvtx 类</a>
</li>
      <li><a href="generated/torch.empty.html#torch.empty">torch 模块中的 empty()</a>

      <ul>
        <li><a href="distributed.tensor.html#torch.distributed.tensor.empty">torch.distributed.tensor 模块中的()</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache">torch.cuda 模块中的 empty_cache()</a>

      <ul>
        <li><a href="generated/torch.mps.empty_cache.html#torch.mps.empty_cache">（在模块 torch.mps 中）</a>
</li>
        <li><a href="generated/torch.mtia.empty_cache.html#torch.mtia.empty_cache">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="generated/torch.xpu.empty_cache.html#torch.xpu.empty_cache">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.empty_like.html#torch.empty_like">torch 模块中的 empty_like()</a>
</li>
      <li><a href="generated/torch.empty_strided.html#torch.empty_strided">torch 模块中的 empty_strided()</a>
</li>
      <li><a href="package.html#torch.package.EmptyMatchError">torch.package 中的 EmptyMatchError（类）</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.enable">torch.cuda.tunable 模块中的 enable()函数</a>

      <ul>
        <li><a href="generated/torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants.enable">torch.sparse.check_sparse_tensor_invariants 静态方法</a>
</li>
      </ul></li>
      <li><a href="cuda._sanitizer.html#torch.cuda._sanitizer.enable_cuda_sanitizer">torch.cuda._sanitizer 模块中的 enable_cuda_sanitizer()函数</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.enable_cudnn_sdp">torch.backends.cuda 模块中的 enable_cudnn_sdp()函数</a>
</li>
      <li><a href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.enable_debug_mode">enable_debug_mode() (torch.cuda.CUDAGraph 方法)</a>
</li>
      <li><a href="onnx_dynamo.html#torch.onnx.enable_fake_mode">enable_fake_mode() (在模块 torch.onnx 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.enable_fake_quant.html#torch.ao.quantization.fake_quantize.enable_fake_quant">enable_fake_quant (torch.ao.quantization.fake_quantize 类)</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.enable_flash_sdp">enable_flash_sdp() (在模块 torch.backends.cuda 中)</a>
</li>
      <li><a href="generated/torch.enable_grad.html#torch.enable_grad">torch 中的 enable_grad（类）</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.enable_math_sdp">torch.backends.cuda 模块中的 enable_math_sdp()</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.enable_mem_efficient_sdp">torch.backends.cuda 模块中的 enable_mem_efficient_sdp()</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.enable_observer.html#torch.ao.quantization.fake_quantize.enable_observer">torch.ao.quantization.fake_quantize 中的 enable_observer（类）</a>
</li>
      <li><a href="generated/torch.jit.enable_onednn_fusion.html#torch.jit.enable_onednn_fusion">torch.jit 模块中的 enable_onednn_fusion()函数</a>
</li>
      <li><a href="backends.html#torch.backends.cudnn.enabled">torch.backends.cudnn 模块中的启用状态</a>

      <ul>
        <li><a href="backends.html#torch.backends.opt_einsum.enabled">torch.backends.opt_einsum 模块中的内容</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.profiler.EnforceUnique.html#torch.autograd.profiler.EnforceUnique">torch.autograd.profiler 模块中的 EnforceUnique 类</a>
</li>
      <li><a href="generated/torch.autograd.forward_ad.enter_dual_level.html#torch.autograd.forward_ad.enter_dual_level">enter_dual_level() (在模块 torch.autograd.forward_ad 中)</a>
</li>
      <li><a href="special.html#torch.special.entr">entr() (在模块 torch.special 中)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.entropy">entropy() (torch.distributions.bernoulli.Bernoulli 方法)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.entropy">(torch.distributions.beta.Beta 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.entropy">(torch.distributions.binomial.Binomial 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.entropy">(torch.distributions.categorical.Categorical 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.entropy">(torch.distributions.cauchy.Cauchy 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.entropy">(torch.distributions.dirichlet.Dirichlet 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.entropy">(torch.distributions.distribution.Distribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exp_family.ExponentialFamily.entropy">(torch.distributions.exp_family.ExponentialFamily 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.entropy">(torch.distributions.exponential.Exponential 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.entropy">(torch.distributions.gamma.Gamma 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.entropy">(torch.distributions.geometric.Geometric 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.entropy">(torch.distributions.gumbel.Gumbel 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_cauchy.HalfCauchy.entropy">(torch.distributions.half_cauchy.HalfCauchy 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_normal.HalfNormal.entropy">(torch.distributions.half_normal.HalfNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.entropy">(torch.distributions.independent.Independent 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.inverse_gamma.InverseGamma.entropy">(torch.distributions.inverse_gamma.InverseGamma 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.entropy">(torch.distributions.kumaraswamy.Kumaraswamy 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.entropy">(torch.distributions.laplace.Laplace 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.entropy">(torch.distributions.log_normal.LogNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy">(torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.entropy">(torch.distributions.multinomial.Multinomial 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.entropy">(torch.distributions.multivariate_normal.MultivariateNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.entropy">(torch.distributions.normal.Normal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.entropy">(torch.distributions.one_hot_categorical.OneHotCategorical 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.pareto.Pareto.entropy">(torch.distributions.pareto.Pareto 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.entropy">(torch.distributions.studentT.StudentT 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.entropy">(torch.distributions.uniform.Uniform 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.weibull.Weibull.entropy">(torch.distributions.weibull.Weibull 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.wishart.Wishart.entropy">(torch.distributions.wishart.Wishart 方法)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.enumerate_support">enumerate_support() (torch.distributions.bernoulli.Bernoulli 方法)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.enumerate_support">(torch.distributions.binomial.Binomial 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.enumerate_support">(torch.distributions.categorical.Categorical 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.enumerate_support">(torch.distributions.distribution.Distribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.enumerate_support">(torch.distributions.independent.Independent 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support">(torch.distributions.one_hot_categorical.OneHotCategorical 方法)</a>
</li>
      </ul></li>
      <li>环境变量<ul>
        <li><a href="jit.html#envvar-PYTORCH_JIT">PYTORCH_JIT</a>
</li>
        <li><a href="torch.compiler.config.html#index-0">TORCH_COMPILE_JOB_ID</a>
</li>
      </ul></li>
      <li><a href="generated/torch.eq.html#torch.eq">eq() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.eq.html#torch.Tensor.eq">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.eq_.html#torch.Tensor.eq_">eq_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.equal.html#torch.equal">equal() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.equal.html#torch.Tensor.equal">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.EqualityConstraint.html#torch.fx.experimental.symbolic_shapes.EqualityConstraint">EqualityConstraint (torch.fx.experimental.symbolic_shapes 中的类)</a>
</li>
      <li><a href="fx.html#torch.fx.Graph.erase_node">erase_node() (torch.fx.Graph 方法)</a>
</li>
      <li><a href="generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker.erase_step_count">erase_step_count() (torch.autograd.profiler.KinetoStepTracker 类方法)</a>
</li>
      <li><a href="generated/torch.erf.html#torch.erf">erf() (在 torch 模块中)</a>

      <ul>
        <li><a href="special.html#torch.special.erf">（在 torch.special 模块中）</a>
</li>
        <li><a href="generated/torch.Tensor.erf.html#torch.Tensor.erf">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.erf_.html#torch.Tensor.erf_">erf_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.erfc.html#torch.erfc">erfc() (在 torch 模块中)</a>

      <ul>
        <li><a href="special.html#torch.special.erfc">（在 torch.special 模块中）</a>
</li>
        <li><a href="generated/torch.Tensor.erfc.html#torch.Tensor.erfc">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.erfc_.html#torch.Tensor.erfc_">erfc_() (torch.Tensor 方法)</a>
</li>
      <li><a href="special.html#torch.special.erfcx">erfcx() (在 torch.special 模块中)</a>
</li>
      <li><a href="generated/torch.erfinv.html#torch.erfinv">erfinv() (在 torch 模块中)</a>

      <ul>
        <li><a href="special.html#torch.special.erfinv">（在 torch.special 模块中）</a>
</li>
        <li><a href="generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.erfinv_.html#torch.Tensor.erfinv_">erfinv_() (torch.Tensor 方法)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="elastic/errors.html#torch.distributed.elastic.multiprocessing.errors.ErrorHandler">torch.distributed.elastic.multiprocessing.errors 中的 ErrorHandler（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend">torch.distributed.elastic.rendezvous.etcd_rendezvous_backend 中的 EtcdRendezvousBackend（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler">torch.distributed.elastic.rendezvous.etcd_rendezvous 中的 EtcdRendezvousHandler（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_server.EtcdServer">torch.distributed.elastic.rendezvous.etcd_server 中的 EtcdServer（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore">torch.distributed.elastic.rendezvous.etcd_store 中的 EtcdStore 类</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.eval">torch.jit.ScriptModule 方法中的 eval()</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.eval">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.eval">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.evaluate_guards_expression">evaluate_guards_expression() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.evaluate_guards_for_args">evaluate_guards_for_args() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.evaluate_sym_node">evaluate_sym_node() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.evaluate_symexpr">evaluate_symexpr() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.Event.html#torch.Event">torch 中的事件（类）</a>

      <ul>
        <li><a href="generated/torch.cuda.Event.html#torch.cuda.Event">(torch.cuda 中的类)</a>
</li>
        <li><a href="elastic/events.html#torch.distributed.elastic.events.api.Event">torch.distributed.elastic.events.api 中的类</a>
</li>
        <li><a href="monitor.html#torch.monitor.Event">torch.monitor 中的类</a>
</li>
        <li><a href="generated/torch.mps.event.Event.html#torch.mps.event.Event">(torch.mps.event 类)</a>
</li>
        <li><a href="generated/torch.mtia.Event.html#torch.mtia.Event">(torch.mtia 中的类)</a>
</li>
        <li><a href="generated/torch.xpu.Event.html#torch.xpu.Event">（torch.xpu 中的类）</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.distribution.Distribution.event_shape">event_shape (torch.distributions.distribution.Distribution 属性)</a>
</li>
      <li><a href="monitor.html#torch.monitor.EventHandlerHandle">torch.monitor 中的 EventHandlerHandle（类）</a>
</li>
      <li><a href="elastic/events.html#torch.distributed.elastic.events.api.EventMetadataValue">torch.distributed.elastic.events.api 模块中的 EventMetadataValue</a>
</li>
      <li><a href="profiler.html#torch.profiler._KinetoProfile.events">torch.profiler._KinetoProfile 方法中的 events()</a>
</li>
      <li><a href="elastic/events.html#torch.distributed.elastic.events.api.EventSource">torch.distributed.elastic.events.api 模块中的 EventSource（类）</a>
</li>
      <li><a href="distributed.html#torch.distributed.Work.exception">exception() (torch.distributed.Work 方法)</a>
</li>
      <li><a href="generated/torch.autograd.forward_ad.exit_dual_level.html#torch.autograd.forward_ad.exit_dual_level">exit_dual_level() (在模块 torch.autograd.forward_ad 中)</a>
</li>
      <li><a href="generated/torch.exp.html#torch.exp">exp() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.exp.html#torch.Tensor.exp">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.exp2.html#torch.exp2">torch 模块中的 exp2()函数</a>

      <ul>
        <li><a href="special.html#torch.special.exp2">（在 torch.special 模块中）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.exp_.html#torch.Tensor.exp_">torch.Tensor 方法中的 exp_()函数</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.expand">torch.distributions.bernoulli.Bernoulli 方法中的 expand()函数</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.expand">(torch.distributions.beta.Beta 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.expand">(torch.distributions.binomial.Binomial 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.expand">(torch.distributions.categorical.Categorical 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.expand">(torch.distributions.cauchy.Cauchy 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.chi2.Chi2.expand">(torch.distributions.chi2.Chi2 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.expand">(torch.distributions.dirichlet.Dirichlet 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.expand">(torch.distributions.distribution.Distribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.expand">(torch.distributions.exponential.Exponential 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.expand">(torch.distributions.fishersnedecor.FisherSnedecor 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.expand">(torch.distributions.gamma.Gamma 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.expand">(torch.distributions.geometric.Geometric 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.expand">(torch.distributions.gumbel.Gumbel 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_cauchy.HalfCauchy.expand">(torch.distributions.half_cauchy.HalfCauchy 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_normal.HalfNormal.expand">(torch.distributions.half_normal.HalfNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.expand">(torch.distributions.independent.Independent 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.inverse_gamma.InverseGamma.expand">(torch.distributions.inverse_gamma.InverseGamma 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.expand">(torch.distributions.kumaraswamy.Kumaraswamy 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.expand">(torch.distributions.laplace.Laplace 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lkj_cholesky.LKJCholesky.expand">(torch.distributions.lkj_cholesky.LKJCholesky 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.expand">(torch.distributions.log_normal.LogNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand">(torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.expand">(torch.distributions.mixture_same_family.MixtureSameFamily 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.expand">(torch.distributions.multinomial.Multinomial 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.expand">(torch.distributions.multivariate_normal.MultivariateNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.negative_binomial.NegativeBinomial.expand">(torch.distributions.negative_binomial.NegativeBinomial 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.expand">(torch.distributions.normal.Normal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.expand">(torch.distributions.one_hot_categorical.OneHotCategorical 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.pareto.Pareto.expand">(torch.distributions.pareto.Pareto 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.expand">(torch.distributions.poisson.Poisson 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand">(torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand">(torch.distributions.relaxed_bernoulli.RelaxedBernoulli 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand">(torch.distributions.relaxed_categorical.RelaxedOneHotCategorical 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.expand">(torch.distributions.studentT.StudentT 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.expand">(torch.distributions.transformed_distribution.TransformedDistribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.expand">(torch.distributions.uniform.Uniform 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.von_mises.VonMises.expand">(torch.distributions.von_mises.VonMises 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.weibull.Weibull.expand">(torch.distributions.weibull.Weibull 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.wishart.Wishart.expand">(torch.distributions.wishart.Wishart 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.expand.html#torch.Tensor.expand">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as">expand_as() (torch.Tensor 方法)</a>
</li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.expires">expires() (在 torch.distributed.elastic.timer 模块中)</a>
</li>
      <li><a href="special.html#torch.special.expit">expit() (在 torch.special 模块中)</a>
</li>
      <li><a href="generated/torch.expm1.html#torch.expm1">expm1() (在 torch 模块中)</a>

      <ul>
        <li><a href="special.html#torch.special.expm1">（在 torch.special 模块中）</a>
</li>
        <li><a href="generated/torch.Tensor.expm1.html#torch.Tensor.expm1">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.expm1_.html#torch.Tensor.expm1_">expm1_() (torch.Tensor 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.exponential.Exponential">指数（torch.distributions.exponential 中的类）</a>
</li>
      <li><a href="generated/torch.signal.windows.exponential.html#torch.signal.windows.exponential">torch.signal.windows 模块中的 exponential()函数</a>
</li>
      <li><a href="generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_">torch.Tensor 方法中的 exponential_()</a>
</li>
      <li><a href="distributions.html#torch.distributions.exp_family.ExponentialFamily">torch.distributions.exp_family 中的 ExponentialFamily 类</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR">torch.optim.lr_scheduler 中的 ExponentialLR 类</a>
</li>
      <li><a href="export.html#torch.export.export">torch.export()（在 torch.export 模块中）</a>

      <ul>
        <li><a href="jit.html#torch.jit.export">(在 torch.jit 模块中)</a>
</li>
        <li><a href="onnx_torchscript.html#torch.onnx.export">（在 torch.onnx 模块中）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.profiler.profile.export_chrome_trace.html#torch.autograd.profiler.profile.export_chrome_trace">export_chrome_trace()（torch.autograd.profiler.profile 方法）</a>

      <ul>
        <li><a href="profiler.html#torch.profiler._KinetoProfile.export_chrome_trace">(torch.profiler._KinetoProfile 方法)</a>
</li>
      </ul></li>
      <li><a href="profiler.html#torch.profiler._KinetoProfile.export_memory_timeline">export_memory_timeline() (torch.profiler._KinetoProfile 方法)</a>
</li>
      <li><a href="profiler.html#torch.profiler._KinetoProfile.export_stacks">export_stacks() (torch.profiler._KinetoProfile 方法)</a>
</li>
      <li><a href="export.html#torch.export.ExportBackwardSignature">ExportBackwardSignature (torch.export 中的类)</a>
</li>
      <li><a href="export.html#torch.export.ExportedProgram">torch.export 中的 ExportedProgram（类）</a>
</li>
      <li><a href="export.html#torch.export.ExportGraphSignature">torch.export 中的 ExportGraphSignature（类）</a>

      <ul>
        <li><a href="export.html#torch.export.graph_signature.ExportGraphSignature">torch.export.graph_signature 中的（类）</a>
</li>
      </ul></li>
      <li><a href="onnx_dynamo.html#torch.onnx.ExportOptions">torch.onnx 中的 ExportOptions（类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.ExpTransform">torch.distributions.transforms 中的 ExpTransform（类）</a>
</li>
      <li><a href="generated/torch.nn.ModuleList.html#torch.nn.ModuleList.extend">torch.nn.ModuleList 方法中的 extend()</a>

      <ul>
        <li><a href="generated/torch.nn.ParameterList.html#torch.nn.ParameterList.extend">torch.nn.ParameterList 方法</a>
</li>
      </ul></li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.extend_logger_results_with_comparison">torch.ao.ns._numeric_suite_fx 模块中的 extend_logger_results_with_comparison()</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Compare.extend_results">extend_results() (torch.utils.benchmark.Compare 方法)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.extern">extern() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream">ExternalStream (torch.cuda 中的类)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.externed_modules">externed_modules() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.html#torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.extra_repr">extra_repr() (torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize 方法)</a>

      <ul>
        <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.extra_repr">(torch.jit.ScriptModule 方法)</a>
</li>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.extra_repr">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.nn.modules.normalization.RMSNorm.html#torch.nn.modules.normalization.RMSNorm.extra_repr">(torch.nn.modules.normalization.RMSNorm 方法)</a>
</li>
        <li><a href="generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm.extra_repr">(torch.nn.RMSNorm 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.extra_repr">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.extract_logger_info">extract_logger_info() (在 torch.ao.ns._numeric_suite_fx 模块中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.extract_results_from_loggers.html#torch.ao.quantization.extract_results_from_loggers">extract_results_from_loggers (torch.ao.quantization 中的类)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.extract_results_n_shadows_model">extract_results_n_shadows_model() (在模块 torch.ao.ns._numeric_suite_fx 中)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.extract_shadow_logger_info">extract_shadow_logger_info() (在模块 torch.ao.ns._numeric_suite_fx 中)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.extract_weights">extract_weights() (在模块 torch.ao.ns._numeric_suite_fx 中)</a>
</li>
      <li><a href="generated/torch.eye.html#torch.eye">eye() (在模块 torch 中)</a>
</li>
      <li><a href="nn.init.html#torch.nn.init.eye_">eye_() (在模块 torch.nn.init 中)</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="F">F</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine">fake_quantize_per_channel_affine() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine">fake_quantize_per_tensor_affine() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.FakeQuantize.html#torch.ao.quantization.fake_quantize.FakeQuantize">FakeQuantize (类，位于 torch.ao.quantization.fake_quantize)</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.FakeQuantizeBase.html#torch.ao.quantization.fake_quantize.FakeQuantizeBase">torch.ao.quantization.fake_quantize 中的 FakeQuantizeBase (类)</a>
</li>
      <li><a href="library.html#torch.library.Library.fallback">torch.library.Library 方法中的 fallback()</a>
</li>
      <li><a href="library.html#torch.library.fallthrough_kernel">torch.library 模块中的 fallthrough_kernel()</a>
</li>
      <li><a href="generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine.fast_forward">torch.quasirandom.SobolEngine 方法中的 fast_forward()</a>
</li>
      <li><a href="generated/torch.nn.functional.feature_alpha_dropout.html#torch.nn.functional.feature_alpha_dropout">feature_alpha_dropout()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.nn.FeatureAlphaDropout.html#torch.nn.FeatureAlphaDropout">FeatureAlphaDropout（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html#torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.fetch_args_kwargs_from_env">fetch_args_kwargs_from_env()（torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts 方法）</a>

      <ul>
        <li><a href="fx.html#torch.fx.Interpreter.fetch_args_kwargs_from_env">(torch.fx.Interpreter 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html#torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.fetch_attr">fetch_attr() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts 方法)</a>

      <ul>
        <li><a href="fx.html#torch.fx.Interpreter.fetch_attr">(torch.fx.Interpreter 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fft.fft.html#torch.fft.fft">fft() (在模块 torch.fft 中)</a>
</li>
      <li><a href="generated/torch.fft.fft2.html#torch.fft.fft2">fft2() (在模块 torch.fft 中)</a>
</li>
      <li><a href="generated/torch.fft.fftfreq.html#torch.fft.fftfreq">fftfreq()（在 torch.fft 模块中）</a>
</li>
      <li><a href="generated/torch.fft.fftn.html#torch.fft.fftn">fftn()（在 torch.fft 模块中）</a>
</li>
      <li><a href="generated/torch.fft.fftshift.html#torch.fft.fftshift">fftshift()（在 torch.fft 模块中）</a>
</li>
      <li><a href="package.html#torch.package.PackageImporter.file_structure">file_structure()（torch.package.PackageImporter 方法）</a>
</li>
      <li><a href="storage.html#torch.TypedStorage.filename">torch.TypedStorage 属性</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.filename">torch.UntypedStorage 属性</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.FileStore">torch.distributed 中的 FileStore 类</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.FileSystemReader">torch.distributed.checkpoint 中的 FileSystemReader 类</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.FileSystemWriter">torch.distributed.checkpoint 中的 FileSystemWriter（类）</a>
</li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.FileTimerClient">torch.distributed.elastic.timer 中的 FileTimerClient（类）</a>
</li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.FileTimerServer">torch.distributed.elastic.timer 中的 FileTimerServer（类）</a>
</li>
      <li><a href="generated/torch.Tensor.fill_.html#torch.Tensor.fill_">torch.Tensor 方法 fill_()</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.fill_">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.fill_">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.fill_diagonal_.html#torch.Tensor.fill_diagonal_">fill_diagonal_() (torch.Tensor 方法)</a>
</li>
      <li><a href="deterministic.html#torch.utils.deterministic.fill_uninitialized_memory">fill_uninitialized_memory (在模块 torch.utils.deterministic 中)</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.FunctionCounts.filter">filter() (torch.utils.benchmark.FunctionCounts 方法)</a>
</li>
      <li><a href="onnx_verification.html#torch.onnx.verification.find_mismatch">find_mismatch() (在 torch.onnx.verification 模块中)</a>
</li>
      <li><a href="fx.html#torch.fx.Graph.find_nodes">find_nodes() (torch.fx.Graph 方法)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.finish">finish() (torch.distributed.checkpoint.StorageWriter 方法)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.finish_plan">finish_plan() (torch.distributed.checkpoint.LoadPlanner 方法)</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.SavePlanner.finish_plan">(torch.distributed.checkpoint.SavePlanner 方法)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor">FisherSnedecor (torch.distributions.fishersnedecor 中的类)</a>
</li>
      <li><a href="generated/torch.fix.html#torch.fix">fix() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.fix.html#torch.Tensor.fix">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.fix_.html#torch.Tensor.fix_">fix_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.html#torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize">FixedQParamsFakeQuantize (torch.ao.quantization.fake_quantize 中的类)</a>
</li>
      <li><a href="backends.html#torch.backends.nnpack.flags">flags() (在 torch.backends.nnpack 模块中)</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.flash_sdp_enabled">torch.backends.cuda 模块中的 flash_sdp_enabled()</a>
</li>
      <li><a href="export.html#torch.export.unflatten.FlatArgsAdapter">torch.export.unflatten 中的 FlatArgsAdapter 类</a>
</li>
      <li><a href="generated/torch.nn.Flatten.html#torch.nn.Flatten">torch.nn 中的 Flatten 类</a>
</li>
      <li><a href="generated/torch.flatten.html#torch.flatten">torch 模块中的 flatten()</a>

      <ul>
        <li><a href="generated/torch.Tensor.flatten.html#torch.Tensor.flatten">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.RNNBase.html#torch.nn.RNNBase.flatten_parameters">flatten_parameters() (torch.nn.RNNBase 方法)</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.flatten_sharded_optim_state_dict">flatten_sharded_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel 静态方法)</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.flex_attention">flex_attention() (在模块 torch.nn.attention.flex_attention 中)</a>
</li>
      <li><a href="generated/torch.flip.html#torch.flip">torch 模块中的 flip()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.flip.html#torch.Tensor.flip">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fliplr.html#torch.fliplr">torch 模块中的 fliplr()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.fliplr.html#torch.Tensor.fliplr">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.flipud.html#torch.flipud">torch 模块中的 flipud()</a>

      <ul>
        <li><a href="generated/torch.Tensor.flipud.html#torch.Tensor.flipud">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.float">torch.jit.ScriptModule 方法中的 float()</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.float">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.float">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.float.html#torch.Tensor.float">torch.Tensor 方法</a>
</li>
        <li><a href="storage.html#torch.TypedStorage.float">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.float">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.qconfig.float16_dynamic_qconfig.html#torch.ao.quantization.qconfig.float16_dynamic_qconfig">float16_dynamic_qconfig（在 torch.ao.quantization.qconfig 模块中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig.float16_static_qconfig.html#torch.ao.quantization.qconfig.float16_static_qconfig">float16_static_qconfig（在 torch.ao.quantization.qconfig 模块中）</a>
</li>
      <li><a href="storage.html#torch.TypedStorage.float8_e4m3fn">float8_e4m3fn()（torch.TypedStorage 方法）</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.float8_e4m3fn">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.TypedStorage.float8_e4m3fnuz">float8_e4m3fnuz() (torch.TypedStorage 方法)</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.float8_e4m3fnuz">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.TypedStorage.float8_e5m2">float8_e5m2() (torch.TypedStorage 方法)</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.float8_e5m2">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.TypedStorage.float8_e5m2fnuz">float8_e5m2fnuz() (torch.TypedStorage 方法)</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.float8_e5m2fnuz">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.float_power.html#torch.float_power">float_power() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.float_power.html#torch.Tensor.float_power">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.float_power_.html#torch.Tensor.float_power_">float_power_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig.html#torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig">float_qparams_weight_only_qconfig (在模块 torch.ao.quantization.qconfig 中)</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.FloatFunctional.html#torch.ao.nn.quantized.FloatFunctional">FloatFunctional (torch.ao.nn.quantized 中的类)</a>
</li>
      <li><a href="storage.html#torch.FloatStorage">FloatStorage (torch 中的类)</a>
</li>
      <li><a href="generated/torch.floor.html#torch.floor">torch 模块中的 floor()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.floor.html#torch.Tensor.floor">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.floor_.html#torch.Tensor.floor_">torch.Tensor 方法中的 floor_()</a>
</li>
      <li><a href="generated/torch.floor_divide.html#torch.floor_divide">torch 模块中的 floor_divide()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_">floor_divide_() (torch.Tensor 方法)</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.flush">flush() (torch.utils.tensorboard.writer.SummaryWriter 方法)</a>
</li>
      <li><a href="generated/torch.fmax.html#torch.fmax">fmax() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.fmax.html#torch.Tensor.fmax">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fmin.html#torch.fmin">fmin()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.fmin.html#torch.Tensor.fmin">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fmod.html#torch.fmod">fmod()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.fmod.html#torch.Tensor.fmod">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.fmod_.html#torch.Tensor.fmod_">fmod_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.nn.Fold.html#torch.nn.Fold">Fold (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.functional.fold.html#torch.nn.functional.fold">fold() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints.forced_specializations">forced_specializations() (torch.fx.experimental.symbolic_shapes.DimConstraints 方法)</a>
</li>
      <li><a href="generated/torch.jit.fork.html#torch.jit.fork">fork() (在 torch.jit 模块中)</a>
</li>
      <li><a href="random.html#torch.random.fork_rng">fork_rng() (在 torch.random 模块中)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.format_guards">format_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.Node.format_node">format_node() (torch.fx.Node 方法)</a>
</li>
      <li><a href="generated/torch.ao.nn.quantizable.MultiheadAttention.html#torch.ao.nn.quantizable.MultiheadAttention.forward">forward() (torch.ao.nn.quantizable.MultiheadAttention 方法)</a>

      <ul>
        <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Logger.forward">(torch.ao.ns._numeric_suite.Logger 方法)</a>
</li>
        <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.OutputLogger.forward">(torch.ao.ns._numeric_suite.OutputLogger 方法)</a>
</li>
        <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.forward">(torch.ao.ns._numeric_suite.Shadow 方法)</a>
</li>
        <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.ShadowLogger.forward">(torch.ao.ns._numeric_suite.ShadowLogger 方法)</a>
</li>
        <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.OutputComparisonLogger.forward">(torch.ao.ns._numeric_suite_fx.OutputComparisonLogger 方法)</a>
</li>
        <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.OutputLogger.forward">(torch.ao.ns._numeric_suite_fx.OutputLogger 方法)</a>
</li>
        <li><a href="generated/torch.ao.quantization.observer.AffineQuantizedObserverBase.html#torch.ao.quantization.observer.AffineQuantizedObserverBase.forward">(torch.ao.quantization.observer.仿射量化观察者基方法)</a>
</li>
        <li><a href="generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver.forward">(torch.ao.quantization.observer.最小-最大观察者方法)</a>
</li>
        <li><a href="generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward">(torch.autograd.函数静态方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.forward">(torch.autograd.function.内联函数静态方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.forward">(torch.autograd.function.NestedIOFunction 方法)</a>
</li>
        <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.forward">(torch.distributed.fsdp.FullyShardedDataParallel 方法)</a>
</li>
        <li><a href="generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag.forward">(torch.nn.EmbeddingBag 方法)</a>
</li>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.forward">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.nn.modules.normalization.RMSNorm.html#torch.nn.modules.normalization.RMSNorm.forward">(torch.nn.modules.normalization.RMSNorm 方法)</a>
</li>
        <li><a href="generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.forward">(torch.nn.MultiheadAttention 方法)</a>
</li>
        <li><a href="generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm.forward">(torch.nn.RMSNorm 方法)</a>
</li>
        <li><a href="generated/torch.nn.Transformer.html#torch.nn.Transformer.forward">(torch.nn.Transformer 方法)</a>
</li>
        <li><a href="generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder.forward">(torch.nn.TransformerDecoder 方法)</a>
</li>
        <li><a href="generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer.forward">(torch.nn.TransformerDecoderLayer 方法)</a>
</li>
        <li><a href="generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder.forward">(torch.nn.TransformerEncoder 方法)</a>
</li>
        <li><a href="generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer.forward">(torch.nn.TransformerEncoderLayer 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.forward">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.forward_extended">forward_extended() (torch.autograd.function.NestedIOFunction 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.Transform.forward_shape">forward_shape() (torch.distributions.transforms.Transform 方法)</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.fp16_bf16_reduction_math_sdp_allowed">fp16_bf16_reduction_math_sdp_allowed() (在 torch.backends.cuda 模块中)</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook">fp16_compress_hook() (在模块 torch.distributed.algorithms.ddp_comm_hooks.default_hooks 中)</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper">fp16_compress_wrapper() (在模块 torch.distributed.algorithms.ddp_comm_hooks.default_hooks 中)</a>
</li>
      <li><a href="generated/torch.frac.html#torch.frac">frac() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.frac.html#torch.Tensor.frac">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.frac_.html#torch.Tensor.frac_">torch.Tensor 方法 frac_()</a>
</li>
      <li><a href="generated/torch.nn.functional.fractional_max_pool2d.html#torch.nn.functional.fractional_max_pool2d">torch.nn.functional 模块中的 fractional_max_pool2d()</a>
</li>
      <li><a href="generated/torch.nn.functional.fractional_max_pool3d.html#torch.nn.functional.fractional_max_pool3d">torch.nn.functional 模块中的 fractional_max_pool3d()</a>
</li>
      <li><a href="generated/torch.nn.FractionalMaxPool2d.html#torch.nn.FractionalMaxPool2d">torch.nn 中的 FractionalMaxPool2d 类</a>
</li>
      <li><a href="generated/torch.nn.FractionalMaxPool3d.html#torch.nn.FractionalMaxPool3d">FractionalMaxPool3d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.jit.freeze.html#torch.jit.freeze">freeze()（在 torch.jit 模块中）</a>

      <ul>
        <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.freeze">（torch.fx.experimental.symbolic_shapes.ShapeEnv 方法）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats.html#torch.ao.nn.intrinsic.qat.freeze_bn_stats">freeze_bn_stats（torch.ao.nn.intrinsic.qat 中的类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.freeze_runtime_asserts">freeze_runtime_asserts() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.frexp.html#torch.frexp">frexp() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.frexp.html#torch.Tensor.frexp">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.from_backend">from_backend() (torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler 类方法)</a>
</li>
      <li><a href="storage.html#torch.TypedStorage.from_buffer">from_buffer() (torch.TypedStorage 类方法)</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.from_buffer">(torch.UntypedStorage 静态方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.attention.bias.CausalVariant.html#torch.nn.attention.bias.CausalVariant.from_bytes">from_bytes() (torch.nn.attention.bias.CausalVariant 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig.from_dict">from_dict() (torch.ao.quantization.backend_config.BackendConfig 类方法)</a>

      <ul>
        <li><a href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.from_dict">(torch.ao.quantization.backend_config.BackendPatternConfig 类方法)</a>
</li>
        <li><a href="generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig.from_dict">(torch.ao.quantization.backend_config.DTypeConfig 类方法)</a>
</li>
        <li><a href="generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.from_dict">(torch.ao.quantization.fx.custom_config.ConvertCustomConfig 类方法)</a>
</li>
        <li><a href="generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html#torch.ao.quantization.fx.custom_config.FuseCustomConfig.from_dict">(torch.ao.quantization.fx.custom_config.FuseCustomConfig 类方法)</a>
</li>
        <li><a href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.from_dict">(torch.ao.quantization.fx.custom_config.PrepareCustomConfig 类方法)</a>
</li>
        <li><a href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.from_dict">(torch.ao.quantization.qconfig_mapping.QConfigMapping 类方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.from_dlpack.html#torch.from_dlpack">from_dlpack() (在模块 torch 中)</a>

      <ul>
        <li><a href="dlpack.html#torch.utils.dlpack.from_dlpack">(在模块 torch.utils.dlpack 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.from_dtype">from_dtype()（torch.onnx.JitScalarType 类方法）</a>
</li>
      <li><a href="generated/torch.from_file.html#torch.from_file">from_file()（在 torch 模块中）</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.from_file">（torch.TypedStorage 类方法）</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.from_file">(torch.UntypedStorage 静态方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.qat.Linear.html#torch.ao.nn.qat.Linear.from_float">torch.ao.nn.qat.Linear 类的 from_float() 方法</a>

      <ul>
        <li><a href="generated/torch.ao.nn.quantized.Conv1d.html#torch.ao.nn.quantized.Conv1d.from_float">torch.ao.nn.quantized.Conv1d 类的方法</a>
</li>
        <li><a href="generated/torch.ao.nn.quantized.Conv2d.html#torch.ao.nn.quantized.Conv2d.from_float">torch.ao.nn.quantized.Conv2d 类的方法</a>
</li>
        <li><a href="generated/torch.ao.nn.quantized.Conv3d.html#torch.ao.nn.quantized.Conv3d.from_float">torch.ao.nn.quantized.Conv3d 类的方法</a>
</li>
        <li><a href="generated/torch.ao.nn.quantized.dynamic.Linear.html#torch.ao.nn.quantized.dynamic.Linear.from_float">(torch.ao.nn.quantized.dynamic.Linear 类方法)</a>
</li>
        <li><a href="generated/torch.ao.nn.quantized.Embedding.html#torch.ao.nn.quantized.Embedding.from_float">(torch.ao.nn.quantized.Embedding 类方法)</a>
</li>
        <li><a href="generated/torch.ao.nn.quantized.EmbeddingBag.html#torch.ao.nn.quantized.EmbeddingBag.from_float">(torch.ao.nn.quantized.EmbeddingBag 类方法)</a>
</li>
        <li><a href="generated/torch.ao.nn.quantized.Linear.html#torch.ao.nn.quantized.Linear.from_float">(torch.ao.nn.quantized.Linear 类方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.device_mesh.DeviceMesh.from_group">from_group() (torch.distributed.device_mesh.DeviceMesh 静态方法)</a>
</li>
      <li><a href="generated/torch.cuda.Event.html#torch.cuda.Event.from_ipc_handle">from_ipc_handle() (torch.cuda.Event 类方法)</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.from_kv_blocks">from_kv_blocks() (torch.nn.attention.flex_attention.BlockMask 类方法)</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.DTensor.from_local">from_local() (torch.distributed.tensor.DTensor 静态方法)</a>
</li>
      <li><a href="generated/torch.from_numpy.html#torch.from_numpy">torch 模块中的 from_numpy()函数</a>
</li>
      <li><a href="generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.from_onnx_type">torch.onnx.JitScalarType 类的方法 from_onnx_type()</a>
</li>
      <li><a href="generated/torch.nn.Embedding.html#torch.nn.Embedding.from_pretrained">torch.nn.Embedding 类的方法 from_pretrained()</a>

      <ul>
        <li><a href="generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag.from_pretrained">torch.nn.EmbeddingBag 类的方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.dynamic.Linear.html#torch.ao.nn.quantized.dynamic.Linear.from_reference">from_reference() (torch.ao.nn.quantized.dynamic.Linear 类方法)</a>

      <ul>
        <li><a href="generated/torch.ao.nn.quantized.Linear.html#torch.ao.nn.quantized.Linear.from_reference">(torch.ao.nn.quantized.Linear 类方法)</a>
</li>
      </ul></li>
      <li><a href="onnx_verification.html#torch.onnx.verification.VerificationInfo.from_tensors">from_tensors() (torch.onnx.verification.VerificationInfo 类方法)</a>
</li>
      <li><a href="generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.from_value">from_value() (torch.onnx.JitScalarType 类方法)</a>
</li>
      <li><a href="generated/torch.frombuffer.html#torch.frombuffer">torch 模块中的 frombuffer()</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.fromkeys">torch.autograd.profiler_util.StringTable 方法中的 fromkeys()</a>

      <ul>
        <li><a href="generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.fromkeys">(torch.nn.ParameterDict 方法)</a>
</li>
      </ul></li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules">torch.distributed.fsdp.FullyShardedDataParallel 静态方法中的 fsdp_modules()</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule">torch.distributed.fsdp 中的 FSDPModule 类</a>
</li>
      <li><a href="generated/torch.full.html#torch.full">torch 模块中的 full()函数</a>

      <ul>
        <li><a href="distributed.tensor.html#torch.distributed.tensor.full">torch.distributed.tensor 模块中的()</a>
</li>
      </ul></li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.full_kv_indices">torch.nn.attention.flex_attention.BlockMask 属性中的 full_kv_indices</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.full_kv_num_blocks">torch.nn.attention.flex_attention.BlockMask 属性中的 full_kv_num_blocks</a>
</li>
      <li><a href="generated/torch.full_like.html#torch.full_like">torch 模块中的 full_like()函数</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict">full_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel 静态方法)</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.full_q_indices">full_q_indices (torch.nn.attention.flex_attention.BlockMask 属性)</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.full_q_num_blocks">full_q_num_blocks (torch.nn.attention.flex_attention.BlockMask 属性)</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.DTensor.full_tensor">full_tensor() (torch.distributed.tensor.DTensor 方法)</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullOptimStateDictConfig">FullOptimStateDictConfig (torch.distributed.fsdp 中的类)</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullStateDictConfig">FullStateDictConfig (torch.distributed.fsdp 中的类)</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.fully_shard">torch.distributed.fsdp 模块中的 fully_shard()函数</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel">torch.distributed.fsdp 模块中的 FullyShardedDataParallel 类</a>
</li>
      <li><a href="autograd.html#torch.autograd.Function">torch.autograd 模块中的 Function 类</a>
</li>
      <li><a href="generated/torch.func.functional_call.html#torch.func.functional_call">torch.func 模块中的 functional_call()函数</a>

      <ul>
        <li><a href="generated/torch.nn.utils.stateless.functional_call.html#torch.nn.utils.stateless.functional_call">(在模块 torch.nn.utils.stateless 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.func.functionalize.html#torch.func.functionalize">functionalize() (在模块 torch.func 中)</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.FunctionCounts">FunctionCounts (类，在 torch.utils.benchmark 中)</a>
</li>
      <li><a href="generated/torch.nn.utils.fuse_conv_bn_eval.html#torch.nn.utils.fuse_conv_bn_eval">fuse_conv_bn_eval() (在模块 torch.nn.utils 中)</a>
</li>
      <li><a href="generated/torch.nn.utils.fuse_conv_bn_weights.html#torch.nn.utils.fuse_conv_bn_weights">torch.nn.utils 模块中的 fuse_conv_bn_weights() 函数</a>
</li>
      <li><a href="generated/torch.ao.quantization.quantize_fx.fuse_fx.html#torch.ao.quantization.quantize_fx.fuse_fx">torch.ao.quantization.quantize_fx 模块中的 fuse_fx 类</a>
</li>
      <li><a href="generated/torch.nn.utils.fuse_linear_bn_eval.html#torch.nn.utils.fuse_linear_bn_eval">torch.nn.utils 模块中的 fuse_linear_bn_eval() 函数</a>
</li>
      <li><a href="generated/torch.nn.utils.fuse_linear_bn_weights.html#torch.nn.utils.fuse_linear_bn_weights">torch.nn.utils 模块中的 fuse_linear_bn_weights() 函数</a>
</li>
      <li><a href="generated/torch.ao.quantization.fuse_modules.fuse_modules.html#torch.ao.quantization.fuse_modules.fuse_modules">torch.ao.quantization.fuse_modules 中的 fuse_modules（类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html#torch.ao.quantization.fx.custom_config.FuseCustomConfig">torch.ao.quantization.fx.custom_config 中的 FuseCustomConfig（类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.html#torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize">torch.ao.quantization.fake_quantize 中的 FusedMovingAvgObsFakeQuantize（类）</a>
</li>
      <li><a href="futures.html#torch.futures.Future">torch.futures 中的 Future（类）</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.FXFloatFunctional.html#torch.ao.nn.quantized.FXFloatFunctional">torch.ao.nn.quantized 中的 FXFloatFunctional（类）</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="G">G</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributions.html#torch.distributions.gamma.Gamma">torch.distributions.gamma 中的 Gamma（类）</a>
</li>
      <li><a href="special.html#torch.special.gammainc">torch.special 模块中的 gammainc()</a>
</li>
      <li><a href="special.html#torch.special.gammaincc">torch.special 模块中的 gammaincc()</a>
</li>
      <li><a href="special.html#torch.special.gammaln">gammaln()（在 torch.special 模块中）</a>
</li>
      <li><a href="generated/torch.gather.html#torch.gather">gather()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.cuda.comm.gather.html#torch.cuda.comm.gather">（在 torch.cuda.comm 模块中）</a>
</li>
        <li><a href="distributed.html#torch.distributed.gather">（在 torch.distributed 模块中）</a>
</li>
        <li><a href="generated/torch.Tensor.gather.html#torch.Tensor.gather">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.gather_object">gather_object()（在 torch.distributed 模块中）</a>
</li>
      <li><a href="generated/torch.signal.windows.gaussian.html#torch.signal.windows.gaussian">gaussian()（在 torch.signal.windows 模块中）</a>
</li>
      <li><a href="generated/torch.nn.functional.gaussian_nll_loss.html#torch.nn.functional.gaussian_nll_loss">gaussian_nll_loss()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.nn.GaussianNLLLoss.html#torch.nn.GaussianNLLLoss">torch.nn 中的 GaussianNLLLoss（类）</a>
</li>
      <li><a href="generated/torch.gcd.html#torch.gcd">torch 模块中的 gcd()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.gcd.html#torch.Tensor.gcd">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.gcd_.html#torch.Tensor.gcd_">gcd_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.cuda.gds.gds_deregister_buffer.html#torch.cuda.gds.gds_deregister_buffer">torch.cuda.gds 模块中的 gds_deregister_buffer()</a>
</li>
      <li><a href="generated/torch.cuda.gds.gds_register_buffer.html#torch.cuda.gds.gds_register_buffer">torch.cuda.gds 模块中的 gds_register_buffer()</a>
</li>
      <li><a href="generated/torch.cuda.gds.GdsFile.html#torch.cuda.gds.GdsFile">torch.cuda.gds 模块中的 GdsFile 类</a>
</li>
      <li><a href="generated/torch.ge.html#torch.ge">torch 模块中的 ge()</a>

      <ul>
        <li><a href="generated/torch.Tensor.ge.html#torch.Tensor.ge">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.ge_.html#torch.Tensor.ge_">ge_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.nn.GELU.html#torch.nn.GELU">GELU (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.functional.gelu.html#torch.nn.functional.gelu">gelu() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="generated/torch.signal.windows.general_cosine.html#torch.signal.windows.general_cosine">general_cosine() (在模块 torch.signal.windows 中)</a>
</li>
      <li><a href="generated/torch.signal.windows.general_hamming.html#torch.signal.windows.general_hamming">general_hamming() (在模块 torch.signal.windows 中)</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.debug.CommDebugMode.generate_comm_debug_tracing_table">generate_comm_debug_tracing_table() (torch.distributed.tensor.debug.CommDebugMode 方法)</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.debug.CommDebugMode.generate_json_dump">generate_json_dump() (torch.distributed.tensor.debug.CommDebugMode 方法)</a>
</li>
      <li><a href="generated/torch.utils.generate_methods_for_privateuse1_backend.html#torch.utils.generate_methods_for_privateuse1_backend">generate_methods_for_privateuse1_backend() (在模块 torch.utils 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.generate_numeric_debug_handle.html#torch.ao.quantization.generate_numeric_debug_handle">generate_numeric_debug_handle (torch.ao.quantization 类中)</a>
</li>
      <li><a href="generated/torch.nn.Transformer.html#torch.nn.Transformer.generate_square_subsequent_mask">generate_square_subsequent_mask() (torch.nn.Transformer 静态方法)</a>
</li>
      <li><a href="generated/torch.Generator.html#torch.Generator">Generator (torch 类中)</a>
</li>
      <li><a href="distributions.html#torch.distributions.geometric.Geometric">torch.distributions.geometric 中的几何分布（类）</a>
</li>
      <li><a href="generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_">torch.Tensor 方法 geometric_()</a>
</li>
      <li><a href="generated/torch.geqrf.html#torch.geqrf">torch 模块中的 geqrf()</a>

      <ul>
        <li><a href="generated/torch.Tensor.geqrf.html#torch.Tensor.geqrf">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ger.html#torch.ger">torch 模块中的 ger()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.ger.html#torch.Tensor.ger">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.get">torch.autograd.profiler_util.StringTable 方法中的 get()</a>

      <ul>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.get">(torch.distributed.elastic.rendezvous.etcd_store.EtcdStore 方法)</a>
</li>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousParameters.get">(torch.distributed.elastic.rendezvous.RendezvousParameters 方法)</a>
</li>
        <li><a href="distributed.html#torch.distributed.Store.get">(torch.distributed.Store 方法)</a>
</li>
        <li><a href="generated/torch.fx.experimental.symbolic_shapes.CallMethodKey.html#torch.fx.experimental.symbolic_shapes.CallMethodKey.get">(torch.fx.experimental.symbolic_shapes.CallMethodKey 方法)</a>
</li>
        <li><a href="generated/torch.fx.experimental.symbolic_shapes.ConvertIntKey.html#torch.fx.experimental.symbolic_shapes.ConvertIntKey.get">(torch.fx.experimental.symbolic_shapes.ConvertIntKey 方法)</a>
</li>
        <li><a href="generated/torch.fx.experimental.symbolic_shapes.DivideByKey.html#torch.fx.experimental.symbolic_shapes.DivideByKey.get">(torch.fx.experimental.symbolic_shapes.DivideByKey 方法)</a>
</li>
        <li><a href="generated/torch.fx.experimental.symbolic_shapes.InnerTensorKey.html#torch.fx.experimental.symbolic_shapes.InnerTensorKey.get">(torch.fx.experimental.symbolic_shapes.InnerTensorKey 方法)</a>
</li>
        <li><a href="monitor.html#torch.monitor.Stat.get">(torch.monitor.Stat 方法)</a>
</li>
        <li><a href="generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.get">(torch.nn.ParameterDict 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.device_mesh.DeviceMesh.get_all_groups">get_all_groups() (torch.distributed.device_mesh.DeviceMesh 方法)</a>
</li>
      <li><a href="multiprocessing.html#torch.multiprocessing.get_all_sharing_strategies">get_all_sharing_strategies() (在模块 torch.multiprocessing 中)</a>
</li>
      <li><a href="generated/torch.cuda.get_allocator_backend.html#torch.cuda.get_allocator_backend">get_allocator_backend() (在模块 torch.cuda 中)</a>
</li>
      <li><a href="generated/torch.cuda.get_arch_list.html#torch.cuda.get_arch_list">get_arch_list() (在模块 torch.cuda 中)</a>

      <ul>
        <li><a href="generated/torch.xpu.get_arch_list.html#torch.xpu.get_arch_list">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_bool">get_as_bool() (torch.distributed.elastic.rendezvous.RendezvousParameters 方法)</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_int">get_as_int() (torch.distributed.elastic.rendezvous.RendezvousParameters 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html#torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.get_attr">get_attr() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts 方法)</a>

      <ul>
        <li><a href="fx.html#torch.fx.Graph.get_attr">(torch.fx.Graph 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Interpreter.get_attr">(torch.fx.Interpreter 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Transformer.get_attr">(torch.fx.Transformer 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.get_axioms">get_axioms() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.get_backend">torch.distributed 模块中的 get_backend()函数</a>

      <ul>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.get_backend">(torch.distributed.elastic.rendezvous.RendezvousHandler 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.observer.get_block_size.html#torch.ao.quantization.observer.get_block_size">torch.ao.quantization.observer 类中的 get_block_size 方法</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.get_buffer">torch.jit.ScriptModule 类中的 get_buffer()方法</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.get_buffer">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.get_buffer">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.debug.CommDebugMode.get_comm_counts">get_comm_counts() (torch.distributed.tensor.debug.CommDebugMode 方法)</a>
</li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.get_compiler_abi_compatibility_and_version">get_compiler_abi_compatibility_and_version() (在模块 torch.utils.cpp_extension 中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.device_mesh.DeviceMesh.get_coordinate">get_coordinate() (torch.distributed.device_mesh.DeviceMesh 方法)</a>
</li>
      <li><a href="generated/torch.utils.get_cpp_backtrace.html#torch.utils.get_cpp_backtrace">get_cpp_backtrace() (在模块 torch.utils 中)</a>
</li>
      <li><a href="backends.html#torch.backends.cpu.get_cpu_capability">get_cpu_capability() (在模块 torch.backends.cpu 中)</a>
</li>
      <li><a href="notes/serialization.html#torch.serialization.get_crc32_options">get_crc32_options() (在模块 torch.serialization 中)</a>
</li>
      <li><a href="library.html#torch.library.get_ctx">torch.library 模块中的 get_ctx()</a>
</li>
      <li><a href="generated/torch.jit.ScriptFunction.html#torch.jit.ScriptFunction.get_debug_state">torch.jit.ScriptFunction 方法中的 get_debug_state()</a>
</li>
      <li><a href="generated/torch.get_default_device.html#torch.get_default_device">torch 模块中的 get_default_device()</a>
</li>
      <li><a href="generated/torch.get_default_dtype.html#torch.get_default_dtype">torch 模块中的 get_default_dtype()</a>
</li>
      <li><a href="notes/serialization.html#torch.serialization.get_default_load_endianness">torch.serialization 模块中的 get_default_load_endianness()函数</a>
</li>
      <li><a href="notes/serialization.html#torch.serialization.get_default_mmap_options">torch.serialization 模块中的 get_default_mmap_options()函数</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping.html#torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping">torch.ao.quantization.qconfig_mapping 类中的 get_default_qat_qconfig_mapping</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping.html#torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping">torch.ao.quantization.qconfig_mapping 类中的 get_default_qconfig_mapping</a>
</li>
      <li><a href="generated/torch.get_deterministic_debug_mode.html#torch.get_deterministic_debug_mode">torch 模块中的 get_deterministic_debug_mode()函数</a>
</li>
      <li><a href="generated/torch.Tensor.get_device.html#torch.Tensor.get_device">torch.Tensor 方法中的 get_device()</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.get_device">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.get_device">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.get_device_capability.html#torch.cuda.get_device_capability">get_device_capability() (在模块 torch.cuda 中)</a>

      <ul>
        <li><a href="generated/torch.mtia.get_device_capability.html#torch.mtia.get_device_capability">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="generated/torch.xpu.get_device_capability.html#torch.xpu.get_device_capability">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.get_device_module.html#torch.get_device_module">get_device_module() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.cuda.get_device_name.html#torch.cuda.get_device_name">get_device_name() (在模块 torch.cuda 中)</a>

      <ul>
        <li><a href="generated/torch.xpu.get_device_name.html#torch.xpu.get_device_name">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.get_device_properties.html#torch.cuda.get_device_properties">get_device_properties() (在模块 torch.cuda 中)</a>

      <ul>
        <li><a href="generated/torch.xpu.get_device_properties.html#torch.xpu.get_device_properties">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="hub.html#torch.hub.get_dir">torch.hub 模块中的 get_dir()函数</a>
</li>
      <li><a href="optim.html#torch.optim.swa_utils.get_ema_multi_avg_fn">torch.optim.swa_utils 模块中的 get_ema_multi_avg_fn()函数</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.WorkerSpec.get_entrypoint_name">torch.distributed.elastic.agent.server.WorkerSpec 方法中的 get_entrypoint_name()</a>
</li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.TimerServer.get_expired_timers">torch.distributed.elastic.timer.TimerServer 方法中的 get_expired_timers()</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.get_extra_state">get_extra_state() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.get_extra_state">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.get_extra_state">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="backends.html#torch.backends.mha.get_fastpath_enabled">get_fastpath_enabled() (在模块 torch.backends.mha 中)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.get_filename">torch.cuda.tunable 模块中的 get_filename()函数</a>
</li>
      <li><a href="generated/torch.get_float32_matmul_precision.html#torch.get_float32_matmul_precision">torch 模块中的 get_float32_matmul_precision()函数</a>
</li>
      <li><a href="fx.html#torch.fx.Tracer.get_fresh_qualname">torch.fx.Tracer 方法中的 get_fresh_qualname()</a>
</li>
      <li><a href="distributed.html#torch.distributed.Work.get_future">torch.distributed.Work 方法中的 get_future()</a>
</li>
      <li><a href="distributed.html#torch.distributed.Work.get_future_result">get_future_result() (torch.distributed.Work 方法)</a>
</li>
      <li><a href="generated/torch.cuda.get_gencode_flags.html#torch.cuda.get_gencode_flags">get_gencode_flags() (在模块 torch.cuda 中)</a>

      <ul>
        <li><a href="generated/torch.xpu.get_gencode_flags.html#torch.xpu.get_gencode_flags">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.get_global_rank">get_global_rank() (在模块 torch.distributed 中)</a>
</li>
      <li><a href="autograd.html#torch.autograd.graph.get_gradient_edge">get_gradient_edge() (在模块 torch.autograd.graph 中)</a>
</li>
      <li><a href="rpc.html#torch.distributed.autograd.get_gradients">get_gradients() (在模块 torch.distributed.autograd 中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.device_mesh.DeviceMesh.get_group">get_group() (torch.distributed.device_mesh.DeviceMesh 方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.get_group_rank">get_group_rank() (在模块 torch.distributed 中)</a>
</li>
      <li><a href="torch.overrides.html#torch.overrides.get_ignored_functions">get_ignored_functions() (在模块 torch.overrides 中)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.get_implications">get_implications() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler.get_last_lr">get_last_lr() (torch.optim.lr_scheduler.ChainedScheduler 方法)</a>

      <ul>
        <li><a href="generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR.get_last_lr">(torch.optim.lr_scheduler.ConstantLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR.get_last_lr">(torch.optim.lr_scheduler.CosineAnnealingLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.get_last_lr">(torch.optim.lr_scheduler.CosineAnnealingWarmRestarts 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR.get_last_lr">(torch.optim.lr_scheduler.CyclicLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR.get_last_lr">(torch.optim.lr_scheduler.ExponentialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR.get_last_lr">(torch.optim.lr_scheduler.LambdaLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR.get_last_lr">(torch.optim.lr_scheduler.LinearLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler.get_last_lr">(torch.optim.lr_scheduler.LRScheduler 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR.get_last_lr">(torch.optim.lr_scheduler.MultiplicativeLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR.get_last_lr">(torch.optim.lr_scheduler.MultiStepLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR.get_last_lr">(torch.optim.lr_scheduler.OneCycleLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR.get_last_lr">(torch.optim.lr_scheduler.PolynomialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau.get_last_lr">(torch.optim.lr_scheduler.ReduceLROnPlateau 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.get_last_lr">(torch.optim.lr_scheduler.SequentialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR.get_last_lr">(torch.optim.lr_scheduler.StepLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.SWALR.html#torch.optim.swa_utils.SWALR.get_last_lr">(torch.optim.swa_utils.SWALR 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.device_mesh.DeviceMesh.get_local_rank">get_local_rank() (torch.distributed.device_mesh.DeviceMesh 方法)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.get_logger_dict">get_logger_dict() (在模块 torch.ao.ns._numeric_suite 中)</a>
</li>
      <li><a href="elastic/events.html#torch.distributed.elastic.events.get_logging_handler">get_logging_handler() (在模块 torch.distributed.elastic.events 中)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler.get_lr">get_lr() (torch.optim.lr_scheduler.ChainedScheduler 方法)</a>

      <ul>
        <li><a href="generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR.get_lr">(torch.optim.lr_scheduler.ConstantLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR.get_lr">(torch.optim.lr_scheduler.CosineAnnealingLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.get_lr">(torch.optim.lr_scheduler.CosineAnnealingWarmRestarts 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR.get_lr">(torch.optim.lr_scheduler.CyclicLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR.get_lr">(torch.optim.lr_scheduler.ExponentialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR.get_lr">(torch.optim.lr_scheduler.LambdaLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR.get_lr">(torch.optim.lr_scheduler.LinearLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler.get_lr">(torch.optim.lr_scheduler.LRScheduler 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR.get_lr">(torch.optim.lr_scheduler.MultiplicativeLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR.get_lr">(torch.optim.lr_scheduler.MultiStepLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR.get_lr">(torch.optim.lr_scheduler.OneCycleLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR.get_lr">(torch.optim.lr_scheduler.PolynomialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau.get_lr">(torch.optim.lr_scheduler.ReduceLROnPlateau 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.get_lr">(torch.optim.lr_scheduler.SequentialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR.get_lr">(torch.optim.lr_scheduler.StepLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.SWALR.html#torch.optim.swa_utils.SWALR.get_lr">(torch.optim.swa_utils.SWALR 方法)</a>
</li>
      </ul></li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.get_matching_activations">get_matching_activations() (在模块 torch.ao.ns._numeric_suite 中)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.get_max_tuning_duration">get_max_tuning_duration() (在模块 torch.cuda.tunable 中)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.get_max_tuning_iterations">get_max_tuning_iterations() (在模块 torch.cuda.tunable 中)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_model_state_dict">get_model_state_dict() (在模块 torch.distributed.checkpoint.state_dict 中)</a>
</li>
      <li><a href="rpc.html#torch.distributed.nn.api.remote_module.RemoteModule.get_module_rref">get_module_rref() (torch.distributed.nn.api.remote_module.RemoteModule 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.get_nontrivial_guards">get_nontrivial_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.get_num_interop_threads.html#torch.get_num_interop_threads">get_num_interop_threads() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.get_num_threads.html#torch.get_num_threads">get_num_threads() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.get_observer_state_dict.html#torch.ao.quantization.observer.get_observer_state_dict">get_observer_state_dict (torch.ao.quantization.observer 类)</a>
</li>
      <li><a href="onnx_dynamo.html#torch.onnx.OnnxRegistry.get_op_functions">get_op_functions() (torch.onnx.OnnxRegistry 方法)</a>
</li>
      <li><a href="backends.html#torch.backends.opt_einsum.get_opt_einsum">get_opt_einsum() (在模块 torch.backends.opt_einsum 中)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_optimizer_state_dict">get_optimizer_state_dict() (在模块 torch.distributed.checkpoint.state_dict 中)</a>
</li>
      <li><a href="torch.overrides.html#torch.overrides.get_overridable_functions">get_overridable_functions() (在模块 torch.overrides 中)</a>
</li>
      <li><a href="future_mod.html#torch.__future__.get_overwrite_module_params_on_conversion">get_overwrite_module_params_on_conversion() (在模块 torch.__future__ 中)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.get_parameter">get_parameter() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.get_parameter">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.get_parameter">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.debug.CommDebugMode.get_parameter_info">get_parameter_info() (torch.distributed.tensor.debug.CommDebugMode 方法)</a>
</li>
      <li><a href="generated/torch.cuda.get_per_process_memory_fraction.html#torch.cuda.get_per_process_memory_fraction">get_per_process_memory_fraction() (在模块 torch.cuda 中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.get_process_group_ranks">get_process_group_ranks() (在模块 torch.distributed 中)</a>
</li>
      <li><a href="generated/torch.fx.experimental.proxy_tensor.get_proxy_mode.html#torch.fx.experimental.proxy_tensor.get_proxy_mode">get_proxy_mode() (在模块 torch.fx.experimental.proxy_tensor 中)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.get_pruned_guards">get_pruned_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.get_rank">get_rank() (在模块 torch.distributed 中)</a>

      <ul>
        <li><a href="distributed.html#torch.distributed.device_mesh.DeviceMesh.get_rank">(torch.distributed.device_mesh.DeviceMesh 方法)</a>
</li>
      </ul></li>
      <li><a href="package.html#torch.package.PackageExporter.get_rdeps">get_rdeps() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="export.html#torch.export.graph_signature.ExportGraphSignature.get_replace_hook">get_replace_hook() (torch.export.graph_signature.ExportGraphSignature 方法)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.get_results">get_results() (在模块 torch.cuda.tunable 中)</a>
</li>
      <li><a href="generated/torch.get_rng_state.html#torch.get_rng_state">get_rng_state() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.cuda.get_rng_state.html#torch.cuda.get_rng_state">(在模块 torch.cuda 中)</a>
</li>
        <li><a href="generated/torch.mps.get_rng_state.html#torch.mps.get_rng_state">（在模块 torch.mps 中）</a>
</li>
        <li><a href="generated/torch.mtia.get_rng_state.html#torch.mtia.get_rng_state">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="random.html#torch.random.get_rng_state">(在模块 torch.random 中)</a>
</li>
        <li><a href="generated/torch.xpu.get_rng_state.html#torch.xpu.get_rng_state">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.get_rng_state_all.html#torch.cuda.get_rng_state_all">get_rng_state_all() (在模块 torch.cuda 中)</a>

      <ul>
        <li><a href="generated/torch.xpu.get_rng_state_all.html#torch.xpu.get_rng_state_all">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.get_rotating_buffer_size">get_rotating_buffer_size() (在模块 torch.cuda.tunable 中)</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.get_run_id">get_run_id() (torch.distributed.elastic.rendezvous.RendezvousHandler 方法)</a>
</li>
      <li><a href="notes/serialization.html#torch.serialization.get_safe_globals">torch.serialization 模块中的 get_safe_globals()</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.debug.CommDebugMode.get_sharding_info">torch.distributed.tensor.debug.CommDebugMode 方法中的 get_sharding_info()</a>
</li>
      <li><a href="multiprocessing.html#torch.multiprocessing.get_sharing_strategy">torch.multiprocessing 模块中的 get_sharing_strategy()</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.get_state">torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend 方法中的 get_state()</a>

      <ul>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.get_state">(torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend 方法)</a>
</li>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.get_state">(torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend 方法)</a>
</li>
        <li><a href="generated/torch.Generator.html#torch.Generator.get_state">(torch.Generator 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict">get_state_dict() (在 torch.distributed.checkpoint.state_dict 模块中)</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.get_state_dict_type">get_state_dict_type() (torch.distributed.fsdp.FullyShardedDataParallel 静态方法)</a>
</li>
      <li><a href="generated/torch.cuda.get_stream_from_external.html#torch.cuda.get_stream_from_external">get_stream_from_external() (在模块 torch.cuda 中)</a>

      <ul>
        <li><a href="generated/torch.xpu.get_stream_from_external.html#torch.xpu.get_stream_from_external">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.get_submodule">get_submodule() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.get_submodule">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.get_submodule">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="elastic/subprocess_handler.html#torch.distributed.elastic.multiprocessing.subprocess_handler.handlers.get_subprocess_handler">get_subprocess_handler() (在模块 torch.distributed.elastic.multiprocessing.subprocess_handler.handlers 中)</a>
</li>
      <li><a href="future_mod.html#torch.__future__.get_swap_module_params_on_conversion">get_swap_module_params_on_conversion() (在模块 torch.__future__ 中)</a>
</li>
      <li><a href="generated/torch.cuda.get_sync_debug_mode.html#torch.cuda.get_sync_debug_mode">torch.cuda 模块中的 get_sync_debug_mode()</a>
</li>
      <li><a href="torch.overrides.html#torch.overrides.get_testing_overrides">torch.overrides 模块中的 get_testing_overrides()</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.debug.CommDebugMode.get_total_counts">torch.distributed.tensor.debug.CommDebugMode 方法中的 get_total_counts()</a>
</li>
      <li><a href="generated/torch.nn.utils.get_total_norm.html#torch.nn.utils.get_total_norm">torch.nn.utils 模块中的 get_total_norm()</a>
</li>
      <li><a href="profiler.html#torch.profiler.profile.get_trace_id">get_trace_id() (torch.profiler.profile 方法)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.get_unique_id">get_unique_id() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="notes/serialization.html#torch.serialization.get_unsafe_globals_in_checkpoint">get_unsafe_globals_in_checkpoint() (在模块 torch.serialization 中)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.get_validators">get_validators() (在模块 torch.cuda.tunable 中)</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.ElasticAgent.get_worker_group">get_worker_group() (torch.distributed.elastic.agent.server.ElasticAgent 方法)</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.get_worker_info">get_worker_info() (在模块 torch.distributed.rpc 中)</a>

      <ul>
        <li><a href="data.html#torch.utils.data.get_worker_info">(在模块 torch.utils.data 中)</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.get_world_size">get_world_size() (在模块 torch.distributed 中)</a>
</li>
      <li><a href="fx.html#torch.fx.Tracer.getattr">getattr() (torch.fx.Tracer 方法)</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.global_unstructured.html#torch.nn.utils.prune.global_unstructured">global_unstructured() (在 torch.nn.utils.prune 模块中)</a>
</li>
      <li><a href="generated/torch.nn.GLU.html#torch.nn.GLU">GLU (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.functional.glu.html#torch.nn.functional.glu">glu() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="generated/torch.Tensor.grad.html#torch.Tensor.grad">grad (torch.Tensor 属性)</a>
</li>
      <li><a href="generated/torch.autograd.grad.html#torch.autograd.grad">grad() (在 torch.autograd 模块中)</a>

      <ul>
        <li><a href="generated/torch.func.grad.html#torch.func.grad">(在 torch.func 模块中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.func.grad_and_value.html#torch.func.grad_and_value">grad_and_value() (在 torch.func 模块中)</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.GradBucket">GradBucket（torch.distributed 中的类）</a>
</li>
      <li><a href="generated/torch.autograd.gradcheck.gradcheck.html#torch.autograd.gradcheck.gradcheck">gradcheck()（在 torch.autograd.gradcheck 模块中）</a>
</li>
      <li><a href="generated/torch.autograd.gradcheck.GradcheckError.html#torch.autograd.gradcheck.GradcheckError">GradcheckError</a>
</li>
      <li><a href="generated/torch.autograd.gradcheck.gradgradcheck.html#torch.autograd.gradcheck.gradgradcheck">gradgradcheck()（在 torch.autograd.gradcheck 模块中）</a>
</li>
      <li><a href="generated/torch.gradient.html#torch.gradient">torch 模块中的 gradient()</a>
</li>
      <li><a href="autograd.html#torch.autograd.graph.GradientEdge">torch.autograd.graph 模块中的 GradientEdge 类</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.GradBucket.gradients">torch.distributed.GradBucket 模块中的 gradients()</a>
</li>
      <li><a href="amp.html#torch.cpu.amp.GradScaler">torch.cpu.amp 模块中的 GradScaler 类</a>

      <ul>
        <li><a href="amp.html#torch.cuda.amp.GradScaler">(torch.cuda.amp 中的类)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.observer.Granularity.html#torch.ao.quantization.observer.Granularity">粒度（torch.ao.quantization.observer 中的类）</a>
</li>
      <li><a href="generated/torch.cuda.graph.html#torch.cuda.graph">图（torch.cuda 中的类）</a>
</li>
      <li><a href="fx.html#torch.fx.Graph">图（torch.fx 中的类）</a>
</li>
      <li><a href="fx.html#torch.fx.GraphModule.graph">torch.fx.GraphModule 属性</a>

      <ul>
        <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.graph">torch.jit.ScriptModule 属性</a>
</li>
      </ul></li>
      <li><a href="fx.html#torch.fx.Graph.graph_copy">torch.fx.Graph 方法 graph_copy()</a>
</li>
      <li><a href="generated/torch.cuda.graph_pool_handle.html#torch.cuda.graph_pool_handle">torch.cuda 中的 graph_pool_handle()</a>
</li>
      <li><a href="onnx_verification.html#torch.onnx.verification.GraphInfo">torch.onnx.verification 中的 GraphInfo（类）</a>
</li>
      <li><a href="onnx_verification.html#torch.onnx.verification.GraphInfoPrettyPrinter">torch.onnx.verification 中的 GraphInfoPrettyPrinter（类）</a>
</li>
      <li><a href="fx.html#torch.fx.GraphModule">torch.fx 中的 GraphModule（类）</a>
</li>
      <li><a href="generated/torch.Generator.html#torch.Generator.graphsafe_get_state">torch.Generator 方法 graphsafe_get_state()</a>
</li>
      <li><a href="generated/torch.Generator.html#torch.Generator.graphsafe_set_state">graphsafe_set_state() (torch.Generator 方法)</a>
</li>
      <li><a href="generated/torch.greater.html#torch.greater">greater() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.greater.html#torch.Tensor.greater">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.greater_.html#torch.Tensor.greater_">greater_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.greater_equal.html#torch.greater_equal">torch（模块）中的 greater_equal()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.greater_equal_.html#torch.Tensor.greater_equal_">torch.Tensor 方法中的 greater_equal_()</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.greater_than">torch.distributions.constraints 模块中的 greater_than（大于）</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.greater_than_eq">torch.distributions.constraints 模块中的 greater_than_eq（大于等于）</a>
</li>
      <li><a href="generated/torch.nn.functional.grid_sample.html#torch.nn.functional.grid_sample">torch.nn.functional 模块中的 grid_sample()函数</a>
</li>
      <li><a href="generated/torch.nn.functional.group_norm.html#torch.nn.functional.group_norm">torch.nn.functional 模块中的 group_norm()函数</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.GroupNorm.html#torch.ao.nn.quantized.GroupNorm">torch.ao.nn.quantized 中的 GroupNorm（类）</a>

      <ul>
        <li><a href="generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.dynamic.GRU.html#torch.ao.nn.quantized.dynamic.GRU">torch.ao.nn.quantized.dynamic 中的 GRU（类）</a>

      <ul>
        <li><a href="generated/torch.nn.GRU.html#torch.nn.GRU">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.dynamic.GRUCell.html#torch.ao.nn.quantized.dynamic.GRUCell">GRUCell（torch.ao.nn.quantized.dynamic 类）</a>

      <ul>
        <li><a href="generated/torch.nn.GRUCell.html#torch.nn.GRUCell">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.gt.html#torch.gt">gt()（torch 模块）</a>

      <ul>
        <li><a href="generated/torch.Tensor.gt.html#torch.Tensor.gt">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.gt_.html#torch.Tensor.gt_">gt_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.guard_size_oblivious.html#torch.fx.experimental.symbolic_shapes.guard_size_oblivious">guard_size_oblivious() (在 torch.fx.experimental.symbolic_shapes 模块中)</a>
</li>
      <li><a href="distributions.html#torch.distributions.gumbel.Gumbel">Gumbel (torch.distributions.gumbel 中的类)</a>
</li>
      <li><a href="generated/torch.nn.functional.gumbel_softmax.html#torch.nn.functional.gumbel_softmax">gumbel_softmax() (在 torch.nn.functional 模块中)</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="H">H</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.H">H (torch.Tensor 属性)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.half">half() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.half">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.half">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.half.html#torch.Tensor.half">torch.Tensor 方法</a>
</li>
        <li><a href="storage.html#torch.TypedStorage.half">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.half">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.constraints.half_open_interval">half_open_interval（在 torch.distributions.constraints 模块中）</a>
</li>
      <li><a href="distributions.html#torch.distributions.half_cauchy.HalfCauchy">HalfCauchy（torch.distributions.half_cauchy 中的类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.half_normal.HalfNormal">HalfNormal（torch.distributions.half_normal 中的类）</a>
</li>
      <li><a href="storage.html#torch.HalfStorage">HalfStorage（torch 中的类）</a>
</li>
      <li><a href="generated/torch.signal.windows.hamming.html#torch.signal.windows.hamming">hamming()（torch.signal.windows 模块中）</a>
</li>
      <li><a href="generated/torch.hamming_window.html#torch.hamming_window">torch 模块中的 hamming_window()函数</a>
</li>
      <li><a href="generated/torch.fx.experimental.proxy_tensor.handle_sym_dispatch.html#torch.fx.experimental.proxy_tensor.handle_sym_dispatch">torch.fx.experimental.proxy_tensor 模块中的 handle_sym_dispatch()函数</a>
</li>
      <li><a href="torch.overrides.html#torch.overrides.handle_torch_function">torch.overrides 模块中的 handle_torch_function()函数</a>
</li>
      <li><a href="generated/torch.signal.windows.hann.html#torch.signal.windows.hann">torch.signal.windows 模块中的 hann()函数</a>
</li>
      <li><a href="generated/torch.hann_window.html#torch.hann_window">torch 模块中的 hann_window()</a>
</li>
      <li><a href="generated/torch.nn.Hardshrink.html#torch.nn.Hardshrink">torch.nn 中的 Hardshrink 类</a>
</li>
      <li><a href="generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink">torch.nn.functional 模块中的 hardshrink()</a>

      <ul>
        <li><a href="generated/torch.Tensor.hardshrink.html#torch.Tensor.hardshrink">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.functional.hardsigmoid.html#torch.ao.nn.quantized.functional.hardsigmoid">hardsigmoid（torch.ao.nn.quantized.functional 中的类）</a>
</li>
      <li><a href="generated/torch.nn.Hardsigmoid.html#torch.nn.Hardsigmoid">Hardsigmoid（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.functional.hardsigmoid.html#torch.nn.functional.hardsigmoid">hardsigmoid()（torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.Hardswish.html#torch.ao.nn.quantized.Hardswish">Hardswish（torch.ao.nn.quantized 中的类）</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.hardswish.html#torch.ao.nn.quantized.functional.hardswish">hardswish（torch.ao.nn.quantized.functional 类）</a>
</li>
      <li><a href="generated/torch.nn.Hardswish.html#torch.nn.Hardswish">Hardswish（torch.nn 类）</a>
</li>
      <li><a href="generated/torch.nn.functional.hardswish.html#torch.nn.functional.hardswish">hardswish()（torch.nn.functional 模块）</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.hardtanh.html#torch.ao.nn.quantized.functional.hardtanh">hardtanh（torch.ao.nn.quantized.functional 类）</a>
</li>
      <li><a href="generated/torch.nn.Hardtanh.html#torch.nn.Hardtanh">torch.nn 中的 hardtanh（类）</a>
</li>
      <li><a href="generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh">torch.nn.functional 模块中的 hardtanh()函数</a>
</li>
      <li><a href="generated/torch.nn.functional.hardtanh_.html#torch.nn.functional.hardtanh_">torch.nn.functional 模块中的 hardtanh_()函数</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.has_enumerate_support">torch.distributions.bernoulli.Bernoulli 属性中的 has_enumerate_support</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.has_enumerate_support">(torch.distributions.binomial.Binomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.has_enumerate_support">(torch.distributions.categorical.Categorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.has_enumerate_support">(torch.distributions.independent.Independent 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support">(torch.distributions.one_hot_categorical.OneHotCategorical 属性)</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.Store.has_extended_api">has_extended_api() (torch.distributed.Store 方法)</a>
</li>
      <li><a href="package.html#torch.package.Directory.has_file">has_file() (torch.package.Directory 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.has_free_symbols.html#torch.fx.experimental.symbolic_shapes.has_free_symbols">has_free_symbols() (在模块 torch.fx.experimental.symbolic_shapes 中)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.has_free_unbacked_symbols.html#torch.fx.experimental.symbolic_shapes.has_free_unbacked_symbols">has_free_unbacked_symbols() (在模块 torch.fx.experimental.symbolic_shapes 中)</a>
</li>
      <li><a href="distributions.html#torch.distributions.beta.Beta.has_rsample">has_rsample (torch.distributions.beta.Beta 属性)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.has_rsample">(torch.distributions.cauchy.Cauchy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.has_rsample">(torch.distributions.dirichlet.Dirichlet 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.has_rsample">(torch.distributions.exponential.Exponential 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample">(torch.distributions.fishersnedecor.FisherSnedecor 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.has_rsample">(torch.distributions.gamma.Gamma 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_cauchy.HalfCauchy.has_rsample">(torch.distributions.half_cauchy.HalfCauchy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_normal.HalfNormal.has_rsample">(torch.distributions.half_normal.HalfNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.has_rsample">(torch.distributions.independent.Independent 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.inverse_gamma.InverseGamma.has_rsample">(torch.distributions.inverse_gamma.InverseGamma 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.has_rsample">(torch.distributions.kumaraswamy.Kumaraswamy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.has_rsample">(torch.distributions.laplace.Laplace 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.has_rsample">(torch.distributions.log_normal.LogNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample">(torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample">(torch.distributions.mixture_same_family.MixtureSameFamily 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample">(torch.distributions.multivariate_normal.MultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.has_rsample">(torch.distributions.normal.Normal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample">(torch.distributions.relaxed_bernoulli.RelaxedBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample">(torch.distributions.relaxed_categorical.RelaxedOneHotCategorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.has_rsample">(torch.distributions.studentT.StudentT 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample">(torch.distributions.transformed_distribution.转换分布属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.has_rsample">(torch.distributions.uniform.Uniform 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.von_mises.VonMises.has_rsample">(torch.distributions.von_mises.VonMises 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.wishart.Wishart.has_rsample">(torch.distributions.wishart.Wishart 属性)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="torch.overrides.html#torch.overrides.has_torch_function">has_torch_function() (在 torch.overrides 模块中)</a>
</li>
      <li><a href="generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params">has_uninitialized_params() (torch.nn.modules.lazy.LazyModuleMixin 方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.HashStore">HashStore (torch.distributed 中的类)</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.health_check_server.HealthCheckServer">torch.distributed.elastic.agent.server.health_check_server 中的 HealthCheckServer（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.heartbeat">torch.distributed.elastic.rendezvous.dynamic_rendezvous 中的 RendezvousTimeout 属性的心跳</a>
</li>
      <li><a href="generated/torch.heaviside.html#torch.heaviside">torch 模块中的 heaviside()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.heaviside.html#torch.Tensor.heaviside">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="hub.html#torch.hub.help">help() (在模块 torch.hub 中)</a>
</li>
      <li><a href="generated/torch.autograd.functional.hessian.html#torch.autograd.functional.hessian">hessian() (在模块 torch.autograd.functional 中)</a>

      <ul>
        <li><a href="generated/torch.func.hessian.html#torch.func.hessian">(在 torch.func 模块中)</a>
</li>
      </ul></li>
      <li><a href="torch.html#torch.SymFloat.hex">hex() (torch.SymFloat 方法)</a>
</li>
      <li><a href="generated/torch.fft.hfft.html#torch.fft.hfft">hfft()（在 torch.fft 模块中）</a>
</li>
      <li><a href="generated/torch.fft.hfft2.html#torch.fft.hfft2">hfft2()（在 torch.fft 模块中）</a>
</li>
      <li><a href="generated/torch.fft.hfftn.html#torch.fft.hfftn">hfftn()（在 torch.fft 模块中）</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Compare.highlight_warnings">highlight_warnings()（torch.utils.benchmark.Compare 方法）</a>
</li>
      <li><a href="generated/torch.nn.functional.hinge_embedding_loss.html#torch.nn.functional.hinge_embedding_loss">hinge_embedding_loss()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss">HingeEmbeddingLoss（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.hint_int.html#torch.fx.experimental.symbolic_shapes.hint_int">hint_int()（在 torch.fx.experimental.symbolic_shapes 模块中）</a>
</li>
      <li><a href="generated/torch.histc.html#torch.histc">histc()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.histc.html#torch.Tensor.histc">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.histogram.html#torch.histogram">histogram()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.histogram.html#torch.Tensor.histogram">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.histogramdd.html#torch.histogramdd">histogramdd()（在 torch 模块中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.HistogramObserver.html#torch.ao.quantization.observer.HistogramObserver">torch.ao.quantization.observer 中的 HistogramObserver（类）</a>
</li>
      <li><a href="distributed.html#torch.distributed.TCPStore.host">torch.distributed.TCPStore 属性中的 host</a>
</li>
      <li><a href="generated/torch.cuda.host_memory_stats.html#torch.cuda.host_memory_stats">torch.cuda 模块中的 host_memory_stats()</a>
</li>
      <li><a href="generated/torch.linalg.householder_product.html#torch.linalg.householder_product">torch.linalg 模块中的 householder_product()</a>
</li>
      <li><a href="storage.html#torch.TypedStorage.hpu">hpu() (torch.TypedStorage 方法)</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.hpu">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.hsplit.html#torch.hsplit">hsplit() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.hspmm.html#torch.hspmm">torch 模块中的 hspmm()</a>
</li>
      <li><a href="generated/torch.hstack.html#torch.hstack">torch 模块中的 hstack()</a>
</li>
      <li><a href="generated/torch.nn.functional.huber_loss.html#torch.nn.functional.huber_loss">torch.nn.functional 模块中的 huber_loss()</a>
</li>
      <li><a href="generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss">torch.nn 中的 HuberLoss 类</a>
</li>
      <li><a href="generated/torch.autograd.functional.hvp.html#torch.autograd.functional.hvp">hvp()（在模块 torch.autograd.functional 中）</a>
</li>
      <li><a href="generated/torch.hypot.html#torch.hypot">hypot()（在模块 torch 中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.hypot.html#torch.Tensor.hypot">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.hypot_.html#torch.Tensor.hypot_">hypot_()（torch.Tensor 方法）</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="I">I</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.i0.html#torch.i0">torch 模块中的 i0()函数</a>

      <ul>
        <li><a href="special.html#torch.special.i0">（在 torch.special 模块中）</a>
</li>
        <li><a href="generated/torch.Tensor.i0.html#torch.Tensor.i0">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.i0_.html#torch.Tensor.i0_">torch.Tensor 方法 i0_()</a>
</li>
      <li><a href="special.html#torch.special.i0e">torch.special 模块中的 i0e()</a>
</li>
      <li><a href="special.html#torch.special.i1">torch.special 模块中的 i1()</a>
</li>
      <li><a href="special.html#torch.special.i1e">torch.special 模块中的 i1e()</a>
</li>
      <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.icdf">torch.distributions.cauchy.Cauchy 方法中的 icdf()</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.icdf">(torch.distributions.distribution.Distribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.icdf">(torch.distributions.exponential.Exponential 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_cauchy.HalfCauchy.icdf">(torch.distributions.half_cauchy.HalfCauchy 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_normal.HalfNormal.icdf">(torch.distributions.half_normal.HalfNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.icdf">(torch.distributions.laplace.Laplace 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.icdf">(torch.distributions.normal.Normal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.icdf">(torch.distributions.transformed_distribution.TransformedDistribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.icdf">(torch.distributions.uniform.Uniform 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.MemPool.html#torch.cuda.MemPool.id">id (torch.cuda.MemPool 属性)</a>

      <ul>
        <li><a href="rpc.html#torch.distributed.rpc.WorkerInfo.id">(torch.distributed.rpc.WorkerInfo 属性)</a>
</li>
      </ul></li>
      <li><a href="package.html#torch.package.PackageImporter.id">id() (torch.package.PackageImporter 方法)</a>
</li>
      <li><a href="generated/torch.nn.Identity.html#torch.nn.Identity">torch.nn 中的 Identity（类）</a>

      <ul>
        <li><a href="generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity">torch.nn.utils.prune 中的（类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.utils.prune.identity.html#torch.nn.utils.prune.identity">torch.nn.utils.prune 模块中的 identity()函数</a>
</li>
      <li><a href="generated/torch.fft.ifft.html#torch.fft.ifft">torch.fft 模块中的 ifft()函数</a>
</li>
      <li><a href="generated/torch.fft.ifft2.html#torch.fft.ifft2">ifft2()（在 torch.fft 模块中）</a>
</li>
      <li><a href="generated/torch.fft.ifftn.html#torch.fft.ifftn">ifftn()（在 torch.fft 模块中）</a>
</li>
      <li><a href="generated/torch.fft.ifftshift.html#torch.fft.ifftshift">ifftshift()（在 torch.fft 模块中）</a>
</li>
      <li><a href="generated/torch.igamma.html#torch.igamma">igamma()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.igamma.html#torch.Tensor.igamma">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.igamma_.html#torch.Tensor.igamma_">igamma_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.igammac.html#torch.igammac">igammac() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.igammac.html#torch.Tensor.igammac">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.igammac_.html#torch.Tensor.igammac_">igammac_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.jit.ignore.html#torch.jit.ignore">ignore() (在 torch.jit 模块中)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.ignore_fresh_unbacked_symbols">ignore_fresh_unbacked_symbols() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.fft.ihfft.html#torch.fft.ihfft">ihfft() (在 torch.fft 模块中)</a>
</li>
      <li><a href="generated/torch.fft.ihfft2.html#torch.fft.ihfft2">ihfft2()（在 torch.fft 模块中）</a>
</li>
      <li><a href="generated/torch.fft.ihfftn.html#torch.fft.ihfftn">ihfftn()（在 torch.fft 模块中）</a>
</li>
      <li><a href="generated/torch.nn.attention.bias.CausalVariant.html#torch.nn.attention.bias.CausalVariant.imag">imag（torch.nn.attention.bias.CausalVariant 属性）</a>

      <ul>
        <li><a href="generated/torch.Tensor.imag.html#torch.Tensor.imag">（torch.Tensor 属性）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.imag.html#torch.imag">torch 模块中的 imag()函数</a>
</li>
      <li><a href="library.html#torch.library.impl">torch.library 模块中的 impl()函数</a>

      <ul>
        <li><a href="library.html#torch.library.Library.impl">torch.library 的 Library 方法</a>
</li>
      </ul></li>
      <li><a href="library.html#torch.library.impl_abstract">torch.library 模块中的 impl_abstract()函数</a>
</li>
      <li><a href="package.html#torch.package.PackageImporter.import_module">import_module() (torch.package.PackageImporter 方法)</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.MemRecordsAcc.html#torch.autograd.profiler_util.MemRecordsAcc.in_interval">in_interval() (torch.autograd.profiler_util.MemRecordsAcc 方法)</a>
</li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.include_paths">include_paths() (在模块 torch.utils.cpp_extension 中)</a>
</li>
      <li><a href="generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker.increment_step">increment_step() (torch.autograd.profiler.KinetoStepTracker 类方法)</a>
</li>
      <li><a href="generated/torch.autograd.graph.increment_version.html#torch.autograd.graph.increment_version">torch.autograd.graph 模块中的 increment_version()函数</a>
</li>
      <li><a href="distributions.html#torch.distributions.independent.Independent">torch.distributions.independent 模块中的 Independent 类</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.independent">torch.distributions.constraints 模块中的 independent</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.IndependentTransform">torch.distributions.transforms 模块中的 IndependentTransform 类</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.GradBucket.index">torch.distributed.GradBucket 模块中的 index()函数</a>

      <ul>
        <li><a href="generated/torch.autograd.forward_ad.UnpackedDualTensor.html#torch.autograd.forward_ad.UnpackedDualTensor.index">torch.autograd.forward_ad.UnpackedDualTensor 方法</a>
</li>
        <li><a href="generated/torch.autograd.profiler_util.Kernel.html#torch.autograd.profiler_util.Kernel.index">torch.autograd.profiler_util.Kernel 方法</a>
</li>
        <li><a href="generated/torch.jit.Attribute.html#torch.jit.Attribute.index">torch.jit.Attribute 方法</a>
</li>
        <li><a href="generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.index">(torch.nn.utils.rnn.PackedSequence 方法)</a>
</li>
        <li><a href="size.html#torch.Size.index">(torch.Size 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.index_add.html#torch.index_add">index_add()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.index_add.html#torch.Tensor.index_add">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_">index_add_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.index_copy.html#torch.index_copy">index_copy() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.index_copy.html#torch.Tensor.index_copy">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_">index_copy_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.index_fill.html#torch.Tensor.index_fill">index_fill() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_">index_fill_(...) (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.index_put.html#torch.Tensor.index_put">index_put() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_">index_put_(...) (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.index_reduce.html#torch.index_reduce">torch 模块中的 index_reduce()</a>

      <ul>
        <li><a href="generated/torch.Tensor.index_reduce.html#torch.Tensor.index_reduce">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_">index_reduce_(torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.index_select.html#torch.index_select">index_select() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.index_select.html#torch.Tensor.index_select">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.indices.html#torch.Tensor.indices">indices() (torch.Tensor 方法)</a>
</li>
      <li><a href="library.html#torch.library.infer_schema">infer_schema() (模块 torch.library 中)</a>
</li>
      <li><a href="generated/torch.autograd.grad_mode.inference_mode.html#torch.autograd.grad_mode.inference_mode">inference_mode (类，位于 torch.autograd.grad_mode)</a>
</li>
      <li><a href="generated/torch.cuda.init.html#torch.cuda.init">init()（在 torch.cuda 模块中）</a>

      <ul>
        <li><a href="generated/torch.mtia.init.html#torch.mtia.init">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="generated/torch.xpu.init.html#torch.xpu.init">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.device_mesh.init_device_mesh">init_device_mesh()（在 torch.distributed.device_mesh 模块中）</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.RpcBackendOptions.init_method">init_method (torch.distributed.rpc.RpcBackendOptions 属性)</a>

      <ul>
        <li><a href="rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method">(torch.distributed.rpc.TensorPipeRpcBackendOptions 属性)</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.init_process_group">init_process_group() (在模块 torch.distributed 中)</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.init_rpc">init_rpc() (在模块 torch.distributed.rpc 中)</a>
</li>
      <li><a href="generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker.init_step_count">init_step_count() (torch.autograd.profiler.KinetoStepTracker 类方法)</a>
</li>
      <li><a href="generated/torch.initial_seed.html#torch.initial_seed">initial_seed() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.cuda.initial_seed.html#torch.cuda.initial_seed">(在模块 torch.cuda 中)</a>
</li>
        <li><a href="random.html#torch.random.initial_seed">(在模块 torch.random 中)</a>
</li>
        <li><a href="generated/torch.xpu.initial_seed.html#torch.xpu.initial_seed">(在模块 torch.xpu 中)</a>
</li>
        <li><a href="generated/torch.Generator.html#torch.Generator.initial_seed">(torch.Generator 方法)</a>
</li>
      </ul></li>
      <li><a href="onnx_dynamo.html#torch.onnx.ONNXProgram.initialize_inference_session">initialize_inference_session() (torch.onnx.ONNXProgram 方法)</a>
</li>
      <li><a href="generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters">initialize_parameters() (torch.nn.modules.lazy.LazyModuleMixin 方法)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.inlined_graph">inlined_graph (torch.jit.ScriptModule 属性)</a>
</li>
      <li><a href="generated/torch.inner.html#torch.inner">inner() (模块 torch 中的函数)</a>

      <ul>
        <li><a href="generated/torch.Tensor.inner.html#torch.Tensor.inner">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.InnerTensorKey.html#torch.fx.experimental.symbolic_shapes.InnerTensorKey">InnerTensorKey (torch.fx.experimental.symbolic_shapes 中的类)</a>
</li>
      <li><a href="generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction">torch.autograd.function 中的 InplaceFunction（类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.ObservationType.html#torch.ao.quantization.backend_config.ObservationType.INPUT_OUTPUT_NOT_OBSERVED">torch.ao.quantization.backend_config.ObservationType 属性中的 INPUT_OUTPUT_NOT_OBSERVED</a>
</li>
      <li><a href="export.html#torch.export.graph_signature.InputKind">torch.export.graph_signature 中的 InputKind（类）</a>
</li>
      <li><a href="export.html#torch.export.graph_signature.InputSpec">torch.export.graph_signature 中的 InputSpec（类）</a>
</li>
      <li><a href="generated/torch.nn.ModuleList.html#torch.nn.ModuleList.insert">insert() (torch.nn.ModuleList 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.Node.insert_arg">insert_arg() (torch.fx.Node 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.Graph.inserting_after">inserting_after() (torch.fx.Graph 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.Graph.inserting_before">inserting_before() (torch.fx.Graph 方法)</a>
</li>
      <li><a href="generated/torch.nn.functional.instance_norm.html#torch.nn.functional.instance_norm">instance_norm()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.InstanceNorm1d.html#torch.ao.nn.quantized.InstanceNorm1d">InstanceNorm1d（torch.ao.nn.quantized 中的类）</a>

      <ul>
        <li><a href="generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.InstanceNorm2d.html#torch.ao.nn.quantized.InstanceNorm2d">InstanceNorm2d（torch.ao.nn.quantized 中的类）</a>

      <ul>
        <li><a href="generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.InstanceNorm3d.html#torch.ao.nn.quantized.InstanceNorm3d">InstanceNorm3d (torch.ao.nn.quantized 中的类)</a>

      <ul>
        <li><a href="generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.int.html#torch.Tensor.int">int() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.int">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.int">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr">int_repr() (torch.Tensor 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.integer_interval">integer_interval (在 torch.distributions.constraints 模块中)</a>
</li>
      <li><a href="generated/torch.jit.interface.html#torch.jit.interface">torch.jit 模块中的 interface()函数</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.intern">torch.package.PackageExporter 方法中的 intern()函数</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.interned_modules">torch.package.PackageExporter 方法中的 interned_modules()函数</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.interpolate.html#torch.ao.nn.quantized.functional.interpolate">torch.ao.nn.quantized.functional 类中的 interpolate 方法</a>
</li>
      <li><a href="generated/torch.nn.functional.interpolate.html#torch.nn.functional.interpolate">torch.nn.functional 模块中的 interpolate()函数</a>
</li>
      <li><a href="fx.html#torch.fx.Interpreter">torch.fx 中的解释器（类）</a>
</li>
      <li><a href="export.html#torch.export.unflatten.InterpreterModule">torch.export.unflatten 中的 InterpreterModule（类）</a>
</li>
      <li><a href="export.html#torch.export.unflatten.InterpreterModuleDispatcher">torch.export.unflatten 中的 InterpreterModuleDispatcher（类）</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.Interval.html#torch.autograd.profiler_util.Interval">torch.autograd.profiler_util 中的 Interval（类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.interval">torch.distributions.constraints 中的 interval</a>
</li>
      <li><a href="storage.html#torch.IntStorage">torch 中的 IntStorage（类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.Transform.inv">torch.distributions.transforms 中的 Transform 属性 inv</a>
</li>
      <li><a href="generated/torch.linalg.inv.html#torch.linalg.inv">inv()（在 torch.linalg 模块中）</a>
</li>
      <li><a href="generated/torch.linalg.inv_ex.html#torch.linalg.inv_ex">inv_ex()（在 torch.linalg 模块中）</a>
</li>
      <li><a href="generated/torch.inverse.html#torch.inverse">inverse()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.inverse.html#torch.Tensor.inverse">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.transforms.Transform.inverse_shape">inverse_shape() (torch.distributions.transforms.Transform 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.inverse_gamma.InverseGamma">InverseGamma (torch.distributions.inverse_gamma 中的类)</a>
</li>
      <li><a href="generated/torch.cuda.ipc_collect.html#torch.cuda.ipc_collect">ipc_collect() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="generated/torch.cuda.Event.html#torch.cuda.Event.ipc_handle">ipc_handle() (torch.cuda.Event 方法)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.ipu">ipu() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.ipu">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.ipu">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributed.html#torch.distributed.irecv">irecv() (in 模块 torch.distributed)</a>
</li>
      <li><a href="generated/torch.fft.irfft.html#torch.fft.irfft">irfft()（在 torch.fft 模块中）</a>
</li>
      <li><a href="generated/torch.fft.irfft2.html#torch.fft.irfft2">irfft2()（在 torch.fft 模块中）</a>
</li>
      <li><a href="generated/torch.fft.irfftn.html#torch.fft.irfftn">irfftn()（在 torch.fft 模块中）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.is_accessor_node.html#torch.fx.experimental.symbolic_shapes.is_accessor_node">is_accessor_node()（在 torch.fx.experimental.symbolic_shapes 模块中）</a>
</li>
      <li><a href="amp.html#torch.amp.autocast_mode.is_autocast_available">is_autocast_available() (在模块 torch.amp.autocast_mode 中)</a>
</li>
      <li><a href="generated/torch.accelerator.is_available.html#torch.accelerator.is_available">is_available() (在模块 torch.accelerator 中)</a>

      <ul>
        <li><a href="backends.html#torch.backends.cudnn.is_available">(在 torch.backends.cudnn 模块中)</a>
</li>
        <li><a href="backends.html#torch.backends.cusparselt.is_available">（在 torch.backends.cusparselt 模块中）</a>
</li>
        <li><a href="backends.html#torch.backends.mkl.is_available">(在模块 torch.backends.mkl 中)</a>
</li>
        <li><a href="backends.html#torch.backends.mkldnn.is_available">(在模块 torch.backends.mkldnn 中)</a>
</li>
        <li><a href="backends.html#torch.backends.mps.is_available">（在模块 torch.backends.mps 中）</a>
</li>
        <li><a href="backends.html#torch.backends.nnpack.is_available">（在模块 torch.backends.nnpack 中）</a>
</li>
        <li><a href="backends.html#torch.backends.openmp.is_available">（在模块 torch.backends.openmp 中）</a>
</li>
        <li><a href="backends.html#torch.backends.opt_einsum.is_available">（在模块 torch.backends.opt_einsum 中）</a>
</li>
        <li><a href="generated/torch.cpu.is_available.html#torch.cpu.is_available">(在模块 torch.cpu 中)</a>
</li>
        <li><a href="generated/torch.cuda.is_available.html#torch.cuda.is_available">(在模块 torch.cuda 中)</a>
</li>
        <li><a href="distributed.html#torch.distributed.is_available">（在 torch.distributed 模块中）</a>
</li>
        <li><a href="generated/torch.mtia.is_available.html#torch.mtia.is_available">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="profiler.html#torch.profiler.itt.is_available">torch.profiler.itt 模块中</a>
</li>
        <li><a href="generated/torch.xpu.is_available.html#torch.xpu.is_available">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="backends.html#torch.backends.cuda.is_built">torch.backends.cuda 中的 is_built()</a>

      <ul>
        <li><a href="backends.html#torch.backends.mps.is_built">torch.backends.mps 中</a>
</li>
      </ul></li>
      <li><a href="generated/torch.mps.profiler.is_capturing_metal.html#torch.mps.profiler.is_capturing_metal">torch.mps.profiler 中的 is_capturing_metal()</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.is_closed">torch.distributed.elastic.rendezvous.RendezvousHandler 方法中的 is_closed()</a>
</li>
      <li><a href="generated/torch.Tensor.is_coalesced.html#torch.Tensor.is_coalesced">is_coalesced() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.compiler.is_compiling.html#torch.compiler.is_compiling">is_compiling() (在模块 torch.compiler 中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.Work.is_completed">is_completed() (torch.distributed.Work 方法)</a>
</li>
      <li><a href="generated/torch.is_complex.html#torch.is_complex">is_complex() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.is_complex.html#torch.Tensor.is_complex">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.is_concrete_bool.html#torch.fx.experimental.symbolic_shapes.is_concrete_bool">torch.fx.experimental.symbolic_shapes 模块中的 is_concrete_bool()</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.is_concrete_float.html#torch.fx.experimental.symbolic_shapes.is_concrete_float">torch.fx.experimental.symbolic_shapes 模块中的 is_concrete_float()</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.is_concrete_int.html#torch.fx.experimental.symbolic_shapes.is_concrete_int">torch.fx.experimental.symbolic_shapes 模块中的 is_concrete_int()</a>
</li>
      <li><a href="generated/torch.is_conj.html#torch.is_conj">torch 模块中的 is_conj()</a>

      <ul>
        <li><a href="generated/torch.Tensor.is_conj.html#torch.Tensor.is_conj">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.is_contiguous.html#torch.Tensor.is_contiguous">torch.Tensor 方法 is_contiguous()</a>
</li>
      <li><a href="generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.is_cuda">torch.nn.utils.rnn.PackedSequence 属性 is_cuda</a>

      <ul>
        <li><a href="generated/torch.Tensor.is_cuda.html#torch.Tensor.is_cuda">（torch.Tensor 属性）</a>
</li>
        <li><a href="storage.html#torch.TypedStorage.is_cuda">(torch.TypedStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.is_cuda">torch.UntypedStorage 属性</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.is_current_stream_capturing.html#torch.cuda.is_current_stream_capturing">torch.cuda 模块中 is_current_stream_capturing()方法</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.is_dependent">torch.distributions.constraints 模块中 is_dependent()方法</a>
</li>
      <li><a href="generated/torch.is_deterministic_algorithms_warn_only_enabled.html#torch.is_deterministic_algorithms_warn_only_enabled">torch 模块中的 is_deterministic_algorithms_warn_only_enabled()函数</a>
</li>
      <li><a href="generated/torch.compiler.is_dynamo_compiling.html#torch.compiler.is_dynamo_compiling">torch.compiler 模块中的 is_dynamo_compiling()函数</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.is_enabled">torch.cuda.tunable 模块中的 is_enabled()函数</a>

      <ul>
        <li><a href="generated/torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants.is_enabled">torch.sparse.check_sparse_tensor_invariants 静态方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.compiler.is_exporting.html#torch.compiler.is_exporting">is_exporting() (在模块 torch.compiler)</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.is_flash_attention_available">torch.backends.cuda 模块中的 is_flash_attention_available()</a>
</li>
      <li><a href="generated/torch.is_floating_point.html#torch.is_floating_point">torch 模块中的 is_floating_point()</a>

      <ul>
        <li><a href="generated/torch.Tensor.is_floating_point.html#torch.Tensor.is_floating_point">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.is_gloo_available">torch.distributed 模块中的 is_gloo_available()</a>
</li>
      <li><a href="generated/torch.is_grad_enabled.html#torch.is_grad_enabled">torch 模块中的 is_grad_enabled()</a>
</li>
      <li><a href="storage.html#torch.TypedStorage.is_hpu">is_hpu (torch.TypedStorage 属性)</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.is_hpu">torch.UntypedStorage 属性</a>
</li>
      </ul></li>
      <li><a href="fx.html#torch.fx.Node.is_impure">is_impure() (torch.fx.Node 方法)</a>
</li>
      <li><a href="onnx_torchscript.html#torch.onnx.is_in_onnx_export">is_in_onnx_export() (在模块 torch.onnx 中)</a>
</li>
      <li><a href="generated/torch.Tensor.is_inference.html#torch.Tensor.is_inference">is_inference() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled">torch 模块中的 is_inference_mode_enabled()函数</a>
</li>
      <li><a href="generated/torch.cuda.is_initialized.html#torch.cuda.is_initialized">torch.cuda 模块中的 is_initialized()函数</a>

      <ul>
        <li><a href="distributed.html#torch.distributed.is_initialized">（在 torch.distributed 模块中）</a>
</li>
        <li><a href="generated/torch.mtia.is_initialized.html#torch.mtia.is_initialized">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="generated/torch.xpu.is_initialized.html#torch.xpu.is_initialized">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.attention.bias.CausalVariant.html#torch.nn.attention.bias.CausalVariant.is_integer">is_integer() (torch.nn.attention.bias.CausalVariant 方法)</a>

      <ul>
        <li><a href="torch.html#torch.SymFloat.is_integer">(torch.SymFloat 方法)</a>
</li>
      </ul></li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.GradBucket.is_last">is_last() (在模块 torch.distributed.GradBucket 中)</a>
</li>
      <li><a href="generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf">is_leaf (torch.Tensor 属性)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.NSTracer.is_leaf_module">is_leaf_module() (torch.ao.ns._numeric_suite_fx.NSTracer 方法)</a>

      <ul>
        <li><a href="fx.html#torch.fx.Tracer.is_leaf_module">(torch.fx.Tracer 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.is_meta.html#torch.Tensor.is_meta">is_meta (torch.Tensor 属性)</a>
</li>
      <li><a href="generated/torch.mps.profiler.is_metal_capture_enabled.html#torch.mps.profiler.is_metal_capture_enabled">is_metal_capture_enabled() (在 torch.mps.profiler 模块中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.is_mpi_available">is_mpi_available() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.is_nccl_available">is_nccl_available() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.is_ninja_available">torch.utils.cpp_extension 模块中的 is_ninja_available() 函数</a>
</li>
      <li><a href="generated/torch.is_nonzero.html#torch.is_nonzero">torch 模块中的 is_nonzero() 函数</a>
</li>
      <li><a href="onnx_dynamo_onnxruntime_backend.html#torch.onnx.is_onnxrt_backend_supported">torch.onnx 模块中的 is_onnxrt_backend_supported() 函数</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.PyRRef.is_owner">torch.distributed.rpc.PyRRef 方法中的 is_owner() 函数</a>
</li>
      <li><a href="generated/torch.nn.utils.parametrize.is_parametrized.html#torch.nn.utils.parametrize.is_parametrized">torch.nn.utils.parametrize 模块中的 is_parametrized() 函数</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.placement_types.Placement.is_partial">torch.distributed.tensor.placement_types.Placement 方法中的 is_partial() 函数</a>
</li>
      <li><a href="generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.is_pinned">torch.nn.utils.rnn.PackedSequence 方法中的 is_pinned() 函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.is_pinned.html#torch.Tensor.is_pinned">torch.Tensor 方法</a>
</li>
        <li><a href="storage.html#torch.TypedStorage.is_pinned">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.is_pinned">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.utils.prune.is_pruned.html#torch.nn.utils.prune.is_pruned">torch.nn.utils.prune 模块中的 is_pruned() 函数</a>
</li>
      <li><a href="generated/torch.Tensor.is_quantized.html#torch.Tensor.is_quantized">torch.Tensor 属性 is_quantized</a>
</li>
      <li><a href="onnx_dynamo.html#torch.onnx.OnnxRegistry.is_registered_op">torch.onnx.OnnxRegistry 方法 is_registered_op()</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.placement_types.Placement.is_replicate">torch.distributed.tensor.placement_types.Placement 方法 is_replicate()</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.WorkerState.is_running">torch.distributed.elastic.agent.server.WorkerState 静态方法 is_running()</a>
</li>
      <li><a href="jit_language_reference.html#torch.jit.is_scripting">torch.jit 模块中的 is_scripting()</a>
</li>
      <li><a href="generated/torch.Tensor.is_set_to.html#torch.Tensor.is_set_to">torch.Tensor 方法中的 is_set_to()</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.placement_types.Placement.is_shard">torch.distributed.tensor.placement_types.Placement 方法中的 is_shard()</a>
</li>
      <li><a href="generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared">torch.Tensor 方法中的 is_shared()</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.is_shared">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.is_shared">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.is_signed.html#torch.Tensor.is_signed">is_signed() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse">is_sparse (torch.Tensor 属性)</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.is_sparse">(torch.TypedStorage 属性)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.is_sparse">(torch.UntypedStorage 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.is_sparse_csr.html#torch.Tensor.is_sparse_csr">torch.Tensor 属性 is_sparse_csr</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.is_sparse_csr">(torch.UntypedStorage 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.is_storage.html#torch.is_storage">torch 模块中的 is_storage()</a>
</li>
      <li><a href="distributed.html#torch.distributed.Work.is_success">torch.distributed.Work 方法中的 is_success()</a>
</li>
      <li><a href="generated/torch.is_tensor.html#torch.is_tensor">torch 模块中的 is_tensor()</a>
</li>
      <li><a href="torch.overrides.html#torch.overrides.is_tensor_like">torch.overrides 模块中的 is_tensor_like()</a>
</li>
      <li><a href="torch.overrides.html#torch.overrides.is_tensor_method_or_property">torch.overrides 模块中的 is_tensor_method_or_property()</a>
</li>
      <li><a href="generated/torch.cuda.is_tf32_supported.html#torch.cuda.is_tf32_supported">torch.cuda 模块中的 is_tf32_supported()</a>
</li>
      <li><a href="distributed.html#torch.distributed.is_torchelastic_launched">torch.distributed 模块中的 is_torchelastic_launched()</a>
</li>
      <li><a href="jit_language_reference.html#torch.jit.is_tracing">torch.jit 模块中的 is_tracing()</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.is_unbacked_symint">torch.fx.experimental.symbolic_shapes.ShapeEnv 方法中的 is_unbacked_symint()</a>
</li>
      <li><a href="generated/torch.is_warn_always_enabled.html#torch.is_warn_always_enabled">torch 模块中的 is_warn_always_enabled()</a>
</li>
      <li><a href="distributed.html#torch.distributed.distributed_c10d.is_xccl_available">torch.distributed.distributed_c10d 模块中的 is_xccl_available()</a>
</li>
      <li><a href="generated/torch.isclose.html#torch.isclose">torch 模块中的 isclose()</a>

      <ul>
        <li><a href="generated/torch.Tensor.isclose.html#torch.Tensor.isclose">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.isend">torch.distributed 模块中的 isend()</a>
</li>
      <li><a href="generated/torch.isfinite.html#torch.isfinite">torch 模块中的 isfinite()</a>

      <ul>
        <li><a href="generated/torch.Tensor.isfinite.html#torch.Tensor.isfinite">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.isin.html#torch.isin">torch 模块中的 isin()</a>
</li>
      <li><a href="generated/torch.isinf.html#torch.isinf">isinf()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.isinf.html#torch.Tensor.isinf">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.isinstance.html#torch.jit.isinstance">isinstance()（在 torch.jit 模块中）</a>
</li>
      <li><a href="generated/torch.isnan.html#torch.isnan">isnan()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.isnan.html#torch.Tensor.isnan">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.isneginf.html#torch.isneginf">isneginf()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.isneginf.html#torch.Tensor.isneginf">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.isposinf.html#torch.isposinf">isposinf()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.isposinf.html#torch.Tensor.isposinf">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.isreal.html#torch.isreal">torch 模块中的 isreal()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.isreal.html#torch.Tensor.isreal">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.istft.html#torch.istft">torch 模块中的 istft()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.istft.html#torch.Tensor.istft">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.item.html#torch.Tensor.item">torch.Tensor 方法中的 item()</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.items">torch.autograd.profiler_util.StringTable 方法中的 items()</a>

      <ul>
        <li><a href="export.html#torch.export.decomp_utils.CustomDecompTable.items">(torch.export.decomp_utils.CustomDecompTable 方法)</a>
</li>
        <li><a href="generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.items">(torch.nn.ModuleDict 方法)</a>
</li>
        <li><a href="generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.items">(torch.nn.ParameterDict 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.itemsize.html#torch.Tensor.itemsize">torch.Tensor 属性 itemsize</a>
</li>
      <li><a href="fx.html#torch.fx.Tracer.iter">torch.fx.Tracer 方法 iter()</a>
</li>
      <li><a href="data.html#torch.utils.data.IterableDataset">torch.utils.data 中的 IterableDataset 类</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="J">J</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.func.jacfwd.html#torch.func.jacfwd">torch.func 模块中的 jacfwd() 函数</a>
</li>
      <li><a href="generated/torch.autograd.functional.jacobian.html#torch.autograd.functional.jacobian">torch.autograd.functional 模块中的 jacobian()</a>
</li>
      <li><a href="generated/torch.func.jacrev.html#torch.func.jacrev">torch.func 模块中的 jacrev()</a>
</li>
      <li><a href="generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType">torch.onnx 模块中的 JitScalarType 类</a>
</li>
      <li><a href="torch.compiler.config.html#torch.compiler.config.job_id">torch.compiler.config 模块中的 job_id</a>
</li>
      <li><a href="distributed.algorithms.join.html#torch.distributed.algorithms.Join">torch.distributed.algorithms 中的 Join（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.join">torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout 属性中的 join</a>
</li>
      <li><a href="multiprocessing.html#torch.multiprocessing.SpawnContext.join">torch.multiprocessing.SpawnContext 方法中的 join()</a>

      <ul>
        <li><a href="generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join">(torch.nn.parallel.DistributedDataParallel 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.algorithms.join.html#torch.distributed.algorithms.Joinable.join_device">join_device (torch.distributed.algorithms.Joinable 属性)</a>

      <ul>
        <li><a href="distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.join_device">(torch.distributed.optim.ZeroRedundancyOptimizer 属性)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributed.algorithms.join.html#torch.distributed.algorithms.Joinable.join_hook">join_hook() (torch.distributed.algorithms.Joinable 方法)</a>

      <ul>
        <li><a href="distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.join_hook">(torch.distributed.optim.ZeroRedundancyOptimizer 方法)</a>
</li>
        <li><a href="generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join_hook">(torch.nn.parallel.DistributedDataParallel 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.algorithms.join.html#torch.distributed.algorithms.Joinable.join_process_group">join_process_group (torch.distributed.algorithms.Joinable 属性)</a>

      <ul>
        <li><a href="distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.join_process_group">(torch.distributed.optim.ZeroRedundancyOptimizer 属性)</a>
</li>
      </ul></li>
      <li><a href="distributed.algorithms.join.html#torch.distributed.algorithms.Joinable">Joinable (torch.distributed.algorithms 中的类)</a>
</li>
      <li><a href="distributed.algorithms.join.html#torch.distributed.algorithms.JoinHook">torch.distributed.algorithms 中的 JoinHook（类）</a>
</li>
      <li><a href="generated/torch.autograd.functional.jvp.html#torch.autograd.functional.jvp">torch.autograd.functional 模块中的 jvp()函数</a>

      <ul>
        <li><a href="generated/torch.func.jvp.html#torch.func.jvp">(在 torch.func 模块中)</a>
</li>
        <li><a href="generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp">(torch.autograd.函数静态方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.jvp">(torch.autograd.function.内联函数静态方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.jvp">(torch.autograd.function.NestedIOFunction 静态方法)</a>
</li>
      </ul></li>
  </ul></td>
</tr></tbody></table>

<h2 id="K">K</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.init.html#torch.nn.init.kaiming_normal_">kaiming_normal_() (在模块 torch.nn.init 中)</a>
</li>
      <li><a href="nn.init.html#torch.nn.init.kaiming_uniform_">kaiming_uniform_() (在模块 torch.nn.init 中)</a>
</li>
      <li><a href="generated/torch.signal.windows.kaiser.html#torch.signal.windows.kaiser">kaiser()（在 torch.signal.windows 模块中）</a>
</li>
      <li><a href="generated/torch.kaiser_window.html#torch.kaiser_window">kaiser_window()（在 torch 模块中）</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.Kernel.html#torch.autograd.profiler_util.Kernel">核（torch.autograd.profiler_util 类）</a>
</li>
      <li><a href="generated/torch.autograd.profiler.profile.key_averages.html#torch.autograd.profiler.profile.key_averages">key_averages()（torch.autograd.profiler.profile 方法）</a>

      <ul>
        <li><a href="profiler.html#torch.profiler._KinetoProfile.key_averages">(torch.profiler._KinetoProfile 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.keys">keys() (torch.autograd.profiler_util.StringTable 方法)</a>

      <ul>
        <li><a href="export.html#torch.export.decomp_utils.CustomDecompTable.keys">(torch.export.decomp_utils.CustomDecompTable 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Tracer.keys">(torch.fx.Tracer 方法)</a>
</li>
        <li><a href="generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.keys">(torch.nn.ModuleDict 方法)</a>
</li>
        <li><a href="generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.keys">(torch.nn.ParameterDict 方法)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.autograd.profiler.KinetoStepTracker.html#torch.autograd.profiler.KinetoStepTracker">KinetoStepTracker (torch.autograd.profiler 中的类)</a>
</li>
      <li><a href="generated/torch.nn.functional.kl_div.html#torch.nn.functional.kl_div">kl_div() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="distributions.html#torch.distributions.kl.kl_divergence">kl_divergence()（在 torch.distributions.kl 模块中）</a>
</li>
      <li><a href="generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss">KLDivLoss（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.kron.html#torch.kron">kron()（在 torch 模块中）</a>
</li>
      <li><a href="generated/torch.kthvalue.html#torch.kthvalue">kthvalue()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.kthvalue.html#torch.Tensor.kthvalue">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.kumaraswamy.Kumaraswamy">Kumaraswamy（torch.distributions.kumaraswamy 类）</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.kv_indices">kv_indices（torch.nn.attention.flex_attention.BlockMask 属性）</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.kv_num_blocks">kv_num_blocks（torch.nn.attention.flex_attention.BlockMask 属性）</a>
</li>
      <li><a href="fx.html#torch.fx.Node.kwargs">kwargs (torch.fx.Node 属性)</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="L">L</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss">l1_loss() (在模块 torch.nn.functional 中)</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.l1_unstructured.html#torch.nn.utils.prune.l1_unstructured">l1_unstructured() (在模块 torch.nn.utils.prune 中)</a>
</li>
      <li><a href="generated/torch.nn.L1Loss.html#torch.nn.L1Loss">L1Loss (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured">torch.nn.utils.prune 中的 L1Unstructured（类）</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR">torch.optim.lr_scheduler 中的 LambdaLR（类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.laplace.Laplace">torch.distributions.laplace 中的 Laplace（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.last_call">torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout 的 last_call 属性</a>
</li>
      <li><a href="generated/torch.nn.functional.layer_norm.html#torch.nn.functional.layer_norm">torch.nn.functional 中的 layer_norm()</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.LayerNorm.html#torch.ao.nn.quantized.LayerNorm">torch.ao.nn.quantized 中的 LayerNorm 类</a>

      <ul>
        <li><a href="generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="tensor_attributes.html#torch.layout">torch 中的 layout 类</a>
</li>
      <li><a href="generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d">LazyBatchNorm1d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d">LazyBatchNorm2d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d">LazyBatchNorm3d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d">LazyConv1d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d">LazyConv2d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d">LazyConv3d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d">LazyConvTranspose1d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d">LazyConvTranspose2d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d">LazyConvTranspose3d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.LazyInstanceNorm1d.html#torch.nn.LazyInstanceNorm1d">LazyInstanceNorm1d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.LazyInstanceNorm2d.html#torch.nn.LazyInstanceNorm2d">LazyInstanceNorm2d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.LazyInstanceNorm3d.html#torch.nn.LazyInstanceNorm3d">LazyInstanceNorm3d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear">LazyLinear（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin">LazyModuleMixin（torch.nn.modules.lazy 中的类）</a>
</li>
      <li><a href="generated/torch.optim.LBFGS.html#torch.optim.LBFGS">LBFGS（torch.optim 中的类）</a>
</li>
      <li><a href="generated/torch.lcm.html#torch.lcm">lcm()（torch 模块中的函数）</a>

      <ul>
        <li><a href="generated/torch.Tensor.lcm.html#torch.Tensor.lcm">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.lcm_.html#torch.Tensor.lcm_">lcm_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.ldexp.html#torch.ldexp">ldexp() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.ldexp_.html#torch.Tensor.ldexp_">ldexp_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.linalg.ldl_factor.html#torch.linalg.ldl_factor">ldl_factor() (在 torch.linalg 模块中)</a>
</li>
      <li><a href="generated/torch.linalg.ldl_factor_ex.html#torch.linalg.ldl_factor_ex">ldl_factor_ex() (在 torch.linalg 模块中)</a>
</li>
      <li><a href="generated/torch.linalg.ldl_solve.html#torch.linalg.ldl_solve">ldl_solve() (在 torch.linalg 模块中)</a>
</li>
      <li><a href="generated/torch.le.html#torch.le">torch 模块中的 le()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.le.html#torch.Tensor.le">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.le_.html#torch.Tensor.le_">torch.Tensor 方法 le_()</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.leaky_relu.html#torch.ao.nn.quantized.functional.leaky_relu">torch.ao.nn.quantized.functional 中的 leaky_relu 类</a>
</li>
      <li><a href="generated/torch.nn.functional.leaky_relu.html#torch.nn.functional.leaky_relu">leaky_relu()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.nn.functional.leaky_relu_.html#torch.nn.functional.leaky_relu_">leaky_relu_()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.LeakyReLU.html#torch.ao.nn.quantized.LeakyReLU">LeakyReLU（torch.ao.nn.quantized 模块中的类）</a>

      <ul>
        <li><a href="generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.lerp.html#torch.lerp">torch 模块中的 lerp()</a>

      <ul>
        <li><a href="generated/torch.Tensor.lerp.html#torch.Tensor.lerp">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.lerp_.html#torch.Tensor.lerp_">torch.Tensor 方法 lerp_()</a>
</li>
      <li><a href="generated/torch.less.html#torch.less">torch 模块中的 less()</a>

      <ul>
        <li><a href="generated/torch.Tensor.less.html#torch.Tensor.less">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.less_.html#torch.Tensor.less_">less_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.less_equal.html#torch.less_equal">less_equal() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.less_equal_.html#torch.Tensor.less_equal_">less_equal_() (torch.Tensor 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.less_than">less_than (在 torch.distributions.constraints 模块中)</a>
</li>
      <li><a href="generated/torch.lgamma.html#torch.lgamma">lgamma() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.lgamma_.html#torch.Tensor.lgamma_">lgamma_() (torch.Tensor 方法)</a>
</li>
      <li><a href="library.html#torch.library.Library">库 (torch.library 中的类)</a>
</li>
      <li><a href="distributed.html#torch.distributed.TCPStore.libuvBackend">libuvBackend (torch.distributed.TCPStore 属性)</a>
</li>
      <li><a href="generated/torch.ao.nn.qat.Linear.html#torch.ao.nn.qat.Linear">Linear (torch.ao.nn.qat 中的类)</a>

      <ul>
        <li><a href="generated/torch.ao.nn.qat.dynamic.Linear.html#torch.ao.nn.qat.dynamic.Linear">(torch.ao.nn.qat.dynamic 类)</a>
</li>
        <li><a href="generated/torch.ao.nn.quantized.Linear.html#torch.ao.nn.quantized.Linear">(torch.ao.nn.quantized 类)</a>
</li>
        <li><a href="generated/torch.ao.nn.quantized.dynamic.Linear.html#torch.ao.nn.quantized.dynamic.Linear">(torch.ao.nn.quantized.dynamic 类)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.functional.linear.html#torch.ao.nn.quantized.functional.linear">线性 (torch.ao.nn.quantized.functional 类)</a>
</li>
      <li><a href="generated/torch.nn.Linear.html#torch.nn.Linear">torch.nn 中的线性（类）</a>
</li>
      <li><a href="generated/torch.nn.functional.linear.html#torch.nn.functional.linear">torch.nn.functional 中的 linear()函数</a>
</li>
      <li><a href="generated/torch.func.linearize.html#torch.func.linearize">torch.func 模块中的 linearize()函数</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR">torch.optim.lr_scheduler 中的 LinearLR 类</a>
</li>
      <li><a href="generated/torch.ao.nn.intrinsic.LinearReLU.html#torch.ao.nn.intrinsic.LinearReLU">torch.ao.nn.intrinsic 中的 LinearReLU（类）</a>

      <ul>
        <li><a href="generated/torch.ao.nn.intrinsic.qat.LinearReLU.html#torch.ao.nn.intrinsic.qat.LinearReLU">torch.ao.nn.intrinsic.qat 中的（类）</a>
</li>
        <li><a href="generated/torch.ao.nn.intrinsic.quantized.LinearReLU.html#torch.ao.nn.intrinsic.quantized.LinearReLU">torch.ao.nn.intrinsic.quantized 中的（类）</a>
</li>
        <li><a href="generated/torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU.html#torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU">torch.ao.nn.intrinsic.quantized.dynamic 中的（类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.linspace.html#torch.linspace">linspace()（在 torch 模块中）</a>
</li>
      <li><a href="fx.html#torch.fx.Graph.lint">lint()（torch.fx.Graph 方法）</a>
</li>
      <li><a href="hub.html#torch.hub.list">list()（在 torch.hub 模块中）</a>
</li>
      <li><a href="generated/torch.compiler.list_backends.html#torch.compiler.list_backends">list_backends()（在 torch.compiler 模块中）</a>
</li>
      <li><a href="generated/torch.cuda.list_gpu_processes.html#torch.cuda.list_gpu_processes">torch.cuda 模块中的 list_gpu_processes()</a>
</li>
      <li><a href="distributions.html#torch.distributions.lkj_cholesky.LKJCholesky">torch.distributions.lkj_cholesky 中的 LKJCholesky 类</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.ln_structured.html#torch.nn.utils.prune.ln_structured">torch.nn.utils.prune 模块中的 ln_structured()</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured">torch.nn.utils.prune 中的 LnStructured 类</a>
</li>
      <li><a href="generated/torch.load.html#torch.load">torch 模块中的 load()函数</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.state_dict_loader.load">torch.distributed.checkpoint.state_dict_loader 模块中</a>
</li>
        <li><a href="export.html#torch.export.load">torch 模块中的 export 函数</a>
</li>
        <li><a href="hub.html#torch.hub.load">torch.hub 模块中</a>
</li>
        <li><a href="generated/torch.jit.load.html#torch.jit.load">(在 torch.jit 模块中)</a>
</li>
        <li><a href="cpp_extension.html#torch.utils.cpp_extension.load">(在 torch.utils.cpp_extension 模块中)</a>
</li>
      </ul></li>
      <li><a href="package.html#torch.package.PackageImporter.load_binary">load_binary() (torch.package.PackageImporter 方法)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.load_bytes">load_bytes() (torch.distributed.checkpoint.LoadPlanner 方法)</a>
</li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.load_inline">torch.utils.cpp_extension 模块中的 load_inline() 函数</a>
</li>
      <li><a href="generated/torch.autograd.profiler.load_nvprof.html#torch.autograd.profiler.load_nvprof">torch.autograd.profiler 模块中的 load_nvprof() 函数</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.load_observer_state_dict.html#torch.ao.quantization.observer.load_observer_state_dict">torch.ao.quantization.observer 类中的 load_observer_state_dict 方法</a>
</li>
      <li><a href="package.html#torch.package.PackageImporter.load_pickle">torch.package.PackageImporter 方法中的 load_pickle() 函数</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.state_dict_loader.load_state_dict">load_state_dict() (在模块 torch.distributed.checkpoint.state_dict_loader 中)</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.stateful.Stateful.load_state_dict">(torch.distributed.checkpoint.stateful.Stateful 方法)</a>
</li>
        <li><a href="distributed.optim.html#torch.distributed.optim.PostLocalSGDOptimizer.load_state_dict">(torch.distributed.optim.PostLocalSGDOptimizer 方法)</a>
</li>
        <li><a href="distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.load_state_dict">(torch.distributed.optim.ZeroRedundancyOptimizer 方法)</a>
</li>
        <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.load_state_dict">(torch.jit.ScriptModule 方法)</a>
</li>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.load_state_dict">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adadelta.html#torch.optim.Adadelta.load_state_dict">(torch.optim.Adadelta 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adafactor.html#torch.optim.Adafactor.load_state_dict">(torch.optim.Adafactor 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adagrad.html#torch.optim.Adagrad.load_state_dict">(torch.optim.Adagrad 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adam.html#torch.optim.Adam.load_state_dict">(torch.optim.Adam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adamax.html#torch.optim.Adamax.load_state_dict">(torch.optim.Adamax 方法)</a>
</li>
        <li><a href="generated/torch.optim.AdamW.html#torch.optim.AdamW.load_state_dict">(torch.optim.AdamW 方法)</a>
</li>
        <li><a href="generated/torch.optim.ASGD.html#torch.optim.ASGD.load_state_dict">(torch.optim.ASGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.LBFGS.html#torch.optim.LBFGS.load_state_dict">(torch.optim.LBFGS 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler.load_state_dict">(torch.optim.lr_scheduler.ChainedScheduler 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR.load_state_dict">(torch.optim.lr_scheduler.ConstantLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR.load_state_dict">(torch.optim.lr_scheduler.CosineAnnealingLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.load_state_dict">(torch.optim.lr_scheduler.CosineAnnealingWarmRestarts 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR.load_state_dict">(torch.optim.lr_scheduler.CyclicLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR.load_state_dict">(torch.optim.lr_scheduler.ExponentialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR.load_state_dict">(torch.optim.lr_scheduler.LambdaLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR.load_state_dict">(torch.optim.lr_scheduler.LinearLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler.load_state_dict">(torch.optim.lr_scheduler.LRScheduler 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict">(torch.optim.lr_scheduler.MultiplicativeLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR.load_state_dict">(torch.optim.lr_scheduler.MultiStepLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR.load_state_dict">(torch.optim.lr_scheduler.OneCycleLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR.load_state_dict">(torch.optim.lr_scheduler.PolynomialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau.load_state_dict">(torch.optim.lr_scheduler.ReduceLROnPlateau 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.load_state_dict">(torch.optim.lr_scheduler.SequentialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR.load_state_dict">(torch.optim.lr_scheduler.StepLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.NAdam.html#torch.optim.NAdam.load_state_dict">(torch.optim.NAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Optimizer.load_state_dict.html#torch.optim.Optimizer.load_state_dict">(torch.optim.Optimizer 方法)</a>
</li>
        <li><a href="generated/torch.optim.RAdam.html#torch.optim.RAdam.load_state_dict">(torch.optim.RAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.RMSprop.html#torch.optim.RMSprop.load_state_dict">(torch.optim.RMSprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.Rprop.html#torch.optim.Rprop.load_state_dict">(torch.optim.Rprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.SGD.html#torch.optim.SGD.load_state_dict">(torch.optim.SGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.load_state_dict">(torch.optim.SparseAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.load_state_dict">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.SWALR.html#torch.optim.swa_utils.SWALR.load_state_dict">(torch.optim.swa_utils.SWALR 方法)</a>
</li>
      </ul></li>
      <li><a href="hub.html#torch.hub.load_state_dict_from_url">load_state_dict_from_url() (在模块 torch.hub 中)</a>
</li>
      <li><a href="generated/torch.cuda.gds.GdsFile.html#torch.cuda.gds.GdsFile.load_storage">load_storage() (torch.cuda.gds.GdsFile 方法)</a>
</li>
      <li><a href="package.html#torch.package.PackageImporter.load_text">load_text() (torch.package.PackageImporter 方法)</a>
</li>
      <li><a href="model_zoo.html#torch.utils.model_zoo.load_url">load_url() (在模块 torch.utils.model_zoo 中)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlan">LoadPlan (torch.distributed.checkpoint 中的类)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner">torch.distributed.checkpoint 中的 LoadPlanner（类）</a>
</li>
      <li><a href="generated/torch.lobpcg.html#torch.lobpcg">torch 模块中的 lobpcg()</a>
</li>
      <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.loc">torch.distributions.log_normal.LogNormal 属性中的 loc</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.experimental.local_map">torch.distributed.tensor.experimental 模块中的 local_map()</a>
</li>
      <li><a href="generated/torch.nn.functional.local_response_norm.html#torch.nn.functional.local_response_norm">torch.nn.functional 模块中的 local_response_norm()函数</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.PyRRef.local_value">torch.distributed.rpc.PyRRef 方法中的 local_value()</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent">torch.distributed.elastic.agent.server.local_elastic_agent 中的 LocalElasticAgent 类</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.LocalOptimStateDictConfig">torch.distributed.fsdp 中的 LocalOptimStateDictConfig 类</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.nn.LocalResponseNorm.html#torch.nn.LocalResponseNorm">torch.nn 中的 LocalResponseNorm（类）</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.LocalStateDictConfig">torch.distributed.fsdp 中的 LocalStateDictConfig（类）</a>
</li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.LocalTimerClient">torch.distributed.elastic.timer 中的 LocalTimerClient（类）</a>
</li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.LocalTimerServer">torch.distributed.elastic.timer 中的 LocalTimerServer（类）</a>
</li>
      <li><a href="generated/torch.log.html#torch.log">torch 模块中的 log()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.log.html#torch.Tensor.log">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.log10.html#torch.log10">torch 模块中的 log10()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.log10.html#torch.Tensor.log10">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.log10_.html#torch.Tensor.log10_">log10_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.log1p.html#torch.log1p">torch 模块中的 log1p()</a>

      <ul>
        <li><a href="special.html#torch.special.log1p">torch.special 模块中</a>
</li>
        <li><a href="generated/torch.Tensor.log1p.html#torch.Tensor.log1p">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_">log1p_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.log2.html#torch.log2">log2() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.log2.html#torch.Tensor.log2">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.log2_.html#torch.Tensor.log2_">log2_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.log_.html#torch.Tensor.log_">log_() (torch.Tensor 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.Transform.log_abs_det_jacobian">log_abs_det_jacobian() (torch.distributions.transforms 转换方法)</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.debug.CommDebugMode.log_comm_debug_tracing_table_to_file">log_comm_debug_tracing_table_to_file() (torch.distributed.tensor.debug.CommDebugMode 方法)</a>
</li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.debug_info_logging.log_debug_info_for_expired_timers">log_debug_info_for_expired_timers() (在模块 torch.distributed.elastic.timer.debug_info_logging 中)</a>
</li>
      <li><a href="monitor.html#torch.monitor.log_event">torch.monitor 模块中的 log_event()函数</a>
</li>
      <li><a href="special.html#torch.special.log_ndtr">torch.special 模块中的 log_ndtr()函数</a>
</li>
      <li><a href="generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_">torch.Tensor 方法中的 log_normal_()</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.log_prob">torch.distributions.bernoulli.Bernoulli 方法中的 log_prob()函数</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.log_prob">(torch.distributions.beta.Beta 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.log_prob">(torch.distributions.binomial.Binomial 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.log_prob">(torch.distributions.categorical.Categorical 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.log_prob">(torch.distributions.cauchy.Cauchy 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.log_prob">(torch.distributions.dirichlet.Dirichlet 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.log_prob">(torch.distributions.distribution.Distribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.log_prob">(torch.distributions.exponential.Exponential 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.log_prob">(torch.distributions.fishersnedecor.FisherSnedecor 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.log_prob">(torch.distributions.gamma.Gamma 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.log_prob">(torch.distributions.geometric.Geometric 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.log_prob">(torch.distributions.gumbel.Gumbel 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_cauchy.HalfCauchy.log_prob">(torch.distributions.half_cauchy.HalfCauchy 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_normal.HalfNormal.log_prob">(torch.distributions.half_normal.HalfNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.log_prob">(torch.distributions.independent.Independent 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.log_prob">(torch.distributions.laplace.Laplace 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lkj_cholesky.LKJCholesky.log_prob">(torch.distributions.lkj_cholesky.LKJCholesky 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob">(torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.log_prob">(torch.distributions.mixture_same_family.MixtureSameFamily 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.log_prob">(torch.distributions.multinomial.Multinomial 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.log_prob">(torch.distributions.multivariate_normal.MultivariateNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.negative_binomial.NegativeBinomial.log_prob">(torch.distributions.negative_binomial.NegativeBinomial 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.log_prob">(torch.distributions.normal.Normal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob">(torch.distributions.one_hot_categorical.OneHotCategorical 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.log_prob">(torch.distributions.poisson.Poisson 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob">(torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.log_prob">(torch.distributions.studentT.StudentT 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.log_prob">(torch.distributions.transformed_distribution.TransformedDistribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.log_prob">(torch.distributions.uniform.Uniform 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.von_mises.VonMises.log_prob">(torch.distributions.von_mises.VonMises 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.wishart.Wishart.log_prob">(torch.distributions.wishart.Wishart 方法)</a>
</li>
        <li><a href="generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob">(torch.nn.AdaptiveLogSoftmaxWithLoss 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax">log_softmax()（在 torch.nn.functional 模块中）</a>

      <ul>
        <li><a href="generated/torch.sparse.log_softmax.html#torch.sparse.log_softmax">（在 torch.sparse 模块中）</a>
</li>
        <li><a href="special.html#torch.special.log_softmax">（在 torch.special 模块中）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.logaddexp.html#torch.logaddexp">logaddexp()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.logaddexp.html#torch.Tensor.logaddexp">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.logaddexp2.html#torch.logaddexp2">logaddexp2() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.logaddexp2.html#torch.Tensor.logaddexp2">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.logcumsumexp.html#torch.logcumsumexp">logcumsumexp() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.logcumsumexp.html#torch.Tensor.logcumsumexp">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.logdet.html#torch.logdet">logdet() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.logdet.html#torch.Tensor.logdet">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Logger">Logger (torch.ao.ns._numeric_suite 中的类)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.loggers_set_enabled">torch.ao.ns._numeric_suite_fx 模块中的 loggers_set_enabled() 函数</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.loggers_set_save_activations">torch.ao.ns._numeric_suite_fx 模块中的 loggers_set_save_activations() 函数</a>
</li>
      <li><a href="generated/torch.logical_and.html#torch.logical_and">torch 模块中的 logical_and() 函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.logical_and_.html#torch.Tensor.logical_and_">logical_and_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.logical_not.html#torch.logical_not">logical_not() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.logical_not_.html#torch.Tensor.logical_not_">logical_not_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.logical_or.html#torch.logical_or">torch 模块中的 logical_or()</a>

      <ul>
        <li><a href="generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.logical_or_.html#torch.Tensor.logical_or_">torch.Tensor 方法中的 logical_or_()</a>
</li>
      <li><a href="generated/torch.logical_xor.html#torch.logical_xor">torch 模块中的 logical_xor()</a>

      <ul>
        <li><a href="generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.logical_xor_.html#torch.Tensor.logical_xor_">torch.Tensor 方法中的 logical_xor_()</a>
</li>
      <li><a href="generated/torch.logit.html#torch.logit">torch 模块中的 logit()</a>

      <ul>
        <li><a href="special.html#torch.special.logit">（在 torch.special 模块中）</a>
</li>
        <li><a href="generated/torch.Tensor.logit.html#torch.Tensor.logit">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.logit_.html#torch.Tensor.logit_">torch.Tensor 方法中的 logit_()</a>
</li>
      <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli">torch.distributions.relaxed_bernoulli 中的 LogitRelaxedBernoulli（类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.logits">torch.distributions.bernoulli.Bernoulli 属性：logits</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.logits">(torch.distributions.binomial.Binomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.logits">(torch.distributions.categorical.Categorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.logits">(torch.distributions.geometric.Geoemtric 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.logits">(torch.distributions.multinomial.Multinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.negative_binomial.NegativeBinomial.logits">(torch.distributions.negative_binomial.NegativeBinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.logits">(torch.distributions.one_hot_categorical.OneHotCategorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits">(torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits">(torch.distributions.relaxed_bernoulli.RelaxedBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits">(torch.distributions.relaxed_categorical.RelaxedOneHotCategorical 属性)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.log_normal.LogNormal">LogNormal（torch.distributions.log_normal 中的类）</a>
</li>
      <li><a href="elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.LogsDest">LogsDest（torch.distributed.elastic.multiprocessing.api 中的类）</a>
</li>
      <li><a href="generated/torch.nn.LogSigmoid.html#torch.nn.LogSigmoid">LogSigmoid（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.functional.logsigmoid.html#torch.nn.functional.logsigmoid">logsigmoid()（torch.nn.functional 模块中的函数）</a>
</li>
      <li><a href="generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax">LogSoftmax（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.logspace.html#torch.logspace">torch 模块中的 logspace()</a>
</li>
      <li><a href="elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.LogsSpecs">torch.distributed.elastic.multiprocessing.api 中的 LogsSpecs 类</a>
</li>
      <li><a href="generated/torch.logsumexp.html#torch.logsumexp">torch 模块中的 logsumexp()</a>

      <ul>
        <li><a href="special.html#torch.special.logsumexp">（在 torch.special 模块中）</a>
</li>
        <li><a href="generated/torch.Tensor.logsumexp.html#torch.Tensor.logsumexp">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.long.html#torch.Tensor.long">torch.Tensor 方法中的 long()</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.long">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.long">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.LongStorage">LongStorage（torch 中的类）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.DefaultSavePlanner.lookup_object">lookup_object()（torch.distributed.checkpoint.DefaultSavePlanner 方法）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.DefaultLoadPlanner.lookup_tensor">lookup_tensor()（torch.distributed.checkpoint.DefaultLoadPlanner 方法）</a>
</li>
      <li><a href="distributed.tensor.parallel.html#torch.distributed.tensor.parallel.loss_parallel">loss_parallel() (在模块 torch.distributed.tensor.parallel 中)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.LowerCholeskyTransform">LowerCholeskyTransform (torch.distributions.transforms 中的类)</a>
</li>
      <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal">torch.distributions.lowrank_multivariate_normal（类）的 LowRankMultivariateNormal</a>
</li>
      <li><a href="generated/torch.nn.functional.lp_pool1d.html#torch.nn.functional.lp_pool1d">torch.nn.functional 模块中的 lp_pool1d()函数</a>
</li>
      <li><a href="generated/torch.nn.functional.lp_pool2d.html#torch.nn.functional.lp_pool2d">torch.nn.functional 模块中的 lp_pool2d()</a>
</li>
      <li><a href="generated/torch.nn.functional.lp_pool3d.html#torch.nn.functional.lp_pool3d">torch.nn.functional 模块中的 lp_pool3d()</a>
</li>
      <li><a href="generated/torch.nn.LPPool1d.html#torch.nn.LPPool1d">torch.nn 模块中的 LPPool1d 类</a>
</li>
      <li><a href="generated/torch.nn.LPPool2d.html#torch.nn.LPPool2d">torch.nn 模块中的 LPPool2d 类</a>
</li>
      <li><a href="generated/torch.nn.LPPool3d.html#torch.nn.LPPool3d">torch.nn 中的 LPPool3d（类）</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler">torch.optim.lr_scheduler 中的 LRScheduler（类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.lru_cache.html#torch.fx.experimental.symbolic_shapes.lru_cache">torch.fx.experimental.symbolic_shapes 模块中的 lru_cache()函数</a>
</li>
      <li><a href="generated/torch.ao.nn.quantizable.LSTM.html#torch.ao.nn.quantizable.LSTM">torch.ao.nn.quantizable 中的 LSTM（类）</a>

      <ul>
        <li><a href="generated/torch.ao.nn.quantized.dynamic.LSTM.html#torch.ao.nn.quantized.dynamic.LSTM">(torch.ao.nn.quantized.dynamic 类)</a>
</li>
        <li><a href="generated/torch.nn.LSTM.html#torch.nn.LSTM">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.dynamic.LSTMCell.html#torch.ao.nn.quantized.dynamic.LSTMCell">LSTMCell（torch.ao.nn.quantized.dynamic 类）</a>

      <ul>
        <li><a href="generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.linalg.lstsq.html#torch.linalg.lstsq">lstsq()（torch.linalg 模块）</a>
</li>
      <li><a href="generated/torch.lt.html#torch.lt">lt()（torch 模块）</a>

      <ul>
        <li><a href="generated/torch.Tensor.lt.html#torch.Tensor.lt">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.lt_.html#torch.Tensor.lt_">lt_()（torch.Tensor 方法）</a>
</li>
      <li><a href="generated/torch.lu.html#torch.lu">torch 模块中的 lu()函数</a>

      <ul>
        <li><a href="generated/torch.linalg.lu.html#torch.linalg.lu">(在 torch.linalg 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.lu.html#torch.Tensor.lu">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor">torch.linalg 模块中的 lu_factor()函数</a>
</li>
      <li><a href="generated/torch.linalg.lu_factor_ex.html#torch.linalg.lu_factor_ex">torch.linalg 模块中的 lu_factor_ex()函数</a>
</li>
      <li><a href="generated/torch.lu_solve.html#torch.lu_solve">torch 模块中的 lu_solve()函数</a>

      <ul>
        <li><a href="generated/torch.linalg.lu_solve.html#torch.linalg.lu_solve">(在 torch.linalg 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.lu_solve.html#torch.Tensor.lu_solve">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.lu_unpack.html#torch.lu_unpack">torch 模块中的 lu_unpack()</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="M">M</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributed.algorithms.join.html#torch.distributed.algorithms.JoinHook.main_hook">torch.distributed.algorithms.JoinHook 方法中的 main_hook()</a>
</li>
      <li><a href="generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual">torch.autograd.forward_ad 模块中的 make_dual()</a>
</li>
      <li><a href="generated/torch.fx.experimental.proxy_tensor.make_fx.html#torch.fx.experimental.proxy_tensor.make_fx">torch.fx.experimental.proxy_tensor 模块中的 make_fx()</a>
</li>
      <li><a href="generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables">make_graphed_callables()（在 torch.cuda 模块中）</a>
</li>
      <li><a href="testing.html#torch.testing.make_tensor">make_tensor()（在 torch.testing 模块中）</a>
</li>
      <li><a href="generated/torch.manual_seed.html#torch.manual_seed">manual_seed()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.cuda.manual_seed.html#torch.cuda.manual_seed">(在模块 torch.cuda 中)</a>
</li>
        <li><a href="generated/torch.mps.manual_seed.html#torch.mps.manual_seed">（在模块 torch.mps 中）</a>
</li>
        <li><a href="random.html#torch.random.manual_seed">(在模块 torch.random 中)</a>
</li>
        <li><a href="generated/torch.xpu.manual_seed.html#torch.xpu.manual_seed">(在模块 torch.xpu 中)</a>
</li>
        <li><a href="generated/torch.Generator.html#torch.Generator.manual_seed">(torch.Generator 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.manual_seed_all.html#torch.cuda.manual_seed_all">manual_seed_all()（在 torch.cuda 模块中）</a>

      <ul>
        <li><a href="generated/torch.xpu.manual_seed_all.html#torch.xpu.manual_seed_all">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.map_.html#torch.Tensor.map_">map_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html#torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.map_nodes_to_values">map_nodes_to_values() (torch.fx.experimental.symbolic_shapes PropagateUnbackedSymInts 方法)</a>

      <ul>
        <li><a href="fx.html#torch.fx.Interpreter.map_nodes_to_values">(torch.fx.Interpreter 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.observer.MappingType.html#torch.ao.quantization.observer.MappingType">MappingType (torch.ao.quantization.observer 类)</a>
</li>
      <li><a href="generated/torch.nn.functional.margin_ranking_loss.html#torch.nn.functional.margin_ranking_loss">margin_ranking_loss() (模块 torch.nn.functional 中的函数)</a>
</li>
      <li><a href="generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss">MarginRankingLoss (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.cuda.nvtx.mark.html#torch.cuda.nvtx.mark">mark() (torch.cuda.nvtx 模块中)</a>

      <ul>
        <li><a href="profiler.html#torch.profiler.itt.mark">torch.profiler.itt 模块中</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.mark_dirty">mark_dirty() (torch.autograd.function.BackwardCFunction 方法)</a>

      <ul>
        <li><a href="generated/torch.autograd.function.FunctionCtx.mark_dirty.html#torch.autograd.function.FunctionCtx.mark_dirty">(torch.autograd.function.FunctionCtx 方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.mark_dirty">(torch.autograd.function.InplaceFunction 方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.mark_dirty">(torch.autograd.function.NestedIOFunction 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.mark_non_differentiable">mark_non_differentiable() (torch.autograd.function.BackwardCFunction 方法)</a>

      <ul>
        <li><a href="generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html#torch.autograd.function.FunctionCtx.mark_non_differentiable">(torch.autograd.function.FunctionCtx 方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.mark_non_differentiable">(torch.autograd.function.InplaceFunction 方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.mark_non_differentiable">(torch.autograd.function.NestedIOFunction 方法)</a>
</li>
      </ul></li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.mask_mod">mask_mod (torch.nn.attention.flex_attention.BlockMask 属性)</a>
</li>
      <li><a href="generated/torch.Tensor.masked_fill.html#torch.Tensor.masked_fill">masked_fill() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_">masked_fill_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.masked_scatter.html#torch.Tensor.masked_scatter">masked_scatter() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_">masked_scatter_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.masked_select.html#torch.masked_select">masked_select() (在 torch 模块中)</a>

      <ul>
        <li><a href="nested.html#torch.nested.masked_select">torch.nested 模块中</a>
</li>
        <li><a href="generated/torch.Tensor.masked_select.html#torch.Tensor.masked_select">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="export.html#torch.export.decomp_utils.CustomDecompTable.materialize">materialize() (torch.export.decomp_utils.CustomDecompTable 方法)</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.math_sdp_enabled">math_sdp_enabled() (在 torch.backends.cuda 模块中)</a>
</li>
      <li><a href="generated/torch.matmul.html#torch.matmul">matmul()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.linalg.matmul.html#torch.linalg.matmul">(在 torch.linalg 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.matmul.html#torch.Tensor.matmul">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.matrix_exp.html#torch.matrix_exp">torch 模块中的 matrix_exp()</a>

      <ul>
        <li><a href="generated/torch.linalg.matrix_exp.html#torch.linalg.matrix_exp">(在 torch.linalg 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.matrix_exp.html#torch.Tensor.matrix_exp">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.linalg.matrix_norm.html#torch.linalg.matrix_norm">torch.linalg 模块中的 matrix_norm()</a>
</li>
      <li><a href="generated/torch.matrix_power.html#torch.matrix_power">torch 模块中的 matrix_power()</a>

      <ul>
        <li><a href="generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power">(在 torch.linalg 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.linalg.matrix_rank.html#torch.linalg.matrix_rank">torch.linalg 模块中的 matrix_rank()</a>
</li>
      <li><a href="generated/torch.max.html#torch.max">torch 模块中的 max()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.max.html#torch.Tensor.max">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated">torch.cuda 模块中的 max_memory_allocated()函数</a>

      <ul>
        <li><a href="generated/torch.xpu.max_memory_allocated.html#torch.xpu.max_memory_allocated">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.max_memory_cached.html#torch.cuda.max_memory_cached">torch.cuda 模块中的 max_memory_cached()函数</a>
</li>
      <li><a href="generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved">torch.cuda 模块中的 max_memory_reserved()函数</a>

      <ul>
        <li><a href="generated/torch.xpu.max_memory_reserved.html#torch.xpu.max_memory_reserved">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.functional.max_pool1d.html#torch.ao.nn.quantized.functional.max_pool1d">max_pool1d（torch.ao.nn.quantized.functional 中的类）</a>
</li>
      <li><a href="generated/torch.nn.functional.max_pool1d.html#torch.nn.functional.max_pool1d">max_pool1d()（torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.max_pool2d.html#torch.ao.nn.quantized.functional.max_pool2d">max_pool2d（torch.ao.nn.quantized.functional 中的类）</a>
</li>
      <li><a href="generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d">max_pool2d()（在模块 torch.nn.functional 中）</a>
</li>
      <li><a href="generated/torch.nn.functional.max_pool3d.html#torch.nn.functional.max_pool3d">max_pool3d()（在模块 torch.nn.functional 中）</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.cufft_plan_cache.max_size">max_size（在模块 torch.backends.cuda.cufft_plan_cache 中）</a>
</li>
      <li><a href="generated/torch.nn.functional.max_unpool1d.html#torch.nn.functional.max_unpool1d">max_unpool1d()（在模块 torch.nn.functional 中）</a>
</li>
      <li><a href="generated/torch.nn.functional.max_unpool2d.html#torch.nn.functional.max_unpool2d">max_unpool2d()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.nn.functional.max_unpool3d.html#torch.nn.functional.max_unpool3d">max_unpool3d()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.maximum.html#torch.maximum">maximum()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.maximum.html#torch.Tensor.maximum">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d">MaxPool1d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d">MaxPool2d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d">MaxPool3d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.MaxUnpool1d.html#torch.nn.MaxUnpool1d">MaxUnpool1d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d">MaxUnpool2d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.MaxUnpool3d.html#torch.nn.MaxUnpool3d">MaxUnpool3d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.proxy_tensor.maybe_disable_thunkify.html#torch.fx.experimental.proxy_tensor.maybe_disable_thunkify">maybe_disable_thunkify()（在 torch.fx.experimental.proxy_tensor 模块中）</a>
</li>
      <li><a href="generated/torch.fx.experimental.proxy_tensor.maybe_enable_thunkify.html#torch.fx.experimental.proxy_tensor.maybe_enable_thunkify">maybe_enable_thunkify()（在 torch.fx.experimental.proxy_tensor 模块中）</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.mean">mean (torch.distributions.bernoulli.Bernoulli 属性)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.mean">(torch.distributions.beta.Beta 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.mean">(torch.distributions.binomial.Binomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.mean">(torch.distributions.categorical.Categorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.mean">(torch.distributions.cauchy.Cauchy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.mean">(torch.distributions.dirichlet.Dirichlet 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.mean">(torch.distributions.distribution.Distribution 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.mean">(torch.distributions.exponential.Exponential 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.mean">(torch.distributions.fishersnedecor.FisherSnedecor 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.mean">(torch.distributions.gamma.Gamma 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.mean">(torch.distributions.geometric.Geoemtric 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.mean">(torch.distributions.gumbel.Gumbel 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_cauchy.HalfCauchy.mean">(torch.distributions.half_cauchy.HalfCauchy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_normal.HalfNormal.mean">(torch.distributions.half_normal.HalfNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.mean">(torch.distributions.independent.Independent 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.inverse_gamma.InverseGamma.mean">(torch.distributions.inverse_gamma.InverseGamma 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.mean">(torch.distributions.kumaraswamy.Kumaraswamy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.mean">(torch.distributions.laplace.Laplace 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.mean">(torch.distributions.log_normal.LogNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean">(torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.mean">(torch.distributions.mixture_same_family.MixtureSameFamily 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.mean">(torch.distributions.multinomial.Multinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.mean">(torch.distributions.multivariate_normal.MultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.negative_binomial.NegativeBinomial.mean">(torch.distributions.negative_binomial.NegativeBinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.mean">(torch.distributions.normal.Normal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.mean">(torch.distributions.one_hot_categorical.OneHotCategorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.pareto.Pareto.mean">(torch.distributions.pareto.Pareto 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.mean">(torch.distributions.poisson.Poisson 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.mean">(torch.distributions.studentT.StudentT 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.mean">(torch.distributions.uniform.Uniform 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.von_mises.VonMises.mean">(torch.distributions.von_mises.VonMises 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.weibull.Weibull.mean">(torch.distributions.weibull.Weibull 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.wishart.Wishart.mean">(torch.distributions.wishart.Wishart 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.mean.html#torch.mean">mean() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.mean.html#torch.Tensor.mean">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Measurement">torch.utils.benchmark 中的测量（类）</a>
</li>
      <li><a href="generated/torch.median.html#torch.median">torch 模块中的 median()</a>

      <ul>
        <li><a href="generated/torch.Tensor.median.html#torch.Tensor.median">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="backends.html#torch.backends.cuda.mem_efficient_sdp_enabled">torch.backends.cuda 模块中的 mem_efficient_sdp_enabled()</a>
</li>
      <li><a href="generated/torch.cuda.mem_get_info.html#torch.cuda.mem_get_info">torch.cuda 模块中的 mem_get_info()</a>

      <ul>
        <li><a href="generated/torch.xpu.mem_get_info.html#torch.xpu.mem_get_info">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated">torch.cuda 模块中的 memory_allocated()</a>

      <ul>
        <li><a href="generated/torch.xpu.memory_allocated.html#torch.xpu.memory_allocated">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.memory_cached.html#torch.cuda.memory_cached">torch.cuda 模块中的 memory_cached()</a>
</li>
      <li><a href="tensor_attributes.html#torch.memory_format">torch 中的 memory_format 类</a>
</li>
      <li><a href="generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved">torch.cuda 模块中的 memory_reserved()</a>

      <ul>
        <li><a href="generated/torch.xpu.memory_reserved.html#torch.xpu.memory_reserved">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.memory_snapshot.html#torch.cuda.memory_snapshot">torch.cuda 模块中的 memory_snapshot()</a>
</li>
      <li><a href="generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats">torch.cuda 模块中的 memory_stats()</a>

      <ul>
        <li><a href="generated/torch.mtia.memory_stats.html#torch.mtia.memory_stats">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="generated/torch.mtia.memory.memory_stats.html#torch.mtia.memory.memory_stats">torch.mtia.memory 模块中</a>
</li>
        <li><a href="generated/torch.xpu.memory_stats.html#torch.xpu.memory_stats">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.xpu.memory_stats_as_nested_dict.html#torch.xpu.memory_stats_as_nested_dict">memory_stats_as_nested_dict() (在 torch.xpu 模块中)</a>
</li>
      <li><a href="generated/torch.cuda.memory_summary.html#torch.cuda.memory_summary">memory_summary() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="generated/torch.cuda.memory_usage.html#torch.cuda.memory_usage">memory_usage() (在 torch.cuda 模块中)</a>
</li>
      <li><a href="generated/torch.cuda.MemPool.html#torch.cuda.MemPool">torch.cuda 中的 MemPool（类）</a>
</li>
      <li><a href="generated/torch.cuda.MemPoolContext.html#torch.cuda.MemPoolContext">torch.cuda 中的 MemPoolContext（类）</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.MemRecordsAcc.html#torch.autograd.profiler_util.MemRecordsAcc">torch.autograd.profiler_util 中的 MemRecordsAcc（类）</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Measurement.merge">merge()（torch.utils.benchmark.Measurement 静态方法）</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.microbatch.merge_chunks">merge_chunks() (在模块 torch.distributed.pipelining.microbatch 中)</a>
</li>
      <li><a href="generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention.merge_masks">merge_masks() (torch.nn.MultiheadAttention 方法)</a>
</li>
      <li><a href="generated/torch.meshgrid.html#torch.meshgrid">torch 模块中的 meshgrid()</a>
</li>
      <li><a href="generated/torch.autograd.graph.Node.metadata.html#torch.autograd.graph.Node.metadata">torch.autograd.graph.Node 方法中的 metadata()</a>
</li>
      <li><a href="generated/torch.mps.profiler.metal_capture.html#torch.mps.profiler.metal_capture">torch.mps.profiler 模块中的 metal_capture()</a>
</li>
      <li><a href="elastic/metrics.html#torch.distributed.elastic.metrics.api.MetricHandler">torch.distributed.elastic.metrics.api 中的 MetricHandler 类</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.mgpu_tune_gemm_in_file">torch.cuda.tunable 模块中的 mgpu_tune_gemm_in_file()函数</a>
</li>
      <li><a href="tensors.html#torch.Tensor.mH">torch.Tensor 属性 mH</a>
</li>
      <li><a href="generated/torch.min.html#torch.min">torch 模块中的 min()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.min.html#torch.Tensor.min">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.minimum.html#torch.minimum">torch 模块中的 minimum()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.minimum.html#torch.Tensor.minimum">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver">torch.ao.quantization.observer 中的 MinMaxObserver 类</a>
</li>
      <li><a href="generated/torch.nn.Mish.html#torch.nn.Mish">torch.nn 中的 Mish 类</a>
</li>
      <li><a href="generated/torch.nn.functional.mish.html#torch.nn.functional.mish">torch.nn.functional 模块中的 mish()函数</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.MixedPrecision">torch.distributed.fsdp 中的 MixedPrecision 类</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.MixedPrecisionPolicy">torch.distributed.fsdp 中的 MixedPrecisionPolicy 类</a>
</li>
      <li><a href="distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution">torch.distributions.mixture_same_family.MixtureSameFamily 属性中的 mixture_distribution</a>
</li>
      <li><a href="distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily">torch.distributions.mixture_same_family 中的 MixtureSameFamily（类）</a>
</li>
      <li><a href="generated/torch.mm.html#torch.mm">torch 模块中的 mm()函数</a>

      <ul>
        <li><a href="generated/torch.sparse.mm.html#torch.sparse.mm">（在 torch.sparse 模块中）</a>
</li>
        <li><a href="generated/torch.Tensor.mm.html#torch.Tensor.mm">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="package.html#torch.package.PackageExporter.mock">mock() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.mocked_modules">mocked_modules() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.mode">mode (torch.distributions.bernoulli.Bernoulli 属性)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.mode">(torch.distributions.beta.Beta 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.mode">(torch.distributions.binomial.Binomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.mode">(torch.distributions.categorical.Categorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.mode">(torch.distributions.cauchy.Cauchy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.mode">(torch.distributions.dirichlet.Dirichlet 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.mode">(torch.distributions.distribution.Distribution 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.mode">(torch.distributions.exponential.Exponential 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.mode">(torch.distributions.fishersnedecor.FisherSnedecor 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.mode">(torch.distributions.gamma.Gamma 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.mode">(torch.distributions.geometric.Geoemtric 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.mode">(torch.distributions.gumbel.Gumbel 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_cauchy.HalfCauchy.mode">(torch.distributions.half_cauchy.HalfCauchy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_normal.HalfNormal.mode">(torch.distributions.half_normal.HalfNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.mode">(torch.distributions.independent.Independent 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.inverse_gamma.InverseGamma.mode">(torch.distributions.inverse_gamma.InverseGamma 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.mode">(torch.distributions.kumaraswamy.Kumaraswamy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.mode">(torch.distributions.laplace.Laplace 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.mode">(torch.distributions.log_normal.LogNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mode">(torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.mode">(torch.distributions.multivariate_normal.MultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.negative_binomial.NegativeBinomial.mode">(torch.distributions.negative_binomial.NegativeBinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.mode">(torch.distributions.normal.Normal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.mode">(torch.distributions.one_hot_categorical.OneHotCategorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.pareto.Pareto.mode">(torch.distributions.pareto.Pareto 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.mode">(torch.distributions.poisson.Poisson 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.mode">(torch.distributions.studentT.StudentT 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.mode">(torch.distributions.uniform.Uniform 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.von_mises.VonMises.mode">(torch.distributions.von_mises.VonMises 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.weibull.Weibull.mode">(torch.distributions.weibull.Weibull 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.wishart.Wishart.mode">(torch.distributions.wishart.Wishart 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.mode.html#torch.mode">mode() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.mode.html#torch.Tensor.mode">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.pt2e.export_utils.model_is_exported.html#torch.ao.quantization.pt2e.export_utils.model_is_exported">model_is_exported (torch.ao.quantization.pt2e.export_utils 类)</a>
</li>
      <li><a href="onnx_dynamo.html#torch.onnx.ONNXProgram.model_proto">model_proto (torch.onnx.ONNXProgram 属性)</a>
</li>
      <li>模块<ul>
        <li><a href="torch.html#module-torch">torch</a>
</li>
        <li><a href="config_mod.html#module-torch.__config__">torch.__config__</a>
</li>
        <li><a href="future_mod.html#module-torch.__future__">torch.__future__</a>
</li>
        <li><a href="logging.html#module-torch._logging">torch._logging</a>
</li>
        <li><a href="accelerator.html#module-torch.accelerator">torch.accelerator</a>
</li>
        <li><a href="amp.html#module-torch.amp">torch.amp</a>
</li>
        <li><a href="amp.html#module-torch.amp.autocast_mode">torch.amp.autocast_mode</a>
</li>
        <li><a href="amp.html#module-torch.amp.grad_scaler">torch.amp.grad_scaler</a>
</li>
        <li><a href="quantization.html#module-torch.ao">torch.ao</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn">torch.ao.nn</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic">torch.ao.nn.intrinsic</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.modules">torch.ao.nn.intrinsic.modules</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.modules.fused">torch.ao.nn.intrinsic.modules.fused</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.qat">torch.ao.nn.intrinsic.qat</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.qat.modules">torch.ao.nn.intrinsic.qat.modules</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.qat.modules.conv_fused">torch.ao.nn.intrinsic.qat.modules.conv_fused</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.qat.modules.linear_fused">torch.ao.nn.intrinsic.qat.modules.linear_fused</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.qat.modules.linear_relu">torch.ao.nn.intrinsic.qat.modules.linear_relu</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.quantized">torch.ao.nn.intrinsic.quantized</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.quantized.dynamic">torch.ao.nn.intrinsic.quantized.dynamic</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.quantized.dynamic.modules">torch.ao.nn.intrinsic.quantized.dynamic.modules</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu">torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.quantized.modules">torch.ao.nn.intrinsic.quantized.modules</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.bn_relu">torch.ao.nn.intrinsic.quantized.modules.bn_relu</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.conv_add">torch.ao.nn.intrinsic.quantized.modules.conv_add</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.conv_relu">torch.ao.nn.intrinsic.quantized.modules.conv_relu</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.linear_relu">torch.ao.nn.intrinsic.quantized.modules.linear_relu</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.qat">torch.ao.nn.qat</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.qat.dynamic">torch.ao.nn.qat.dynamic</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.qat.dynamic.modules">torch.ao.nn.qat.dynamic.modules</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.qat.dynamic.modules.linear">torch.ao.nn.qat.dynamic.modules.linear</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.qat.modules">torch.ao.nn.qat.modules</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.qat.modules.conv">torch.ao.nn.qat.modules.conv</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.qat.modules.embedding_ops">torch.ao.nn.qat.modules.embedding_ops</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.qat.modules.linear">torch.ao.nn.qat.模块.linear</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantizable">torch.ao.nn.quantizable</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantizable.modules">torch.ao.nn.quantizable.模块</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantizable.modules.activation">torch.ao.nn.quantizable.模块.activation</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantizable.modules.rnn">torch.ao.nn.quantizable.modules.rnn</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized">torch.ao.nn.quantized</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.quantized.dynamic">torch.ao.nn.quantized.dynamic</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.quantized.dynamic.modules">torch.ao.nn.quantized.dynamic.modules</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.dynamic.modules.conv">torch.ao.nn.quantized.dynamic.modules.conv</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.dynamic.modules.linear">torch.ao.nn.quantized.dynamic.modules.linear</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.dynamic.modules.rnn">torch.ao.nn.quantized.dynamic.modules.rnn</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.quantized.functional">torch.ao.nn.quantized.functional</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.nn.quantized.modules">torch.ao.nn.quantized.modules</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.activation">torch.ao.nn.quantized.modules.activation</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.batchnorm">torch.ao.nn.quantized.modules.batchnorm</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.conv">torch.ao.nn.quantized.modules.conv</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.dropout">torch.ao.nn.quantized.modules.dropout</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.embedding_ops">torch.ao.nn.quantized.modules.embedding_ops</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.functional_modules">torch.ao.nn.quantized.modules.functional_modules</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.linear">torch.ao.nn.quantized.modules.linear</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.normalization">torch.ao.nn.quantized.modules.normalization</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.rnn">torch.ao.nn.quantized.modules.rnn</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.utils">torch.ao.nn.quantized.modules.utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference">torch.ao.nn.quantized.reference</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference.modules">torch.ao.nn.quantized.reference.modules</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference.modules.conv">torch.ao.nn.quantized.reference.modules.conv</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference.modules.linear">torch.ao.nn.quantized.reference.modules.linear</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference.modules.rnn">torch.ao.nn.quantized.reference.modules.rnn</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference.modules.sparse">torch.ao.nn.quantized.reference.modules.sparse</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference.modules.utils">torch.ao.nn.quantized.reference.modules.utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.sparse">torch.ao.nn.sparse</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.sparse.quantized">torch.ao.nn.sparse.quantized</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.sparse.quantized.dynamic">torch.ao.nn.sparse.quantized.dynamic</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.sparse.quantized.dynamic.linear">torch.ao.nn.sparse.quantized.dynamic.linear</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.sparse.quantized.linear">torch.ao.nn.sparse.quantized.linear</a>
</li>
        <li><a href="quantization.html#module-torch.ao.nn.sparse.quantized.utils">torch.ao.nn.sparse.quantized.utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.ns">torch.ao.ns</a>
</li>
        <li><a href="torch.ao.ns._numeric_suite.html#module-torch.ao.ns._numeric_suite">torch.ao.ns._numeric_suite</a>
</li>
        <li><a href="torch.ao.ns._numeric_suite_fx.html#module-torch.ao.ns._numeric_suite_fx">torch.ao.ns._numeric_suite_fx</a>
</li>
        <li><a href="quantization.html#module-torch.ao.ns.fx">torch.ao.ns.fx</a>
</li>
        <li><a href="quantization.html#module-torch.ao.ns.fx.graph_matcher">torch.ao.ns.fx.graph_matcher</a>
</li>
        <li><a href="quantization.html#module-torch.ao.ns.fx.graph_passes">torch.ao.ns.fx.graph_passes</a>
</li>
        <li><a href="quantization.html#module-torch.ao.ns.fx.mappings">torch.ao.ns.fx.mappings</a>
</li>
        <li><a href="quantization.html#module-torch.ao.ns.fx.n_shadows_utils">torch.ao.ns.fx.n_shadows_utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.ns.fx.ns_types">torch.ao.ns.fx.ns_types</a>
</li>
        <li><a href="quantization.html#module-torch.ao.ns.fx.pattern_utils">torch.ao.ns.fx.pattern_utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.ns.fx.qconfig_multi_mapping">torch.ao.ns.fx.qconfig_multi_mapping</a>
</li>
        <li><a href="quantization.html#module-torch.ao.ns.fx.utils">torch.ao.ns.fx.utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.ns.fx.weight_utils">torch.ao.ns.fx.weight_utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.pruning">torch.ao.pruning</a>
</li>
        <li><a href="quantization.html#module-torch.ao.pruning.scheduler">torch.ao.pruning.scheduler</a>
</li>
        <li><a href="quantization.html#module-torch.ao.pruning.scheduler.base_scheduler">torch.ao.pruning.scheduler.base_scheduler</a>
</li>
        <li><a href="quantization.html#module-torch.ao.pruning.scheduler.cubic_scheduler">torch.ao.pruning.scheduler.立方调度器</a>
</li>
        <li><a href="quantization.html#module-torch.ao.pruning.scheduler.lambda_scheduler">torch.ao.pruning.scheduler.lambda 调度器</a>
</li>
        <li><a href="quantization.html#module-torch.ao.pruning.sparsifier">torch.ao.pruning.稀疏化器</a>
</li>
        <li><a href="quantization.html#module-torch.ao.pruning.sparsifier.base_sparsifier">torch.ao.pruning.稀疏化器.基础稀疏化器</a>
</li>
        <li><a href="quantization.html#module-torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier">torch.ao.pruning.sparsifier.近似对角稀疏化器</a>
</li>
        <li><a href="quantization.html#module-torch.ao.pruning.sparsifier.utils">torch.ao.pruning.sparsifier.utils 工具</a>
</li>
        <li><a href="quantization.html#module-torch.ao.pruning.sparsifier.weight_norm_sparsifier">torch.ao.pruning.sparsifier.weight_norm_sparsifier 权重归一稀疏化器</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization">torch.ao.quantization 量化</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config">torch.ao.quantization.backend_config</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.backend_config">torch.ao.quantization.backend_config.backend_config</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.executorch">torch.ao.quantization.backend_config.executorch</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.fbgemm">torch.ao.quantization.backend_config.fbgemm</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.native">torch.ao.quantization.backend_config.native</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.observation_type">torch.ao.quantization.backend_config.observation_type</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.onednn">torch.ao.quantization.backend_config.onednn</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.qnnpack">torch.ao.quantization.backend_config.qnnpack</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.tensorrt">torch.ao.quantization.backend_config.tensorrt</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.utils">torch.ao.quantization.backend_config.utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.x86">torch.ao.quantization.backend_config.x86</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fake_quantize">torch.ao.quantization.fake_quantize</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fuse_modules">torch.ao.quantization.fuse_modules</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fuser_method_mappings">torch.ao.quantization.fuser_method_mappings</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx">torch.ao.quantization.fx</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.convert">torch.ao.quantization.fx.convert</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.custom_config">torch.ao.quantization.fx.custom_config</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.fuse">torch.ao.quantization.fx.fuse</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.fuse_handler">torch.ao.quantization.fx.fuse_handler</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.graph_module">torch.ao.quantization.fx.graph_module</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.lower_to_fbgemm">torch.ao.quantization.fx.从低精度到 fbgemm</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.lower_to_qnnpack">torch.ao.quantization.fx.从低精度到 qnnpack</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.lstm_utils">torch.ao.quantization.fx.lstm_utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.match_utils">torch.ao.quantization.fx.match_utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.pattern_utils">torch.ao.quantization.fx.pattern_utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.prepare">torch.ao.quantization.fx.prepare</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.qconfig_mapping_utils">torch.ao.quantization.fx.qconfig_mapping_utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.quantize_handler">torch.ao.quantization.fx.quantize_handler</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.tracer">torch.ao.quantization.fx.tracer</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.utils">torch.ao.quantization.fx.utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.observer">torch.ao.quantization.observer</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.quantization.pt2e">torch.ao.quantization.pt2e</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.duplicate_dq_pass">torch.ao.quantization.pt2e.duplicate_dq_pass</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.export_utils">torch.ao.quantization.pt2e.export_utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.graph_utils">torch.ao.quantization.pt2e.graph_utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.port_metadata_pass">torch.ao.quantization.pt2e.port_metadata_pass</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.prepare">torch.ao.quantization.pt2e.prepare</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.qat_utils">torch.ao.quantization.pt2e.qat_utils</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.quantization.pt2e.representation">torch.ao.quantization.pt2e.representation</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.representation.rewrite">torch.ao.quantization.pt2e.representation.rewrite</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.utils"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">torch.ao.quantization.pt2e.utils
torch.ao.量化.pt2e.工具</font></font></font></a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.qconfig">torch.ao.quantization.qconfig</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.qconfig_mapping">torch.ao.quantization.qconfig_mapping</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.quant_type">torch.ao.quantization.quant_type</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.quantization_mappings">torch.ao.quantization.quantization_mappings</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.quantize_fx">torch.ao.quantization.quantize_fx</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.quantize_jit">torch.ao.quantization.quantize_jit</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.quantize_pt2e">torch.ao.quantization.quantize_pt2e</a>
</li>
        <li><a href="quantization-support.html#module-torch.ao.quantization.quantizer">torch.ao.quantization.quantizer</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.composable_quantizer">torch.ao.quantization.quantizer.composable_quantizer</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.embedding_quantizer">torch.ao.quantization.quantizer.embedding_quantizer</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.quantizer">torch.ao.quantization.quantizer.quantizer</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.utils">torch.ao.quantization.quantizer.utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.x86_inductor_quantizer">torch.ao.quantization.quantizer.x86_inductor_quantizer</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.xnnpack_quantizer">torch.ao.quantization.quantizer.xnnpack_quantizer</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.xnnpack_quantizer_utils">torch.ao.quantization.quantizer.xnnpack_quantizer_utils</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.xpu_inductor_quantizer">torch.ao.quantization.quantizer.xpu_inductor_quantizer</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.stubs">torch.ao.quantization.stubs</a>
</li>
        <li><a href="quantization.html#module-torch.ao.quantization.utils">torch.ao.quantization.utils</a>
</li>
        <li><a href="autograd.html#module-torch.autograd">torch.autograd</a>
</li>
        <li><a href="autograd.html#module-torch.autograd.anomaly_mode">torch.autograd.anomaly_mode</a>
</li>
        <li><a href="autograd.html#module-torch.autograd.forward_ad">torch.autograd.forward_ad</a>
</li>
        <li><a href="autograd.html#module-torch.autograd.function">torch.autograd.function</a>
</li>
        <li><a href="autograd.html#module-torch.autograd.functional">torch.autograd.functional</a>
</li>
        <li><a href="autograd.html#module-torch.autograd.grad_mode">torch.autograd.grad_mode</a>
</li>
        <li><a href="autograd.html#module-torch.autograd.gradcheck">torch.autograd.gradcheck</a>
</li>
        <li><a href="autograd.html#module-torch.autograd.graph">torch.autograd.graph</a>
</li>
        <li><a href="autograd.html#module-torch.autograd.profiler">torch.autograd.profiler</a>
</li>
        <li><a href="autograd.html#module-torch.autograd.profiler_legacy">torch.autograd.profiler_legacy</a>
</li>
        <li><a href="autograd.html#module-torch.autograd.profiler_util">torch.autograd.profiler_util</a>
</li>
        <li><a href="autograd.html#module-torch.autograd.variable">torch.autograd.variable</a>
</li>
        <li><a href="backends.html#module-torch.backends">torch.backends</a>
</li>
        <li><a href="backends.html#module-torch.backends.cpu">torch.backends.cpu</a>
</li>
        <li><a href="backends.html#module-torch.backends.cuda">torch.backends.cuda</a>
</li>
        <li><a href="backends.html#module-torch.backends.cudnn">torch.backends.cudnn</a>
</li>
        <li><a href="backends.html#module-torch.backends.cudnn.rnn">torch.backends.cudnn.rnn</a>
</li>
        <li><a href="backends.html#module-torch.backends.cusparselt">torch.backends.cusparselt</a>
</li>
        <li><a href="backends.html#module-torch.backends.kleidiai">torch.backends.kleidiai</a>
</li>
        <li><a href="backends.html#module-torch.backends.mha">torch.backends.mha</a>
</li>
        <li><a href="backends.html#module-torch.backends.mkl">torch.backends.mkl</a>
</li>
        <li><a href="backends.html#module-torch.backends.mkldnn">torch.backends.mkldnn</a>
</li>
        <li><a href="backends.html#module-torch.backends.mps">torch.backends.mps</a>
</li>
        <li><a href="backends.html#module-torch.backends.nnpack">torch.backends.nnpack</a>
</li>
        <li><a href="backends.html#module-torch.backends.openmp">torch.backends.openmp</a>
</li>
        <li><a href="backends.html#module-torch.backends.opt_einsum">torch.backends.opt_einsum</a>
</li>
        <li><a href="backends.html#module-torch.backends.quantized">torch.backends.quantized</a>
</li>
        <li><a href="backends.html#module-torch.backends.xeon">torch.backends.xeon</a>
</li>
        <li><a href="backends.html#module-torch.backends.xeon.run_cpu">torch.backends.xeon.run_cpu</a>
</li>
        <li><a href="backends.html#module-torch.backends.xnnpack">torch.backends.xnnpack</a>
</li>
        <li><a href="torch.compiler_api.html#module-torch.compiler">torch.compiler</a>
</li>
        <li><a href="torch.compiler.config.html#module-torch.compiler.config">torch.compiler.config</a>
</li>
        <li><a href="torch.html#module-torch.contrib">torch.contrib</a>
</li>
        <li><a href="cpu.html#module-torch.cpu">torch.cpu</a>
</li>
        <li><a href="amp.html#module-torch.cpu.amp">torch.cpu.amp</a>
</li>
        <li><a href="amp.html#module-torch.cpu.amp.autocast_mode">torch.cpu.amp.autocast_mode</a>
</li>
        <li><a href="amp.html#module-torch.cpu.amp.grad_scaler">torch.cpu.amp.grad_scaler</a>
</li>
        <li><a href="cuda.html#module-torch.cuda">torch.cuda</a>
</li>
        <li><a href="cuda._sanitizer.html#module-torch.cuda._sanitizer">torch.cuda._sanitizer</a>
</li>
        <li><a href="amp.html#module-torch.cuda.amp">torch.cuda.amp</a>
</li>
        <li><a href="amp.html#module-torch.cuda.amp.autocast_mode">torch.cuda.amp.autocast_mode</a>
</li>
        <li><a href="amp.html#module-torch.cuda.amp.common">torch.cuda.amp.common</a>
</li>
        <li><a href="amp.html#module-torch.cuda.amp.grad_scaler">torch.cuda.amp.grad_scaler</a>
</li>
        <li><a href="cuda.html#module-torch.cuda.comm">torch.cuda.comm</a>
</li>
        <li><a href="cuda.html#module-torch.cuda.error">torch.cuda.error</a>
</li>
        <li><a href="cuda.html#module-torch.cuda.gds">torch.cuda.gds</a>
</li>
        <li><a href="cuda.html#module-torch.cuda.graphs">torch.cuda.graphs</a>
</li>
        <li><a href="cuda.html#module-torch.cuda.jiterator">torch.cuda.jiterator</a>
</li>
        <li><a href="cuda.html#module-torch.cuda.memory">torch.cuda.memory</a>
</li>
        <li><a href="cuda.html#module-torch.cuda.nccl">torch.cuda.nccl</a>
</li>
        <li><a href="cuda.html#module-torch.cuda.nvtx">torch.cuda.nvtx</a>
</li>
        <li><a href="cuda.html#module-torch.cuda.profiler">torch.cuda.profiler</a>
</li>
        <li><a href="cuda.html#module-torch.cuda.random">torch.cuda.random</a>
</li>
        <li><a href="cuda.html#module-torch.cuda.sparse">torch.cuda.sparse</a>
</li>
        <li><a href="cuda.html#module-torch.cuda.streams">torch.cuda.streams</a>
</li>
        <li><a href="cuda.tunable.html#module-torch.cuda.tunable">torch.cuda.tunable</a>
</li>
        <li><a href="distributed.html#module-torch.distributed">torch.distributed</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms">torch.distributed.algorithms</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks">torch.distributed.algorithms.ddp_comm_hooks</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook">torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks">torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.default_hooks">torch.distributed.algorithms.ddp_comm_hooks.default_hooks</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks">torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks">torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook">torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook">torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks">torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.join">torch.distributed.algorithms.join</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.model_averaging">torch.distributed.algorithms.model_averaging</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.model_averaging.averagers">torch.distributed.algorithms.model_averaging.averagers</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.model_averaging.hierarchical_model_averager">torch.distributed.algorithms.model_averaging.hierarchical_model_averager</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.algorithms.model_averaging.utils">torch.distributed.algorithms.model_averaging.utils</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.argparse_util">torch.distributed.argparse_util</a>
</li>
        <li><a href="rpc.html#module-torch.distributed.autograd">torch.distributed.autograd</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.c10d_logger">torch.distributed.c10d_logger</a>
</li>
        <li><a href="distributed.checkpoint.html#module-torch.distributed.checkpoint">torch.distributed.checkpoint</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.api">torch.distributed.checkpoint.api</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.default_planner">torch.distributed.checkpoint.default_planner</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.filesystem">torch.distributed.checkpoint.filesystem</a>
</li>
        <li><a href="distributed.checkpoint.html#module-torch.distributed.checkpoint.format_utils">torch.distributed.checkpoint.format_utils</a>
</li>
        <li><a href="distributed.checkpoint.html#module-torch.distributed.checkpoint.logger">torch.distributed.checkpoint.logger</a>
</li>
        <li><a href="distributed.checkpoint.html#module-torch.distributed.checkpoint.logging_handlers">torch.distributed.checkpoint.logging_handlers</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.metadata">torch.distributed.checkpoint.metadata</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.optimizer">torch.distributed.checkpoint.optimizer</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.planner">torch.distributed.checkpoint.planner</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.planner_helpers">torch.distributed.checkpoint.planner_helpers</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.resharding">torch.distributed.checkpoint.resharding</a>
</li>
        <li><a href="distributed.checkpoint.html#module-torch.distributed.checkpoint.staging">torch.distributed.checkpoint.staging</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.state_dict">torch.distributed.checkpoint.state_dict</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.state_dict_loader">torch.distributed.checkpoint.state_dict_loader</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.state_dict_saver">torch.distributed.checkpoint.state_dict_saver</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.stateful">torch.distributed.checkpoint.stateful</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.storage">torch.distributed.checkpoint.storage</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.utils">torch.distributed.checkpoint.utils</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.collective_utils">torch.distributed.collective_utils</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.constants">torch.distributed.constants</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.device_mesh">torch.distributed.device_mesh</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.distributed_c10d">torch.distributed.distributed_c10d</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic">torch.distributed.elastic</a>
</li>
        <li><a href="elastic/agent.html#module-torch.distributed.elastic.agent">torch.distributed.elastic.agent</a>
</li>
        <li><a href="elastic/agent.html#module-torch.distributed.elastic.agent.server">torch.distributed.elastic.agent.server</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.agent.server.api">torch.distributed.elastic.agent.server.api</a>
</li>
        <li><a href="elastic/agent.html#module-torch.distributed.elastic.agent.server.health_check_server">torch.distributed.elastic.agent.server.健康检查服务器</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.agent.server.local_elastic_agent">torch.distributed.elastic.agent.server.本地弹性代理</a>
</li>
        <li><a href="elastic/control_plane.html#module-torch.distributed.elastic.control_plane">torch.distributed.elastic.控制平面</a>
</li>
        <li><a href="elastic/events.html#module-torch.distributed.elastic.events">torch.distributed.elastic.事件</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.events.api">torch.distributed.elastic.events.api</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.events.handlers">torch.distributed.elastic.events.handlers</a>
</li>
        <li><a href="elastic/metrics.html#module-torch.distributed.elastic.metrics">torch.distributed.elastic.metrics</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.metrics.api">torch.distributed.elastic.metrics.api</a>
</li>
        <li><a href="elastic/multiprocessing.html#module-torch.distributed.elastic.multiprocessing">torch.distributed.elastic.multiprocessing</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.multiprocessing.api">torch.distributed.elastic.multiprocessing.api</a>
</li>
        <li><a href="elastic/errors.html#module-torch.distributed.elastic.multiprocessing.errors">torch.distributed.elastic.multiprocessing.errors</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.multiprocessing.errors.error_handler">torch.distributed.elastic.multiprocessing 错误处理器</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.multiprocessing.errors.handlers">torch.distributed.elastic.multiprocessing 错误处理程序</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.multiprocessing.redirects">torch.distributed.elastic.multiprocessing 重定向</a>
</li>
        <li><a href="elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler">torch.distributed.elastic.multiprocessing 子进程处理器</a>
</li>
        <li><a href="elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler.handlers">torch.distributed.elastic.多进程子进程处理器.处理器</a>
</li>
        <li><a href="elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler">torch.distributed.elastic.多进程子进程处理器.子进程处理器</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.multiprocessing.tail_log">torch.distributed.elastic.多进程尾部日志</a>
</li>
        <li><a href="elastic/rendezvous.html#module-torch.distributed.elastic.rendezvous">torch.distributed.elastic. rendezvous</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.api">torch.distributed.elastic.rendezvous.api</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.c10d_rendezvous_backend">torch.distributed.elastic.rendezvous.c10d_rendezvous_backend</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.dynamic_rendezvous">torch.distributed.elastic.rendezvous.dynamic_rendezvous</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.etcd_rendezvous">torch.distributed.elastic.rendezvous.etcd_rendezvous</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.etcd_rendezvous_backend">torch.distributed.elastic.rendezvous.etcd_rendezvous_backend</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.etcd_server">torch.distributed.elastic.rendezvous.etcd_server</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.etcd_store">torch.distributed.elastic.rendezvous.etcd_store</a>
</li>
        <li><a href="elastic/rendezvous.html#module-torch.distributed.elastic.rendezvous.registry">torch.distributed.elastic.rendezvous.registry</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.static_tcp_rendezvous">torch.distributed.elastic.rendezvous.static_tcp_rendezvous</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.utils">torch.distributed.elastic.rendezvous.utils</a>
</li>
        <li><a href="elastic/timer.html#module-torch.distributed.elastic.timer">torch.distributed.elastic.timer</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.timer.api">torch.distributed.elastic.timer.api</a>
</li>
        <li><a href="elastic/timer.html#module-torch.distributed.elastic.timer.debug_info_logging"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">torch.distributed.elastic.timer.debug_info_logging
torch 分布式弹性定时器调试信息记录</font></font></font></a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.timer.file_based_local_timer">torch.distributed.elastic.timer.file_based_local_timer</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.timer.local_timer">torch.distributed.elastic.timer.local_timer</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils">torch.distributed.elastic.utils</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.api">torch.distributed.elastic.utils.api</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.data">torch.distributed.elastic.utils.data</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.data.cycling_iterator">torch.distributed.elastic.utils.data.cycling_iterator</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.data.elastic_distributed_sampler">torch.distributed.elastic.utils.data.elastic_distributed_sampler</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.distributed">torch.distributed.elastic.utils.distributed</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.log_level">torch.distributed.elastic.utils.log_level</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.logging">torch.distributed.elastic.utils.logging</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.store">torch.distributed.elastic.utils.store</a>
</li>
        <li><a href="fsdp.html#module-torch.distributed.fsdp">torch.distributed.fsdp</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.fsdp.api">torch.distributed.fsdp.api</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.fsdp.fully_sharded_data_parallel">torch.distributed.fsdp.全分片数据并行</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.fsdp.sharded_grad_scaler">torch.distributed.fsdp.分片梯度缩放器</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.fsdp.wrap">torch.distributed.fsdp.wrap</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.launch">torch.distributed.launch</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.launcher">torch.distributed.launcher</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.launcher.api">torch.distributed.launcher.api</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.logging_handlers">torch.distributed.logging_handlers</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.nn">torch.distributed.nn</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.nn.api">torch.distributed.nn.api</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.nn.api.remote_module">torch.distributed.nn.api.remote_module</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.nn.functional">torch.distributed.nn.functional</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.nn.jit">torch.distributed.nn.jit</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.nn.jit.instantiator">torch.distributed.nn.jit.instantiator</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.nn.jit.templates">torch.distributed.nn.jit.templates</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.nn.jit.templates.remote_module_template">torch.distributed.nn.jit.模板.remote_module_template</a>
</li>
        <li><a href="distributed.optim.html#module-torch.distributed.optim">torch.distributed.optim</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.apply_optimizer_in_backward">torch.distributed.optim.apply_optimizer_in_backward</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_adadelta">torch.distributed.optim.functional_adadelta</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_adagrad">torch.distributed.optim.functional_adagrad</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_adam">torch.distributed.optim.functional_adam</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_adamax">torch.distributed.optim.functional_adamax</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_adamw">torch.distributed.optim.functional_adamw</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_rmsprop">torch.distributed.optim.functional_rmsprop</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_rprop">torch.distributed.optim.functional_rprop</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_sgd">torch.distributed.optim.functional_sgd</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.named_optimizer">torch.distributed.optim.named_optimizer</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.optimizer">torch.distributed.optim.optimizer</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.post_localSGD_optimizer">torch.distributed.optim.post_localSGD_optimizer</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.utils">torch.distributed.optim.utils</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.optim.zero_redundancy_optimizer">torch.distributed.optim.zero_redundancy_optimizer</a>
</li>
        <li><a href="distributed.pipelining.html#module-torch.distributed.pipelining">torch.distributed.pipelining</a>
</li>
        <li><a href="distributed.pipelining.html#module-torch.distributed.pipelining.microbatch">torch.distributed.pipelining.微批处理</a>
</li>
        <li><a href="distributed.pipelining.html#module-torch.distributed.pipelining.schedules">torch.distributed.pipelining.schedules</a>
</li>
        <li><a href="distributed.pipelining.html#module-torch.distributed.pipelining.stage">torch.distributed.pipelining.stage</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.remote_device">torch.distributed.remote_device</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.rendezvous">torch.distributed.rendezvous</a>
</li>
        <li><a href="rpc.html#module-torch.distributed.rpc">torch.distributed.rpc</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.rpc.api">torch.distributed.rpc.api</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.rpc.backend_registry">torch.distributed.rpc.backend_registry</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.rpc.constants">torch.distributed.rpc.constants</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.rpc.functions">torch.distributed.rpc.functions</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.rpc.internal">torch.distributed.rpc.internal</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.rpc.options">torch.distributed.rpc.options</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.rpc.rref_proxy">torch.distributed.rpc.rref_proxy</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.rpc.server_process_global_profiler">torch.distributed.rpc.server_process_global_profiler</a>
</li>
        <li><a href="elastic/run.html#module-torch.distributed.run">torch.distributed.run</a>
</li>
        <li><a href="distributed.tensor.html#module-torch.distributed.tensor">torch.distributed.tensor</a>
</li>
        <li><a href="distributed.tensor.html#module-torch.distributed.tensor.debug">torch.distributed.tensor.debug</a>
</li>
        <li><a href="distributed.tensor.html#module-torch.distributed.tensor.device_mesh">torch.distributed.tensor.device_mesh</a>
</li>
        <li><a href="distributed.tensor.html#module-torch.distributed.tensor.experimental">torch.distributed.tensor.experimental</a>
</li>
        <li><a href="distributed.tensor.parallel.html#module-torch.distributed.tensor.parallel">torch.distributed.tensor.parallel</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.tensor.parallel.api">torch.distributed.tensor.parallel.api</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.tensor.parallel.ddp">torch.distributed.tensor.parallel.ddp</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.tensor.parallel.fsdp">torch.distributed.tensor.parallel.fsdp</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.tensor.parallel.input_reshard">torch.distributed.tensor.parallel.input_reshard</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.tensor.parallel.loss">torch.distributed.tensor.parallel.loss</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.tensor.parallel.style">torch.distributed.tensor.parallel.style</a>
</li>
        <li><a href="distributed.tensor.html#module-torch.distributed.tensor.placement_types">torch.distributed.tensor.placement_types</a>
</li>
        <li><a href="distributed.html#module-torch.distributed.utils">torch.distributed.utils</a>
</li>
        <li><a href="distributions.html#module-torch.distributions">torch.distributions</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.bernoulli">torch.distributions.伯努利</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.beta">torch.distributions.贝塔</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.binomial">torch.distributions.binomial</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.categorical">torch.distributions.categorical</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.cauchy">torch.distributions.cauchy</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.chi2">torch.distributions.chi2</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.constraint_registry">torch.distributions 约束注册表</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.constraints">torch.distributions 约束</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.continuous_bernoulli">torch.distributions.continuous_bernoulli</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.dirichlet">torch.distributions.dirichlet</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.distribution"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">torch.distributions.distribution

分布库</font></font></font></a>
</li>
        <li><a href="distributions.html#module-torch.distributions.exp_family">torch.distributions.exp_family</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.exponential">torch.distributions.exponential</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.fishersnedecor">torch.distributions.fishersnedecor</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.gamma">torch.distributions.gamma</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.geometric">torch.distributions.geometric</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.gumbel">torch.distributions.gumbel</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.half_cauchy">torch.distributions.half_cauchy</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.half_normal">torch.distributions.half_normal</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.independent">torch.distributions.independent</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.inverse_gamma">torch.distributions.inverse_gamma</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.kl">torch.distributions.kl</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.kumaraswamy">torch.distributions.kumaraswamy</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.laplace">torch.distributions.laplace</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.lkj_cholesky">torch.distributions.lkj_cholesky</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.log_normal">torch.distributions.log_normal</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.logistic_normal">torch.distributions.logistic_normal</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.lowrank_multivariate_normal">torch.distributions.lowrank_multivariate_normal</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.mixture_same_family">torch.distributions.mixture_same_family</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.multinomial">torch.distributions.multinomial</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.multivariate_normal">torch.distributions.multivariate_normal</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.negative_binomial">torch.distributions.negative_binomial</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.normal">torch.distributions.normal</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.one_hot_categorical">torch.distributions.one_hot_categorical</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.pareto">torch.distributions.pareto</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.poisson">torch.distributions.poisson</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.relaxed_bernoulli">torch.distributions.relaxed_bernoulli</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.relaxed_categorical">torch.distributions.relaxed_categorical</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.studentT">torch.distributions.studentT</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.transformed_distribution">torch.distributions.transformed_distribution</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.transforms">torch.distributions.transforms</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.uniform">torch.distributions.uniform</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.utils">torch.distributions.utils</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.von_mises">torch.distributions.von_mises</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.weibull">torch.distributions.weibull</a>
</li>
        <li><a href="distributions.html#module-torch.distributions.wishart">torch.distributions.wishart</a>
</li>
        <li><a href="export.html#module-torch.export">torch.export</a>
</li>
        <li><a href="export.html#module-torch.export.custom_obj">torch.export.custom_obj</a>
</li>
        <li><a href="export.html#module-torch.export.custom_ops">torch.export.custom_ops</a>
</li>
        <li><a href="export.html#module-torch.export.decomp_utils">torch.export.decomp_utils</a>
</li>
        <li><a href="export.html#module-torch.export.dynamic_shapes">torch.export.dynamic_shapes</a>
</li>
        <li><a href="export.html#module-torch.export.experimental">torch.export.experimental</a>
</li>
        <li><a href="export.html#module-torch.export.exported_program">torch.export.exported_program</a>
</li>
        <li><a href="export.html#module-torch.export.graph_signature">torch.export.graph_signature</a>
</li>
        <li><a href="export.html#module-torch.export.passes">torch.export.passes</a>
</li>
        <li><a href="export.html#module-torch.export.unflatten">torch.export.unflatten</a>
</li>
        <li><a href="fft.html#module-torch.fft">torch.fft</a>
</li>
        <li><a href="func.api.html#module-torch.func">torch.func</a>
</li>
        <li><a href="torch.html#module-torch.functional">torch.functional</a>
</li>
        <li><a href="futures.html#module-torch.futures">torch.futures</a>
</li>
        <li><a href="fx.html#module-torch.fx">torch.fx</a>
</li>
        <li><a href="fx.html#module-torch.fx.annotate">torch.fx 注解</a>
</li>
        <li><a href="fx.html#module-torch.fx.config">torch.fx 配置</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental">torch.fx.experimental</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.accelerator_partitioner">torch.fx.experimental.accelerator_partitioner</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.const_fold">torch.fx.experimental.const_fold</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.debug">torch.fx.experimental.debug</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.graph_gradual_typechecker">torch.fx.experimental.graph_gradual_typechecker</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.merge_matmul">torch.fx.experimental.merge_matmul</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.meta_tracer">torch.fx.experimental.meta_tracer</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types">torch.fx.experimental.migrate_gradual_types</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint">torch.fx.experimental.migrate_gradual_types.constraint</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint_generator">torch.fx.experimental.migrate_gradual_types.constraint_generator</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint_transformation">torch.fx.experimental.migrate_gradual_types.constraint_transformation</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.operation">torch.fx.experimental.migrate_gradual_types.operation</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.transform_to_z3">torch.fx.experimental.migrate_gradual_types.transform_to_z3</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.util">torch.fx.experimental.migrate_gradual_types.util</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.z3_types">torch.fx.experimental.migrate_gradual_types.z3_types</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.normalize">torch.fx 实验性的规范化</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.optimization">torch.fx.experimental.optimization</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.partitioner_utils">torch.fx.experimental.partitioner_utils</a>
</li>
        <li><a href="fx.experimental.html#module-torch.fx.experimental.proxy_tensor">torch.fx.experimental.proxy_tensor</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.recording">torch.fx.experimental.recording</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.refinement_types">torch.fx.experimental.refinement_types</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.rewriter">torch.fx.experimental.rewriter</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.schema_type_annotation">torch.fx 实验性模式类型注解</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.sym_node">torch.fx.experimental.sym_node</a>
</li>
        <li><a href="fx.experimental.html#module-torch.fx.experimental.symbolic_shapes">torch.fx.experimental.symbolic_shapes</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification">torch.fx.experimental.unification</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification.core">torch.fx.experimental.unification.core</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification.dispatch">torch.fx.experimental.unification.dispatch</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification.match">torch.fx.experimental.unification.match</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification.more">torch.fx.experimental.unification.more</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification.multipledispatch">torch.fx.experimental.unification.multipledispatch</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification.multipledispatch.conflict">torch.fx.experimental.unification.multipledispatch.conflict</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification.multipledispatch.core">torch.fx.experimental.unification.multipledispatch.core</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification.multipledispatch.dispatcher">torch.fx.experimental.unification.multipledispatch.dispatcher</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification.multipledispatch.utils">torch.fx.experimental.unification.multipledispatch.utils</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification.multipledispatch.variadic">torch.fx.experimental.unification.multipledispatch.variadic</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification.unification_tools">torch.fx.experimental.unification.unification_tools</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification.utils">torch.fx.experimental.unification.utils</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unification.variable">torch.fx.experimental.unification.variable</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.unify_refinements">torch.fx.experimental.unify_refinements</a>
</li>
        <li><a href="fx.html#module-torch.fx.experimental.validator">torch.fx.experimental.validator</a>
</li>
        <li><a href="fx.html#module-torch.fx.graph">torch.fx.graph</a>
</li>
        <li><a href="fx.html#module-torch.fx.graph_module">torch.fx.graph_module</a>
</li>
        <li><a href="fx.html#module-torch.fx.immutable_collections">torch.fx 不可变集合</a>
</li>
        <li><a href="fx.html#module-torch.fx.interpreter">torch.fx.interpreter</a>
</li>
        <li><a href="fx.html#module-torch.fx.node">torch.fx 节点</a>
</li>
        <li><a href="fx.html#module-torch.fx.operator_schemas">torch.fx 算子模式</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes">torch.fx.passes</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.annotate_getitem_nodes">torch.fx.passes.annotate_getitem_nodes</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.backends">torch.fx.passes.backends</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.backends.cudagraphs">torch.fx.passes.backends.cudagraphs</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.dialect">torch.fx.passes.dialect</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.dialect.common">torch.fx.passes.dialect.common</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.dialect.common.cse_pass">torch.fx.passes.dialect.common.cse_pass</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.fake_tensor_prop">torch.fx.passes.伪造张量属性</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.graph_drawer">torch.fx.passes.graph_drawer</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.graph_manipulation">torch.fx.passes 图操作</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.graph_transform_observer">torch.fx.passes 图转换观察者</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.infra">torch.fx.passes.infra</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.infra.partitioner">torch.fx.passes.infra.partitioner</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.infra.pass_base">torch.fx.passes.infra.pass_base</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.infra.pass_manager">torch.fx.passes.infra.pass_manager</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.net_min_base">torch.fx.passes.net_min_base</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.operator_support">torch.fx.passes.operator_support</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.param_fetch">torch.fx.passes.param_fetch</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.pass_manager">torch.fx.passes.pass_manager</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.reinplace">torch.fx.passes.reinplace</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.runtime_assert">torch.fx.passes.runtime_assert</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.shape_prop">torch.fx.passes.shape_prop</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.split_module">torch.fx.passes.split_module</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.split_utils">torch.fx.passes.split_utils</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.splitter_base">torch.fx.passes.splitter_base</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.tests">torch.fx.passes.tests</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.tests.test_pass_manager">torch.fx.passes 单元测试.test_pass_manager</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.tools_common">torch.fx.passes 工具.common</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.utils">torch.fx.passes 工具.utils</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.utils.common">torch.fx.passes 工具.common</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.utils.fuser_utils">torch.fx.passes.utils.fuser_utils</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.utils.matcher_utils">torch.fx.passes.utils.matcher_utils</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.utils.matcher_with_name_node_map_utils">torch.fx.passes.utils.matcher_with_name_node_map_utils</a>
</li>
        <li><a href="fx.html#module-torch.fx.passes.utils.source_matcher_utils">torch.fx.passes.utils.source_matcher_utils</a>
</li>
        <li><a href="fx.html#module-torch.fx.proxy">torch.fx.proxy</a>
</li>
        <li><a href="fx.html#module-torch.fx.subgraph_rewriter">torch.fx.subgraph_rewriter</a>
</li>
        <li><a href="fx.html#module-torch.fx.tensor_type">torch.fx.tensor_type</a>
</li>
        <li><a href="fx.html#module-torch.fx.traceback">torch.fx.traceback</a>
</li>
        <li><a href="hub.html#module-torch.hub">torch.hub</a>
</li>
        <li><a href="jit.html#module-torch.jit">torch.jit</a>
</li>
        <li><a href="jit.html#module-torch.jit.annotations">torch.jit.annotations</a>
</li>
        <li><a href="jit.html#module-torch.jit.frontend">torch.jit.frontend</a>
</li>
        <li><a href="jit.html#module-torch.jit.generate_bytecode">torch.jit.generate_bytecode</a>
</li>
        <li><a href="jit.html#module-torch.jit.mobile">torch.jit.mobile</a>
</li>
        <li><a href="jit.html#module-torch.jit.quantized">torch.jit.quantized</a>
</li>
        <li><a href="jit_builtin_functions.html#module-torch.jit.supported_ops">torch.jit.supported_ops</a>
</li>
        <li><a href="jit_unsupported.html#module-torch.jit.unsupported_tensor_ops">torch.jit.unsupported_tensor_ops</a>
</li>
        <li><a href="library.html#module-torch.library">torch.library</a>
</li>
        <li><a href="linalg.html#module-torch.linalg">torch.linalg</a>
</li>
        <li><a href="masked.html#module-torch.masked">torch.masked</a>
</li>
        <li><a href="masked.html#module-torch.masked.maskedtensor">torch.masked.maskedtensor</a>
</li>
        <li><a href="masked.html#module-torch.masked.maskedtensor.binary">torch.masked.maskedtensor.binary</a>
</li>
        <li><a href="masked.html#module-torch.masked.maskedtensor.core">torch.masked.maskedtensor.core</a>
</li>
        <li><a href="masked.html#module-torch.masked.maskedtensor.creation">torch.masked.maskedtensor.creation</a>
</li>
        <li><a href="masked.html#module-torch.masked.maskedtensor.passthrough">torch.masked.maskedtensor.passthrough</a>
</li>
        <li><a href="masked.html#module-torch.masked.maskedtensor.reductions">torch.masked.maskedtensor.reductions</a>
</li>
        <li><a href="masked.html#module-torch.masked.maskedtensor.unary">torch.masked.maskedtensor.unary</a>
</li>
        <li><a href="monitor.html#module-torch.monitor">torch.monitor</a>
</li>
        <li><a href="mps.html#module-torch.mps">torch.mps</a>
</li>
        <li><a href="mps.html#module-torch.mps.event">torch.mps.event</a>
</li>
        <li><a href="mps.html#module-torch.mps.profiler">torch.mps.profiler</a>
</li>
        <li><a href="mtia.html#module-torch.mtia">torch.mtia</a>
</li>
        <li><a href="mtia.memory.html#module-torch.mtia.memory">torch.mtia.memory</a>
</li>
        <li><a href="multiprocessing.html#module-torch.multiprocessing">torch.multiprocessing</a>
</li>
        <li><a href="multiprocessing.html#module-torch.multiprocessing.pool">torch.multiprocessing.pool</a>
</li>
        <li><a href="multiprocessing.html#module-torch.multiprocessing.queue">torch.multiprocessing.queue</a>
</li>
        <li><a href="multiprocessing.html#module-torch.multiprocessing.reductions">torch.multiprocessing.reductions</a>
</li>
        <li><a href="multiprocessing.html#module-torch.multiprocessing.spawn">torch.multiprocessing.spawn</a>
</li>
        <li><a href="nested.html#module-torch.nested">torch.nested</a>
</li>
        <li><a href="nn.html#module-torch.nn">torch.nn</a>
</li>
        <li><a href="nn.attention.html#module-torch.nn.attention">torch.nn.attention</a>
</li>
        <li><a href="nn.attention.bias.html#module-torch.nn.attention.bias">torch.nn.attention.bias</a>
</li>
        <li><a href="nn.attention.experimental.html#module-torch.nn.attention.experimental">torch.nn.attention.experimental</a>
</li>
        <li><a href="nn.attention.flex_attention.html#module-torch.nn.attention.flex_attention">torch.nn.attention.flex_attention</a>
</li>
        <li><a href="nn.html#module-torch.nn.backends">torch.nn.backends</a>
</li>
        <li><a href="nn.html#module-torch.nn.backends.thnn">torch.nn.backends.thnn</a>
</li>
        <li><a href="nn.html#module-torch.nn.common_types">torch.nn.common_types</a>
</li>
        <li><a href="nn.html#module-torch.nn.cpp">torch.nn.cpp</a>
</li>
        <li><a href="nn.html#module-torch.nn.functional">torch.nn.functional</a>
</li>
        <li><a href="nn.html#module-torch.nn.grad">torch.nn.grad</a>
</li>
        <li><a href="nn.html#module-torch.nn.init">torch.nn.init</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic">torch.nn.intrinsic</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.modules">torch.nn.intrinsic.modules</a>
</li>
        <li><a href="quantization.html#module-torch.nn.intrinsic.modules.fused">torch.nn.intrinsic.modules.fused</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.qat">torch.nn.intrinsic.qat</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.qat.modules">torch.nn.intrinsic.qat.modules</a>
</li>
        <li><a href="quantization.html#module-torch.nn.intrinsic.qat.modules.conv_fused">torch.nn.intrinsic.qat.modules.conv_fused</a>
</li>
        <li><a href="quantization.html#module-torch.nn.intrinsic.qat.modules.linear_fused">torch.nn.intrinsic.qat.modules.linear_fused</a>
</li>
        <li><a href="quantization.html#module-torch.nn.intrinsic.qat.modules.linear_relu">torch.nn.intrinsic.qat.modules.linear_relu</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.quantized">torch.nn.intrinsic.quantized</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.quantized.dynamic">torch.nn.intrinsic.quantized.dynamic</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.quantized.dynamic.modules">torch.nn.intrinsic.quantized.dynamic.modules</a>
</li>
        <li><a href="quantization.html#module-torch.nn.intrinsic.quantized.dynamic.modules.linear_relu">torch.nn.intrinsic.quantized.dynamic.modules.linear_relu</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.quantized.modules">torch.nn.intrinsic.quantized.modules</a>
</li>
        <li><a href="quantization.html#module-torch.nn.intrinsic.quantized.modules.bn_relu">torch.nn.intrinsic.quantized.modules.bn_relu</a>
</li>
        <li><a href="quantization.html#module-torch.nn.intrinsic.quantized.modules.conv_relu">torch.nn.intrinsic.quantized.modules.conv_relu</a>
</li>
        <li><a href="quantization.html#module-torch.nn.intrinsic.quantized.modules.linear_relu">torch.nn.intrinsic.quantized.modules.linear_relu</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules">torch.nn.modules</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.activation">torch.nn.modules.activation</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.adaptive">torch.nn.modules.adaptive</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.batchnorm">torch.nn.modules.batchnorm</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.channelshuffle">torch.nn.modules.channelshuffle</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.container">torch.nn.modules.container</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.conv">torch.nn.modules.conv</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.distance">torch.nn.modules.distance</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.dropout">torch.nn.modules.dropout</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.flatten">torch.nn.modules.flatten</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.fold">torch.nn.modules.fold</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.instancenorm">torch.nn.modules.instancenorm</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.lazy">torch.nn.modules.lazy</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.linear">torch.nn.modules.linear</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.loss">torch.nn.modules.loss</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.module">torch.nn.modules.module</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.normalization">torch.nn.modules.normalization</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.padding">torch.nn.modules.padding</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.pixelshuffle">torch.nn.modules.pixelshuffle</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.pooling">torch.nn.modules.pooling</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.rnn">torch.nn.modules.rnn</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.sparse">torch.nn.modules.sparse</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.transformer">torch.nn.modules.transformer</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.upsampling">torch.nn.modules.upsampling</a>
</li>
        <li><a href="nn.html#module-torch.nn.modules.utils">torch.nn.modules.utils</a>
</li>
        <li><a href="nn.html#module-torch.nn.parallel">torch.nn.parallel</a>
</li>
        <li><a href="nn.html#module-torch.nn.parallel.comm">torch.nn.parallel.comm</a>
</li>
        <li><a href="nn.html#module-torch.nn.parallel.distributed">torch.nn.parallel.distributed 分布式并行</a>
</li>
        <li><a href="nn.html#module-torch.nn.parallel.parallel_apply">torch.nn.parallel.parallel_apply 并行应用</a>
</li>
        <li><a href="nn.html#module-torch.nn.parallel.replicate">torch.nn.parallel.replicate 复制并行</a>
</li>
        <li><a href="nn.html#module-torch.nn.parallel.scatter_gather">torch.nn.parallel.scatter_gather 扩散收集</a>
</li>
        <li><a href="nn.html#module-torch.nn.parameter">torch.nn.parameter</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.qat">torch.nn.qat</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.qat.dynamic">torch.nn.qat.dynamic</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.qat.dynamic.modules">torch.nn.qat.dynamic.modules</a>
</li>
        <li><a href="quantization.html#module-torch.nn.qat.dynamic.modules.linear">torch.nn.qat 动态模块线性</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.qat.modules">torch.nn.qat 模块</a>
</li>
        <li><a href="quantization.html#module-torch.nn.qat.modules.conv">torch.nn.qat 模块卷积</a>
</li>
        <li><a href="quantization.html#module-torch.nn.qat.modules.embedding_ops">torch.nn.qat.modules.embedding_ops</a>
</li>
        <li><a href="quantization.html#module-torch.nn.qat.modules.linear">torch.nn.qat.modules.linear</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.quantizable">torch.nn.quantizable</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.quantizable.modules">torch.nn.quantizable.modules</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantizable.modules.activation">torch.nn.quantizable.modules.activation</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantizable.modules.rnn">torch.nn.quantizable.modules.rnn</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.quantized">torch.nn.quantized</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.quantized.dynamic">torch.nn.quantized.dynamic</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.quantized.dynamic.modules">torch.nn.quantized.dynamic.modules</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.dynamic.modules.conv">torch.nn.quantized.dynamic.modules.conv</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.dynamic.modules.linear">torch.nn.quantized.dynamic.modules.linear</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.dynamic.modules.rnn">torch.nn.quantized.dynamic.modules.rnn</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.functional">torch.nn.quantized.functional</a>
</li>
        <li><a href="quantization-support.html#module-torch.nn.quantized.modules">torch.nn.quantized.modules</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.activation">torch.nn.quantized.modules.activation</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.batchnorm">torch.nn.quantized.modules.batchnorm</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.conv">torch.nn.quantized.modules.conv</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.dropout">torch.nn.quantized.modules.dropout</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.embedding_ops">torch.nn.quantized.modules.embedding_ops</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.functional_modules">torch.nn.quantized.modules.functional_modules</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.linear">torch.nn.quantized.modules.linear</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.normalization">torch.nn.quantized.modules.normalization</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.rnn">torch.nn.quantized.modules.rnn</a>
</li>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.utils">torch.nn.quantized.modules.utils</a>
</li>
        <li><a href="nn.html#module-torch.nn.utils">torch.nn.utils</a>
</li>
        <li><a href="nn.html#module-torch.nn.utils.clip_grad">torch.nn.utils.clip_grad</a>
</li>
        <li><a href="nn.html#module-torch.nn.utils.convert_parameters">torch.nn.utils.convert_parameters</a>
</li>
        <li><a href="nn.html#module-torch.nn.utils.fusion">torch.nn.utils.fusion</a>
</li>
        <li><a href="nn.html#module-torch.nn.utils.init">torch.nn.utils.init</a>
</li>
        <li><a href="nn.html#module-torch.nn.utils.memory_format">torch.nn.utils.memory_format</a>
</li>
        <li><a href="nn.html#module-torch.nn.utils.parametrizations"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">torch.nn.utils.parametrizations
torch.nn.utils 参数化</font></font></font></a>
</li>
        <li><a href="nn.html#module-torch.nn.utils.parametrize">torch.nn.utils.parametrize</a>
</li>
        <li><a href="nn.html#module-torch.nn.utils.prune">torch.nn.utils.prune</a>
</li>
        <li><a href="nn.html#module-torch.nn.utils.rnn">torch.nn.utils.rnn</a>
</li>
        <li><a href="nn.html#module-torch.nn.utils.stateless">torch.nn.utils.stateless</a>
</li>
        <li><a href="onnx_torchscript.html#module-torch.onnx">torch.onnx</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.errors">torch.onnx.errors</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.operators">torch.onnx.operators</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_caffe2">torch.onnx 符号化 caffe2</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_helper">torch.onnx 符号化助手</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset10">torch.onnx 符号操作集 10</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset11">torch.onnx 符号操作集 11</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset12">torch.onnx 符号操作集 12</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset13">torch.onnx 符号算子集 13</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset14">torch.onnx 符号算子集 14</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset15">torch.onnx 符号算子集 15</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset16">torch.onnx 符号算子集 16</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset17">torch.onnx 符号算子集 17</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset18">torch.onnx 符号算子集 18</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset19">torch.onnx 符号算子集 19</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset20">torch.onnx 符号操作集 20</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset7">torch.onnx 符号操作集 7</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset8">torch.onnx 符号操作集 8</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset9">torch.onnx 符号操作集 9</a>
</li>
        <li><a href="onnx.html#module-torch.onnx.utils">torch.onnx 工具</a>
</li>
        <li><a href="onnx_verification.html#module-torch.onnx.verification">torch.onnx 验证</a>
</li>
        <li><a href="optim.html#module-torch.optim">torch.optim</a>
</li>
        <li><a href="optim.html#module-torch.optim.adadelta">torch.optim.adadelta</a>
</li>
        <li><a href="optim.html#module-torch.optim.adagrad">torch.optim.adagrad</a>
</li>
        <li><a href="optim.html#module-torch.optim.adam">torch.optim.adam</a>
</li>
        <li><a href="optim.html#module-torch.optim.adamax">torch.optim.adamax</a>
</li>
        <li><a href="optim.html#module-torch.optim.adamw">torch.optim.adamw</a>
</li>
        <li><a href="optim.html#module-torch.optim.asgd">torch.optim.asgd</a>
</li>
        <li><a href="optim.html#module-torch.optim.lbfgs">torch.optim.lbfgs</a>
</li>
        <li><a href="optim.html#module-torch.optim.lr_scheduler">torch.optim.lr_scheduler</a>
</li>
        <li><a href="optim.html#module-torch.optim.nadam">torch.optim.nadam</a>
</li>
        <li><a href="optim.html#module-torch.optim.optimizer">torch.optim.optimizer</a>
</li>
        <li><a href="optim.html#module-torch.optim.radam">torch.optim.radam</a>
</li>
        <li><a href="optim.html#module-torch.optim.rmsprop">torch.optim.rmsprop</a>
</li>
        <li><a href="optim.html#module-torch.optim.rprop">torch.optim.rprop</a>
</li>
        <li><a href="optim.html#module-torch.optim.sgd">torch.optim.sgd</a>
</li>
        <li><a href="optim.html#module-torch.optim.sparse_adam">torch.optim.sparse_adam</a>
</li>
        <li><a href="optim.html#module-torch.optim.swa_utils">torch.optim.swa_utils</a>
</li>
        <li><a href="torch.overrides.html#module-torch.overrides">torch.overrides</a>
</li>
        <li><a href="package.html#module-torch.package">torch.package</a>
</li>
        <li><a href="package.html#module-torch.package.analyze">torch.package.analyze</a>
</li>
        <li><a href="package.html#module-torch.package.analyze.find_first_use_of_broken_modules">torch.package.analyze.find_first_use_of_broken_modules</a>
</li>
        <li><a href="package.html#module-torch.package.analyze.is_from_package">torch.package.analyze.is_from_package</a>
</li>
        <li><a href="package.html#module-torch.package.analyze.trace_dependencies">torch.package.analyze.trace_dependencies</a>
</li>
        <li><a href="package.html#module-torch.package.file_structure_representation">torch.package.file_structure_representation</a>
</li>
        <li><a href="package.html#module-torch.package.find_file_dependencies">torch.package.find_file_dependencies</a>
</li>
        <li><a href="package.html#module-torch.package.glob_group">torch.package.glob_group</a>
</li>
        <li><a href="package.html#module-torch.package.importer">torch.package.importer</a>
</li>
        <li><a href="package.html#module-torch.package.package_exporter">torch.package.package_exporter</a>
</li>
        <li><a href="package.html#module-torch.package.package_importer">torch.package.package_importer</a>
</li>
        <li><a href="profiler.html#module-torch.profiler">torch.profiler</a>
</li>
        <li><a href="profiler.html#module-torch.profiler.itt">torch.profiler.itt</a>
</li>
        <li><a href="profiler.html#module-torch.profiler.profiler">torch.profiler.profiler</a>
</li>
        <li><a href="profiler.html#module-torch.profiler.python_tracer">torch.profiler.python_tracer</a>
</li>
        <li><a href="quantization-support.html#module-torch.quantization">torch.quantization</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.fake_quantize">torch.quantization.fake_quantize</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.fuse_modules">torch.quantization.fuse_modules</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.fuser_method_mappings">torch.quantization.fuser_method_mappings</a>
</li>
        <li><a href="quantization-support.html#module-torch.quantization.fx">torch.quantization.fx</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.fx.convert">torch.quantization.fx.convert</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.fx.fuse">torch.quantization.fx.fuse</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.fx.fusion_patterns">torch.quantization.fx.fusion_patterns</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.fx.graph_module">torch.quantization.fx.graph_module</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.fx.match_utils">torch.quantization.fx.match_utils</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.fx.pattern_utils">torch.quantization.fx.pattern_utils</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.fx.prepare">torch.quantization.fx 准备</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.fx.quantization_patterns">torch.quantization.fx 量化模式</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.fx.quantization_types">torch.quantization.fx 量化类型</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.fx.utils">torch.quantization.fx 工具</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.observer">torch.quantization.observer</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.qconfig">torch.quantization.qconfig</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.quant_type">torch 量化量化类型</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.quantization_mappings">torch 量化量化映射</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.quantize">torch.quantization.quantize</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.quantize_fx">torch.quantization.quantize_fx</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.quantize_jit">torch.quantization.quantize_jit</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.stubs">torch 量化存根</a>
</li>
        <li><a href="quantization.html#module-torch.quantization.utils">torch 量化工具</a>
</li>
        <li><a href="torch.html#module-torch.quasirandom">torch.quasirandom</a>
</li>
        <li><a href="random.html#module-torch.random">torch.random</a>
</li>
        <li><a href="torch.html#module-torch.return_types">torch.return_types</a>
</li>
        <li><a href="torch.html#module-torch.serialization">torch.serialization</a>
</li>
        <li><a href="signal.html#module-torch.signal">torch.signal</a>
</li>
        <li><a href="signal.html#module-torch.signal.windows">torch.signal.windows</a>
</li>
        <li><a href="torch.html#module-torch.signal.windows.windows">torch.signal.windows.windows</a>
</li>
        <li><a href="sparse.html#module-torch.sparse">torch.sparse</a>
</li>
        <li><a href="torch.html#module-torch.sparse.semi_structured">torch.sparse.semi_structured</a>
</li>
        <li><a href="special.html#module-torch.special">torch.special</a>
</li>
        <li><a href="torch.html#module-torch.storage">torch.storage</a>
</li>
        <li><a href="testing.html#module-torch.testing">torch.testing</a>
</li>
        <li><a href="torch.html#module-torch.torch_version">torch 版本</a>
</li>
        <li><a href="torch.html#module-torch.types">torch 类型</a>
</li>
        <li><a href="utils.html#module-torch.utils">torch.utils</a>
</li>
        <li><a href="torch.html#module-torch.utils.backcompat">torch.utils.backcompat</a>
</li>
        <li><a href="utils.html#module-torch.utils.backend_registration">torch.utils.backend_registration</a>
</li>
        <li><a href="benchmark_utils.html#module-torch.utils.benchmark">torch.utils.benchmark</a>
</li>
        <li><a href="benchmark_utils.html#module-torch.utils.benchmark.examples">torch.utils.benchmark.examples</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.examples.blas_compare_setup">torch.utils.benchmark.examples.blas_compare_setup</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.examples.compare">torch.utils.benchmark.example 比较</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.examples.fuzzer">torch.utils.benchmark.example.fuzzer</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.examples.op_benchmark">torch.utils.benchmark 示例.op_benchmark</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.examples.simple_timeit">torch.utils.benchmark 示例.simple_timeit</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.examples.spectral_ops_fuzz_test">torch.utils.benchmark.examples.spectral_ops_fuzz_test</a>
</li>
        <li><a href="benchmark_utils.html#module-torch.utils.benchmark.op_fuzzers">torch.utils.benchmark.op_fuzzers</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.op_fuzzers.binary">torch.utils.benchmark.op_fuzzers.binary</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.op_fuzzers.sparse_binary">torch.utils.benchmark 操作模糊器.sparse_binary</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.op_fuzzers.sparse_unary">torch.utils.benchmark 操作模糊器.sparse_unary</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.op_fuzzers.spectral"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">torch.utils.benchmark.op_fuzzers.spectral

torch.utils.benchmark.op_fuzzers.频谱</font></font></font></a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.op_fuzzers.unary">torch.utils.benchmark.op_fuzzers.一元操作模糊器</a>
</li>
        <li><a href="benchmark_utils.html#module-torch.utils.benchmark.utils">torch.utils.benchmark.utils 工具</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.common">torch.utils.benchmark.utils.common</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.compare">torch.utils.benchmark.utils.compare</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.compile">torch.utils.benchmark.utils.compile</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.cpp_jit">torch.utils.benchmark.utils.cpp_jit</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.fuzzer">torch.utils.benchmark.utils.fuzzer</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.sparse_fuzzer">torch.utils.benchmark.utils.sparse_fuzzer</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.timer">torch.utils.benchmark.utils.timer</a>
</li>
        <li><a href="benchmark_utils.html#module-torch.utils.benchmark.utils.valgrind_wrapper">torch.utils.benchmark.utils.valgrind_wrapper</a>
</li>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.valgrind_wrapper.timer_interface">torch.utils.benchmark.utils.valgrind_wrapper.timer_interface</a>
</li>
        <li><a href="bottleneck.html#module-torch.utils.bottleneck">torch.utils.bottleneck</a>
</li>
        <li><a href="utils.html#module-torch.utils.bundled_inputs">torch.utils.bundled_inputs</a>
</li>
        <li><a href="utils.html#module-torch.utils.checkpoint">torch.utils.checkpoint</a>
</li>
        <li><a href="utils.html#module-torch.utils.collect_env">torch.utils.collect_env</a>
</li>
        <li><a href="utils.html#module-torch.utils.cpp_backtrace">torch.utils.cpp_backtrace</a>
</li>
        <li><a href="utils.html#module-torch.utils.cpp_extension">torch.utils.cpp_extension</a>
</li>
        <li><a href="data.html#module-torch.utils.data">torch.utils.data</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.backward_compatibility">torch.utils.data.backward_compatibility</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.dataloader">torch.utils.data.dataloader</a>
</li>
        <li><a href="data.html#module-torch.utils.data.datapipes">torch.utils.data.datapipes</a>
</li>
        <li><a href="data.html#module-torch.utils.data.datapipes.dataframe">torch.utils.data.datapipes.dataframe</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.dataframe.dataframe_wrapper">torch.utils.data.datapipes.dataframe.dataframe_wrapper</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.dataframe.dataframes">torch.utils.data.datapipes.dataframe.dataframes</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.dataframe.datapipes">torch.utils.data.datapipes.dataframe.datapipes</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.dataframe.structures">torch.utils.data.datapipes.dataframe.structures</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.datapipe">torch.utils.data.datapipes.datapipe</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.gen_pyi">torch.utils.data.datapipes.gen_pyi</a>
</li>
        <li><a href="data.html#module-torch.utils.data.datapipes.iter">torch.utils.data.datapipes.iter</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.callable">torch.utils.data.datapipes.iter.callable</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.combinatorics">torch.utils.data.datapipes.iter.combinatorics</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.combining">torch.utils.data.datapipes.iter.combining</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.filelister">torch.utils.data.datapipes.iter.filelister</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.fileopener">torch.utils.data.datapipes.iter.fileopener</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.grouping">torch.utils.data.datapipes.iter.grouping</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.routeddecoder">torch.utils.data.datapipes.iter.routeddecoder</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.selecting">torch.utils.data.datapipes.iter.selecting</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.sharding">torch.utils.data.datapipes.iter.sharding</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.streamreader">torch.utils.data.datapipes.iter.streamreader</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.utils"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">torch.utils.data.datapipes.iter.utils
torch.utils.data.datapipes.iter.utils</font></font></font></a>
</li>
        <li><a href="data.html#module-torch.utils.data.datapipes.map">torch.utils.data.datapipes.map</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.map.callable">torch.utils.data.datapipes.map.callable</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.map.combinatorics">torch.utils.data.datapipes.map.combinatorics</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.map.combining">torch.utils.data.datapipes.map.combining</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.map.grouping">torch.utils.data.datapipes.map.grouping</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.map.utils">torch.utils.data.datapipes.map.utils</a>
</li>
        <li><a href="data.html#module-torch.utils.data.datapipes.utils">torch.utils.data.datapipes.utils</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.utils.common">torch.utils.data.datapipes.utils.common</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.utils.decoder">torch.utils.data.datapipes.utils 解码器</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.datapipes.utils.snapshot">torch.utils.data.datapipes.utils 快照</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.dataset">torch.utils.data.dataset</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.distributed">torch.utils.data.distributed</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.graph">torch.utils.data.graph</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.graph_settings">torch.utils.data.graph_settings</a>
</li>
        <li><a href="utils.html#module-torch.utils.data.sampler">torch.utils.data.sampler</a>
</li>
        <li><a href="deterministic.html#module-torch.utils.deterministic">torch.utils.deterministic</a>
</li>
        <li><a href="utils.html#module-torch.utils.dlpack">torch.utils.dlpack</a>
</li>
        <li><a href="utils.html#module-torch.utils.file_baton">torch.utils.file_baton</a>
</li>
        <li><a href="utils.html#module-torch.utils.flop_counter">torch.utils.flop_counter</a>
</li>
        <li><a href="torch.html#module-torch.utils.hipify">torch.utils.hipify</a>
</li>
        <li><a href="utils.html#module-torch.utils.hipify.constants">torch.utils.hipify.常量</a>
</li>
        <li><a href="utils.html#module-torch.utils.hipify.cuda_to_hip_mappings">torch.utils.hipify.cuda_to_hip_mappings</a>
</li>
        <li><a href="utils.html#module-torch.utils.hipify.hipify_python">torch.utils.hipify.hipify_python</a>
</li>
        <li><a href="utils.html#module-torch.utils.hipify.version">torch.utils.hipify.version</a>
</li>
        <li><a href="utils.html#module-torch.utils.hooks">torch.utils.hooks</a>
</li>
        <li><a href="jit_utils.html#module-torch.utils.jit">torch.utils.jit</a>
</li>
        <li><a href="utils.html#module-torch.utils.jit.log_extract">torch.utils.jit.log_extract</a>
</li>
        <li><a href="utils.html#module-torch.utils.mkldnn">torch.utils.mkldnn</a>
</li>
        <li><a href="utils.html#module-torch.utils.mobile_optimizer">torch.utils.mobile_optimizer</a>
</li>
        <li><a href="torch.html#module-torch.utils.model_dump">torch.utils.model_dump</a>
</li>
        <li><a href="model_zoo.html#module-torch.utils.model_zoo">torch.utils.model_zoo</a>
</li>
        <li><a href="module_tracker.html#module-torch.utils.module_tracker">torch.utils.module_tracker</a>
</li>
        <li><a href="notes/serialization.html#module-torch.utils.serialization">torch.utils.serialization</a>
</li>
        <li><a href="notes/serialization.html#module-torch.utils.serialization.config">torch.utils.serialization.config</a>
</li>
        <li><a href="utils.html#module-torch.utils.show_pickle">torch.utils.show_pickle</a>
</li>
        <li><a href="tensorboard.html#module-torch.utils.tensorboard">torch.utils.tensorboard</a>
</li>
        <li><a href="utils.html#module-torch.utils.tensorboard.summary">torch.utils.tensorboard.summary</a>
</li>
        <li><a href="utils.html#module-torch.utils.tensorboard.writer">torch.utils.tensorboard.writer</a>
</li>
        <li><a href="utils.html#module-torch.utils.throughput_benchmark">torch.utils.throughput_benchmark</a>
</li>
        <li><a href="torch.html#module-torch.utils.viz">torch.utils.viz</a>
</li>
        <li><a href="utils.html#module-torch.utils.weak">torch.utils.weak</a>
</li>
        <li><a href="torch.html#module-torch.version">torch 版本</a>
</li>
        <li><a href="xpu.html#module-torch.xpu">torch.xpu</a>
</li>
        <li><a href="xpu.html#module-torch.xpu.memory">torch.xpu.memory</a>
</li>
        <li><a href="xpu.html#module-torch.xpu.random">torch.xpu.random</a>
</li>
        <li><a href="xpu.html#module-torch.xpu.streams">torch.xpu.streams</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.nn.Module.html#torch.nn.Module">torch.nn 中的模块（类）</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.module">torch.distributed.fsdp.FullyShardedDataParallel 属性</a>
</li>
      <li><a href="export.html#torch.export.ExportedProgram.module">torch.export.ExportedProgram 方法中的 module()</a>
</li>
      <li><a href="generated/torch.Tensor.module_load.html#torch.Tensor.module_load">torch.Tensor 方法中的 module_load()</a>
</li>
      <li><a href="export.html#torch.export.ModuleCallEntry">torch.export 中的 ModuleCallEntry（类）</a>
</li>
      <li><a href="export.html#torch.export.ModuleCallSignature">torch.export 中的 ModuleCallSignature（类）</a>
</li>
      <li><a href="generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict">torch.nn 中的 ModuleDict（类）</a>
</li>
      <li><a href="generated/torch.nn.ModuleList.html#torch.nn.ModuleList">torch.nn 中的 ModuleList（类）</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.modules">modules() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.modules">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.modules">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="module_tracker.html#torch.utils.module_tracker.ModuleTracker">ModuleTracker (torch.utils.module_tracker 中的类)</a>
</li>
      <li><a href="distributed.html#torch.distributed.monitored_barrier">monitored_barrier() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="export.html#torch.export.passes.move_to_device_pass">move_to_device_pass() (在 torch.export.passes 模块中)</a>
</li>
      <li><a href="generated/torch.moveaxis.html#torch.moveaxis">torch 模块中的 moveaxis()</a>

      <ul>
        <li><a href="generated/torch.Tensor.moveaxis.html#torch.Tensor.moveaxis">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.movedim.html#torch.movedim">torch 模块中的 movedim()</a>

      <ul>
        <li><a href="generated/torch.Tensor.movedim.html#torch.Tensor.movedim">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.observer.MovingAverageMinMaxObserver.html#torch.ao.quantization.observer.MovingAverageMinMaxObserver">torch.ao.quantization.observer 中的 MovingAverageMinMaxObserver 类</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver.html#torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver">torch.ao.quantization.observer 中的 MovingAveragePerChannelMinMaxObserver 类</a>
</li>
      <li><a href="storage.html#torch.UntypedStorage.mps">mps() (torch.UntypedStorage 方法)</a>
</li>
      <li><a href="generated/torch.nn.functional.mse_loss.html#torch.nn.functional.mse_loss">mse_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="generated/torch.nn.MSELoss.html#torch.nn.MSELoss">MSELoss (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.msort.html#torch.msort">msort() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.msort.html#torch.Tensor.msort">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="tensors.html#torch.Tensor.mT">mT（torch.Tensor 属性）</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.mtia">mtia() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.mtia">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.mtia">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.mul.html#torch.mul">mul() (在模块 torch 中)</a>

      <ul>
        <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.mul">(torch.ao.ns._numeric_suite.Shadow 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.mul.html#torch.Tensor.mul">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.mul_.html#torch.Tensor.mul_">mul_() (torch.Tensor 方法)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow.mul_scalar">mul_scalar() (torch.ao.ns._numeric_suite.Shadow 方法)</a>
</li>
      <li><a href="generated/torch.linalg.multi_dot.html#torch.linalg.multi_dot">multi_dot() (在 torch.linalg 模块中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.Store.multi_get">multi_get() (torch.distributed.Store 方法)</a>
</li>
      <li><a href="generated/torch.nn.functional.multi_margin_loss.html#torch.nn.functional.multi_margin_loss">multi_margin_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.Store.multi_set">multi_set() (torch.distributed.Store 方法)</a>
</li>
      <li><a href="special.html#torch.special.multigammaln">multigammaln() (在 torch.special 模块中)</a>
</li>
      <li><a href="generated/torch.ao.nn.quantizable.MultiheadAttention.html#torch.ao.nn.quantizable.MultiheadAttention">MultiheadAttention (torch.ao.nn.quantizable 中的类)</a>

      <ul>
        <li><a href="generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.functional.multilabel_margin_loss.html#torch.nn.functional.multilabel_margin_loss">multilabel_margin_loss() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="generated/torch.nn.functional.multilabel_soft_margin_loss.html#torch.nn.functional.multilabel_soft_margin_loss">torch.nn.functional 模块中的 multilabel_soft_margin_loss()</a>
</li>
      <li><a href="generated/torch.nn.MultiLabelMarginLoss.html#torch.nn.MultiLabelMarginLoss">torch.nn 中的 MultiLabelMarginLoss 类</a>
</li>
      <li><a href="generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss">torch.nn 中的 MultiLabelSoftMarginLoss 类</a>
</li>
      <li><a href="generated/torch.nn.MultiMarginLoss.html#torch.nn.MultiMarginLoss">torch.nn 中的 MultiMarginLoss 类</a>
</li>
      <li><a href="distributions.html#torch.distributions.multinomial.Multinomial">torch.distributions.multinomial 中的多项式（类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraints.multinomial">torch.distributions.constraints 模块中的多项式</a>
</li>
      <li><a href="generated/torch.multinomial.html#torch.multinomial">torch 模块中的 multinomial()</a>

      <ul>
        <li><a href="generated/torch.Tensor.multinomial.html#torch.Tensor.multinomial">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR">torch.optim.lr_scheduler 中的 MultiplicativeLR（类）</a>
</li>
      <li><a href="generated/torch.multiply.html#torch.multiply">torch 模块中的 multiply()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.multiply.html#torch.Tensor.multiply">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.multiply_.html#torch.Tensor.multiply_">torch.Tensor 方法中的 multiply_()</a>
</li>
      <li><a href="elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.MultiprocessContext">torch.distributed.elastic.multiprocessing.api 中的 MultiprocessContext 类</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR">torch.optim.lr_scheduler 中的 MultiStepLR 类</a>
</li>
      <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal">torch.distributions.multivariate_normal 中的 MultivariateNormal（类）</a>
</li>
      <li><a href="generated/torch.mv.html#torch.mv">torch 模块中的 mv()</a>

      <ul>
        <li><a href="generated/torch.Tensor.mv.html#torch.Tensor.mv">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.mvlgamma.html#torch.mvlgamma">torch 模块中的 mvlgamma()</a>

      <ul>
        <li><a href="generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.mvlgamma_.html#torch.Tensor.mvlgamma_">torch.Tensor 方法中的 mvlgamma_()</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="N">N</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.optim.NAdam.html#torch.optim.NAdam">torch.optim 中的 NAdam（类）</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.Kernel.html#torch.autograd.profiler_util.Kernel.name">name（torch.autograd.profiler_util.Kernel 属性）</a>

      <ul>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.name">torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend 属性</a>
</li>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.name">torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend 属性</a>
</li>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.name">(torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend 属性)</a>
</li>
        <li><a href="rpc.html#torch.distributed.rpc.WorkerInfo.name">(torch.distributed.rpc.WorkerInfo 属性)</a>
</li>
        <li><a href="monitor.html#torch.monitor.Aggregation.name">(torch.monitor.Aggregation 属性)</a>
</li>
        <li><a href="monitor.html#torch.monitor.Event.name">(torch.monitor.Event 属性)</a>
</li>
        <li><a href="monitor.html#torch.monitor.Stat.name">(torch.monitor.Stat 属性)</a>
</li>
        <li><a href="generated/torch.nn.attention.SDPBackend.html#torch.nn.attention.SDPBackend.name">(torch.nn.attention.SDPBackend 属性)</a>
</li>
        <li><a href="profiler.html#torch.profiler.ProfilerActivity.name">(torch.profiler.ProfilerActivity 属性)</a>
</li>
        <li><a href="torch.html#torch.Tag.name">(torch.Tag 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.graph.Node.name.html#torch.autograd.graph.Node.name">name() (torch.autograd.graph.Node 方法)</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.named_buffers">named_buffers() (torch.distributed.fsdp.FullyShardedDataParallel 方法)</a>

      <ul>
        <li><a href="export.html#torch.export.ExportedProgram.named_buffers">(torch.export.ExportedProgram 方法)</a>
</li>
        <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.named_buffers">(torch.jit.ScriptModule 方法)</a>
</li>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.named_buffers">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.named_buffers">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.named_children">named_children() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.named_children">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.named_children">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.named_modules">named_modules() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.named_modules">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.named_modules">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.named_parameters">named_parameters() (torch.distributed.fsdp.FullyShardedDataParallel 方法)</a>

      <ul>
        <li><a href="export.html#torch.export.ExportedProgram.named_parameters">(torch.export.ExportedProgram 方法)</a>
</li>
        <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.named_parameters">(torch.jit.ScriptModule 方法)</a>
</li>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.named_parameters">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.named_parameters">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.Unflatten.html#torch.nn.Unflatten.NamedShape">NamedShape（torch.nn.Unflatten 属性）</a>
</li>
      <li><a href="named_tensor.html#torch.Tensor.names">names（torch.Tensor 属性）</a>
</li>
      <li><a href="generated/torch.nan_to_num.html#torch.nan_to_num">nan_to_num()（模块 torch 中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.nan_to_num_.html#torch.Tensor.nan_to_num_">nan_to_num_()（torch.Tensor 方法）</a>
</li>
      <li><a href="generated/torch.nanmean.html#torch.nanmean">torch 模块中的 nanmean()</a>

      <ul>
        <li><a href="generated/torch.Tensor.nanmean.html#torch.Tensor.nanmean">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nanmedian.html#torch.nanmedian">torch 模块中的 nanmedian()</a>

      <ul>
        <li><a href="generated/torch.Tensor.nanmedian.html#torch.Tensor.nanmedian">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nanquantile.html#torch.nanquantile">torch 模块中的 nanquantile()</a>

      <ul>
        <li><a href="generated/torch.Tensor.nanquantile.html#torch.Tensor.nanquantile">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nansum.html#torch.nansum">torch 模块中的 nansum()</a>

      <ul>
        <li><a href="generated/torch.Tensor.nansum.html#torch.Tensor.nansum">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.narrow.html#torch.narrow">torch 模块中的 narrow()</a>

      <ul>
        <li><a href="nested.html#torch.nested.narrow">torch.nested 模块中</a>
</li>
        <li><a href="generated/torch.Tensor.narrow.html#torch.Tensor.narrow">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.narrow_copy.html#torch.narrow_copy">torch 模块中的 narrow_copy()</a>

      <ul>
        <li><a href="generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.nbytes.html#torch.Tensor.nbytes">torch.Tensor 属性中的 nbytes</a>
</li>
      <li><a href="storage.html#torch.TypedStorage.nbytes">nbytes() (torch.TypedStorage 方法)</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.nbytes">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.ndim.html#torch.Tensor.ndim">ndim (torch.Tensor 属性)</a>
</li>
      <li><a href="generated/torch.Tensor.ndimension.html#torch.Tensor.ndimension">ndimension() (torch.Tensor 方法)</a>
</li>
      <li><a href="special.html#torch.special.ndtr">ndtr() (在 torch.special 模块中)</a>
</li>
      <li><a href="special.html#torch.special.ndtri">torch.special 模块中的 ndtri()函数</a>
</li>
      <li><a href="generated/torch.ne.html#torch.ne">torch 模块中的 ne()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.ne.html#torch.Tensor.ne">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.ne_.html#torch.Tensor.ne_">torch.Tensor 方法中的 ne_()函数</a>
</li>
      <li><a href="generated/torch.neg.html#torch.neg">torch 模块中的 neg()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.neg.html#torch.Tensor.neg">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.neg_.html#torch.Tensor.neg_">neg_() (torch.Tensor 方法)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.negative.html#torch.negative">negative() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.negative.html#torch.Tensor.negative">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.negative_.html#torch.Tensor.negative_">negative_() (torch.Tensor 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.negative_binomial.NegativeBinomial">NegativeBinomial (torch.distributions.negative_binomial 中的类)</a>
</li>
      <li><a href="generated/torch.Tensor.nelement.html#torch.Tensor.nelement">torch.Tensor 方法 nelement()</a>
</li>
      <li><a href="nested.html#torch.nested.nested_tensor">torch.nested 模块中的 nested_tensor()</a>
</li>
      <li><a href="nested.html#torch.nested.nested_tensor_from_jagged">torch.nested 模块中的 nested_tensor_from_jagged()</a>
</li>
      <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction">torch.autograd.function 中的 NestedIOFunction 类</a>
</li>
      <li><a href="storage.html#torch.UntypedStorage.new">new() (torch.UntypedStorage 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.new_empty.html#torch.Tensor.new_empty">new_empty() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.new_full.html#torch.Tensor.new_full">new_full() (torch.Tensor 方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.new_group">new_group() (在 torch.distributed 模块中)</a>
</li>
      <li><a href="generated/torch.Tensor.new_ones.html#torch.Tensor.new_ones">new_ones() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.new_tensor.html#torch.Tensor.new_tensor">new_tensor() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.new_zeros.html#torch.Tensor.new_zeros">new_zeros() (torch.Tensor 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.Node.next">next (torch.fx.Node 属性)</a>
</li>
      <li><a href="generated/torch.autograd.graph.Node.next_functions.html#torch.autograd.graph.Node.next_functions">next_functions (torch.autograd.graph.Node 属性)</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.next_rendezvous">next_rendezvous() (torch.distributed.elastic.rendezvous.RendezvousHandler 方法)</a>
</li>
      <li><a href="generated/torch.nextafter.html#torch.nextafter">nextafter() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.nextafter_.html#torch.Tensor.nextafter_">nextafter_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.nn.functional.nll_loss.html#torch.nn.functional.nll_loss">nll_loss() (在模块 torch.nn.functional 中)</a>
</li>
      <li><a href="generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss">NLLLoss（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.no_grad.html#torch.no_grad">torch 中的 no_grad（类）</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.no_sync">torch.distributed.fsdp.FullyShardedDataParallel 的 no_sync()方法</a>

      <ul>
        <li><a href="generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync">(torch.nn.parallel.DistributedDataParallel 方法)</a>
</li>
      </ul></li>
      <li><a href="fx.html#torch.fx.Node">torch.fx 中的 Node 类</a>
</li>
      <li><a href="fx.html#torch.fx.Graph.node_copy">torch.fx.Graph 的 node_copy()方法</a>
</li>
      <li><a href="fx.html#torch.fx.Graph.nodes">节点（torch.fx 图属性）</a>
</li>
      <li><a href="generated/torch.nonzero.html#torch.nonzero">nonzero()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.nonzero.html#torch.Tensor.nonzero">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook">noop_hook()（在 torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks 模块中）</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.noop_mask">noop_mask()（在 torch.nn.attention.flex_attention 模块中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.NoopObserver.html#torch.ao.quantization.observer.NoopObserver">NoopObserver（torch.ao.quantization.observer 中的类）</a>
</li>
      <li><a href="generated/torch.norm.html#torch.norm">torch 模块中的 norm()函数</a>

      <ul>
        <li><a href="generated/torch.linalg.norm.html#torch.linalg.norm">(在 torch.linalg 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.norm.html#torch.Tensor.norm">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.normal.Normal">普通（torch.distributions.normal 中的类）</a>
</li>
      <li><a href="generated/torch.normal.html#torch.normal">normal() (在 torch 模块中)</a>
</li>
      <li><a href="nn.init.html#torch.nn.init.normal_">normal_() (在模块 torch.nn.init 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.normal_.html#torch.Tensor.normal_">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.functional.normalize.html#torch.nn.functional.normalize">normalize() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="fx.html#torch.fx.Node.normalized_arguments">normalized_arguments() (torch.fx 节点方法)</a>
</li>
      <li><a href="generated/torch.not_equal.html#torch.not_equal">不等于()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.not_equal_.html#torch.Tensor.not_equal_">not_equal_() (torch.Tensor 方法)</a>
</li>
      <li><a href="distributed.algorithms.join.html#torch.distributed.algorithms.Join.notify_join_context">notify_join_context() (torch.distributed.algorithms.Join 静态方法)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.NSTracer">torch.ao.ns._numeric_suite_fx 中的 NSTracer（类）</a>
</li>
      <li><a href="elastic/metrics.html#torch.distributed.elastic.metrics.api.NullMetricHandler">torch.distributed.elastic.metrics.api 中的 NullMetricHandler（类）</a>
</li>
      <li><a href="distributed.html#torch.distributed.Store.num_keys">num_keys() (torch.distributed.Store 方法)</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.num_nodes_waiting">num_nodes_waiting() (torch.distributed.elastic.rendezvous.RendezvousHandler 方法)</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads">num_worker_threads (torch.distributed.rpc.TensorPipeRpcBackendOptions 属性)</a>
</li>
      <li><a href="generated/torch.numel.html#torch.numel">numel() (在模块 torch 中)</a>

      <ul>
        <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.numel">(torch.nn.attention.flex_attention.BlockMask 方法)</a>
</li>
        <li><a href="size.html#torch.Size.numel">(torch.Size 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.numel.html#torch.Tensor.numel">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.attention.bias.CausalVariant.html#torch.nn.attention.bias.CausalVariant.numerator">分子（torch.nn.attention.bias.CausalVariant 属性）</a>
</li>
      <li><a href="generated/torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY.html#torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY">NUMERIC_DEBUG_HANDLE_KEY（在 torch.ao.quantization 模块中）</a>
</li>
      <li><a href="generated/torch.Tensor.numpy.html#torch.Tensor.numpy">numpy() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.signal.windows.nuttall.html#torch.signal.windows.nuttall">nuttall()（在 torch.signal.windows 模块中）</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="O">O</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.ao.quantization.backend_config.ObservationType.html#torch.ao.quantization.backend_config.ObservationType">观察类型（torch.ao.quantization.backend_config 中的类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.ObserverBase.html#torch.ao.quantization.observer.ObserverBase">ObserverBase（torch.ao.quantization.observer 中的类）</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.OffloadPolicy">OffloadPolicy（torch.distributed.fsdp 中的类）</a>
</li>
      <li><a href="fx.html#torch.fx.Graph.on_generate_code">on_generate_code()（torch.fx.Graph 方法）</a>
</li>
      <li><a href="generated/torch.autograd.function.once_differentiable.html#torch.autograd.function.once_differentiable">once_differentiable()（在 torch.autograd.function 模块中）</a>
</li>
      <li><a href="generated/torch.nn.functional.one_hot.html#torch.nn.functional.one_hot">torch.nn.functional 模块中的 one_hot()函数</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR">torch.optim.lr_scheduler 中的 OneCycleLR 类</a>
</li>
      <li><a href="generated/torch.jit.onednn_fusion_enabled.html#torch.jit.onednn_fusion_enabled">torch.jit 模块中的 onednn_fusion_enabled()</a>
</li>
      <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical">torch.distributions.one_hot_categorical 中的 OneHotCategorical 类</a>
</li>
      <li><a href="generated/torch.ones.html#torch.ones">torch 模块中的 ones()</a>

      <ul>
        <li><a href="distributed.tensor.html#torch.distributed.tensor.ones">torch.distributed.tensor 模块中的()</a>
</li>
      </ul></li>
      <li><a href="nn.init.html#torch.nn.init.ones_">ones_() (在模块 torch.nn.init 中)</a>
</li>
      <li><a href="generated/torch.ones_like.html#torch.ones_like">ones_like() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.onnx_compatible">onnx_compatible() (torch.onnx.JitScalarType 方法)</a>
</li>
      <li><a href="generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.onnx_type">onnx_type() (torch.onnx.JitScalarType 方法)</a>
</li>
      <li><a href="onnx_verification.html#torch.onnx.verification.OnnxBackend">torch.onnx.verification 中的 OnnxBackend（类）</a>
</li>
      <li><a href="onnx_dynamo.html#torch.onnx.OnnxExporterError">torch.onnx 中的 OnnxExporterError（类）</a>
</li>
      <li><a href="onnx_dynamo.html#torch.onnx.ONNXProgram">torch.onnx 中的 ONNXProgram（类）</a>
</li>
      <li><a href="onnx_dynamo.html#torch.onnx.OnnxRegistry">torch.onnx 中的 OnnxRegistry（类）</a>
</li>
      <li><a href="onnx_dynamo.html#torch.onnx.ONNXRuntimeOptions">ONNXRuntimeOptions（torch.onnx 中的类）</a>
</li>
      <li><a href="onnx_verification.html#torch.onnx.verification.OnnxTestCaseRepro">OnnxTestCaseRepro（torch.onnx.verification 中的类）</a>
</li>
      <li><a href="library.html#torch.library.opcheck">opcheck()（torch.library 模块中的函数）</a>
</li>
      <li><a href="onnx_dynamo.html#torch.onnx.OnnxRegistry.opset_version">opset_version（torch.onnx.OnnxRegistry 属性）</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict">optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel 静态方法)</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_to_load">optim_state_dict_to_load() (torch.distributed.fsdp.FullyShardedDataParallel 静态方法)</a>
</li>
      <li><a href="onnx_dynamo.html#torch.onnx.ONNXProgram.optimize">optimize() (torch.onnx.ONNXProgram 方法)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.jit.optimize_for_inference.html#torch.jit.optimize_for_inference">optimize_for_inference() (模块 torch.jit 中)</a>
</li>
      <li><a href="mobile_optimizer.html#torch.utils.mobile_optimizer.optimize_for_mobile">torch.utils.mobile_optimizer 模块中的 optimize_for_mobile()</a>
</li>
      <li><a href="optim.html#torch.optim.Optimizer">torch.optim 模块中的 Optimizer 类</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.OptimStateDictConfig">torch.distributed.fsdp 模块中的 OptimStateDictConfig 类</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.or_masks">torch.nn.attention.flex_attention 模块中的 or_masks()</a>
</li>
      <li><a href="generated/torch.orgqr.html#torch.orgqr">orgqr()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.orgqr.html#torch.Tensor.orgqr">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ormqr.html#torch.ormqr">ormqr()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.ormqr.html#torch.Tensor.ormqr">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.utils.parametrizations.orthogonal.html#torch.nn.utils.parametrizations.orthogonal">orthogonal()（在 torch.nn.utils.parametrizations 模块中）</a>
</li>
      <li><a href="nn.init.html#torch.nn.init.orthogonal_">orthogonal_()（在 torch.nn.init 模块中）</a>
</li>
      <li><a href="generated/torch.outer.html#torch.outer">outer() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.outer.html#torch.Tensor.outer">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.OutOfMemoryError.html#torch.cuda.OutOfMemoryError">内存溢出错误</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html#torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.output">输出() (torch.fx.experimental.symbolic_shapes.传播未备份符号整数的 PropagateUnbackedSymInts 方法)</a>

      <ul>
        <li><a href="fx.html#torch.fx.Graph.output">(torch.fx.Graph 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Interpreter.output">(torch.fx.Interpreter 方法)</a>
</li>
      </ul></li>
      <li><a href="fx.html#torch.fx.Graph.output_node">output_node() (torch.fx.Graph 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.ObservationType.html#torch.ao.quantization.backend_config.ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT">OUTPUT_SHARE_OBSERVER_WITH_INPUT (torch.ao.quantization.backend_config.ObservationType 属性)</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.ObservationType.html#torch.ao.quantization.backend_config.ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT">OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT (torch.ao.quantization.backend_config.ObservationType 属性)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.OutputComparisonLogger">OutputComparisonLogger (torch.ao.ns._numeric_suite_fx 中的类)</a>
</li>
      <li><a href="export.html#torch.export.graph_signature.OutputKind">OutputKind (torch.export.graph_signature 中的类)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.OutputLogger">torch.ao.ns._numeric_suite 中的 OutputLogger（类）</a>

      <ul>
        <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.OutputLogger">torch.ao.ns._numeric_suite_fx 中的（类）</a>
</li>
      </ul></li>
      <li><a href="export.html#torch.export.graph_signature.OutputSpec">torch.export.graph_signature 中的 OutputSpec（类）</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.PyRRef.owner">torch.distributed.rpc.PyRRef 方法中的 owner()</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.PyRRef.owner_name">torch.distributed.rpc.PyRRef 方法中的 owner_name()</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="P">P</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributed.html#torch.distributed.P2POp">P2POp（torch.distributed 中的类）</a>
</li>
      <li><a href="generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence">pack_padded_sequence()（在 torch.nn.utils.rnn 模块中）</a>
</li>
      <li><a href="generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence">pack_sequence()（在 torch.nn.utils.rnn 模块中）</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter">PackageExporter（torch.package 中的类）</a>
</li>
      <li><a href="package.html#torch.package.PackageImporter">PackageImporter（torch.package 中的类）</a>
</li>
      <li><a href="package.html#torch.package.PackagingError">包装错误（torch.package 中的类）</a>
</li>
      <li><a href="generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence">torch.nn.utils.rnn 中的 PackedSequence（类）</a>
</li>
      <li><a href="generated/torch.nn.functional.pad.html#torch.nn.functional.pad">torch.nn.functional 模块中的 pad()函数</a>
</li>
      <li><a href="generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence">torch.nn.utils.rnn 模块中的 pad_packed_sequence()函数</a>
</li>
      <li><a href="generated/torch.nn.utils.rnn.pad_sequence.html#torch.nn.utils.rnn.pad_sequence">torch.nn.utils.rnn 模块中的 pad_sequence()函数</a>
</li>
      <li><a href="generated/torch.nn.functional.pairwise_distance.html#torch.nn.functional.pairwise_distance">torch.nn.functional 模块中的 pairwise_distance()函数</a>
</li>
      <li><a href="generated/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance">torch.nn 中的 PairwiseDistance 类</a>
</li>
      <li><a href="config_mod.html#torch.__config__.parallel_info">parallel_info() (在模块 torch.__config__ 中)</a>
</li>
      <li><a href="distributed.tensor.parallel.html#torch.distributed.tensor.parallel.parallelize_module">parallelize_module() (在模块 torch.distributed.tensor.parallel 中)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.param_shape">参数形状（torch.distributions.bernoulli.Bernoulli 属性）</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.param_shape">(torch.distributions.binomial.Binomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.param_shape">(torch.distributions.categorical.Categorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.param_shape">(torch.distributions.multinomial.Multinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.negative_binomial.NegativeBinomial.param_shape">(torch.distributions.negative_binomial.NegativeBinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape">(torch.distributions.one_hot_categorical.OneHotCategorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape">(torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter">torch.nn.parameter 中的参数（类）</a>
</li>
      <li><a href="generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict">torch.nn 中的 ParameterDict（类）</a>
</li>
      <li><a href="generated/torch.nn.ParameterList.html#torch.nn.ParameterList">torch.nn 中的 ParameterList（类）</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.GradBucket.parameters">torch.distributed.GradBucket 中的 parameters()（方法）</a>

      <ul>
        <li><a href="export.html#torch.export.ExportedProgram.parameters">(torch.export.ExportedProgram 方法)</a>
</li>
        <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.parameters">(torch.jit.ScriptModule 方法)</a>
</li>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.parameters">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.parameters">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.utils.parameters_to_vector.html#torch.nn.utils.parameters_to_vector">参数_to_向量()（在 torch.nn.utils 模块中）</a>
</li>
      <li><a href="generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList">torch.nn.utils.parametrize 中的 ParametrizationList（类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.pareto.Pareto">torch.distributions.pareto 中的 Pareto（类）</a>
</li>
      <li><a href="generated/torch.autograd.profiler.parse_nvprof_trace.html#torch.autograd.profiler.parse_nvprof_trace">torch.autograd.profiler 模块中的 parse_nvprof_trace() 函数</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.placement_types.Partial">torch.distributed.tensor.placement_types 中的 Partial（类）</a>
</li>
      <li><a href="distributed.html#torch.distributed.FileStore.path">torch.distributed.FileStore 属性路径</a>
</li>
      <li><a href="fx.html#torch.fx.Tracer.path_of_module">torch.fx.Tracer 方法 path_of_module()</a>
</li>
      <li><a href="generated/torch.pca_lowrank.html#torch.pca_lowrank">torch 模块中的 pca_lowrank()</a>
</li>
      <li><a href="elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.PContext">torch.distributed.elastic.multiprocessing.api 中的 PContext 类</a>
</li>
      <li><a href="generated/torch.nn.functional.pdist.html#torch.nn.functional.pdist">torch.nn.functional 模块中的 pdist()函数</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig.per_channel_dynamic_qconfig.html#torch.ao.quantization.qconfig.per_channel_dynamic_qconfig">torch.ao.quantization.qconfig 模块中的 per_channel_dynamic_qconfig</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.PerAxis.html#torch.ao.quantization.observer.PerAxis">PerAxis（torch.ao.quantization.observer 类中的类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.PerBlock.html#torch.ao.quantization.observer.PerBlock">PerBlock（torch.ao.quantization.observer 类中的类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html#torch.ao.quantization.observer.PerChannelMinMaxObserver">torch.ao.quantization.observer 中的 PerChannelMinMaxObserver（类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.PerGroup.html#torch.ao.quantization.observer.PerGroup">torch.ao.quantization.observer 中的 PerGroup（类）</a>
</li>
      <li><a href="generated/torch.permute.html#torch.permute">torch 模块中的 permute()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.permute.html#torch.Tensor.permute">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.distribution.Distribution.perplexity">torch.distributions.distribution.Distribution 方法中的 perplexity()</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.PerRow.html#torch.ao.quantization.observer.PerRow">PerRow（torch.ao.quantization.observer 类中的类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.PerTensor.html#torch.ao.quantization.observer.PerTensor">PerTensor（torch.ao.quantization.observer 类中的类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.PerToken.html#torch.ao.quantization.observer.PerToken">PerToken（torch.ao.quantization.observer 类中的类）</a>
</li>
      <li><a href="storage.html#torch.TypedStorage.pickle_storage_type">pickle_storage_type()（torch.TypedStorage 方法）</a>
</li>
      <li><a href="generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory">pin_memory() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.pin_memory">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.pin_memory">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.linalg.pinv.html#torch.linalg.pinv">pinv() (在 torch.linalg 模块中)</a>
</li>
      <li><a href="generated/torch.pinverse.html#torch.pinverse">pinverse() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.pinverse.html#torch.Tensor.pinverse">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.Pipe">管道（torch.distributed.pipelining 类中的类）</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.pipe_split">pipe_split() (在模块 torch.distributed.pipelining 中)</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.pipeline">pipeline() (在模块 torch.distributed.pipelining 中)</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.schedules.PipelineScheduleMulti">PipelineScheduleMulti (torch.distributed.pipelining.schedules 中的类)</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.schedules.PipelineScheduleSingle">torch.distributed.pipelining.schedules 中的 PipelineScheduleSingle（类）</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.stage.PipelineStage">torch.distributed.pipelining.stage 中的 PipelineStage（类）</a>
</li>
      <li><a href="generated/torch.nn.functional.pixel_shuffle.html#torch.nn.functional.pixel_shuffle">像素混洗() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="generated/torch.nn.functional.pixel_unshuffle.html#torch.nn.functional.pixel_unshuffle">像素解乱序() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle">torch.nn 中的 PixelShuffle（类）</a>
</li>
      <li><a href="generated/torch.nn.PixelUnshuffle.html#torch.nn.PixelUnshuffle">torch.nn 中的 PixelUnshuffle（类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html#torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.placeholder">占位符() (torch.fx.experimental.symbolic_shapes PropagateUnbackedSymInts 方法)</a>

      <ul>
        <li><a href="fx.html#torch.fx.Graph.placeholder">(torch.fx.Graph 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Interpreter.placeholder">(torch.fx.Interpreter 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Transformer.placeholder">(torch.fx.Transformer 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.observer.PlaceholderObserver.html#torch.ao.quantization.observer.PlaceholderObserver">PlaceholderObserver (torch.ao.quantization.observer 中的类)</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.placement_types.Placement">Placement (torch.distributed.tensor.placement_types 中的类)</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.DTensor.placements">placements (torch.distributed.tensor.DTensor 属性)</a>
</li>
      <li><a href="distributions.html#torch.distributions.poisson.Poisson">PyTorch 中的泊松分布（类）</a>
</li>
      <li><a href="generated/torch.poisson.html#torch.poisson">torch 模块中的 poisson()函数</a>
</li>
      <li><a href="generated/torch.nn.functional.poisson_nll_loss.html#torch.nn.functional.poisson_nll_loss">torch.nn.functional 模块中的 poisson_nll_loss()函数</a>
</li>
      <li><a href="generated/torch.nn.PoissonNLLLoss.html#torch.nn.PoissonNLLLoss">torch.nn 中的 PoissonNLLLoss 类</a>
</li>
      <li><a href="generated/torch.polar.html#torch.polar">torch 模块中的 polar()函数</a>
</li>
      <li><a href="generated/torch.polygamma.html#torch.polygamma">torch 模块中的 polygamma()函数</a>

      <ul>
        <li><a href="special.html#torch.special.polygamma">torch.special 模块中</a>
</li>
        <li><a href="generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.polygamma_.html#torch.Tensor.polygamma_">torch.Tensor 方法中的 polygamma_()函数</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR">torch.optim.lr_scheduler 模块中的 PolynomialLR 类</a>
</li>
      <li><a href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.pool">pool() (torch.cuda.CUDAGraph 方法)</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.pop">pop() (torch.autograd.profiler_util.StringTable 方法)</a>

      <ul>
        <li><a href="export.html#torch.export.decomp_utils.CustomDecompTable.pop">(torch.export.decomp_utils.CustomDecompTable 方法)</a>
</li>
        <li><a href="generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.pop">(torch.nn.ModuleDict 方法)</a>
</li>
        <li><a href="generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.pop">(torch.nn.ParameterDict 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.popitem">popitem() (torch.autograd.profiler_util.StringTable 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.popitem">(torch.nn.ParameterDict 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.TCPStore.port">端口（torch.distributed.TCPStore 属性）</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.positive.html#torch.positive">positive()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.positive.html#torch.Tensor.positive">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.transforms.PositiveDefiniteTransform">PositiveDefiniteTransform（torch.distributions.transforms 中的类）</a>
</li>
      <li><a href="distributed.algorithms.join.html#torch.distributed.algorithms.JoinHook.post_hook">post_hook()（torch.distributed.algorithms.JoinHook 方法）</a>
</li>
      <li><a href="distributed.optim.html#torch.distributed.optim.PostLocalSGDOptimizer">torch.distributed.optim 中的 PostLocalSGDOptimizer（类）</a>
</li>
      <li><a href="generated/torch.pow.html#torch.pow">torch 模块中的 pow()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.pow.html#torch.Tensor.pow">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.pow_.html#torch.Tensor.pow_">torch.Tensor 方法中的 pow_()</a>
</li>
      <li><a href="generated/torch.cuda.power_draw.html#torch.cuda.power_draw">torch.cuda 模块中的 power_draw()函数</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook">powerSGD_hook() (在模块 torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook 中)</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState">PowerSGDState (torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook 中的类)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.PowerTransform">PowerTransform (torch.distributions.transforms 中的类)</a>
</li>
      <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix">precision_matrix (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal 属性)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix">(torch.distributions.multivariate_normal.MultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.wishart.Wishart.precision_matrix">(torch.distributions.wishart.Wishart 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss.predict">predict() (torch.nn.AdaptiveLogSoftmaxWithLoss 方法)</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.preferred_blas_library">preferred_blas_library() (在模块 torch.backends.cuda 中)</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.preferred_linalg_library">preferred_linalg_library() (在模块 torch.backends.cuda 中)</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.preferred_rocm_fa_library">preferred_rocm_fa_library() (在模块 torch.backends.cuda 中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.PrefixStore">PrefixStore（torch.distributed 中的类）</a>
</li>
      <li><a href="generated/torch.nn.PReLU.html#torch.nn.PReLU">PReLU（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.functional.prelu.html#torch.nn.functional.prelu">prelu()（torch.nn.functional 模块中的函数）</a>
</li>
      <li><a href="generated/torch.ao.quantization.prepare.html#torch.ao.quantization.prepare">torch.ao.quantization 中的 prepare（类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.prepare_for_propagation_comparison.html#torch.ao.quantization.prepare_for_propagation_comparison">torch.ao.quantization 中的 prepare_for_propagation_comparison（类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.quantize_fx.prepare_fx.html#torch.ao.quantization.quantize_fx.prepare_fx">torch.ao.quantization.quantize_fx 中的 prepare_fx（类）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.prepare_global_plan">torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader 方法中的 prepare_global_plan()()</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.prepare_global_plan">(torch.distributed.checkpoint.StorageReader 方法)</a>
</li>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.prepare_global_plan">(torch.distributed.checkpoint.StorageWriter 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.prepare_local_plan">prepare_local_plan() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader 方法)</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.prepare_local_plan">(torch.distributed.checkpoint.StorageReader 方法)</a>
</li>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.prepare_local_plan">(torch.distributed.checkpoint.StorageWriter 方法)</a>
</li>
      </ul></li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.prepare_model_outputs">prepare_model_outputs()（在模块 torch.ao.ns._numeric_suite 中）</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.prepare_model_with_stubs">prepare_model_with_stubs()（在模块 torch.ao.ns._numeric_suite 中）</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.prepare_n_shadows_model">prepare_n_shadows_model()（在模块 torch.ao.ns._numeric_suite_fx 中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.prepare_qat.html#torch.ao.quantization.prepare_qat">torch.ao.quantization 中的 prepare_qat（类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html#torch.ao.quantization.quantize_fx.prepare_qat_fx">torch.ao.quantization.quantize_fx 中的 prepare_qat_fx（类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig">torch.ao.quantization.fx.custom_config 中的 PrepareCustomConfig（类）</a>
</li>
      <li><a href="distributed.tensor.parallel.html#torch.distributed.tensor.parallel.PrepareModuleInput">torch.distributed.tensor.parallel 中的 PrepareModuleInput（类）</a>
</li>
      <li><a href="distributed.tensor.parallel.html#torch.distributed.tensor.parallel.PrepareModuleOutput">准备模块输出（torch.distributed.tensor.parallel 中的类）</a>
</li>
      <li><a href="fx.html#torch.fx.Node.prepend">prepend() (torch.fx.Node 方法)</a>
</li>
      <li><a href="profiler.html#torch.profiler._KinetoProfile.preset_metadata_json">preset_metadata_json() (torch.profiler._KinetoProfile 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints.prettify_results">prettify_results() (torch.fx.experimental.symbolic_shapes.DimConstraints 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.Node.prev">prev (torch.fx.Node 属性)</a>
</li>
      <li><a href="generated/torch.autograd.forward_ad.UnpackedDualTensor.html#torch.autograd.forward_ad.UnpackedDualTensor.primal">primal (torch.autograd.forward_ad.UnpackedDualTensor 属性)</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Compare.print">print() (torch.utils.benchmark.Compare 方法)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite_fx.html#torch.ao.ns._numeric_suite_fx.print_comparisons_n_shadows_model">print_comparisons_n_shadows_model() (在模块 torch.ao.ns._numeric_suite_fx 中)</a>
</li>
      <li><a href="fx.html#torch.fx.GraphModule.print_readable">print_readable() (torch.fx.GraphModule 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.Graph.print_tabular">print_tabular() (torch.fx.Graph 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.probs">probs (torch.distributions.bernoulli.Bernoulli 属性)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.probs">(torch.distributions.binomial.Binomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.probs">(torch.distributions.categorical.Categorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.probs">(torch.distributions.geometric.Geoemtric 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.probs">(torch.distributions.multinomial.Multinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.negative_binomial.NegativeBinomial.probs">(torch.distributions.negative_binomial.NegativeBinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.probs">(torch.distributions.one_hot_categorical.OneHotCategorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs">(torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs">(torch.distributions.relaxed_bernoulli.RelaxedBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs">(torch.distributions.relaxed_categorical.RelaxedOneHotCategorical 属性)</a>
</li>
      </ul></li>
      <li><a href="fx.html#torch.fx.Graph.process_inputs">process_inputs() (torch.fx.Graph 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.Graph.process_outputs">process_outputs() (torch.fx.Graph 方法)</a>
</li>
      <li><a href="elastic/errors.html#torch.distributed.elastic.multiprocessing.errors.ProcessFailure">进程失败（torch.distributed.elastic.multiprocessing.errors 类中的类）</a>
</li>
      <li><a href="generated/torch.prod.html#torch.prod">torch 模块中的 prod()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.prod.html#torch.Tensor.prod">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.produce_guards">torch.fx.experimental.symbolic_shapes.ShapeEnv 方法中的 produce_guards()</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.produce_guards_expression">produce_guards_expression() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.produce_guards_verbose">produce_guards_verbose() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="elastic/metrics.html#torch.distributed.elastic.metrics.prof">prof() (在模块 torch.distributed.elastic.metrics 中)</a>
</li>
      <li><a href="autograd.html#torch.autograd.profiler.profile">profile (torch.autograd.profiler 中的类)</a>

      <ul>
        <li><a href="profiler.html#torch.profiler.profile">(torch.profiler 中的类)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.mps.profiler.profile.html#torch.mps.profiler.profile">profile() (在 torch.mps.profiler 模块中)</a>
</li>
      <li><a href="profiler.html#torch.profiler.ProfilerAction">torch.profiler 中的 ProfilerAction（类）</a>
</li>
      <li><a href="profiler.html#torch.profiler.ProfilerActivity">torch.profiler 中的 ProfilerActivity（类）</a>
</li>
      <li><a href="generated/torch.promote_types.html#torch.promote_types">torch 模块中的 promote_types()（函数）</a>
</li>
      <li><a href="generated/torch.ao.quantization.propagate_qconfig_.html#torch.ao.quantization.propagate_qconfig_">torch.ao.quantization 中的 propagate_qconfig_（类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html#torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts">torch.fx.experimental.symbolic_shapes 中的 PropagateUnbackedSymInts（类）</a>
</li>
      <li><a href="fx.html#torch.fx.Proxy">torch.fx 中的 Proxy（类）</a>
</li>
      <li><a href="fx.html#torch.fx.Tracer.proxy">torch.fx.Tracer 方法中的 proxy()</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.prune">torch.nn.utils.prune.BasePruningMethod 方法中的 prune()</a>

      <ul>
        <li><a href="generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.prune">(torch.nn.utils.prune.CustomFromMask 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.prune">(torch.nn.utils.prune.Identity 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.prune">(torch.nn.utils.prune.L1Unstructured 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.prune">(torch.nn.utils.prune.LnStructured 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.prune">(torch.nn.utils.prune.PruningContainer 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.prune">(torch.nn.utils.prune.RandomStructured 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.prune">(torch.nn.utils.prune.RandomUnstructured 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer">PruningContainer (torch.nn.utils.prune 中的类)</a>
</li>
      <li><a href="special.html#torch.special.psi">psi()（在 torch.special 模块中）</a>
</li>
      <li><a href="generated/torch.Tensor.put_.html#torch.Tensor.put_">put_()（torch.Tensor 方法）</a>
</li>
      <li><a href="elastic/metrics.html#torch.distributed.elastic.metrics.put_metric">put_metric()（在 torch.distributed.elastic.metrics 模块中）</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.PyRRef">PyRRef（torch.distributed.rpc 中的类）</a>
</li>
      <li><a href="fx.html#torch.fx.Graph.python_code">torch.fx.Graph 方法 python_code()</a>
</li>
      <li><a href="package.html#torch.package.PackageImporter.python_version">torch.package.PackageImporter 方法 python_version()</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="Q">Q</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.q_indices">q_indices（torch.nn.attention.flex_attention.BlockMask 属性）</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.q_num_blocks">q_num_blocks (torch.nn.attention.flex_attention.BlockMask 属性)</a>
</li>
      <li><a href="generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis">q_per_channel_axis() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales">q_per_channel_scales() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points">q_per_channel_zero_points() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale">q_scale() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point">q_zero_point() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig.QConfig.html#torch.ao.quantization.qconfig.QConfig">QConfig (torch.ao.quantization.qconfig 中的类)</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping">QConfigMapping (torch.ao.quantization.qconfig_mapping 中的类)</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.QFunctional.html#torch.ao.nn.quantized.QFunctional">torch.ao.nn.quantized 中的 QFunctional（类）</a>
</li>
      <li><a href="storage.html#torch.QInt32Storage">torch 中的 QInt32Storage（类）</a>
</li>
      <li><a href="storage.html#torch.QInt8Storage">torch 中的 QInt8Storage（类）</a>
</li>
      <li><a href="generated/torch.qr.html#torch.qr">torch 模块中的 qr()</a>

      <ul>
        <li><a href="generated/torch.linalg.qr.html#torch.linalg.qr">(在 torch.linalg 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.qr.html#torch.Tensor.qr">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.qscheme.html#torch.Tensor.qscheme">qscheme() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.quantile.html#torch.quantile">quantile() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.quantile.html#torch.Tensor.quantile">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.quantize.html#torch.ao.quantization.quantize">quantize (torch.ao.quantization 中的类)</a>
</li>
      <li><a href="generated/torch.ao.quantization.quantize_dynamic.html#torch.ao.quantization.quantize_dynamic">quantize_dynamic (torch.ao.quantization 中的类)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.quantize_per_channel.html#torch.quantize_per_channel">torch 模块中的 quantize_per_channel()</a>
</li>
      <li><a href="generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor">torch 模块中的 quantize_per_tensor()</a>
</li>
      <li><a href="generated/torch.ao.quantization.quantize_qat.html#torch.ao.quantization.quantize_qat">torch.ao.quantization 中的 quantize_qat 类</a>
</li>
      <li><a href="generated/torch.quantized_batch_norm.html#torch.quantized_batch_norm">torch 模块中的 quantized_batch_norm()</a>
</li>
      <li><a href="generated/torch.quantized_max_pool1d.html#torch.quantized_max_pool1d">量化_max_pool1d()（在 torch 模块中）</a>
</li>
      <li><a href="generated/torch.quantized_max_pool2d.html#torch.quantized_max_pool2d">quantized_max_pool2d() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.QuantStub.html#torch.ao.quantization.QuantStub">QuantStub (torch.ao.quantization 中的类)</a>
</li>
      <li><a href="generated/torch.ao.quantization.QuantWrapper.html#torch.ao.quantization.QuantWrapper">QuantWrapper (torch.ao.quantization 中的类)</a>
</li>
      <li><a href="generated/torch.cuda.Event.html#torch.cuda.Event.query">query() (torch.cuda.Event 方法)</a>

      <ul>
        <li><a href="generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream.query">(torch.cuda.ExternalStream 方法)</a>
</li>
        <li><a href="generated/torch.cuda.Stream.html#torch.cuda.Stream.query">(torch.cuda.Stream 方法)</a>
</li>
        <li><a href="generated/torch.Event.html#torch.Event.query">torch.Event 方法</a>
</li>
        <li><a href="generated/torch.mps.event.Event.html#torch.mps.event.Event.query">torch.mps.event.Event 方法</a>
</li>
        <li><a href="generated/torch.mtia.Event.html#torch.mtia.Event.query">(torch.mtia.Event 方法)</a>
</li>
        <li><a href="generated/torch.mtia.Stream.html#torch.mtia.Stream.query">(torch.mtia.Stream 方法)</a>
</li>
        <li><a href="generated/torch.Stream.html#torch.Stream.query">(torch.Stream 方法)</a>
</li>
        <li><a href="generated/torch.xpu.Event.html#torch.xpu.Event.query">(torch.xpu.Event 方法)</a>
</li>
        <li><a href="generated/torch.xpu.Stream.html#torch.xpu.Stream.query">(torch.xpu.Stream 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.QUInt2x4Storage">QUInt2x4Storage (torch 中的类)</a>
</li>
      <li><a href="storage.html#torch.QUInt4x2Storage">torch 中的 QUInt4x2Storage（类）</a>
</li>
      <li><a href="storage.html#torch.QUInt8Storage">torch 中的 QUInt8Storage（类）</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="R">R</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.rad2deg.html#torch.rad2deg">torch 模块中的 rad2deg()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.optim.RAdam.html#torch.optim.RAdam">torch.optim 中的 RAdam（类）</a>
</li>
      <li><a href="generated/torch.rand.html#torch.rand">torch 模块中的 rand()</a>

      <ul>
        <li><a href="distributed.tensor.html#torch.distributed.tensor.rand">torch.distributed.tensor 模块中的()</a>
</li>
      </ul></li>
      <li><a href="generated/torch.rand_like.html#torch.rand_like">torch 模块中的 rand_like()</a>
</li>
      <li><a href="generated/torch.randint.html#torch.randint">torch 模块中的 randint()</a>
</li>
      <li><a href="generated/torch.randint_like.html#torch.randint_like">torch 模块中的 randint_like()</a>
</li>
      <li><a href="generated/torch.randn.html#torch.randn">torch 模块中的 randn()</a>

      <ul>
        <li><a href="distributed.tensor.html#torch.distributed.tensor.randn">torch.distributed.tensor 模块中的()</a>
</li>
      </ul></li>
      <li><a href="generated/torch.randn_like.html#torch.randn_like">torch 模块中的 randn_like()</a>
</li>
      <li><a href="generated/torch.Tensor.random_.html#torch.Tensor.random_">torch.Tensor 方法中的 random_()</a>
</li>
      <li><a href="data.html#torch.utils.data.random_split">torch.utils.data 模块中的 random_split()</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.random_structured.html#torch.nn.utils.prune.random_structured">random_structured() (在模块 torch.nn.utils.prune 中)</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.random_unstructured.html#torch.nn.utils.prune.random_unstructured">random_unstructured() (在模块 torch.nn.utils.prune 中)</a>
</li>
      <li><a href="data.html#torch.utils.data.RandomSampler">RandomSampler (torch.utils.data 中的类)</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured">RandomStructured (torch.nn.utils.prune 中的类)</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured">RandomUnstructured (torch.nn.utils.prune 中的类)</a>
</li>
      <li><a href="generated/torch.randperm.html#torch.randperm">randperm()（在 torch 模块中）</a>
</li>
      <li><a href="generated/torch.range.html#torch.range">range()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.cuda.nvtx.range.html#torch.cuda.nvtx.range">在 torch.cuda.nvtx 模块中</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.nvtx.range_pop.html#torch.cuda.nvtx.range_pop">torch.cuda.nvtx 模块中的 range_pop()</a>

      <ul>
        <li><a href="profiler.html#torch.profiler.itt.range_pop">torch.profiler.itt 模块中</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.nvtx.range_push.html#torch.cuda.nvtx.range_push">torch.cuda.nvtx 模块中的 range_push()</a>

      <ul>
        <li><a href="profiler.html#torch.profiler.itt.range_push">torch.profiler.itt 模块中</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.inverse_gamma.InverseGamma.rate">rate (torch.distributions.inverse_gamma.InverseGamma 属性)</a>
</li>
      <li><a href="generated/torch.ravel.html#torch.ravel">torch.ravel()</a>

      <ul>
        <li><a href="generated/torch.Tensor.ravel.html#torch.Tensor.ravel">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.read_data">读取数据() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader 方法)</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.read_data">(torch.distributed.checkpoint.StorageReader 方法)</a>
</li>
      </ul></li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.read_file">read_file()（在模块 torch.cuda.tunable 中）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.read_metadata">read_metadata() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader 方法)</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.read_metadata">(torch.distributed.checkpoint.StorageReader 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.ReadItem">读取项目（torch.distributed.checkpoint 中的类）</a>
</li>
      <li><a href="generated/torch.nn.attention.bias.CausalVariant.html#torch.nn.attention.bias.CausalVariant.real">torch.nn.attention.bias.CausalVariant 属性（真实）</a>

      <ul>
        <li><a href="generated/torch.Tensor.real.html#torch.Tensor.real">（torch.Tensor 属性）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.real.html#torch.real">torch 模块中的 real()函数</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.rebind_unbacked.html#torch.fx.experimental.symbolic_shapes.rebind_unbacked">torch.fx.experimental.symbolic_shapes 模块中的 rebind_unbacked()函数</a>
</li>
      <li><a href="generated/torch.reciprocal.html#torch.reciprocal">reciprocal() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.reciprocal_.html#torch.Tensor.reciprocal_">reciprocal_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.mps.recommended_max_memory.html#torch.mps.recommended_max_memory">recommended_max_memory() (在模块 torch.mps 中)</a>
</li>
      <li><a href="fx.html#torch.fx.GraphModule.recompile">recompile() (torch.fx.GraphModule 方法)</a>
</li>
      <li><a href="elastic/events.html#torch.distributed.elastic.events.record">record()（在 torch.distributed.elastic.events 模块中）</a>

      <ul>
        <li><a href="elastic/errors.html#torch.distributed.elastic.multiprocessing.errors.record">（在 torch.distributed.elastic.multiprocessing.errors 模块中）</a>
</li>
        <li><a href="generated/torch.cuda.Event.html#torch.cuda.Event.record">（torch.cuda.Event 方法）</a>
</li>
        <li><a href="generated/torch.Event.html#torch.Event.record">torch.Event 方法</a>
</li>
        <li><a href="generated/torch.mps.event.Event.html#torch.mps.event.Event.record">torch.mps.event.Event 方法</a>
</li>
        <li><a href="generated/torch.mtia.Event.html#torch.mtia.Event.record">(torch.mtia.Event 方法)</a>
</li>
        <li><a href="generated/torch.xpu.Event.html#torch.xpu.Event.record">(torch.xpu.Event 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream.record_event">record_event() (torch.cuda.ExternalStream 方法)</a>

      <ul>
        <li><a href="generated/torch.cuda.Stream.html#torch.cuda.Stream.record_event">(torch.cuda.Stream 方法)</a>
</li>
        <li><a href="generated/torch.mtia.Stream.html#torch.mtia.Stream.record_event">(torch.mtia.Stream 方法)</a>
</li>
        <li><a href="generated/torch.Stream.html#torch.Stream.record_event">(torch.Stream 方法)</a>
</li>
        <li><a href="generated/torch.xpu.Stream.html#torch.xpu.Stream.record_event">(torch.xpu.Stream 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.profiler.record_function.html#torch.autograd.profiler.record_function">torch.autograd.profiler 中的 record_function（类）</a>
</li>
      <li><a href="generated/torch.mtia.record_memory_history.html#torch.mtia.record_memory_history">torch.mtia 模块中的 record_memory_history()函数</a>
</li>
      <li><a href="generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream">record_stream() (torch.Tensor 方法)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.record_untuned_enable">record_untuned_enable() (在 torch.cuda.tunable 模块中)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.record_untuned_is_enabled">record_untuned_is_enabled() (在模块 torch.cuda.tunable 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.RecordingObserver.html#torch.ao.quantization.observer.RecordingObserver">RecordingObserver（torch.ao.quantization.observer 类中的类）</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.recursive_undo">recursive_undo() (torch.optim.lr_scheduler.SequentialLR 方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.recv">torch.distributed 模块中的 recv()函数</a>
</li>
      <li><a href="distributed.html#torch.distributed.recv_object_list">torch.distributed 模块中的 recv_object_list()函数</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.DTensor.redistribute">重新分配() (torch.distributed.tensor.DTensor 方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.reduce">torch.distributed 模块中的 reduce()</a>
</li>
      <li><a href="generated/torch.cuda.comm.reduce_add.html#torch.cuda.comm.reduce_add">torch.cuda.comm 模块中的 reduce_add()</a>
</li>
      <li><a href="distributed.html#torch.distributed.reduce_op">torch.distributed 中的 reduce_op（类）</a>

      <ul>
        <li><a href="distributed.tensor.html#torch.distributed.tensor.placement_types.Partial.reduce_op">torch.distributed.tensor.placement_types.Partial 属性</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.reduce_scatter">torch.distributed 模块中的 reduce_scatter()</a>
</li>
      <li><a href="distributed.html#torch.distributed.reduce_scatter_tensor">reduce_scatter_tensor() (在模块 torch.distributed 中)</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau">减少学习率在平台期（torch.optim.lr_scheduler 中的类）</a>
</li>
      <li><a href="distributed.html#torch.distributed.ReduceOp">torch.distributed 中的 ReduceOp（类）</a>
</li>
      <li><a href="export.html#torch.export.dynamic_shapes.refine_dynamic_shapes_from_suggested_fixes">torch.export.dynamic_shapes 模块中的 refine_dynamic_shapes_from_suggested_fixes()函数</a>
</li>
      <li><a href="named_tensor.html#torch.Tensor.refine_names">torch.Tensor 方法中的 refine_names()</a>
</li>
      <li><a href="generated/torch.nn.ReflectionPad1d.html#torch.nn.ReflectionPad1d">torch.nn 中的 ReflectionPad1d（类）</a>
</li>
      <li><a href="generated/torch.nn.ReflectionPad2d.html#torch.nn.ReflectionPad2d">ReflectionPad2d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.ReflectionPad3d.html#torch.nn.ReflectionPad3d">ReflectionPad3d（torch.nn 中的类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.constraint_registry.ConstraintRegistry.register">register()（torch.distributions.constraint_registry.ConstraintRegistry 方法）</a>
</li>
      <li><a href="library.html#torch.library.register_autocast">register_autocast()（在 torch.library 模块中）</a>
</li>
      <li><a href="library.html#torch.library.register_autograd">register_autograd()（在 torch.library 模块中）</a>
</li>
      <li><a href="distributed.html#torch.distributed.Backend.register_backend">register_backend() (torch.distributed.Backend 类方法)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_backward_hook">register_backward_hook() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.register_backward_hook">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.register_backward_hook">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_buffer">register_buffer() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.register_buffer">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.register_buffer">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.register_comm_hook">register_comm_hook() (torch.distributed.fsdp.FullyShardedDataParallel 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.register_comm_hook">(torch.nn.parallel.DistributedDataParallel 方法)</a>
</li>
      </ul></li>
      <li><a href="onnx_torchscript.html#torch.onnx.register_custom_op_symbolic">register_custom_op_symbolic() (在模块 torch.onnx 中)</a>
</li>
      <li><a href="export.html#torch.export.register_dataclass">register_dataclass()（在 torch.export 模块中）</a>
</li>
      <li><a href="monitor.html#torch.monitor.register_event_handler">register_event_handler() (在模块 torch.monitor 中)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.register_extern_hook">register_extern_hook() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="library.html#torch.library.register_fake">register_fake() (在模块 torch.library 中)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_forward_hook">register_forward_hook() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.register_forward_hook">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_forward_pre_hook">register_forward_pre_hook() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.register_forward_pre_hook">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.register_fsdp_forward_method">register_fsdp_forward_method() (在模块 torch.distributed.fsdp 中)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_full_backward_hook">register_full_backward_hook() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.register_full_backward_hook">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_full_backward_pre_hook">register_full_backward_pre_hook() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_pre_hook">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.register_full_backward_pre_hook">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.gds.GdsFile.html#torch.cuda.gds.GdsFile.register_handle">register_handle() (torch.cuda.gds.GdsFile 方法)</a>
</li>
      <li><a href="generated/torch.autograd.graph.Node.register_hook.html#torch.autograd.graph.Node.register_hook">register_hook() (torch.autograd.graph.Node 方法)</a>

      <ul>
        <li><a href="generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="package.html#torch.package.PackageExporter.register_intern_hook">register_intern_hook() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="library.html#torch.library.register_kernel">register_kernel() (在模块 torch.library 中)</a>
</li>
      <li><a href="distributions.html#torch.distributions.kl.register_kl">register_kl() (在模块 torch.distributions.kl 中)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_load_state_dict_post_hook">register_load_state_dict_post_hook() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.register_load_state_dict_post_hook">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adadelta.html#torch.optim.Adadelta.register_load_state_dict_post_hook">(torch.optim.Adadelta 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adafactor.html#torch.optim.Adafactor.register_load_state_dict_post_hook">(torch.optim.Adafactor 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adagrad.html#torch.optim.Adagrad.register_load_state_dict_post_hook">(torch.optim.Adagrad 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adam.html#torch.optim.Adam.register_load_state_dict_post_hook">(torch.optim.Adam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adamax.html#torch.optim.Adamax.register_load_state_dict_post_hook">(torch.optim.Adamax 方法)</a>
</li>
        <li><a href="generated/torch.optim.AdamW.html#torch.optim.AdamW.register_load_state_dict_post_hook">(torch.optim.AdamW 方法)</a>
</li>
        <li><a href="generated/torch.optim.ASGD.html#torch.optim.ASGD.register_load_state_dict_post_hook">(torch.optim.ASGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.LBFGS.html#torch.optim.LBFGS.register_load_state_dict_post_hook">(torch.optim.LBFGS 方法)</a>
</li>
        <li><a href="generated/torch.optim.NAdam.html#torch.optim.NAdam.register_load_state_dict_post_hook">(torch.optim.NAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Optimizer.register_load_state_dict_post_hook.html#torch.optim.Optimizer.register_load_state_dict_post_hook">(torch.optim.Optimizer 方法)</a>
</li>
        <li><a href="generated/torch.optim.RAdam.html#torch.optim.RAdam.register_load_state_dict_post_hook">(torch.optim.RAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.RMSprop.html#torch.optim.RMSprop.register_load_state_dict_post_hook">(torch.optim.RMSprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.Rprop.html#torch.optim.Rprop.register_load_state_dict_post_hook">(torch.optim.Rprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.SGD.html#torch.optim.SGD.register_load_state_dict_post_hook">(torch.optim.SGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.register_load_state_dict_post_hook">(torch.optim.SparseAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.register_load_state_dict_post_hook">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_load_state_dict_pre_hook">register_load_state_dict_pre_hook() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.register_load_state_dict_pre_hook">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adadelta.html#torch.optim.Adadelta.register_load_state_dict_pre_hook">(torch.optim.Adadelta 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adafactor.html#torch.optim.Adafactor.register_load_state_dict_pre_hook">(torch.optim.Adafactor 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adagrad.html#torch.optim.Adagrad.register_load_state_dict_pre_hook">(torch.optim.Adagrad 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adam.html#torch.optim.Adam.register_load_state_dict_pre_hook">(torch.optim.Adam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adamax.html#torch.optim.Adamax.register_load_state_dict_pre_hook">(torch.optim.Adamax 方法)</a>
</li>
        <li><a href="generated/torch.optim.AdamW.html#torch.optim.AdamW.register_load_state_dict_pre_hook">(torch.optim.AdamW 方法)</a>
</li>
        <li><a href="generated/torch.optim.ASGD.html#torch.optim.ASGD.register_load_state_dict_pre_hook">(torch.optim.ASGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.LBFGS.html#torch.optim.LBFGS.register_load_state_dict_pre_hook">(torch.optim.LBFGS 方法)</a>
</li>
        <li><a href="generated/torch.optim.NAdam.html#torch.optim.NAdam.register_load_state_dict_pre_hook">(torch.optim.NAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Optimizer.register_load_state_dict_pre_hook.html#torch.optim.Optimizer.register_load_state_dict_pre_hook">(torch.optim.Optimizer 方法)</a>
</li>
        <li><a href="generated/torch.optim.RAdam.html#torch.optim.RAdam.register_load_state_dict_pre_hook">(torch.optim.RAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.RMSprop.html#torch.optim.RMSprop.register_load_state_dict_pre_hook">(torch.optim.RMSprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.Rprop.html#torch.optim.Rprop.register_load_state_dict_pre_hook">(torch.optim.Rprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.SGD.html#torch.optim.SGD.register_load_state_dict_pre_hook">(torch.optim.SGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.register_load_state_dict_pre_hook">(torch.optim.SparseAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.register_load_state_dict_pre_hook">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="package.html#torch.package.PackageExporter.register_mock_hook">register_mock_hook() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_module">register_module() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.register_module">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.register_module">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.modules.module.register_module_backward_hook.html#torch.nn.modules.module.register_module_backward_hook">register_module_backward_hook() (在模块 torch.nn.modules.module 中)</a>
</li>
      <li><a href="generated/torch.nn.modules.module.register_module_buffer_registration_hook.html#torch.nn.modules.module.register_module_buffer_registration_hook">register_module_buffer_registration_hook() (在模块 torch.nn.modules.module 中)</a>
</li>
      <li><a href="generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook">register_module_forward_hook() (在模块 torch.nn.modules.module 中)</a>
</li>
      <li><a href="generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook">register_module_forward_pre_hook() (在模块 torch.nn.modules.module 中)</a>
</li>
      <li><a href="generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook">register_module_full_backward_hook() (在模块 torch.nn.modules.module 中)</a>
</li>
      <li><a href="generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch.nn.modules.module.register_module_full_backward_pre_hook">register_module_full_backward_pre_hook() (在模块 torch.nn.modules.module 中)</a>
</li>
      <li><a href="generated/torch.nn.modules.module.register_module_module_registration_hook.html#torch.nn.modules.module.register_module_module_registration_hook">register_module_module_registration_hook() (在模块 torch.nn.modules.module 中)</a>
</li>
      <li><a href="generated/torch.nn.modules.module.register_module_parameter_registration_hook.html#torch.nn.modules.module.register_module_parameter_registration_hook">register_module_parameter_registration_hook() (在模块 torch.nn.modules.module 中)</a>
</li>
      <li><a href="autograd.html#torch.autograd.graph.register_multi_grad_hook">register_multi_grad_hook (torch.autograd.graph 类中)</a>
</li>
      <li><a href="onnx_dynamo.html#torch.onnx.OnnxRegistry.register_op">register_op() (torch.onnx.OnnxRegistry 方法)</a>
</li>
      <li><a href="notes/serialization.html#torch.serialization.register_package">register_package() (在模块 torch.serialization 中)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_parameter">register_parameter() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.register_parameter">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.register_parameter">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization">register_parametrization() (在模块 torch.nn.utils.parametrize 中)</a>
</li>
      <li><a href="generated/torch.Tensor.register_post_accumulate_grad_hook.html#torch.Tensor.register_post_accumulate_grad_hook">register_post_accumulate_grad_hook() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.autograd.graph.Node.register_prehook.html#torch.autograd.graph.Node.register_prehook">register_prehook() (torch.autograd.graph.Node 方法)</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.experimental.register_sharding">register_sharding() (在模块 torch.distributed.tensor.experimental 中)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_state_dict_post_hook">register_state_dict_post_hook() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.register_state_dict_post_hook">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adadelta.html#torch.optim.Adadelta.register_state_dict_post_hook">(torch.optim.Adadelta 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adafactor.html#torch.optim.Adafactor.register_state_dict_post_hook">(torch.optim.Adafactor 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adagrad.html#torch.optim.Adagrad.register_state_dict_post_hook">(torch.optim.Adagrad 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adam.html#torch.optim.Adam.register_state_dict_post_hook">(torch.optim.Adam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adamax.html#torch.optim.Adamax.register_state_dict_post_hook">(torch.optim.Adamax 方法)</a>
</li>
        <li><a href="generated/torch.optim.AdamW.html#torch.optim.AdamW.register_state_dict_post_hook">(torch.optim.AdamW 方法)</a>
</li>
        <li><a href="generated/torch.optim.ASGD.html#torch.optim.ASGD.register_state_dict_post_hook">(torch.optim.ASGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.LBFGS.html#torch.optim.LBFGS.register_state_dict_post_hook">(torch.optim.LBFGS 方法)</a>
</li>
        <li><a href="generated/torch.optim.NAdam.html#torch.optim.NAdam.register_state_dict_post_hook">(torch.optim.NAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Optimizer.register_state_dict_post_hook.html#torch.optim.Optimizer.register_state_dict_post_hook">(torch.optim.Optimizer 方法)</a>
</li>
        <li><a href="generated/torch.optim.RAdam.html#torch.optim.RAdam.register_state_dict_post_hook">(torch.optim.RAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.RMSprop.html#torch.optim.RMSprop.register_state_dict_post_hook">(torch.optim.RMSprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.Rprop.html#torch.optim.Rprop.register_state_dict_post_hook">(torch.optim.Rprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.SGD.html#torch.optim.SGD.register_state_dict_post_hook">(torch.optim.SGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.register_state_dict_post_hook">(torch.optim.SparseAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.register_state_dict_post_hook">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.register_state_dict_pre_hook">register_state_dict_pre_hook() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.register_state_dict_pre_hook">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adadelta.html#torch.optim.Adadelta.register_state_dict_pre_hook">(torch.optim.Adadelta 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adafactor.html#torch.optim.Adafactor.register_state_dict_pre_hook">(torch.optim.Adafactor 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adagrad.html#torch.optim.Adagrad.register_state_dict_pre_hook">(torch.optim.Adagrad 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adam.html#torch.optim.Adam.register_state_dict_pre_hook">(torch.optim.Adam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adamax.html#torch.optim.Adamax.register_state_dict_pre_hook">(torch.optim.Adamax 方法)</a>
</li>
        <li><a href="generated/torch.optim.AdamW.html#torch.optim.AdamW.register_state_dict_pre_hook">(torch.optim.AdamW 方法)</a>
</li>
        <li><a href="generated/torch.optim.ASGD.html#torch.optim.ASGD.register_state_dict_pre_hook">(torch.optim.ASGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.LBFGS.html#torch.optim.LBFGS.register_state_dict_pre_hook">(torch.optim.LBFGS 方法)</a>
</li>
        <li><a href="generated/torch.optim.NAdam.html#torch.optim.NAdam.register_state_dict_pre_hook">(torch.optim.NAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Optimizer.register_state_dict_pre_hook.html#torch.optim.Optimizer.register_state_dict_pre_hook">(torch.optim.Optimizer 方法)</a>
</li>
        <li><a href="generated/torch.optim.RAdam.html#torch.optim.RAdam.register_state_dict_pre_hook">(torch.optim.RAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.RMSprop.html#torch.optim.RMSprop.register_state_dict_pre_hook">(torch.optim.RMSprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.Rprop.html#torch.optim.Rprop.register_state_dict_pre_hook">(torch.optim.Rprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.SGD.html#torch.optim.SGD.register_state_dict_pre_hook">(torch.optim.SGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.register_state_dict_pre_hook">(torch.optim.SparseAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.register_state_dict_pre_hook">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.optim.Adadelta.html#torch.optim.Adadelta.register_step_post_hook">register_step_post_hook() (torch.optim.Adadelta 方法)</a>

      <ul>
        <li><a href="generated/torch.optim.Adafactor.html#torch.optim.Adafactor.register_step_post_hook">(torch.optim.Adafactor 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adagrad.html#torch.optim.Adagrad.register_step_post_hook">(torch.optim.Adagrad 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adam.html#torch.optim.Adam.register_step_post_hook">(torch.optim.Adam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adamax.html#torch.optim.Adamax.register_step_post_hook">(torch.optim.Adamax 方法)</a>
</li>
        <li><a href="generated/torch.optim.AdamW.html#torch.optim.AdamW.register_step_post_hook">(torch.optim.AdamW 方法)</a>
</li>
        <li><a href="generated/torch.optim.ASGD.html#torch.optim.ASGD.register_step_post_hook">(torch.optim.ASGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.LBFGS.html#torch.optim.LBFGS.register_step_post_hook">(torch.optim.LBFGS 方法)</a>
</li>
        <li><a href="generated/torch.optim.NAdam.html#torch.optim.NAdam.register_step_post_hook">(torch.optim.NAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Optimizer.register_step_post_hook.html#torch.optim.Optimizer.register_step_post_hook">(torch.optim.Optimizer 方法)</a>
</li>
        <li><a href="generated/torch.optim.RAdam.html#torch.optim.RAdam.register_step_post_hook">(torch.optim.RAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.RMSprop.html#torch.optim.RMSprop.register_step_post_hook">(torch.optim.RMSprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.Rprop.html#torch.optim.Rprop.register_step_post_hook">(torch.optim.Rprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.SGD.html#torch.optim.SGD.register_step_post_hook">(torch.optim.SGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.register_step_post_hook">(torch.optim.SparseAdam 方法)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.optim.Adadelta.html#torch.optim.Adadelta.register_step_pre_hook">register_step_pre_hook() (torch.optim.Adadelta 方法)</a>

      <ul>
        <li><a href="generated/torch.optim.Adafactor.html#torch.optim.Adafactor.register_step_pre_hook">(torch.optim.Adafactor 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adagrad.html#torch.optim.Adagrad.register_step_pre_hook">(torch.optim.Adagrad 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adam.html#torch.optim.Adam.register_step_pre_hook">(torch.optim.Adam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adamax.html#torch.optim.Adamax.register_step_pre_hook">(torch.optim.Adamax 方法)</a>
</li>
        <li><a href="generated/torch.optim.AdamW.html#torch.optim.AdamW.register_step_pre_hook">(torch.optim.AdamW 方法)</a>
</li>
        <li><a href="generated/torch.optim.ASGD.html#torch.optim.ASGD.register_step_pre_hook">(torch.optim.ASGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.LBFGS.html#torch.optim.LBFGS.register_step_pre_hook">(torch.optim.LBFGS 方法)</a>
</li>
        <li><a href="generated/torch.optim.NAdam.html#torch.optim.NAdam.register_step_pre_hook">(torch.optim.NAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Optimizer.register_step_pre_hook.html#torch.optim.Optimizer.register_step_pre_hook">(torch.optim.Optimizer 方法)</a>
</li>
        <li><a href="generated/torch.optim.RAdam.html#torch.optim.RAdam.register_step_pre_hook">(torch.optim.RAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.RMSprop.html#torch.optim.RMSprop.register_step_pre_hook">(torch.optim.RMSprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.Rprop.html#torch.optim.Rprop.register_step_pre_hook">(torch.optim.Rprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.SGD.html#torch.optim.SGD.register_step_pre_hook">(torch.optim.SGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.register_step_pre_hook">(torch.optim.SparseAdam 方法)</a>
</li>
      </ul></li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.TimerServer.register_timers">register_timers() (torch.distributed.elastic.timer.TimerServer 方法)</a>
</li>
      <li><a href="library.html#torch.library.register_torch_dispatch">register_torch_dispatch() (在模块 torch.library 中)</a>
</li>
      <li><a href="library.html#torch.library.register_vmap">register_vmap() (在模块 torch.library 中)</a>
</li>
      <li><a href="elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.DefaultLogsSpecs.reify">reify() (torch.distributed.elastic.multiprocessing.api.DefaultLogsSpecs 方法)</a>

      <ul>
        <li><a href="elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.LogsSpecs.reify">(torch.distributed.elastic.multiprocessing.api.LogsSpecs 方法)</a>
</li>
      </ul></li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict">rekey_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel 静态方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli">RelaxedBernoulli (torch.distributions.relaxed_bernoulli 中的类)</a>
</li>
      <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical">RelaxedOneHotCategorical (torch.distributions.relaxed_categorical 中的类)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.RelaxedUnspecConstraint.html#torch.fx.experimental.symbolic_shapes.RelaxedUnspecConstraint">RelaxedUnspecConstraint (torch.fx.experimental.symbolic_shapes 中的类)</a>
</li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.TimerClient.release">释放() (torch.distributed.elastic.timer.TimerClient 方法)</a>

      <ul>
        <li><a href="onnx_dynamo.html#torch.onnx.ONNXProgram.release">(torch.onnx.ONNXProgram 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.ReLU.html#torch.nn.ReLU">ReLU（torch.nn 类）</a>
</li>
      <li><a href="generated/torch.nn.functional.relu.html#torch.nn.functional.relu">relu()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.ReLU6.html#torch.ao.nn.quantized.ReLU6">ReLU6（torch.ao.nn.quantized 中的类）</a>

      <ul>
        <li><a href="generated/torch.nn.ReLU6.html#torch.nn.ReLU6">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.functional.relu6.html#torch.nn.functional.relu6">relu6()（torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.nn.functional.relu_.html#torch.nn.functional.relu_">relu_()（torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.remainder.html#torch.remainder">remainder()（torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.remainder.html#torch.Tensor.remainder">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.remainder_.html#torch.Tensor.remainder_">remainder_() (torch.Tensor 方法)</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.remote">remote() (在 torch.distributed.rpc 模块中)</a>

      <ul>
        <li><a href="rpc.html#torch.distributed.rpc.PyRRef.remote">(torch.distributed.rpc.PyRRef 方法)</a>
</li>
      </ul></li>
      <li><a href="rpc.html#torch.distributed.nn.api.remote_module.RemoteModule.remote_parameters">remote_parameters() (torch.distributed.nn.api.remote_module.RemoteModule 方法)</a>
</li>
      <li><a href="rpc.html#torch.distributed.nn.api.remote_module.RemoteModule">torch.distributed.nn.api.remote_module 中的 RemoteModule (类)</a>
</li>
      <li><a href="generated/torch.nn.utils.prune.remove.html#torch.nn.utils.prune.remove">torch.nn.utils.prune 中的 remove() 函数</a>

      <ul>
        <li><a href="generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod.remove">torch.nn.utils.prune.BasePruningMethod 方法</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask.remove">(torch.nn.utils.prune.CustomFromMask 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity.remove">(torch.nn.utils.prune.Identity 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured.remove">(torch.nn.utils.prune.L1Unstructured 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured.remove">(torch.nn.utils.prune.LnStructured 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer.remove">(torch.nn.utils.prune.PruningContainer 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured.remove">(torch.nn.utils.prune.RandomStructured 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured.remove">(torch.nn.utils.prune.RandomUnstructured 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch.nn.utils.parametrize.remove_parametrizations">torch.nn.utils.parametrize 中的 remove_parametrizations() 函数</a>
</li>
      <li><a href="generated/torch.nn.utils.remove_spectral_norm.html#torch.nn.utils.remove_spectral_norm">torch.nn.utils 模块中的 remove_spectral_norm()</a>
</li>
      <li><a href="generated/torch.nn.utils.remove_weight_norm.html#torch.nn.utils.remove_weight_norm">torch.nn.utils 模块中的 remove_weight_norm()</a>
</li>
      <li><a href="named_tensor.html#torch.Tensor.rename">torch.Tensor 方法中的 rename()</a>
</li>
      <li><a href="named_tensor.html#torch.Tensor.rename_">rename_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.utils.rename_privateuse1_backend.html#torch.utils.rename_privateuse1_backend">rename_privateuse1_backend() (在 torch.utils 模块中)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint.html#torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint.render">render() (torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint 方法)</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend">RendezvousBackend (torch.distributed.elastic.rendezvous.dynamic_rendezvous 中的类)</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.api.RendezvousClosedError">rendezvous 关闭错误（torch.distributed.elastic.rendezvous.api 中的类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.api.RendezvousConnectionError">torch.distributed.elastic.rendezvous.api 中的 RendezvousConnectionError（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.api.RendezvousError">torch.distributed.elastic.rendezvous.api 中的 RendezvousError（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.api.RendezvousGracefulExitError">torch.distributed.elastic.rendezvous.api 中的 RendezvousGracefulExitError（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler">torch.distributed.elastic.rendezvous 中的 RendezvousHandler（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry">rendezvous 处理器注册表（torch.distributed.elastic.rendezvous 中的类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousInfo">torch.distributed.elastic.rendezvous 中的 RendezvousInfo（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousParameters">torch.distributed.elastic.rendezvous 中的 RendezvousParameters（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.api.RendezvousStateError">torch.distributed.elastic.rendezvous.api 中的 RendezvousStateError（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.api.RendezvousStoreInfo">torch.distributed.elastic.rendezvous.api 中的 RendezvousStoreInfo（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout">torch.distributed.elastic.rendezvous.dynamic_rendezvous 中的 RendezvousTimeout（类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.api.RendezvousTimeoutError">torch.distributed.elastic.rendezvous.api 中的 RendezvousTimeoutError（类）</a>
</li>
      <li><a href="generated/torch.renorm.html#torch.renorm">torch 模块中的 renorm()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.renorm.html#torch.Tensor.renorm">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.renorm_.html#torch.Tensor.renorm_">torch.Tensor 方法中的 renorm_()函数</a>
</li>
      <li><a href="generated/torch.Tensor.repeat.html#torch.Tensor.repeat">torch.Tensor 方法中的 repeat()函数</a>
</li>
      <li><a href="generated/torch.repeat_interleave.html#torch.repeat_interleave">torch 模块中的 repeat_interleave()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.repeat_interleave.html#torch.Tensor.repeat_interleave">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.replace">torch.fx.experimental.symbolic_shapes.ShapeEnv 方法中的 replace()函数</a>
</li>
      <li><a href="generated/torch.func.replace_all_batch_norm_modules_.html#torch.func.replace_all_batch_norm_modules_">替换所有批量归一化模块()（在 torch.func 模块中）</a>
</li>
      <li><a href="export.html#torch.export.graph_signature.ExportGraphSignature.replace_all_uses">replace_all_uses() (torch.export.graph_signature.ExportGraphSignature 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.Node.replace_all_uses_with">replace_all_uses_with() (torch.fx.Node 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.Node.replace_input_with">replace_input_with() (torch.fx.Node 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.replace_pattern">replace_pattern() (在模块 torch.fx 中)</a>
</li>
      <li><a href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.replay">replay() (torch.cuda.CUDAGraph 方法)</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.placement_types.Replicate">Replicate (class in torch.distributed.tensor.placement_types)</a>
</li>
      <li><a href="generated/torch.nn.ReplicationPad1d.html#torch.nn.ReplicationPad1d">ReplicationPad1d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.ReplicationPad2d.html#torch.nn.ReplicationPad2d">ReplicationPad2d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.ReplicationPad3d.html#torch.nn.ReplicationPad3d">ReplicationPad3d（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad">requires_grad (torch.Tensor 属性)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.requires_grad_">requires_grad_() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.requires_grad_">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.requires_grad_">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.compiler.reset.html#torch.compiler.reset">重置()（在 torch.compiler 模块中）</a>

      <ul>
        <li><a href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph.reset">(torch.cuda.CUDAGraph 方法)</a>
</li>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.reset">(torch.distributed.checkpoint.format_utils.广播 TorchSaveReader 方法)</a>
</li>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.reset">(torch.distributed.checkpoint.StorageReader 方法)</a>
</li>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.reset">(torch.distributed.checkpoint.StorageWriter 方法)</a>
</li>
        <li><a href="generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine.reset">(torch.quasirandom.SobolEngine 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.xpu.reset_accumulated_memory_stats.html#torch.xpu.reset_accumulated_memory_stats">重置累积内存统计信息()（在 torch.xpu 模块中）</a>
</li>
      <li><a href="generated/torch.cuda.reset_max_memory_allocated.html#torch.cuda.reset_max_memory_allocated">重置分配的最大内存()（在 torch.cuda 模块中）</a>
</li>
      <li><a href="generated/torch.cuda.reset_max_memory_cached.html#torch.cuda.reset_max_memory_cached">重置最大缓存内存()（在 torch.cuda 模块中）</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver.reset_min_max_vals">重置最小最大值() (torch.ao.quantization.observer.MinMaxObserver 方法)</a>

      <ul>
        <li><a href="generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html#torch.ao.quantization.observer.PerChannelMinMaxObserver.reset_min_max_vals">(torch.ao.quantization.observer.PerChannelMinMaxObserver 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.modules.normalization.RMSNorm.html#torch.nn.modules.normalization.RMSNorm.reset_parameters">reset_parameters() (torch.nn.modules.normalization.RMSNorm 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm.reset_parameters">(torch.nn.RMSNorm 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.reset_peak_host_memory_stats.html#torch.cuda.reset_peak_host_memory_stats">torch.cuda 模块中的 reset_peak_host_memory_stats()</a>
</li>
      <li><a href="generated/torch.cuda.reset_peak_memory_stats.html#torch.cuda.reset_peak_memory_stats">torch.cuda 模块中的 reset_peak_memory_stats()</a>

      <ul>
        <li><a href="generated/torch.xpu.reset_peak_memory_stats.html#torch.xpu.reset_peak_memory_stats">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.reshape.html#torch.reshape">torch 模块中的 reshape()</a>

      <ul>
        <li><a href="generated/torch.Tensor.reshape.html#torch.Tensor.reshape">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as">torch.Tensor 方法中的 reshape_as()</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.ReshapeTransform">torch.distributions.transforms 中的 ReshapeTransform（类）</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.reshard">torch.distributed.fsdp.FSDPModule 方法中的 reshard()</a>
</li>
      <li><a href="storage.html#torch.TypedStorage.resizable">torch.TypedStorage 方法中的 resizable()</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.resizable">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.resize_.html#torch.Tensor.resize_">torch.Tensor 方法中的 resize_()</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.resize_">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.resize_">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_">resize_as_() (torch.Tensor 方法)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.resolve_bytes">resolve_bytes() (torch.distributed.checkpoint.LoadPlanner 方法)</a>
</li>
      <li><a href="generated/torch.resolve_conj.html#torch.resolve_conj">resolve_conj() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.resolve_conj.html#torch.Tensor.resolve_conj">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.SavePlanner.resolve_data">resolve_data() (torch.distributed.checkpoint.SavePlanner 方法)</a>
</li>
      <li><a href="torch.overrides.html#torch.overrides.resolve_name">resolve_name() (在模块 torch.overrides 中)</a>
</li>
      <li><a href="generated/torch.resolve_neg.html#torch.resolve_neg">resolve_neg() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.resolve_neg.html#torch.Tensor.resolve_neg">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.resolve_tensor">resolve_tensor() (torch.distributed.checkpoint.LoadPlanner 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.resolve_unbacked_bindings.html#torch.fx.experimental.symbolic_shapes.resolve_unbacked_bindings">resolve_unbacked_bindings() (在模块 torch.fx.experimental.symbolic_shapes 中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.Work.result">result() (torch.distributed.Work 方法)</a>
</li>
      <li><a href="generated/torch.result_type.html#torch.result_type">result_type() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.Tensor.retain_grad.html#torch.Tensor.retain_grad">retain_grad() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.retains_grad.html#torch.Tensor.retains_grad">retains_grad (torch.Tensor 属性)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints.rewrite_with_congruences">rewrite_with_congruences() (torch.fx.experimental.symbolic_shapes.DimConstraints 方法)</a>
</li>
      <li><a href="generated/torch.fft.rfft.html#torch.fft.rfft">torch.fft 模块中的 rfft()</a>
</li>
      <li><a href="generated/torch.fft.rfft2.html#torch.fft.rfft2">torch.fft 模块中的 rfft2()</a>
</li>
      <li><a href="generated/torch.fft.rfftfreq.html#torch.fft.rfftfreq">torch.fft 模块中的 rfftfreq()</a>
</li>
      <li><a href="generated/torch.fft.rfftn.html#torch.fft.rfftn">torch.fft 模块中的 rfftn()</a>
</li>
      <li><a href="generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList.right_inverse">right_inverse() (torch.nn.utils.parametrize.ParametrizationList 方法)</a>
</li>
      <li><a href="generated/torch.nn.functional.rms_norm.html#torch.nn.functional.rms_norm">rms_norm() (在 torch.nn.functional 模块中)</a>
</li>
      <li><a href="generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm">RMSNorm (torch.nn 中的类)</a>

      <ul>
        <li><a href="generated/torch.nn.modules.normalization.RMSNorm.html#torch.nn.modules.normalization.RMSNorm">(torch.nn.modules.normalization 中的类)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.optim.RMSprop.html#torch.optim.RMSprop">RMSprop（torch.optim 中的类）</a>
</li>
      <li><a href="generated/torch.nn.RNN.html#torch.nn.RNN">RNN（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.RNNBase.html#torch.nn.RNNBase">RNNBase（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.dynamic.RNNCell.html#torch.ao.nn.quantized.dynamic.RNNCell">RNNCell（torch.ao.nn.quantized.dynamic 中的类）</a>

      <ul>
        <li><a href="generated/torch.nn.RNNCell.html#torch.nn.RNNCell">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.roll.html#torch.roll">torch 模块中的 roll()</a>

      <ul>
        <li><a href="generated/torch.Tensor.roll.html#torch.Tensor.roll">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.rot90.html#torch.rot90">torch 模块中的 rot90()</a>

      <ul>
        <li><a href="generated/torch.Tensor.rot90.html#torch.Tensor.rot90">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.round.html#torch.round">torch 模块中的 round()</a>

      <ul>
        <li><a href="special.html#torch.special.round">torch.special 模块中</a>
</li>
        <li><a href="generated/torch.Tensor.round.html#torch.Tensor.round">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.round_.html#torch.Tensor.round_">torch.Tensor 方法中的 round_()</a>
</li>
      <li><a href="generated/torch.Tensor.row_indices.html#torch.Tensor.row_indices">row_indices() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.row_stack.html#torch.row_stack">row_stack() (在 torch 模块中)</a>
</li>
      <li><a href="distributed.tensor.parallel.html#torch.distributed.tensor.parallel.RowwiseParallel">RowwiseParallel (torch.distributed.tensor.parallel 中的类)</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.rpc_async">torch.distributed.rpc 模块中的 rpc_async()函数</a>

      <ul>
        <li><a href="rpc.html#torch.distributed.rpc.PyRRef.rpc_async">(torch.distributed.rpc.PyRRef 方法)</a>
</li>
      </ul></li>
      <li><a href="rpc.html#torch.distributed.rpc.rpc_sync">torch.distributed.rpc 模块中的 rpc_sync()函数</a>

      <ul>
        <li><a href="rpc.html#torch.distributed.rpc.PyRRef.rpc_sync">(torch.distributed.rpc.PyRRef 方法)</a>
</li>
      </ul></li>
      <li><a href="rpc.html#torch.distributed.rpc.RpcBackendOptions.rpc_timeout">rpc_timeout (torch.distributed.rpc.RpcBackendOptions 属性)</a>

      <ul>
        <li><a href="rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout">(torch.distributed.rpc.TensorPipeRpcBackendOptions 属性)</a>
</li>
      </ul></li>
      <li><a href="rpc.html#torch.distributed.rpc.RpcBackendOptions">RpcBackendOptions (torch.distributed.rpc 中的类)</a>
</li>
      <li><a href="generated/torch.optim.Rprop.html#torch.optim.Rprop">Rprop (torch.optim 中的类)</a>
</li>
      <li><a href="generated/torch.nn.RReLU.html#torch.nn.RReLU">RReLU（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.functional.rrelu.html#torch.nn.functional.rrelu">rrelu()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.nn.functional.rrelu_.html#torch.nn.functional.rrelu_">rrelu_() (在模块 torch.nn.functional 中)</a>
</li>
      <li><a href="distributions.html#torch.distributions.beta.Beta.rsample">rsample() (torch.distributions.beta.Beta 方法)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.rsample">(torch.distributions.cauchy.Cauchy 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.rsample">(torch.distributions.dirichlet.Dirichlet 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.rsample">(torch.distributions.distribution.Distribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.rsample">(torch.distributions.exponential.Exponential 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.rsample">(torch.distributions.fishersnedecor.FisherSnedecor 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.rsample">(torch.distributions.gamma.Gamma 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.rsample">(torch.distributions.independent.Independent 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.rsample">(torch.distributions.laplace.Laplace 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample">(torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.rsample">(torch.distributions.multivariate_normal.MultivariateNormal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.rsample">(torch.distributions.normal.Normal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample">(torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.rsample">(torch.distributions.studentT.StudentT 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.rsample">(torch.distributions.transformed_distribution.TransformedDistribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.rsample">(torch.distributions.uniform.Uniform 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.wishart.Wishart.rsample">(torch.distributions.wishart.Wishart 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.rsqrt.html#torch.rsqrt">rsqrt() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.rsqrt_.html#torch.Tensor.rsqrt_">rsqrt_() (torch.Tensor 方法)</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.ElasticAgent.run">run() (torch.distributed.elastic.agent.server.ElasticAgent 方法)</a>

      <ul>
        <li><a href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html#torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.run">(torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts 方法)</a>
</li>
        <li><a href="fx.html#torch.fx.Interpreter.run">(torch.fx.Interpreter 方法)</a>
</li>
      </ul></li>
      <li><a href="export.html#torch.export.ExportedProgram.run_decompositions">run_decompositions() (torch.export.ExportedProgram 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.html#torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts.run_node">run_node() (torch.fx.experimental.symbolic_shapes.PropagateUnbackedSymInts 方法)</a>

      <ul>
        <li><a href="fx.html#torch.fx.Interpreter.run_node">(torch.fx.Interpreter 方法)</a>
</li>
      </ul></li>
      <li><a href="elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.RunProcsResult">torch.distributed.elastic.multiprocessing.api 中的 RunProcsResult（类）</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.api.RunResult">torch.distributed.elastic.agent.server.api 中的 RunResult（类）</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="S">S</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="notes/serialization.html#torch.serialization.safe_globals">torch.serialization 中的 safe_globals（类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.sample">torch.distributions.bernoulli.Bernoulli 方法中的 sample()</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.sample">(torch.distributions.binomial.Binomial 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.sample">(torch.distributions.categorical.Categorical 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.sample">(torch.distributions.distribution.Distribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.sample">(torch.distributions.geometric.Geometric 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.sample">(torch.distributions.independent.Independent 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lkj_cholesky.LKJCholesky.sample">(torch.distributions.lkj_cholesky.LKJCholesky 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.sample">(torch.distributions.mixture_same_family.MixtureSameFamily 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.sample">(torch.distributions.multinomial.Multinomial 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.negative_binomial.NegativeBinomial.sample">(torch.distributions.negative_binomial.NegativeBinomial 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.sample">(torch.distributions.normal.Normal 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.sample">(torch.distributions.one_hot_categorical.OneHotCategorical 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.sample">(torch.distributions.poisson.Poisson 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.sample">(torch.distributions.transformed_distribution.TransformedDistribution 方法)</a>
</li>
        <li><a href="distributions.html#torch.distributions.von_mises.VonMises.sample">(torch.distributions.von_mises.VonMises 方法)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.distribution.Distribution.sample_n">sample_n() (torch.distributions.distribution.Distribution 方法)</a>
</li>
      <li><a href="generated/torch.sparse.sampled_addmm.html#torch.sparse.sampled_addmm">sampled_addmm() (在 torch.sparse 模块中)</a>
</li>
      <li><a href="data.html#torch.utils.data.Sampler">Sampler (torch.utils.data 中的类)</a>
</li>
      <li><a href="generated/torch.save.html#torch.save">save() (在 torch 模块中)</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.state_dict_saver.save">（在模块 torch.distributed.checkpoint.state_dict_saver 中）</a>
</li>
        <li><a href="export.html#torch.export.save">torch 模块中的 export 函数</a>
</li>
        <li><a href="generated/torch.jit.save.html#torch.jit.save">(在 torch.jit 模块中)</a>
</li>
        <li><a href="generated/torch.jit.ScriptFunction.html#torch.jit.ScriptFunction.save">（torch.jit.ScriptFunction 方法）</a>
</li>
        <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.save">(torch.jit.ScriptModule 方法)</a>
</li>
        <li><a href="onnx_dynamo.html#torch.onnx.ONNXProgram.save">(torch.onnx.ONNXProgram 方法)</a>
</li>
      </ul></li>
      <li><a href="package.html#torch.package.PackageExporter.save_binary">save_binary() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.save_for_backward">save_for_backward() (torch.autograd.function.BackwardCFunction 方法)</a>

      <ul>
        <li><a href="generated/torch.autograd.function.FunctionCtx.save_for_backward.html#torch.autograd.function.FunctionCtx.save_for_backward">(torch.autograd.function.FunctionCtx 方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.save_for_backward">(torch.autograd.function.InplaceFunction 方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.save_for_backward">(torch.autograd.function.NestedIOFunction 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.save_for_forward">save_for_forward() (torch.autograd.function.BackwardCFunction 方法)</a>

      <ul>
        <li><a href="generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.save_for_forward">(torch.autograd.function.InplaceFunction 方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.save_for_forward">(torch.autograd.function.NestedIOFunction 方法)</a>
</li>
      </ul></li>
      <li><a href="package.html#torch.package.PackageExporter.save_module">save_module() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="autograd.html#torch.autograd.graph.save_on_cpu">save_on_cpu (torch.autograd.graph 类)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.save_pickle">save_pickle() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.save_source_file">save_source_file() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.save_source_string">save_source_string() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.state_dict_saver.save_state_dict">save_state_dict() (在模块 torch.distributed.checkpoint.state_dict_saver 中)</a>
</li>
      <li><a href="generated/torch.cuda.gds.GdsFile.html#torch.cuda.gds.GdsFile.save_storage">save_storage() (torch.cuda.gds.GdsFile 方法)</a>
</li>
      <li><a href="package.html#torch.package.PackageExporter.save_text">save_text() (torch.package.PackageExporter 方法)</a>
</li>
      <li><a href="generated/torch.jit.ScriptFunction.html#torch.jit.ScriptFunction.save_to_buffer">save_to_buffer() (torch.jit.ScriptFunction 方法)</a>
</li>
      <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.saved_tensors">saved_tensors (torch.autograd.function.NestedIOFunction 属性)</a>
</li>
      <li><a href="autograd.html#torch.autograd.graph.saved_tensors_hooks">saved_tensors_hooks (torch.autograd.graph 中的类)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.SavePlan">SavePlan (torch.distributed.checkpoint 中的类)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.SavePlanner">torch.distributed.checkpoint 中的 SavePlanner（类）</a>
</li>
      <li><a href="generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.scalar_name">torch.onnx.JitScalarType 的 scalar_name()方法</a>
</li>
      <li><a href="distributions.html#torch.distributions.half_cauchy.HalfCauchy.scale">torch.distributions.half_cauchy.HalfCauchy 的 scale 属性</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.half_normal.HalfNormal.scale">(torch.distributions.half_normal.HalfNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.scale">(torch.distributions.log_normal.LogNormal 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR.scale_fn">torch.optim.lr_scheduler.CyclicLR 的 scale_fn()方法</a>
</li>
      <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril">scale_tril (torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal 属性)</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril">(torch.distributions.multivariate_normal.MultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.wishart.Wishart.scale_tril">(torch.distributions.wishart.Wishart 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention">scaled_dot_product_attention() (在模块 torch.nn.functional 中)</a>
</li>
      <li><a href="special.html#torch.special.scaled_modified_bessel_k0">scaled_modified_bessel_k0() (在模块 torch.special 中)</a>
</li>
      <li><a href="special.html#torch.special.scaled_modified_bessel_k1">scaled_modified_bessel_k1() (在模块 torch.special 中)</a>
</li>
      <li><a href="generated/torch.scatter.html#torch.scatter">torch 模块中的 scatter()函数</a>

      <ul>
        <li><a href="generated/torch.cuda.comm.scatter.html#torch.cuda.comm.scatter">（在 torch.cuda.comm 模块中）</a>
</li>
        <li><a href="distributed.html#torch.distributed.scatter">（在 torch.distributed 模块中）</a>
</li>
        <li><a href="generated/torch.Tensor.scatter.html#torch.Tensor.scatter">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_">torch.Tensor 方法中的 scatter_()</a>
</li>
      <li><a href="generated/torch.scatter_add.html#torch.scatter_add">torch 模块中的 scatter_add()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.scatter_add.html#torch.Tensor.scatter_add">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_">torch.Tensor 方法中的 scatter_add_()</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict">scatter_full_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel 静态方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.scatter_object_list">scatter_object_list() (在模块 torch.distributed 中)</a>
</li>
      <li><a href="generated/torch.scatter_reduce.html#torch.scatter_reduce">scatter_reduce() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.scatter_reduce.html#torch.Tensor.scatter_reduce">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_">scatter_reduce_() (torch.Tensor 方法)</a>
</li>
      <li><a href="profiler.html#torch.profiler.schedule">torch.profiler 模块中的 schedule()函数</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.schedules.Schedule1F1B">torch.distributed.pipelining.schedules 中的 Schedule1F1B 类</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.schedules.ScheduleGPipe">torch.distributed.pipelining.schedules 中的 ScheduleGPipe 类</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.schedules.ScheduleInterleaved1F1B">torch.distributed.pipelining.schedules 中的 ScheduleInterleaved1F1B 类</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.schedules.ScheduleInterleavedZeroBubble">torch.distributed.pipelining.schedules 中的 ScheduleInterleavedZeroBubble（类）</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.schedules.ScheduleLoopedBFS">torch.distributed.pipelining.schedules 中的 ScheduleLoopedBFS（类）</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.schedules.ScheduleZBVZeroBubble">torch.distributed.pipelining.schedules 中的 ScheduleZBVZeroBubble（类）</a>
</li>
      <li><a href="generated/torch.jit.script.html#torch.jit.script">torch.jit 模块中的 script() 函数</a>
</li>
      <li><a href="generated/torch.jit.script_if_tracing.html#torch.jit.script_if_tracing">torch.jit 模块中的 script_if_tracing()函数</a>
</li>
      <li><a href="generated/torch.jit.ScriptFunction.html#torch.jit.ScriptFunction">torch.jit 中的 ScriptFunction 类</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule">torch.jit 中的 ScriptModule 类</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.sdp_kernel">sdp_kernel()（在 torch.backends.cuda 模块中）</a>
</li>
      <li><a href="generated/torch.nn.attention.sdpa_kernel.html#torch.nn.attention.sdpa_kernel">sdpa_kernel()（在 torch.nn.attention 模块中）</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.SDPAParams">SDPAParams（torch.backends.cuda 模块中的类）</a>
</li>
      <li><a href="generated/torch.nn.attention.SDPBackend.html#torch.nn.attention.SDPBackend">SDPBackend（torch.nn.attention 模块中的类）</a>
</li>
      <li><a href="generated/torch.searchsorted.html#torch.searchsorted">torch 模块中的 searchsorted()函数</a>
</li>
      <li><a href="generated/torch.autograd.profiler.EnforceUnique.html#torch.autograd.profiler.EnforceUnique.see">torch.autograd.profiler.EnforceUnique 方法中的 see()函数</a>
</li>
      <li><a href="generated/torch.seed.html#torch.seed">torch 模块中的 seed()函数</a>

      <ul>
        <li><a href="generated/torch.cuda.seed.html#torch.cuda.seed">(在模块 torch.cuda 中)</a>
</li>
        <li><a href="generated/torch.mps.seed.html#torch.mps.seed">（在模块 torch.mps 中）</a>
</li>
        <li><a href="random.html#torch.random.seed">(在模块 torch.random 中)</a>
</li>
        <li><a href="generated/torch.xpu.seed.html#torch.xpu.seed">(在模块 torch.xpu 中)</a>
</li>
        <li><a href="generated/torch.Generator.html#torch.Generator.seed">(torch.Generator 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.seed_all.html#torch.cuda.seed_all">seed_all() (在 torch.cuda 模块中)</a>

      <ul>
        <li><a href="generated/torch.xpu.seed_all.html#torch.xpu.seed_all">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.select.html#torch.select">select() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.select.html#torch.Tensor.select">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="onnx_torchscript.html#torch.onnx.select_model_mode_for_export">选择导出模型模式（在 torch.onnx 模块中）</a>
</li>
      <li><a href="generated/torch.select_scatter.html#torch.select_scatter">torch 模块中的 select_scatter()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.select_scatter.html#torch.Tensor.select_scatter">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="checkpoint.html#torch.utils.checkpoint.SelectiveCheckpointContext">torch.utils.checkpoint 模块中的 SelectiveCheckpointContext 类</a>
</li>
      <li><a href="generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total">torch.autograd.profiler.profile 属性的 self_cpu_time_total</a>
</li>
      <li><a href="generated/torch.nn.SELU.html#torch.nn.SELU">torch.nn 模块中的 SELU 类</a>
</li>
      <li><a href="generated/torch.nn.functional.selu.html#torch.nn.functional.selu">selu()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="distributed.html#torch.distributed.send">send()（在 torch.distributed 模块中）</a>
</li>
      <li><a href="distributed.html#torch.distributed.send_object_list">send_object_list()（在 torch.distributed 模块中）</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.seq_lengths">seq_lengths（torch.nn.attention.flex_attention.BlockMask 属性）</a>
</li>
      <li><a href="distributed.tensor.parallel.html#torch.distributed.tensor.parallel.SequenceParallel">SequenceParallel（torch.distributed.tensor.parallel 中的类）</a>
</li>
      <li><a href="generated/torch.nn.Sequential.html#torch.nn.Sequential">Sequential（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR">SequentialLR（torch.optim.lr_scheduler 中的类）</a>
</li>
      <li><a href="data.html#torch.utils.data.SequentialSampler">SequentialSampler（torch.utils.data 中的类）</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.set">torch.distributed.elastic.rendezvous.etcd_store.EtcdStore 方法 set()</a>

      <ul>
        <li><a href="distributed.html#torch.distributed.Store.set">(torch.distributed.Store 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.set_.html#torch.Tensor.set_">torch.Tensor 方法 set_()</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_all_reduce_hook">torch.distributed.fsdp.FSDPModule 方法 set_all_reduce_hook()</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_config">torch.ao.quantization.backend_config.BackendConfig 方法 set_backend_pattern_config()</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_configs">set_backend_pattern_configs() (torch.ao.quantization.backend_config.BackendConfig 方法)</a>
</li>
      <li><a href="ddp_comm_hooks.html#torch.distributed.GradBucket.set_buffer">set_buffer() (在模块 torch.distributed.GradBucket 中)</a>
</li>
      <li><a href="checkpoint.html#torch.utils.checkpoint.set_checkpoint_debug_enabled">set_checkpoint_debug_enabled() (在模块 torch.utils.checkpoint 中)</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.set_closed">set_closed() (torch.distributed.elastic.rendezvous.RendezvousHandler 方法)</a>
</li>
      <li><a href="fx.html#torch.fx.Graph.set_codegen">set_codegen() (torch.fx.Graph 方法)</a>
</li>
      <li><a href="notes/serialization.html#torch.serialization.set_crc32_options">set_crc32_options() (在模块 torch.serialization 中)</a>
</li>
      <li><a href="profiler.html#torch.profiler.profile.set_custom_trace_id_callback">set_custom_trace_id_callback() (torch.profiler.profile 方法)</a>
</li>
      <li><a href="generated/torch.set_default_device.html#torch.set_default_device">set_default_device() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.set_default_dtype.html#torch.set_default_dtype">torch 模块中的 set_default_dtype()</a>
</li>
      <li><a href="notes/serialization.html#torch.serialization.set_default_load_endianness">torch.serialization 模块中的 set_default_load_endianness()</a>
</li>
      <li><a href="notes/serialization.html#torch.serialization.set_default_mmap_options">torch.serialization 模块中的 set_default_mmap_options()</a>
</li>
      <li><a href="generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type">torch 模块中的 set_default_tensor_type()</a>
</li>
      <li><a href="distributions.html#torch.distributions.distribution.Distribution.set_default_validate_args">set_default_validate_args() (torch.distributions.distribution.Distribution 静态方法)</a>
</li>
      <li><a href="autograd.html#torch.autograd.set_detect_anomaly">set_detect_anomaly (torch.autograd 类)</a>
</li>
      <li><a href="generated/torch.set_deterministic_debug_mode.html#torch.set_deterministic_debug_mode">set_deterministic_debug_mode() (模块 torch 中)</a>
</li>
      <li><a href="generated/torch.cpu.set_device.html#torch.cpu.set_device">set_device() (模块 torch.cpu 中)</a>

      <ul>
        <li><a href="generated/torch.cuda.set_device.html#torch.cuda.set_device">(在模块 torch.cuda 中)</a>
</li>
        <li><a href="generated/torch.mtia.set_device.html#torch.mtia.set_device">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="generated/torch.xpu.set_device.html#torch.xpu.set_device">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.accelerator.set_device_idx.html#torch.accelerator.set_device_idx">设置设备索引()（在 torch.accelerator 模块中）</a>
</li>
      <li><a href="generated/torch.accelerator.set_device_index.html#torch.accelerator.set_device_index">set_device_index() (在模块 torch.accelerator 中)</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map">set_device_map() (torch.distributed.rpc.TensorPipeRpcBackendOptions 方法)</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_devices">set_devices() (torch.distributed.rpc.TensorPipeRpcBackendOptions 方法)</a>
</li>
      <li><a href="hub.html#torch.hub.set_dir">set_dir() (在模块 torch.hub 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_dtype_configs">set_dtype_configs() (torch.ao.quantization.backend_config.BackendPatternConfig 方法)</a>
</li>
      <li><a href="futures.html#torch.futures.Future.set_exception">set_exception() (torch.futures.Future 方法)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.set_extra_state">set_extra_state() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.set_extra_state">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.set_extra_state">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="backends.html#torch.backends.mha.set_fastpath_enabled">set_fastpath_enabled() (在模块 torch.backends.mha 中)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.set_filename">torch.cuda.tunable 模块中的 set_filename()</a>
</li>
      <li><a href="backends.html#torch.backends.nnpack.set_flags">torch.backends.nnpack 模块中的 set_flags()</a>
</li>
      <li><a href="generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision">torch 模块中的 set_float32_matmul_precision()</a>
</li>
      <li><a href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_float_to_observed_mapping">torch.ao.quantization.fx.custom_config.PrepareCustomConfig 方法中的 set_float_to_observed_mapping()</a>
</li>
      <li><a href="generated/torch.set_flush_denormal.html#torch.set_flush_denormal">torch 模块中的 set_flush_denormal()</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_fused_module">torch.ao.quantization.backend_config.BackendPatternConfig 方法中的 set_fused_module()</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_fuser_method">torch.ao.quantization.backend_config.BackendPatternConfig 方法中的 set_fuser_method()</a>
</li>
      <li><a href="generated/torch.jit.set_fusion_strategy.html#torch.jit.set_fusion_strategy">torch.jit 模块中的 set_fusion_strategy()</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_global">set_global() (torch.ao.quantization.qconfig_mapping.QConfigMapping 方法)</a>
</li>
      <li><a href="generated/torch.autograd.grad_mode.set_grad_enabled.html#torch.autograd.grad_mode.set_grad_enabled">set_grad_enabled (torch.autograd.grad_mode 类)</a>
</li>
      <li><a href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_input_quantized_indexes">set_input_quantized_indexes() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig 方法)</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_is_last_backward">set_is_last_backward() (torch.distributed.fsdp.FSDPModule 方法)</a>
</li>
      <li><a href="library.html#torch._library.custom_ops.CustomOpDef.set_kernel_enabled">set_kernel_enabled() (torch._library.custom_ops.CustomOpDef 方法)</a>
</li>
      <li><a href="generated/torch._logging.set_logs.html#torch._logging.set_logs">set_logs() (在模块 torch._logging 中)</a>
</li>
      <li><a href="generated/torch.autograd.function.BackwardCFunction.html#torch.autograd.function.BackwardCFunction.set_materialize_grads">set_materialize_grads() (torch.autograd.function.BackwardCFunction 方法)</a>

      <ul>
        <li><a href="generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html#torch.autograd.function.FunctionCtx.set_materialize_grads">(torch.autograd.function.FunctionCtx 方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.set_materialize_grads">(torch.autograd.function.InplaceFunction 方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.set_materialize_grads">(torch.autograd.function.NestedIOFunction 方法)</a>
</li>
      </ul></li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.set_max_tuning_duration">torch.cuda.tunable 模块中的 set_max_tuning_duration()函数</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.set_max_tuning_iterations">torch.cuda.tunable 模块中的 set_max_tuning_iterations()函数</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.set_model_state_dict">set_model_state_dict() (在模块 torch.distributed.checkpoint.state_dict 中)</a>
</li>
      <li><a href="generated/torch.utils.set_module.html#torch.utils.set_module">set_module() (在模块 torch.utils 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name">set_module_name() (torch.ao.quantization.qconfig_mapping.QConfigMapping 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_object_type_order">set_module_name_object_type_order() (torch.ao.quantization.qconfig_mapping.QConfigMapping 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_regex">set_module_name_regex() (torch.ao.quantization.qconfig_mapping.QConfigMapping 方法)</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_modules_to_backward_prefetch">set_modules_to_backward_prefetch() (torch.distributed.fsdp.FSDPModule 方法)</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_modules_to_forward_prefetch">set_modules_to_forward_prefetch() (torch.distributed.fsdp.FSDPModule 方法)</a>
</li>
      <li><a href="generated/torch.autograd.grad_mode.set_multithreading_enabled.html#torch.autograd.grad_mode.set_multithreading_enabled">torch.autograd.grad_mode 类中的 set_multithreading_enabled</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig.set_name">torch.ao.quantization.backend_config.BackendConfig 方法中的 set_name()</a>
</li>
      <li><a href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_classes">torch.ao.quantization.fx.custom_config.PrepareCustomConfig 方法中的 set_non_traceable_module_classes()</a>
</li>
      <li><a href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_names">torch.ao.quantization.fx.custom_config.PrepareCustomConfig 方法中的 set_non_traceable_module_names()</a>
</li>
      <li><a href="generated/torch.set_num_interop_threads.html#torch.set_num_interop_threads">torch 模块中的 set_num_interop_threads()函数</a>
</li>
      <li><a href="generated/torch.set_num_threads.html#torch.set_num_threads">torch 模块中的 set_num_threads()函数</a>
</li>
      <li><a href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_object_type">torch.ao.quantization.qconfig_mapping.QConfigMapping 方法中的 set_object_type()</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_observation_type">torch.ao.quantization.backend_config.BackendPatternConfig 方法中的 set_observation_type()</a>
</li>
      <li><a href="generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_observed_to_quantized_mapping">set_observed_to_quantized_mapping() (torch.ao.quantization.fx.custom_config.ConvertCustomConfig 方法)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.set_optimizer_state_dict">set_optimizer_state_dict() (在模块 torch.distributed.checkpoint.state_dict 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_output_quantized_indexes">set_output_quantized_indexes() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig 方法)</a>
</li>
      <li><a href="future_mod.html#torch.__future__.set_overwrite_module_params_on_conversion">set_overwrite_module_params_on_conversion() (在模块 torch.__future__ 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_pattern">set_pattern() (torch.ao.quantization.backend_config.BackendPatternConfig 方法)</a>
</li>
      <li><a href="generated/torch.cuda.set_per_process_memory_fraction.html#torch.cuda.set_per_process_memory_fraction">set_per_process_memory_fraction() (在模块 torch.cuda 中)</a>

      <ul>
        <li><a href="generated/torch.mps.set_per_process_memory_fraction.html#torch.mps.set_per_process_memory_fraction">（在模块 torch.mps 中）</a>
</li>
      </ul></li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_post_optim_event">set_post_optim_event() (torch.distributed.fsdp.FSDPModule 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_preserved_attributes">set_preserved_attributes() (torch.ao.quantization.fx.custom_config.ConvertCustomConfig 方法)</a>

      <ul>
        <li><a href="generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html#torch.ao.quantization.fx.custom_config.FuseCustomConfig.set_preserved_attributes">(torch.ao.quantization.fx.custom_config.FuseCustomConfig 方法)</a>
</li>
        <li><a href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_preserved_attributes">(torch.ao.quantization.fx.custom_config.PrepareCustomConfig 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.set_printoptions.html#torch.set_printoptions">set_printoptions() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_qat_module">set_qat_module() (torch.ao.quantization.backend_config.BackendPatternConfig 方法)</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_reduce_scatter_divide_factor">set_reduce_scatter_divide_factor() (torch.distributed.fsdp.FSDPModule 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_reference_quantized_module">set_reference_quantized_module() (torch.ao.quantization.backend_config.BackendPatternConfig 方法)</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_requires_all_reduce">set_requires_all_reduce() (torch.distributed.fsdp.FSDPModule 方法)</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_requires_gradient_sync">set_requires_gradient_sync() (torch.distributed.fsdp.FSDPModule 方法)</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_reshard_after_backward">set_reshard_after_backward() (torch.distributed.fsdp.FSDPModule 方法)</a>
</li>
      <li><a href="futures.html#torch.futures.Future.set_result">set_result() (torch.futures.Future 方法)</a>
</li>
      <li><a href="generated/torch.set_rng_state.html#torch.set_rng_state">set_rng_state() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.cuda.set_rng_state.html#torch.cuda.set_rng_state">(在模块 torch.cuda 中)</a>
</li>
        <li><a href="generated/torch.mps.set_rng_state.html#torch.mps.set_rng_state">（在模块 torch.mps 中）</a>
</li>
        <li><a href="generated/torch.mtia.set_rng_state.html#torch.mtia.set_rng_state">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="random.html#torch.random.set_rng_state">(在模块 torch.random 中)</a>
</li>
        <li><a href="generated/torch.xpu.set_rng_state.html#torch.xpu.set_rng_state">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.set_rng_state_all.html#torch.cuda.set_rng_state_all">set_rng_state_all() (在模块 torch.cuda 中)</a>

      <ul>
        <li><a href="generated/torch.xpu.set_rng_state_all.html#torch.xpu.set_rng_state_all">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.set_root_module">set_root_module() (torch.ao.quantization.backend_config.BackendPatternConfig 方法)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.set_rotating_buffer_size">set_rotating_buffer_size() (在模块 torch.cuda.tunable 中)</a>
</li>
      <li><a href="multiprocessing.html#torch.multiprocessing.set_sharing_strategy">set_sharing_strategy() (在模块 torch.multiprocessing 中)</a>
</li>
      <li><a href="generated/torch.compiler.set_stance.html#torch.compiler.set_stance">set_stance() (在模块 torch.compiler 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_class">set_standalone_module_class() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig 方法)</a>
</li>
      <li><a href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_name">set_standalone_module_name() (torch.ao.quantization.fx.custom_config.PrepareCustomConfig 方法)</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.set_state">set_state() (torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend 方法)</a>

      <ul>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.set_state">(torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend 方法)</a>
</li>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.set_state">(torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend 方法)</a>
</li>
        <li><a href="generated/torch.Generator.html#torch.Generator.set_state">(torch.Generator 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.set_state_dict">set_state_dict()（在模块 torch.distributed.checkpoint.state_dict 中）</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type">set_state_dict_type()（torch.distributed.fsdp.FullyShardedDataParallel 静态方法）</a>
</li>
      <li><a href="generated/torch.accelerator.set_stream.html#torch.accelerator.set_stream">set_stream()（在模块 torch.accelerator 中）</a>

      <ul>
        <li><a href="generated/torch.cuda.set_stream.html#torch.cuda.set_stream">(在模块 torch.cuda 中)</a>
</li>
        <li><a href="generated/torch.mtia.set_stream.html#torch.mtia.set_stream">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="generated/torch.xpu.set_stream.html#torch.xpu.set_stream">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.set_submodule">set_submodule() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.set_submodule">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.set_submodule">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="future_mod.html#torch.__future__.set_swap_module_params_on_conversion">set_swap_module_params_on_conversion() (在模块 torch.__future__ 中)</a>
</li>
      <li><a href="generated/torch.cuda.set_sync_debug_mode.html#torch.cuda.set_sync_debug_mode">set_sync_debug_mode() (在模块 torch.cuda 中)</a>
</li>
      <li><a href="distributed.html#torch.distributed.Store.set_timeout">set_timeout() (torch.distributed.Store 方法)</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.set_unbacked_var_to_val">set_unbacked_var_to_val() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_unshard_in_backward">set_unshard_in_backward() (torch.distributed.fsdp.FSDPModule 方法)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner.set_up_planner">set_up_planner() (torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner 方法)</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.LoadPlanner.set_up_planner">(torch.distributed.checkpoint.LoadPlanner 方法)</a>
</li>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.SavePlanner.set_up_planner">(torch.distributed.checkpoint.SavePlanner 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.set_up_storage_reader">set_up_storage_reader() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader 方法)</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.set_up_storage_reader">(torch.distributed.checkpoint.StorageReader 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.set_up_storage_writer">set_up_storage_writer() (torch.distributed.checkpoint.StorageWriter 方法)</a>
</li>
      <li><a href="generated/torch.set_warn_always.html#torch.set_warn_always">set_warn_always() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.setdefault">setdefault() (torch.autograd.profiler_util.StringTable 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.setdefault">(torch.nn.ParameterDict 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.setup_context">setup_context() (torch.autograd.function.InplaceFunction 静态方法)</a>

      <ul>
        <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.setup_context">(torch.autograd.function.NestedIOFunction 静态方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.optim.SGD.html#torch.optim.SGD">SGD (torch.optim 中的类)</a>
</li>
      <li><a href="generated/torch.sgn.html#torch.sgn">sgn() (模块 torch 中的函数)</a>

      <ul>
        <li><a href="generated/torch.Tensor.sgn.html#torch.Tensor.sgn">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.sgn_.html#torch.Tensor.sgn_">sgn_() (torch.Tensor 方法)</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.Shadow">torch.ao.ns._numeric_suite 中的 Shadow（类）</a>
</li>
      <li><a href="torch.ao.ns._numeric_suite.html#torch.ao.ns._numeric_suite.ShadowLogger">torch.ao.ns._numeric_suite 中的 ShadowLogger（类）</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.shape">torch.nn.attention.flex_attention.BlockMask 属性中的 shape</a>

      <ul>
        <li><a href="generated/torch.Tensor.shape.html#torch.Tensor.shape">（torch.Tensor 属性）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv">torch.fx.experimental.symbolic_shapes 中的 ShapeEnv（类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnvSettings.html#torch.fx.experimental.symbolic_shapes.ShapeEnvSettings">torch.fx.experimental.symbolic_shapes 中的 ShapeEnvSettings（类）</a>
</li>
      <li><a href="export.html#torch.export.dynamic_shapes.ShapesCollection">torch.export.dynamic_shapes 中的 ShapesCollection（类）</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.placement_types.Shard">torch.distributed.tensor.placement_types 中的 Shard（类）</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict">torch.distributed.fsdp.FullyShardedDataParallel 的静态方法 shard_full_optim_state_dict()</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict">sharded_optim_state_dict() (torch.distributed.fsdp.FullyShardedDataParallel 静态方法)</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.ShardedOptimStateDictConfig">ShardedOptimStateDictConfig (torch.distributed.fsdp 中的类)</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.ShardedStateDictConfig">ShardedStateDictConfig (torch.distributed.fsdp 中的类)</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.ShardingStrategy">ShardingStrategy (torch.distributed.fsdp 中的类)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.share_memory">share_memory() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.share_memory">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.share_memory">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_">share_memory_() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.share_memory_">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.share_memory_">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.short.html#torch.Tensor.short">short() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.short">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.short">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="storage.html#torch.ShortStorage">ShortStorage (torch 中的类)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.staging.AsyncStager.should_synchronize_after_execute">should_synchronize_after_execute (torch.distributed.checkpoint.staging.AsyncStager 属性)</a>
</li>
      <li><a href="config_mod.html#torch.__config__.show">show() (在模块 torch.__config__ 中)</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.shutdown">shutdown() (在模块 torch.distributed.rpc 中)</a>

      <ul>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.shutdown">(torch.distributed.elastic.rendezvous.RendezvousHandler 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.nn.quantized.Sigmoid.html#torch.ao.nn.quantized.Sigmoid">torch.ao.nn.quantized 中的 Sigmoid（类）</a>

      <ul>
        <li><a href="generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.sigmoid.html#torch.sigmoid">torch 模块中的 sigmoid()</a>

      <ul>
        <li><a href="generated/torch.nn.functional.sigmoid.html#torch.nn.functional.sigmoid">torch.nn.functional 模块中</a>
</li>
        <li><a href="generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.sigmoid_.html#torch.Tensor.sigmoid_">torch.Tensor 方法中的 sigmoid_()</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.SigmoidTransform">torch.distributions.transforms 中的 SigmoidTransform（类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.Transform.sign">torch.distributions.transforms 的 Transform 属性</a>
</li>
      <li><a href="generated/torch.sign.html#torch.sign">torch 模块中的 sign()</a>

      <ul>
        <li><a href="generated/torch.Tensor.sign.html#torch.Tensor.sign">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.sign_.html#torch.Tensor.sign_">torch.Tensor 方法中的 sign_()</a>
</li>
      <li><a href="generated/torch.signbit.html#torch.signbit">torch 模块中的 signbit()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.signbit.html#torch.Tensor.signbit">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Measurement.significant_figures">torch.utils.benchmark.Measurement 属性中的 significant_figures</a>
</li>
      <li><a href="generated/torch.nn.SiLU.html#torch.nn.SiLU">torch.nn 中的 SiLU 类</a>
</li>
      <li><a href="generated/torch.nn.functional.silu.html#torch.nn.functional.silu">silu()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.SimpleElasticAgent">SimpleElasticAgent（torch.distributed.elastic.agent.server 模块中的类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.simplify">simplify()（torch.fx.experimental.symbolic_shapes.ShapeEnv 方法）</a>
</li>
      <li><a href="generated/torch.sin.html#torch.sin">sin()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.sin.html#torch.Tensor.sin">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.sin_.html#torch.Tensor.sin_">sin_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.sinc.html#torch.sinc">sinc() (在 torch 模块中)</a>

      <ul>
        <li><a href="special.html#torch.special.sinc">torch.special 模块中</a>
</li>
        <li><a href="generated/torch.Tensor.sinc.html#torch.Tensor.sinc">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.sinc_.html#torch.Tensor.sinc_">sinc_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.sinh.html#torch.sinh">sinh() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.sinh.html#torch.Tensor.sinh">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.sinh_.html#torch.Tensor.sinh_">torch.Tensor 方法 sinh_()</a>
</li>
      <li><a href="size.html#torch.Size">torch 中的 Size 类</a>
</li>
      <li><a href="backends.html#torch.backends.cuda.cufft_plan_cache.size">torch.backends.cuda.cufft_plan_cache 模块中的 size</a>
</li>
      <li><a href="generated/torch.Tensor.size.html#torch.Tensor.size">torch.Tensor 方法 size()</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.size">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.size">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.size_hint">size_hint() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="notes/serialization.html#torch.serialization.skip_data">skip_data (torch.serialization 中的类)</a>
</li>
      <li><a href="generated/torch.nn.utils.skip_init.html#torch.nn.utils.skip_init">skip_init() (模块 torch.nn.utils 中的方法)</a>
</li>
      <li><a href="generated/torch.slice_scatter.html#torch.slice_scatter">slice_scatter() (模块 torch 中的方法)</a>

      <ul>
        <li><a href="generated/torch.Tensor.slice_scatter.html#torch.Tensor.slice_scatter">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.slogdet.html#torch.slogdet">torch 模块中的 slogdet()函数</a>

      <ul>
        <li><a href="generated/torch.linalg.slogdet.html#torch.linalg.slogdet">(在 torch.linalg 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.slogdet.html#torch.Tensor.slogdet">torch.Tensor 方法</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.smm.html#torch.smm">torch 模块中的 smm()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.smm.html#torch.Tensor.smm">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.functional.smooth_l1_loss.html#torch.nn.functional.smooth_l1_loss">torch.nn.functional 模块中的 smooth_l1_loss()函数</a>
</li>
      <li><a href="generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss">torch.nn 中的 SmoothL1Loss 类</a>
</li>
      <li><a href="generated/torch.mtia.snapshot.html#torch.mtia.snapshot">torch.mtia 模块中的 snapshot()</a>

      <ul>
        <li><a href="generated/torch.cuda.MemPool.html#torch.cuda.MemPool.snapshot">torch.cuda.MemPool 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine">torch.quasirandom 中的 SobolEngine 类</a>
</li>
      <li><a href="generated/torch.nn.functional.soft_margin_loss.html#torch.nn.functional.soft_margin_loss">torch.nn.functional 模块中的 soft_margin_loss()</a>
</li>
      <li><a href="generated/torch.nn.SoftMarginLoss.html#torch.nn.SoftMarginLoss">SoftMarginLoss（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.Softmax.html#torch.nn.Softmax">Softmax（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.softmax.html#torch.softmax">softmax()（torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax">torch.nn.functional 模块中</a>
</li>
        <li><a href="generated/torch.sparse.softmax.html#torch.sparse.softmax">（在 torch.sparse 模块中）</a>
</li>
        <li><a href="special.html#torch.special.softmax">torch.special 模块中</a>
</li>
        <li><a href="generated/torch.Tensor.softmax.html#torch.Tensor.softmax">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.Softmax2d.html#torch.nn.Softmax2d">Softmax2d（torch.nn 中的类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.SoftmaxTransform">SoftmaxTransform（torch.distributions.transforms 中的类）</a>
</li>
      <li><a href="generated/torch.nn.Softmin.html#torch.nn.Softmin">Softmin（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.functional.softmin.html#torch.nn.functional.softmin">softmin()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.nn.Softplus.html#torch.nn.Softplus">Softplus（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.functional.softplus.html#torch.nn.functional.softplus">softplus()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.SoftplusTransform">SoftplusTransform（torch.distributions.transforms 中的类）</a>
</li>
      <li><a href="generated/torch.nn.Softshrink.html#torch.nn.Softshrink">torch.nn 中的 Softshrink（类）</a>
</li>
      <li><a href="generated/torch.nn.functional.softshrink.html#torch.nn.functional.softshrink">torch.nn.functional 模块中的 softshrink()函数</a>
</li>
      <li><a href="generated/torch.nn.Softsign.html#torch.nn.Softsign">torch.nn 中的 Softsign（类）</a>
</li>
      <li><a href="generated/torch.nn.functional.softsign.html#torch.nn.functional.softsign">torch.nn.functional 模块中的 softsign()函数</a>
</li>
      <li><a href="generated/torch.linalg.solve.html#torch.linalg.solve">torch.linalg 模块中的 solve()函数</a>

      <ul>
        <li><a href="generated/torch.fx.experimental.symbolic_shapes.DimConstraints.html#torch.fx.experimental.symbolic_shapes.DimConstraints.solve">(torch.fx.experimental.symbolic_shapes.DimConstraints 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.linalg.solve_ex.html#torch.linalg.solve_ex">torch.linalg 模块中的 solve_ex()函数</a>
</li>
      <li><a href="generated/torch.linalg.solve_triangular.html#torch.linalg.solve_triangular">torch.linalg 模块中的 solve_triangular()函数</a>
</li>
      <li><a href="generated/torch.sort.html#torch.sort">torch 模块中的 sort()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.sort.html#torch.Tensor.sort">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.sorted_indices">torch.nn.utils.rnn.PackedSequence 属性中的 sorted_indices</a>
</li>
      <li><a href="distributed.html#torch.distributed.Work.source_rank">torch.distributed.Work 方法中的 source_rank()</a>
</li>
      <li><a href="nn.init.html#torch.nn.init.sparse_">torch.nn.init 模块中的 sparse_()函数</a>
</li>
      <li><a href="generated/torch.sparse_bsc_tensor.html#torch.sparse_bsc_tensor">torch 模块中的 sparse_bsc_tensor()</a>
</li>
      <li><a href="generated/torch.sparse_bsr_tensor.html#torch.sparse_bsr_tensor">torch 模块中的 sparse_bsr_tensor()</a>
</li>
      <li><a href="generated/torch.sparse_compressed_tensor.html#torch.sparse_compressed_tensor">torch 模块中的 sparse_compressed_tensor()</a>
</li>
      <li><a href="generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor">torch 模块中的 sparse_coo_tensor()</a>
</li>
      <li><a href="generated/torch.sparse_csc_tensor.html#torch.sparse_csc_tensor">torch 模块中的 sparse_csc_tensor()</a>
</li>
      <li><a href="generated/torch.sparse_csr_tensor.html#torch.sparse_csr_tensor">torch 模块中的 sparse_csr_tensor()</a>
</li>
      <li><a href="generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim">torch.Tensor 方法中的 sparse_dim()</a>
</li>
      <li><a href="generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask">sparse_mask() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.sparse_resize_.html#torch.Tensor.sparse_resize_">sparse_resize_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.sparse_resize_and_clear_.html#torch.Tensor.sparse_resize_and_clear_">sparse_resize_and_clear_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam">SparseAdam (torch.optim 中的类)</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.sparsity">sparsity() (torch.nn.attention.flex_attention.BlockMask 方法)</a>
</li>
      <li><a href="multiprocessing.html#torch.multiprocessing.spawn.spawn">spawn() (在 torch.multiprocessing.spawn 模块中)</a>
</li>
      <li><a href="multiprocessing.html#torch.multiprocessing.SpawnContext">SpawnContext (torch.multiprocessing 中的类)</a>
</li>
      <li><a href="generated/torch.sparse.spdiags.html#torch.sparse.spdiags">spdiags()（在 torch.sparse 模块中）</a>
</li>
      <li><a href="generated/torch.nn.utils.spectral_norm.html#torch.nn.utils.spectral_norm">spectral_norm()（在 torch.nn.utils 模块中）</a>

      <ul>
        <li><a href="generated/torch.nn.utils.parametrizations.spectral_norm.html#torch.nn.utils.parametrizations.spectral_norm">（在 torch.nn.utils.parametrizations 模块中）</a>
</li>
      </ul></li>
      <li><a href="special.html#torch.special.spherical_bessel_j0">spherical_bessel_j0()（在 torch.special 模块中）</a>
</li>
      <li><a href="generated/torch.split.html#torch.split">torch 模块中的 split()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.split.html#torch.Tensor.split">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.microbatch.split_args_kwargs_into_chunks">torch.distributed.pipelining.microbatch 模块中的 split_args_kwargs_into_chunks()函数</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.SplitPoint">torch.distributed.pipelining 模块中的 SplitPoint 类</a>
</li>
      <li><a href="generated/torch.sparse.spsolve.html#torch.sparse.spsolve">torch.sparse 模块中的 spsolve()函数</a>
</li>
      <li><a href="generated/torch.sqrt.html#torch.sqrt">torch 模块中的 sqrt()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.sqrt_.html#torch.Tensor.sqrt_">torch.Tensor 方法中的 sqrt_()</a>
</li>
      <li><a href="generated/torch.square.html#torch.square">torch 模块中的 square()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.square.html#torch.Tensor.square">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.square_.html#torch.Tensor.square_">torch.Tensor 方法中的 square_()</a>
</li>
      <li><a href="generated/torch.squeeze.html#torch.squeeze">torch 模块中的 squeeze()</a>

      <ul>
        <li><a href="generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.squeeze_.html#torch.Tensor.squeeze_">torch.Tensor 方法中的 squeeze_()</a>
</li>
      <li><a href="generated/torch.sspaddmm.html#torch.sspaddmm">torch 模块中的 sspaddmm()</a>

      <ul>
        <li><a href="generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.constraints.stack">torch.distributions.constraints 模块中的 stack()</a>
</li>
      <li><a href="generated/torch.stack.html#torch.stack">torch 模块中的 stack()</a>
</li>
      <li><a href="generated/torch.func.stack_module_state.html#torch.func.stack_module_state">torch.func 模块中的 stack_module_state()</a>
</li>
      <li><a href="fx.html#torch.fx.Node.stack_trace">栈跟踪（torch.fx 节点属性）</a>
</li>
      <li><a href="data.html#torch.utils.data.StackDataset">StackDataset（torch.utils.data 中的类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.StackTransform">torch.distributions.transforms 中的 StackTransform（类）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.FileSystemWriter.stage">torch.distributed.checkpoint.FileSystemWriter 方法中的 stage()</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.staging.AsyncStager.stage">(torch.distributed.checkpoint.staging.异步分阶段器方法)</a>
</li>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.staging.BlockingAsyncStager.stage">(torch.distributed.checkpoint.staging.BlockingAsyncStager 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry.html#torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry">torch.ao.quantization.fx.custom_config 中的 StandaloneModuleConfigEntry（类）</a>
</li>
      <li><a href="generated/torch.mps.profiler.start.html#torch.mps.profiler.start">torch.mps.profiler 中的 start()（方法）</a>

      <ul>
        <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.health_check_server.HealthCheckServer.start">(torch.distributed.elastic.agent.server.health_check_server.HealthCheckServer 方法)</a>
</li>
      </ul></li>
      <li><a href="elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.start_processes">start_processes() (在 torch.distributed.elastic.multiprocessing 模块中)</a>
</li>
      <li><a href="monitor.html#torch.monitor.Stat">torch.monitor 中的 Stat（类）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.stateful.Stateful.state_dict">torch.distributed.checkpoint.stateful.Stateful 方法中的 state_dict()</a>

      <ul>
        <li><a href="distributed.optim.html#torch.distributed.optim.PostLocalSGDOptimizer.state_dict">(torch.distributed.optim.PostLocalSGDOptimizer 方法)</a>
</li>
        <li><a href="distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.state_dict">(torch.distributed.optim.ZeroRedundancyOptimizer 方法)</a>
</li>
        <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.state_dict">(torch.jit.ScriptModule 方法)</a>
</li>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.state_dict">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adadelta.html#torch.optim.Adadelta.state_dict">(torch.optim.Adadelta 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adafactor.html#torch.optim.Adafactor.state_dict">(torch.optim.Adafactor 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adagrad.html#torch.optim.Adagrad.state_dict">(torch.optim.Adagrad 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adam.html#torch.optim.Adam.state_dict">(torch.optim.Adam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adamax.html#torch.optim.Adamax.state_dict">(torch.optim.Adamax 方法)</a>
</li>
        <li><a href="generated/torch.optim.AdamW.html#torch.optim.AdamW.state_dict">(torch.optim.AdamW 方法)</a>
</li>
        <li><a href="generated/torch.optim.ASGD.html#torch.optim.ASGD.state_dict">(torch.optim.ASGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.LBFGS.html#torch.optim.LBFGS.state_dict">(torch.optim.LBFGS 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler.state_dict">(torch.optim.lr_scheduler.ChainedScheduler 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR.state_dict">(torch.optim.lr_scheduler.ConstantLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR.state_dict">(torch.optim.lr_scheduler.CosineAnnealingLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.state_dict">(torch.optim.lr_scheduler.CosineAnnealingWarmRestarts 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR.state_dict">(torch.optim.lr_scheduler.ExponentialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR.state_dict">(torch.optim.lr_scheduler.LambdaLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR.state_dict">(torch.optim.lr_scheduler.LinearLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler.state_dict">(torch.optim.lr_scheduler.LRScheduler 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR.state_dict">(torch.optim.lr_scheduler.MultiplicativeLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR.state_dict">(torch.optim.lr_scheduler.MultiStepLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR.state_dict">(torch.optim.lr_scheduler.OneCycleLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR.state_dict">(torch.optim.lr_scheduler.PolynomialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.state_dict">(torch.optim.lr_scheduler.SequentialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR.state_dict">(torch.optim.lr_scheduler.StepLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.NAdam.html#torch.optim.NAdam.state_dict">(torch.optim.NAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Optimizer.state_dict.html#torch.optim.Optimizer.state_dict">(torch.optim.Optimizer 方法)</a>
</li>
        <li><a href="generated/torch.optim.RAdam.html#torch.optim.RAdam.state_dict">(torch.optim.RAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.RMSprop.html#torch.optim.RMSprop.state_dict">(torch.optim.RMSprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.Rprop.html#torch.optim.Rprop.state_dict">(torch.optim.Rprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.SGD.html#torch.optim.SGD.state_dict">(torch.optim.SGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.state_dict">(torch.optim.SparseAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.state_dict">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.SWALR.html#torch.optim.swa_utils.SWALR.state_dict">(torch.optim.swa_utils.SWALR 方法)</a>
</li>
      </ul></li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type">state_dict_type() (torch.distributed.fsdp.FullyShardedDataParallel 静态方法)</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.StateDictConfig">StateDictConfig (torch.distributed.fsdp 中的类)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.StateDictOptions">torch.distributed.checkpoint.state_dict 中的 StateDictOptions（类）</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.StateDictSettings">torch.distributed.fsdp 中的 StateDictSettings（类）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.stateful.Stateful">torch.distributed.checkpoint.stateful 中的 Stateful（类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.StatefulSymbolicContext.html#torch.fx.experimental.symbolic_shapes.StatefulSymbolicContext">torch.fx.experimental.symbolic_shapes 中的 StatefulSymbolicContext（类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.StatelessSymbolicContext.html#torch.fx.experimental.symbolic_shapes.StatelessSymbolicContext">torch.fx.experimental.symbolic_shapes 中的无状态符号上下文（类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.statically_known_true.html#torch.fx.experimental.symbolic_shapes.statically_known_true">torch.fx.experimental.symbolic_shapes 模块中的 statically_known_true() 函数</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.CallgrindStats.stats">torch.utils.benchmark.CallgrindStats 方法中的 stats() 函数</a>
</li>
      <li><a href="generated/torch.std.html#torch.std">torch 模块中的 std() 函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.std.html#torch.Tensor.std">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.std_mean.html#torch.std_mean">torch 模块中的 std_mean()</a>
</li>
      <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev">torch.distributions.continuous_bernoulli.ContinuousBernoulli 属性 stddev</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.stddev">(torch.distributions.distribution.Distribution 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.stddev">(torch.distributions.exponential.Exponential 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.stddev">(torch.distributions.gumbel.Gumbel 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.stddev">(torch.distributions.laplace.Laplace 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.stddev">(torch.distributions.normal.Normal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.stddev">(torch.distributions.uniform.Uniform 属性)</a>
</li>
      </ul></li>
      <li><a href="distributed.optim.html#torch.distributed.optim.DistributedOptimizer.step">step() (torch.distributed.optim.DistributedOptimizer 方法)</a>

      <ul>
        <li><a href="distributed.optim.html#torch.distributed.optim.PostLocalSGDOptimizer.step">(torch.distributed.optim.PostLocalSGDOptimizer 方法)</a>
</li>
        <li><a href="distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer.step">(torch.distributed.optim.ZeroRedundancyOptimizer 方法)</a>
</li>
        <li><a href="distributed.pipelining.html#torch.distributed.pipelining.schedules.PipelineScheduleMulti.step">(torch.distributed.pipelining.schedules.PipelineScheduleMulti 方法)</a>
</li>
        <li><a href="distributed.pipelining.html#torch.distributed.pipelining.schedules.PipelineScheduleSingle.step">(torch.distributed.pipelining.schedules.PipelineScheduleSingle 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adadelta.html#torch.optim.Adadelta.step">(torch.optim.Adadelta 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adafactor.html#torch.optim.Adafactor.step">(torch.optim.Adafactor 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adagrad.html#torch.optim.Adagrad.step">(torch.optim.Adagrad 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adam.html#torch.optim.Adam.step">(torch.optim.Adam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adamax.html#torch.optim.Adamax.step">(torch.optim.Adamax 方法)</a>
</li>
        <li><a href="generated/torch.optim.AdamW.html#torch.optim.AdamW.step">(torch.optim.AdamW 方法)</a>
</li>
        <li><a href="generated/torch.optim.ASGD.html#torch.optim.ASGD.step">(torch.optim.ASGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.LBFGS.html#torch.optim.LBFGS.step">(torch.optim.LBFGS 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ChainedScheduler.html#torch.optim.lr_scheduler.ChainedScheduler.step">(torch.optim.lr_scheduler.ChainedScheduler 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR.step">(torch.optim.lr_scheduler.ConstantLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR.step">(torch.optim.lr_scheduler.CosineAnnealingLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step">(torch.optim.lr_scheduler.CosineAnnealingWarmRestarts 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR.step">(torch.optim.lr_scheduler.CyclicLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR.step">(torch.optim.lr_scheduler.ExponentialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR.step">(torch.optim.lr_scheduler.LambdaLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR.step">(torch.optim.lr_scheduler.LinearLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler.step">(torch.optim.lr_scheduler.LRScheduler 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR.step">(torch.optim.lr_scheduler.MultiplicativeLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR.step">(torch.optim.lr_scheduler.MultiStepLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR.step">(torch.optim.lr_scheduler.OneCycleLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR.step">(torch.optim.lr_scheduler.PolynomialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau.step">(torch.optim.lr_scheduler.ReduceLROnPlateau 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.SequentialLR.html#torch.optim.lr_scheduler.SequentialLR.step">(torch.optim.lr_scheduler.SequentialLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR.step">(torch.optim.lr_scheduler.StepLR 方法)</a>
</li>
        <li><a href="generated/torch.optim.NAdam.html#torch.optim.NAdam.step">(torch.optim.NAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step">(torch.optim.Optimizer 方法)</a>
</li>
        <li><a href="generated/torch.optim.RAdam.html#torch.optim.RAdam.step">(torch.optim.RAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.RMSprop.html#torch.optim.RMSprop.step">(torch.optim.RMSprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.Rprop.html#torch.optim.Rprop.step">(torch.optim.Rprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.SGD.html#torch.optim.SGD.step">(torch.optim.SGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.step">(torch.optim.SparseAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.SWALR.html#torch.optim.swa_utils.SWALR.step">(torch.optim.swa_utils.SWALR 方法)</a>
</li>
        <li><a href="profiler.html#torch.profiler.profile.step">(torch.profiler.profile 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR">StepLR (torch.optim.lr_scheduler 中的类)</a>
</li>
      <li><a href="generated/torch.stft.html#torch.stft">stft() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.stft.html#torch.Tensor.stft">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.transforms.StickBreakingTransform">torch.distributions.transforms 中的 StickBreakingTransform（类）</a>
</li>
      <li><a href="generated/torch.mps.profiler.stop.html#torch.mps.profiler.stop">torch.mps.profiler 模块中的 stop() 函数</a>

      <ul>
        <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.health_check_server.HealthCheckServer.stop">(torch.distributed.elastic.agent.server.health_check_server.HealthCheckServer 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.storage.html#torch.Tensor.storage">torch.Tensor 方法中的 storage()</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.storage_meta">storage_meta() (torch.distributed.checkpoint.StorageWriter 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.storage_offset.html#torch.Tensor.storage_offset">storage_offset() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type">storage_type() (torch.Tensor 方法)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader">torch.distributed.checkpoint 中的 StorageReader（类）</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter">torch.distributed.checkpoint 中的 StorageWriter（类）</a>
</li>
      <li><a href="distributed.html#torch.distributed.Store">torch.distributed 中的 Store（类）</a>
</li>
      <li><a href="backends.html#torch.backends.opt_einsum.strategy">torch.backends.opt_einsum 模块中的策略</a>
</li>
      <li><a href="generated/torch.Stream.html#torch.Stream">torch 中的 Stream（类）</a>

      <ul>
        <li><a href="generated/torch.cpu.Stream.html#torch.cpu.Stream">torch.cpu 中的（类）</a>
</li>
        <li><a href="generated/torch.cuda.Stream.html#torch.cuda.Stream">(torch.cuda 中的类)</a>
</li>
        <li><a href="generated/torch.mtia.Stream.html#torch.mtia.Stream">(torch.mtia 中的类)</a>
</li>
        <li><a href="generated/torch.xpu.Stream.html#torch.xpu.Stream">（torch.xpu 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cpu.stream.html#torch.cpu.stream">torch.cpu 模块中的 stream()函数</a>

      <ul>
        <li><a href="generated/torch.cuda.stream.html#torch.cuda.stream">(在模块 torch.cuda 中)</a>
</li>
        <li><a href="generated/torch.mtia.stream.html#torch.mtia.stream">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="generated/torch.xpu.stream.html#torch.xpu.stream">(在模块 torch.xpu 中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cpu.StreamContext.html#torch.cpu.StreamContext">torch.cpu 中的 StreamContext（类）</a>

      <ul>
        <li><a href="generated/torch.cuda.StreamContext.html#torch.cuda.StreamContext">(torch.cuda 中的类)</a>
</li>
        <li><a href="generated/torch.mtia.StreamContext.html#torch.mtia.StreamContext">(torch.mtia 中的类)</a>
</li>
        <li><a href="generated/torch.xpu.StreamContext.html#torch.xpu.StreamContext">（torch.xpu 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.strict_fusion.html#torch.jit.strict_fusion">torch.jit 中的 strict_fusion（类）</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint.html#torch.fx.experimental.symbolic_shapes.StrictMinMaxConstraint">torch.fx.experimental.symbolic_shapes 中的 StrictMinMaxConstraint（类）</a>
</li>
      <li><a href="generated/torch.Tensor.stride.html#torch.Tensor.stride">torch.Tensor 方法中的 stride()</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable">torch.autograd.profiler_util 中的 StringTable（类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.studentT.StudentT">torch.distributions.studentT 中的 StudentT（类）</a>
</li>
      <li><a href="generated/torch.sub.html#torch.sub">torch 模块中的 sub()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.sub.html#torch.Tensor.sub">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.sub_.html#torch.Tensor.sub_">torch.Tensor 方法中的 sub_()</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.SubclassSymbolicContext.html#torch.fx.experimental.symbolic_shapes.SubclassSymbolicContext">torch.fx.experimental.symbolic_shapes 中的 SubclassSymbolicContext 类</a>
</li>
      <li><a href="elastic/multiprocessing.html#torch.distributed.elastic.multiprocessing.api.SubprocessContext">torch.distributed.elastic.multiprocessing.api 中的 SubprocessContext 类</a>
</li>
      <li><a href="elastic/subprocess_handler.html#torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler.SubprocessHandler">torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler 类（SubprocessHandler）</a>
</li>
      <li><a href="data.html#torch.utils.data.Subset">torch.utils.data 中的 Subset 类</a>
</li>
      <li><a href="data.html#torch.utils.data.SubsetRandomSampler">torch.utils.data 中的 SubsetRandomSampler 类</a>
</li>
      <li><a href="generated/torch.compiler.substitute_in_graph.html#torch.compiler.substitute_in_graph">torch.compiler 模块中的 substitute_in_graph()函数</a>
</li>
      <li><a href="generated/torch.subtract.html#torch.subtract">torch 模块中的 subtract()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.subtract.html#torch.Tensor.subtract">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.subtract_.html#torch.Tensor.subtract_">torch.Tensor 方法中的 subtract_()函数</a>
</li>
      <li><a href="generated/torch.sum.html#torch.sum">torch 模块中的 sum()函数</a>

      <ul>
        <li><a href="generated/torch.sparse.sum.html#torch.sparse.sum">（在 torch.sparse 模块中）</a>
</li>
        <li><a href="generated/torch.Tensor.sum.html#torch.Tensor.sum">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.sum_to_size.html#torch.Tensor.sum_to_size">torch.Tensor 方法中的 sum_to_size()函数</a>
</li>
      <li><a href="tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter">SummaryWriter（torch.utils.tensorboard.writer 中的类）</a>
</li>
      <li><a href="fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params">summon_full_params()（torch.distributed.fsdp.FullyShardedDataParallel 静态方法）</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.support">support（torch.distributions.bernoulli.Bernoulli 属性）</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.support">（torch.distributions.beta.Beta 属性）</a>
</li>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.support">(torch.distributions.binomial.Binomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.support">(torch.distributions.categorical.Categorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.support">(torch.distributions.cauchy.Cauchy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.support">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.support">(torch.distributions.dirichlet.Dirichlet 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.support">(torch.distributions.distribution.Distribution 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.support">(torch.distributions.exponential.Exponential 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.support">(torch.distributions.fishersnedecor.FisherSnedecor 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.support">(torch.distributions.gamma.Gamma 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.support">(torch.distributions.geometric.Geometric 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.support">(torch.distributions.gumbel.Gumbel 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_cauchy.HalfCauchy.support">(torch.distributions.half_cauchy.HalfCauchy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_normal.HalfNormal.support">(torch.distributions.half_normal.HalfNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.support">(torch.distributions.independent.Independent 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.inverse_gamma.InverseGamma.support">(torch.distributions.inverse_gamma.InverseGamma 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.support">(torch.distributions.kumaraswamy.Kumaraswamy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.support">(torch.distributions.laplace.Laplace 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lkj_cholesky.LKJCholesky.support">(torch.distributions.lkj_cholesky.LKJCholesky 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.support">(torch.distributions.log_normal.LogNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support">(torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.support">(torch.distributions.mixture_same_family.MixtureSameFamily 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.support">(torch.distributions.multinomial.Multinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.support">(torch.distributions.multivariate_normal.MultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.negative_binomial.NegativeBinomial.support">(torch.distributions.negative_binomial.NegativeBinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.support">(torch.distributions.normal.Normal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.support">(torch.distributions.one_hot_categorical.OneHotCategorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.pareto.Pareto.support">(torch.distributions.pareto.Pareto 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.support">(torch.distributions.poisson.Poisson 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support">(torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support">(torch.distributions.relaxed_bernoulli.RelaxedBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support">(torch.distributions.relaxed_categorical.RelaxedOneHotCategorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.support">(torch.distributions.studentT.StudentT 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution.support">(torch.distributions.transformed_distribution.转换分布属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.support">(torch.distributions.uniform.Uniform 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.von_mises.VonMises.support">(torch.distributions.von_mises.VonMises 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.weibull.Weibull.support">(torch.distributions.weibull.Weibull 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.wishart.Wishart.support">(torch.distributions.wishart.Wishart 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.ShapeEnv.html#torch.fx.experimental.symbolic_shapes.ShapeEnv.suppress_guards">suppress_guards() (torch.fx.experimental.symbolic_shapes.ShapeEnv 方法)</a>
</li>
      <li><a href="generated/torch.svd.html#torch.svd">svd() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.linalg.svd.html#torch.linalg.svd">(在 torch.linalg 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.svd.html#torch.Tensor.svd">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.svd_lowrank.html#torch.svd_lowrank">svd_lowrank() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.linalg.svdvals.html#torch.linalg.svdvals">svdvals() (在 torch.linalg 模块中)</a>
</li>
      <li><a href="generated/torch.optim.swa_utils.SWALR.html#torch.optim.swa_utils.SWALR">SWALR（torch.optim.swa_utils 中的类）</a>
</li>
      <li><a href="generated/torch.ao.quantization.swap_module.html#torch.ao.quantization.swap_module">swap_module（torch.ao.quantization 中的类）</a>
</li>
      <li><a href="generated/torch.utils.swap_tensors.html#torch.utils.swap_tensors">swap_tensors()（torch.utils 模块中的函数）</a>
</li>
      <li><a href="generated/torch.swapaxes.html#torch.swapaxes">swapaxes()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.swapdims.html#torch.swapdims">swapdims() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.SyclExtension">torch.utils.cpp_extension 模块中的 SyclExtension()</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.sym_eq.html#torch.fx.experimental.symbolic_shapes.sym_eq">torch.fx.experimental.symbolic_shapes 模块中的 sym_eq()</a>
</li>
      <li><a href="generated/torch.sym_float.html#torch.sym_float">torch 模块中的 sym_float()</a>
</li>
      <li><a href="generated/torch.sym_fresh_size.html#torch.sym_fresh_size">torch 模块中的 sym_fresh_size()</a>
</li>
      <li><a href="generated/torch.sym_int.html#torch.sym_int">torch 模块中的 sym_int()</a>
</li>
      <li><a href="generated/torch.sym_ite.html#torch.sym_ite">torch 模块中的 sym_ite()</a>
</li>
      <li><a href="generated/torch.sym_max.html#torch.sym_max">torch 模块中的 sym_max()</a>
</li>
      <li><a href="generated/torch.sym_min.html#torch.sym_min">torch 模块中的 sym_min()</a>
</li>
      <li><a href="generated/torch.sym_not.html#torch.sym_not">torch 模块中的 sym_not()</a>
</li>
      <li><a href="generated/torch.sym_sum.html#torch.sym_sum">torch 模块中的 sym_sum()</a>
</li>
      <li><a href="fx.html#torch.fx.symbolic_trace">torch.fx 模块中的 symbolic_trace()</a>
</li>
      <li><a href="generated/torch.fx.experimental.symbolic_shapes.SymbolicContext.html#torch.fx.experimental.symbolic_shapes.SymbolicContext">torch.fx.experimental.symbolic_shapes 中的 SymbolicContext（类）</a>
</li>
      <li><a href="torch.html#torch.SymBool">torch 中的 SymBool（类）</a>
</li>
      <li><a href="export.html#torch.export.graph_signature.SymBoolArgument">torch.export.graph_signature 中的 SymBoolArgument（类）</a>
</li>
      <li><a href="torch.html#torch.SymFloat">torch 中的 SymFloat（类）</a>
</li>
      <li><a href="export.html#torch.export.graph_signature.SymFloatArgument">torch.export.graph_signature 中的 SymFloatArgument（类）</a>
</li>
      <li><a href="torch.html#torch.SymInt">torch 中的 SymInt（类）</a>
</li>
      <li><a href="export.html#torch.export.graph_signature.SymIntArgument">torch.export.graph_signature 中的 SymIntArgument（类）</a>
</li>
      <li><a href="generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm">torch.nn 中的 SyncBatchNorm（类）</a>
</li>
      <li><a href="generated/torch.accelerator.synchronize.html#torch.accelerator.synchronize">torch.accelerator 模块中的 synchronize()函数</a>

      <ul>
        <li><a href="generated/torch.cpu.synchronize.html#torch.cpu.synchronize">(在模块 torch.cpu 中)</a>
</li>
        <li><a href="generated/torch.cuda.synchronize.html#torch.cuda.synchronize">(在模块 torch.cuda 中)</a>
</li>
        <li><a href="generated/torch.mps.synchronize.html#torch.mps.synchronize">（在模块 torch.mps 中）</a>
</li>
        <li><a href="generated/torch.mtia.synchronize.html#torch.mtia.synchronize">（在 torch.mtia 模块中）</a>
</li>
        <li><a href="generated/torch.xpu.synchronize.html#torch.xpu.synchronize">(在模块 torch.xpu 中)</a>
</li>
        <li><a href="generated/torch.cuda.Event.html#torch.cuda.Event.synchronize">（torch.cuda.Event 方法）</a>
</li>
        <li><a href="generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream.synchronize">(torch.cuda.ExternalStream 方法)</a>
</li>
        <li><a href="generated/torch.cuda.Stream.html#torch.cuda.Stream.synchronize">(torch.cuda.Stream 方法)</a>
</li>
        <li><a href="distributed.html#torch.distributed.Work.synchronize">torch.distributed.Work 方法</a>
</li>
        <li><a href="generated/torch.Event.html#torch.Event.synchronize">torch.Event 方法</a>
</li>
        <li><a href="generated/torch.mps.event.Event.html#torch.mps.event.Event.synchronize">torch.mps.event.Event 方法</a>
</li>
        <li><a href="generated/torch.mtia.Event.html#torch.mtia.Event.synchronize">(torch.mtia.Event 方法)</a>
</li>
        <li><a href="generated/torch.mtia.Stream.html#torch.mtia.Stream.synchronize">(torch.mtia.Stream 方法)</a>
</li>
        <li><a href="generated/torch.Stream.html#torch.Stream.synchronize">(torch.Stream 方法)</a>
</li>
        <li><a href="generated/torch.xpu.Event.html#torch.xpu.Event.synchronize">(torch.xpu.Event 方法)</a>
</li>
        <li><a href="generated/torch.xpu.Stream.html#torch.xpu.Stream.synchronize">(torch.xpu.Stream 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.staging.AsyncStager.synchronize_staging">synchronize_staging() (torch.distributed.checkpoint.staging.AsyncStager 方法)</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.staging.BlockingAsyncStager.synchronize_staging">(torch.distributed.checkpoint.staging.BlockingAsyncStager 方法)</a>
</li>
      </ul></li>
  </ul></td>
</tr></tbody></table>

<h2 id="T">T</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="tensors.html#torch.Tensor.T">T (torch.Tensor 属性)</a>
</li>
      <li><a href="generated/torch.t.html#torch.t">t() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.t.html#torch.Tensor.t">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.t_.html#torch.Tensor.t_">torch.Tensor 方法 t_()</a>
</li>
      <li><a href="torch.html#torch.Tag">torch 中的 Tag 类</a>
</li>
      <li><a href="generated/torch.take.html#torch.take">torch 模块中的 take()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.take.html#torch.Tensor.take">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.take_along_dim.html#torch.take_along_dim">torch 模块中的 take_along_dim()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.take_along_dim.html#torch.Tensor.take_along_dim">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.tan.html#torch.tan">torch 模块中的 tan()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.tan.html#torch.Tensor.tan">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.tan_.html#torch.Tensor.tan_">tan_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.autograd.forward_ad.UnpackedDualTensor.html#torch.autograd.forward_ad.UnpackedDualTensor.tangent">tangent (torch.autograd.forward_ad.UnpackedDualTensor 属性)</a>
</li>
      <li><a href="generated/torch.nn.Tanh.html#torch.nn.Tanh">torch.nn 中的 Tanh 类</a>
</li>
      <li><a href="generated/torch.tanh.html#torch.tanh">torch 模块中的 tanh()</a>

      <ul>
        <li><a href="generated/torch.nn.functional.tanh.html#torch.nn.functional.tanh">torch.nn.functional 模块中</a>
</li>
        <li><a href="generated/torch.Tensor.tanh.html#torch.Tensor.tanh">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.tanh_.html#torch.Tensor.tanh_">torch.Tensor 方法中的 tanh_()</a>
</li>
      <li><a href="generated/torch.nn.Tanhshrink.html#torch.nn.Tanhshrink">torch.nn 中的 Tanhshrink 类</a>
</li>
      <li><a href="generated/torch.nn.functional.tanhshrink.html#torch.nn.functional.tanhshrink">tanhshrink()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="distributions.html#torch.distributions.transforms.TanhTransform">TanhTransform（torch.distributions.transforms 中的类）</a>
</li>
      <li><a href="distributed.html#torch.distributed.TCPStore">TCPStore（torch.distributed 中的类）</a>
</li>
      <li><a href="distributions.html#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature">temperature（torch.distributions.relaxed_bernoulli.RelaxedBernoulli 属性）</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature">(torch.distributions.relaxed_categorical.RelaxedOneHotCategorical 属性)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.temperature.html#torch.cuda.temperature">torch.cuda 中的 temperature()函数</a>
</li>
      <li><a href="tensors.html#torch.Tensor">torch 中的 Tensor 类</a>
</li>
      <li><a href="generated/torch.tensor.html#torch.tensor">torch 中的 tensor()函数</a>
</li>
      <li><a href="generated/torch.tensor_split.html#torch.tensor_split">torch 中的 tensor_split()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.planner.WriteItem.tensor_storage_size">tensor_storage_size() (torch.distributed.checkpoint.planner.WriteItem 方法)</a>
</li>
      <li><a href="profiler.html#torch.profiler.tensorboard_trace_handler">tensorboard_trace_handler() (在模块 torch.profiler 中)</a>
</li>
      <li><a href="monitor.html#torch.monitor.TensorboardEventHandler">TensorboardEventHandler (torch.monitor 中的类)</a>
</li>
      <li><a href="distributed.pipelining.html#torch.distributed.pipelining.microbatch.TensorChunkSpec">TensorChunkSpec (torch.distributed.pipelining.microbatch 中的类)</a>
</li>
      <li><a href="data.html#torch.utils.data.TensorDataset">torch.utils.data 中的 TensorDataset（类）</a>
</li>
      <li><a href="generated/torch.tensordot.html#torch.tensordot">torch 模块中的 tensordot()函数</a>
</li>
      <li><a href="generated/torch.linalg.tensorinv.html#torch.linalg.tensorinv">tensorinv() (在 torch.linalg 模块中)</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions">TensorPipeRpcBackendOptions（torch.distributed.rpc 中的类）</a>
</li>
      <li><a href="generated/torch.linalg.tensorsolve.html#torch.linalg.tensorsolve">tensorsolve() (在 torch.linalg 模块中)</a>
</li>
      <li><a href="futures.html#torch.futures.Future.then">then() (torch.futures.Future 方法)</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.threshold.html#torch.ao.nn.quantized.functional.threshold">threshold (torch.ao.nn.quantized.functional 中的类)</a>
</li>
      <li><a href="generated/torch.nn.Threshold.html#torch.nn.Threshold">Threshold (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.functional.threshold.html#torch.nn.functional.threshold">threshold()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.nn.functional.threshold_.html#torch.nn.functional.threshold_">threshold_()（在 torch.nn.functional 模块中）</a>
</li>
      <li><a href="generated/torch.tile.html#torch.tile">tile()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.tile.html#torch.Tensor.tile">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Timer.timeit">timeit() (torch.utils.benchmark.Timer 方法)</a>
</li>
      <li><a href="distributed.html#torch.distributed.Store.timeout">超时（torch.distributed.Store 属性）</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Timer">计时器（torch.utils.benchmark 中的类）</a>
</li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.TimerClient">torch.distributed.elastic.timer 中的 TimerClient（类）</a>
</li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.TimerRequest">torch.distributed.elastic.timer 中的 TimerRequest（类）</a>
</li>
      <li><a href="elastic/timer.html#torch.distributed.elastic.timer.TimerServer">torch.distributed.elastic.timer 中的 TimerServer（类）</a>
</li>
      <li><a href="monitor.html#torch.monitor.Event.timestamp">torch.monitor.Event 属性中的 timestamp</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.to">torch.jit.ScriptModule 方法 to()</a>

      <ul>
        <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.to">(torch.nn.attention.flex_attention.BlockMask 方法)</a>
</li>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.to">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.to">(torch.nn.utils.rnn.PackedSequence 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.to">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.to.html#torch.Tensor.to">torch.Tensor 方法</a>
</li>
        <li><a href="storage.html#torch.TypedStorage.to">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.to">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="fx.html#torch.fx.Tracer.to_bool">to_bool() (torch.fx.Tracer 方法)</a>
</li>
      <li><a href="generated/torch.nn.attention.bias.CausalVariant.html#torch.nn.attention.bias.CausalVariant.to_bytes">to_bytes() (torch.nn.attention.bias.CausalVariant 方法)</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.to_dense">to_dense() (torch.nn.attention.flex_attention.BlockMask 方法)</a>

      <ul>
        <li><a href="generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig.to_dict">to_dict() (torch.ao.quantization.backend_config.BackendConfig 方法)</a>

      <ul>
        <li><a href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig.to_dict">(torch.ao.quantization.backend_config.BackendPatternConfig 方法)</a>
</li>
        <li><a href="generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig.to_dict">(torch.ao.quantization.backend_config.DTypeConfig 方法)</a>
</li>
        <li><a href="generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.to_dict">(torch.ao.quantization.fx.custom_config.ConvertCustomConfig 方法)</a>
</li>
        <li><a href="generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html#torch.ao.quantization.fx.custom_config.FuseCustomConfig.to_dict">(torch.ao.quantization.fx.custom_config.FuseCustomConfig 方法)</a>
</li>
        <li><a href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.to_dict">(torch.ao.quantization.fx.custom_config.PrepareCustomConfig 方法)</a>
</li>
        <li><a href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping.to_dict">(torch.ao.quantization.qconfig_mapping.QConfigMapping 方法)</a>
</li>
      </ul></li>
      <li><a href="dlpack.html#torch.utils.dlpack.to_dlpack">torch.utils.dlpack 模块中的 to_dlpack()</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.to_empty">torch.jit.ScriptModule 方法中的 to_empty()</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.to_empty">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.to_empty">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="fx.html#torch.fx.GraphModule.to_folder">to_folder() (torch.fx.GraphModule 方法)</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.PyRRef.to_here">to_here() (torch.distributed.rpc.PyRRef 方法)</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.DTensor.to_local">to_local() (torch.distributed.tensor.DTensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.to_mkldnn.html#torch.Tensor.to_mkldnn">to_mkldnn() (torch.Tensor 方法)</a>
</li>
      <li><a href="nested.html#torch.nested.to_padded_tensor">to_padded_tensor() (在模块 torch.nested 中)</a>
</li>
      <li><a href="generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse">to_sparse() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.to_sparse_bsc.html#torch.Tensor.to_sparse_bsc">to_sparse_bsc() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr">to_sparse_bsr() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.to_sparse_coo.html#torch.Tensor.to_sparse_coo">to_sparse_coo() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc">to_sparse_csc() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr">to_sparse_csr() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.attention.flex_attention.html#torch.nn.attention.flex_attention.BlockMask.to_string">to_string() (torch.nn.attention.flex_attention.BlockMask 方法)</a>
</li>
      <li><a href="profiler.html#torch.profiler._KinetoProfile.toggle_collection_dynamic">toggle_collection_dynamic() (torch.profiler._KinetoProfile 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.tolist.html#torch.Tensor.tolist">tolist() (torch.Tensor 方法)</a>

      <ul>
        <li><a href="storage.html#torch.TypedStorage.tolist">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.tolist">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.topk.html#torch.topk">topk() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.topk.html#torch.Tensor.topk">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li>
    torch

      <ul>
        <li><a href="torch.html#module-torch">模块</a>
</li>
      </ul></li>
      <li>
    torch.__config__

      <ul>
        <li><a href="config_mod.html#module-torch.__config__">模块</a>
</li>
      </ul></li>
      <li>
    torch.__future__

      <ul>
        <li><a href="future_mod.html#module-torch.__future__">模块</a>
</li>
      </ul></li>
      <li>
    torch._logging

      <ul>
        <li><a href="logging.html#module-torch._logging">模块</a>
</li>
      </ul></li>
      <li>
    torch.accelerator

      <ul>
        <li><a href="accelerator.html#module-torch.accelerator">模块</a>
</li>
      </ul></li>
      <li>
    torch.amp

      <ul>
        <li><a href="amp.html#module-torch.amp">模块</a>
</li>
      </ul></li>
      <li>
    torch.amp.autocast_mode

      <ul>
        <li><a href="amp.html#module-torch.amp.autocast_mode">模块</a>
</li>
      </ul></li>
      <li>
    torch.amp.grad_scaler

      <ul>
        <li><a href="amp.html#module-torch.amp.grad_scaler">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao

      <ul>
        <li><a href="quantization.html#module-torch.ao">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.modules.fused

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.modules.fused">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.qat

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.qat">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.qat.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.qat.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.qat.modules.conv_fused

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.qat.modules.conv_fused">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.qat.modules.linear_fused

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.qat.modules.linear_fused">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.qat.modules.linear_relu

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.qat.modules.linear_relu">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.quantized

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.quantized">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.quantized.dynamic

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.quantized.dynamic">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.quantized.dynamic.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.quantized.dynamic.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.quantized.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.intrinsic.quantized.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.quantized.modules.bn_relu

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.bn_relu">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.quantized.modules.conv_add

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.conv_add">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.quantized.modules.conv_relu

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.conv_relu">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.intrinsic.quantized.modules.linear_relu

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.intrinsic.quantized.modules.linear_relu">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.qat

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.qat">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.qat.dynamic

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.qat.dynamic">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.qat.dynamic.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.qat.dynamic.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.qat.dynamic.modules.linear

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.qat.dynamic.modules.linear">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.qat.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.qat.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.qat.modules.conv

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.qat.modules.conv">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.qat.modules.embedding_ops

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.qat.modules.embedding_ops">模块</a>
</li>
      </ul></li>
      <li>torch.ao.nn.qat.模块.linear<ul>
        <li><a href="quantization.html#module-torch.ao.nn.qat.modules.linear">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantizable

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantizable">模块</a>
</li>
      </ul></li>
      <li>torch.ao.nn.quantizable.模块<ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantizable.modules">模块</a>
</li>
      </ul></li>
      <li>torch.ao.nn.quantizable.模块.activation<ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantizable.modules.activation">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantizable.modules.rnn

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantizable.modules.rnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.dynamic

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.quantized.dynamic">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.dynamic.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.quantized.dynamic.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.dynamic.modules.conv

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.dynamic.modules.conv">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.dynamic.modules.linear

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.dynamic.modules.linear">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.dynamic.modules.rnn

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.dynamic.modules.rnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.functional

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.quantized.functional">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.nn.quantized.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.modules.activation

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.activation">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.modules.batchnorm

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.batchnorm">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.modules.conv

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.conv">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.modules.dropout

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.dropout">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.modules.embedding_ops

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.embedding_ops">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.modules.functional_modules

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.functional_modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.modules.linear

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.linear">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.modules.normalization

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.normalization">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.modules.rnn

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.rnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.modules.utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.modules.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.reference

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.reference.modules

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.reference.modules.conv

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference.modules.conv">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.reference.modules.linear

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference.modules.linear">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.reference.modules.rnn

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference.modules.rnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.reference.modules.sparse

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference.modules.sparse">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.quantized.reference.modules.utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.quantized.reference.modules.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.sparse

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.sparse">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.sparse.quantized

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.sparse.quantized">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.sparse.quantized.dynamic

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.sparse.quantized.dynamic">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.sparse.quantized.dynamic.linear

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.sparse.quantized.dynamic.linear">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.sparse.quantized.linear

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.sparse.quantized.linear">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.nn.sparse.quantized.utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.nn.sparse.quantized.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.ns

      <ul>
        <li><a href="quantization.html#module-torch.ao.ns">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.ns._numeric_suite

      <ul>
        <li><a href="torch.ao.ns._numeric_suite.html#module-torch.ao.ns._numeric_suite">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.ns._numeric_suite_fx

      <ul>
        <li><a href="torch.ao.ns._numeric_suite_fx.html#module-torch.ao.ns._numeric_suite_fx">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.ns.fx

      <ul>
        <li><a href="quantization.html#module-torch.ao.ns.fx">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.ns.fx.graph_matcher

      <ul>
        <li><a href="quantization.html#module-torch.ao.ns.fx.graph_matcher">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.ns.fx.graph_passes

      <ul>
        <li><a href="quantization.html#module-torch.ao.ns.fx.graph_passes">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.ns.fx.mappings

      <ul>
        <li><a href="quantization.html#module-torch.ao.ns.fx.mappings">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.ns.fx.n_shadows_utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.ns.fx.n_shadows_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.ns.fx.ns_types

      <ul>
        <li><a href="quantization.html#module-torch.ao.ns.fx.ns_types">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.ns.fx.pattern_utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.ns.fx.pattern_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.ns.fx.qconfig_multi_mapping

      <ul>
        <li><a href="quantization.html#module-torch.ao.ns.fx.qconfig_multi_mapping">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.ns.fx.utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.ns.fx.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.ns.fx.weight_utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.ns.fx.weight_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.pruning

      <ul>
        <li><a href="quantization.html#module-torch.ao.pruning">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.pruning.scheduler

      <ul>
        <li><a href="quantization.html#module-torch.ao.pruning.scheduler">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.pruning.scheduler.base_scheduler

      <ul>
        <li><a href="quantization.html#module-torch.ao.pruning.scheduler.base_scheduler">模块</a>
</li>
      </ul></li>
      <li>torch.ao.pruning.scheduler.立方调度器<ul>
        <li><a href="quantization.html#module-torch.ao.pruning.scheduler.cubic_scheduler">模块</a>
</li>
      </ul></li>
      <li>torch.ao.pruning.scheduler.lambda 调度器<ul>
        <li><a href="quantization.html#module-torch.ao.pruning.scheduler.lambda_scheduler">模块</a>
</li>
      </ul></li>
      <li>torch.ao.pruning.稀疏化器<ul>
        <li><a href="quantization.html#module-torch.ao.pruning.sparsifier">模块</a>
</li>
      </ul></li>
      <li>torch.ao.pruning.稀疏化器.基础稀疏化器<ul>
        <li><a href="quantization.html#module-torch.ao.pruning.sparsifier.base_sparsifier">模块</a>
</li>
      </ul></li>
      <li>torch.ao.pruning.sparsifier.近似对角稀疏化器<ul>
        <li><a href="quantization.html#module-torch.ao.pruning.sparsifier.nearly_diagonal_sparsifier">模块</a>
</li>
      </ul></li>
      <li>torch.ao.pruning.sparsifier.utils 工具<ul>
        <li><a href="quantization.html#module-torch.ao.pruning.sparsifier.utils">模块</a>
</li>
      </ul></li>
      <li>torch.ao.pruning.sparsifier.weight_norm_sparsifier 权重归一稀疏化器<ul>
        <li><a href="quantization.html#module-torch.ao.pruning.sparsifier.weight_norm_sparsifier">模块</a>
</li>
      </ul></li>
      <li>torch.ao.quantization 量化<ul>
        <li><a href="quantization.html#module-torch.ao.quantization">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.backend_config

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.backend_config.backend_config

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.backend_config">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.backend_config.executorch

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.executorch">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.backend_config.fbgemm

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.fbgemm">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.backend_config.native

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.native">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.backend_config.observation_type

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.observation_type">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.backend_config.onednn

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.onednn">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.backend_config.qnnpack

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.qnnpack">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.backend_config.tensorrt

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.tensorrt">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.backend_config.utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.backend_config.x86

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.backend_config.x86">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fake_quantize

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fake_quantize">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fuse_modules

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fuse_modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fuser_method_mappings

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fuser_method_mappings">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx.convert

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.convert">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx.custom_config

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.custom_config">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx.fuse

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.fuse">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx.fuse_handler

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.fuse_handler">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx.graph_module

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.graph_module">模块</a>
</li>
      </ul></li>
      <li>torch.ao.quantization.fx.从低精度到 fbgemm<ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.lower_to_fbgemm">模块</a>
</li>
      </ul></li>
      <li>torch.ao.quantization.fx.从低精度到 qnnpack<ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.lower_to_qnnpack">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx.lstm_utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.lstm_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx.match_utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.match_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx.pattern_utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.pattern_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx.prepare

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.prepare">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx.qconfig_mapping_utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.qconfig_mapping_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx.quantize_handler

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.quantize_handler">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx.tracer

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.tracer">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.fx.utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.fx.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.observer

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.observer">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.pt2e

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.quantization.pt2e">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.pt2e.duplicate_dq_pass

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.duplicate_dq_pass">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.pt2e.export_utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.export_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.pt2e.graph_utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.graph_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.pt2e.port_metadata_pass

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.port_metadata_pass">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.pt2e.prepare

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.prepare">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.pt2e.qat_utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.qat_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.pt2e.representation

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.quantization.pt2e.representation">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.pt2e.representation.rewrite

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.representation.rewrite">模块</a>
</li>
      </ul></li>
      <li><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">torch.ao.quantization.pt2e.utils
torch.ao.量化.pt2e.工具</font></font></font><ul>
        <li><a href="quantization.html#module-torch.ao.quantization.pt2e.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.qconfig

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.qconfig">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.qconfig_mapping

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.qconfig_mapping">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quant_type

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.quant_type">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quantization_mappings

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.quantization_mappings">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quantize_fx

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.quantize_fx">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quantize_jit

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.quantize_jit">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quantize_pt2e

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.quantize_pt2e">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quantizer

      <ul>
        <li><a href="quantization-support.html#module-torch.ao.quantization.quantizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quantizer.composable_quantizer

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.composable_quantizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quantizer.embedding_quantizer

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.embedding_quantizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quantizer.quantizer

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.quantizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quantizer.utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quantizer.x86_inductor_quantizer

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.x86_inductor_quantizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quantizer.xnnpack_quantizer

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.xnnpack_quantizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quantizer.xnnpack_quantizer_utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.xnnpack_quantizer_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.quantizer.xpu_inductor_quantizer

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.quantizer.xpu_inductor_quantizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.stubs

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.stubs">模块</a>
</li>
      </ul></li>
      <li>
    torch.ao.quantization.utils

      <ul>
        <li><a href="quantization.html#module-torch.ao.quantization.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.autograd

      <ul>
        <li><a href="autograd.html#module-torch.autograd">模块</a>
</li>
      </ul></li>
      <li>
    torch.autograd.anomaly_mode

      <ul>
        <li><a href="autograd.html#module-torch.autograd.anomaly_mode">模块</a>
</li>
      </ul></li>
      <li>
    torch.autograd.forward_ad

      <ul>
        <li><a href="autograd.html#module-torch.autograd.forward_ad">模块</a>
</li>
      </ul></li>
      <li>
    torch.autograd.function

      <ul>
        <li><a href="autograd.html#module-torch.autograd.function">模块</a>
</li>
      </ul></li>
      <li>
    torch.autograd.functional

      <ul>
        <li><a href="autograd.html#module-torch.autograd.functional">模块</a>
</li>
      </ul></li>
      <li>
    torch.autograd.grad_mode

      <ul>
        <li><a href="autograd.html#module-torch.autograd.grad_mode">模块</a>
</li>
      </ul></li>
      <li>
    torch.autograd.gradcheck

      <ul>
        <li><a href="autograd.html#module-torch.autograd.gradcheck">模块</a>
</li>
      </ul></li>
      <li>
    torch.autograd.graph

      <ul>
        <li><a href="autograd.html#module-torch.autograd.graph">模块</a>
</li>
      </ul></li>
      <li>
    torch.autograd.profiler

      <ul>
        <li><a href="autograd.html#module-torch.autograd.profiler">模块</a>
</li>
      </ul></li>
      <li>
    torch.autograd.profiler_legacy

      <ul>
        <li><a href="autograd.html#module-torch.autograd.profiler_legacy">模块</a>
</li>
      </ul></li>
      <li>
    torch.autograd.profiler_util

      <ul>
        <li><a href="autograd.html#module-torch.autograd.profiler_util">模块</a>
</li>
      </ul></li>
      <li>
    torch.autograd.variable

      <ul>
        <li><a href="autograd.html#module-torch.autograd.variable">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends

      <ul>
        <li><a href="backends.html#module-torch.backends">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.cpu

      <ul>
        <li><a href="backends.html#module-torch.backends.cpu">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.cuda

      <ul>
        <li><a href="backends.html#module-torch.backends.cuda">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.cudnn

      <ul>
        <li><a href="backends.html#module-torch.backends.cudnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.cudnn.rnn

      <ul>
        <li><a href="backends.html#module-torch.backends.cudnn.rnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.cusparselt

      <ul>
        <li><a href="backends.html#module-torch.backends.cusparselt">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.kleidiai

      <ul>
        <li><a href="backends.html#module-torch.backends.kleidiai">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.mha

      <ul>
        <li><a href="backends.html#module-torch.backends.mha">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.mkl

      <ul>
        <li><a href="backends.html#module-torch.backends.mkl">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.mkldnn

      <ul>
        <li><a href="backends.html#module-torch.backends.mkldnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.mps

      <ul>
        <li><a href="backends.html#module-torch.backends.mps">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.nnpack

      <ul>
        <li><a href="backends.html#module-torch.backends.nnpack">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.openmp

      <ul>
        <li><a href="backends.html#module-torch.backends.openmp">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.opt_einsum

      <ul>
        <li><a href="backends.html#module-torch.backends.opt_einsum">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.quantized

      <ul>
        <li><a href="backends.html#module-torch.backends.quantized">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.xeon

      <ul>
        <li><a href="backends.html#module-torch.backends.xeon">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.xeon.run_cpu

      <ul>
        <li><a href="backends.html#module-torch.backends.xeon.run_cpu">模块</a>
</li>
      </ul></li>
      <li>
    torch.backends.xnnpack

      <ul>
        <li><a href="backends.html#module-torch.backends.xnnpack">模块</a>
</li>
      </ul></li>
      <li>
    torch.compiler

      <ul>
        <li><a href="torch.compiler_api.html#module-torch.compiler">模块</a>
</li>
      </ul></li>
      <li>
    torch.compiler.config

      <ul>
        <li><a href="torch.compiler.config.html#module-torch.compiler.config">模块</a>
</li>
      </ul></li>
      <li>
    torch.contrib

      <ul>
        <li><a href="torch.html#module-torch.contrib">模块</a>
</li>
      </ul></li>
      <li>
    torch.cpu

      <ul>
        <li><a href="cpu.html#module-torch.cpu">模块</a>
</li>
      </ul></li>
      <li>
    torch.cpu.amp

      <ul>
        <li><a href="amp.html#module-torch.cpu.amp">模块</a>
</li>
      </ul></li>
      <li>
    torch.cpu.amp.autocast_mode

      <ul>
        <li><a href="amp.html#module-torch.cpu.amp.autocast_mode">模块</a>
</li>
      </ul></li>
      <li>
    torch.cpu.amp.grad_scaler

      <ul>
        <li><a href="amp.html#module-torch.cpu.amp.grad_scaler">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda

      <ul>
        <li><a href="cuda.html#module-torch.cuda">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda._sanitizer

      <ul>
        <li><a href="cuda._sanitizer.html#module-torch.cuda._sanitizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.amp

      <ul>
        <li><a href="amp.html#module-torch.cuda.amp">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.amp.autocast_mode

      <ul>
        <li><a href="amp.html#module-torch.cuda.amp.autocast_mode">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.amp.common

      <ul>
        <li><a href="amp.html#module-torch.cuda.amp.common">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.amp.grad_scaler

      <ul>
        <li><a href="amp.html#module-torch.cuda.amp.grad_scaler">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.comm

      <ul>
        <li><a href="cuda.html#module-torch.cuda.comm">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.error

      <ul>
        <li><a href="cuda.html#module-torch.cuda.error">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.gds

      <ul>
        <li><a href="cuda.html#module-torch.cuda.gds">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.graphs

      <ul>
        <li><a href="cuda.html#module-torch.cuda.graphs">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.jiterator

      <ul>
        <li><a href="cuda.html#module-torch.cuda.jiterator">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.memory

      <ul>
        <li><a href="cuda.html#module-torch.cuda.memory">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.nccl

      <ul>
        <li><a href="cuda.html#module-torch.cuda.nccl">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.nvtx

      <ul>
        <li><a href="cuda.html#module-torch.cuda.nvtx">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.profiler

      <ul>
        <li><a href="cuda.html#module-torch.cuda.profiler">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.random

      <ul>
        <li><a href="cuda.html#module-torch.cuda.random">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.sparse

      <ul>
        <li><a href="cuda.html#module-torch.cuda.sparse">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.streams

      <ul>
        <li><a href="cuda.html#module-torch.cuda.streams">模块</a>
</li>
      </ul></li>
      <li>
    torch.cuda.tunable

      <ul>
        <li><a href="cuda.tunable.html#module-torch.cuda.tunable">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed

      <ul>
        <li><a href="distributed.html#module-torch.distributed">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.ddp_comm_hooks

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.ddp_comm_hooks.default_hooks

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.default_hooks">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.join

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.join">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.model_averaging

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.model_averaging">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.model_averaging.averagers

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.model_averaging.averagers">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.model_averaging.hierarchical_model_averager

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.model_averaging.hierarchical_model_averager">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.algorithms.model_averaging.utils

      <ul>
        <li><a href="distributed.html#module-torch.distributed.algorithms.model_averaging.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.argparse_util

      <ul>
        <li><a href="distributed.html#module-torch.distributed.argparse_util">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.autograd

      <ul>
        <li><a href="rpc.html#module-torch.distributed.autograd">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.c10d_logger

      <ul>
        <li><a href="distributed.html#module-torch.distributed.c10d_logger">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint

      <ul>
        <li><a href="distributed.checkpoint.html#module-torch.distributed.checkpoint">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.api

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.api">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.default_planner

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.default_planner">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.filesystem

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.filesystem">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.format_utils

      <ul>
        <li><a href="distributed.checkpoint.html#module-torch.distributed.checkpoint.format_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.logger

      <ul>
        <li><a href="distributed.checkpoint.html#module-torch.distributed.checkpoint.logger">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.logging_handlers

      <ul>
        <li><a href="distributed.checkpoint.html#module-torch.distributed.checkpoint.logging_handlers">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.metadata

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.metadata">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.optimizer

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.optimizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.planner

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.planner">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.planner_helpers

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.planner_helpers">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.resharding

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.resharding">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.staging

      <ul>
        <li><a href="distributed.checkpoint.html#module-torch.distributed.checkpoint.staging">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.state_dict

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.state_dict">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.state_dict_loader

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.state_dict_loader">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.state_dict_saver

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.state_dict_saver">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.stateful

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.stateful">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.storage

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.storage">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.checkpoint.utils

      <ul>
        <li><a href="distributed.html#module-torch.distributed.checkpoint.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.collective_utils

      <ul>
        <li><a href="distributed.html#module-torch.distributed.collective_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.constants

      <ul>
        <li><a href="distributed.html#module-torch.distributed.constants">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.device_mesh

      <ul>
        <li><a href="distributed.html#module-torch.distributed.device_mesh">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.distributed_c10d

      <ul>
        <li><a href="distributed.html#module-torch.distributed.distributed_c10d">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.agent

      <ul>
        <li><a href="elastic/agent.html#module-torch.distributed.elastic.agent">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.agent.server

      <ul>
        <li><a href="elastic/agent.html#module-torch.distributed.elastic.agent.server">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.agent.server.api

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.agent.server.api">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.elastic.agent.server.健康检查服务器<ul>
        <li><a href="elastic/agent.html#module-torch.distributed.elastic.agent.server.health_check_server">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.elastic.agent.server.本地弹性代理<ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.agent.server.local_elastic_agent">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.elastic.控制平面<ul>
        <li><a href="elastic/control_plane.html#module-torch.distributed.elastic.control_plane">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.elastic.事件<ul>
        <li><a href="elastic/events.html#module-torch.distributed.elastic.events">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.events.api

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.events.api">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.events.handlers

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.events.handlers">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.metrics

      <ul>
        <li><a href="elastic/metrics.html#module-torch.distributed.elastic.metrics">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.metrics.api

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.metrics.api">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.multiprocessing

      <ul>
        <li><a href="elastic/multiprocessing.html#module-torch.distributed.elastic.multiprocessing">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.multiprocessing.api

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.multiprocessing.api">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.multiprocessing.errors

      <ul>
        <li><a href="elastic/errors.html#module-torch.distributed.elastic.multiprocessing.errors">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.elastic.multiprocessing 错误处理器<ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.multiprocessing.errors.error_handler">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.elastic.multiprocessing 错误处理程序<ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.multiprocessing.errors.handlers">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.elastic.multiprocessing 重定向<ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.multiprocessing.redirects">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.elastic.multiprocessing 子进程处理器<ul>
        <li><a href="elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.elastic.多进程子进程处理器.处理器<ul>
        <li><a href="elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler.handlers">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.elastic.多进程子进程处理器.子进程处理器<ul>
        <li><a href="elastic/subprocess_handler.html#module-torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.elastic.多进程尾部日志<ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.multiprocessing.tail_log">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.elastic. rendezvous<ul>
        <li><a href="elastic/rendezvous.html#module-torch.distributed.elastic.rendezvous">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.rendezvous.api

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.api">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.rendezvous.c10d_rendezvous_backend

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.c10d_rendezvous_backend">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.rendezvous.dynamic_rendezvous

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.dynamic_rendezvous">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.rendezvous.etcd_rendezvous

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.etcd_rendezvous">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.rendezvous.etcd_rendezvous_backend

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.etcd_rendezvous_backend">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.rendezvous.etcd_server

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.etcd_server">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.rendezvous.etcd_store

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.etcd_store">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.rendezvous.registry

      <ul>
        <li><a href="elastic/rendezvous.html#module-torch.distributed.elastic.rendezvous.registry">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.rendezvous.static_tcp_rendezvous

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.static_tcp_rendezvous">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.rendezvous.utils

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.rendezvous.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.timer

      <ul>
        <li><a href="elastic/timer.html#module-torch.distributed.elastic.timer">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.timer.api

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.timer.api">模块</a>
</li>
      </ul></li>
      <li><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">torch.distributed.elastic.timer.debug_info_logging
torch 分布式弹性定时器调试信息记录</font></font></font><ul>
        <li><a href="elastic/timer.html#module-torch.distributed.elastic.timer.debug_info_logging">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.timer.file_based_local_timer

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.timer.file_based_local_timer">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.timer.local_timer

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.timer.local_timer">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.utils

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.utils.api

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.api">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.utils.data

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.data">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.utils.data.cycling_iterator

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.data.cycling_iterator">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.utils.data.elastic_distributed_sampler

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.data.elastic_distributed_sampler">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.utils.distributed

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.distributed">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.utils.log_level

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.log_level">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.utils.logging

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.logging">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.elastic.utils.store

      <ul>
        <li><a href="distributed.html#module-torch.distributed.elastic.utils.store">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.fsdp

      <ul>
        <li><a href="fsdp.html#module-torch.distributed.fsdp">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.fsdp.api

      <ul>
        <li><a href="distributed.html#module-torch.distributed.fsdp.api">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.fsdp.全分片数据并行<ul>
        <li><a href="distributed.html#module-torch.distributed.fsdp.fully_sharded_data_parallel">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.fsdp.分片梯度缩放器<ul>
        <li><a href="distributed.html#module-torch.distributed.fsdp.sharded_grad_scaler">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.fsdp.wrap

      <ul>
        <li><a href="distributed.html#module-torch.distributed.fsdp.wrap">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.launch

      <ul>
        <li><a href="distributed.html#module-torch.distributed.launch">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.launcher

      <ul>
        <li><a href="distributed.html#module-torch.distributed.launcher">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.launcher.api

      <ul>
        <li><a href="distributed.html#module-torch.distributed.launcher.api">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.logging_handlers

      <ul>
        <li><a href="distributed.html#module-torch.distributed.logging_handlers">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.nn

      <ul>
        <li><a href="distributed.html#module-torch.distributed.nn">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.nn.api

      <ul>
        <li><a href="distributed.html#module-torch.distributed.nn.api">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.nn.api.remote_module

      <ul>
        <li><a href="distributed.html#module-torch.distributed.nn.api.remote_module">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.nn.functional

      <ul>
        <li><a href="distributed.html#module-torch.distributed.nn.functional">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.nn.jit

      <ul>
        <li><a href="distributed.html#module-torch.distributed.nn.jit">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.nn.jit.instantiator

      <ul>
        <li><a href="distributed.html#module-torch.distributed.nn.jit.instantiator">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.nn.jit.templates

      <ul>
        <li><a href="distributed.html#module-torch.distributed.nn.jit.templates">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.nn.jit.模板.remote_module_template<ul>
        <li><a href="distributed.html#module-torch.distributed.nn.jit.templates.remote_module_template">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim

      <ul>
        <li><a href="distributed.optim.html#module-torch.distributed.optim">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.apply_optimizer_in_backward

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.apply_optimizer_in_backward">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.functional_adadelta

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_adadelta">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.functional_adagrad

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_adagrad">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.functional_adam

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_adam">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.functional_adamax

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_adamax">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.functional_adamw

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_adamw">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.functional_rmsprop

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_rmsprop">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.functional_rprop

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_rprop">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.functional_sgd

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.functional_sgd">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.named_optimizer

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.named_optimizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.optimizer

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.optimizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.post_localSGD_optimizer

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.post_localSGD_optimizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.utils

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.optim.zero_redundancy_optimizer

      <ul>
        <li><a href="distributed.html#module-torch.distributed.optim.zero_redundancy_optimizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.pipelining

      <ul>
        <li><a href="distributed.pipelining.html#module-torch.distributed.pipelining">模块</a>
</li>
      </ul></li>
      <li>torch.distributed.pipelining.微批处理<ul>
        <li><a href="distributed.pipelining.html#module-torch.distributed.pipelining.microbatch">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.pipelining.schedules

      <ul>
        <li><a href="distributed.pipelining.html#module-torch.distributed.pipelining.schedules">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.pipelining.stage

      <ul>
        <li><a href="distributed.pipelining.html#module-torch.distributed.pipelining.stage">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.remote_device

      <ul>
        <li><a href="distributed.html#module-torch.distributed.remote_device">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.rendezvous

      <ul>
        <li><a href="distributed.html#module-torch.distributed.rendezvous">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.rpc

      <ul>
        <li><a href="rpc.html#module-torch.distributed.rpc">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.rpc.api

      <ul>
        <li><a href="distributed.html#module-torch.distributed.rpc.api">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.rpc.backend_registry

      <ul>
        <li><a href="distributed.html#module-torch.distributed.rpc.backend_registry">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.rpc.constants

      <ul>
        <li><a href="distributed.html#module-torch.distributed.rpc.constants">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.rpc.functions

      <ul>
        <li><a href="distributed.html#module-torch.distributed.rpc.functions">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.rpc.internal

      <ul>
        <li><a href="distributed.html#module-torch.distributed.rpc.internal">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.rpc.options

      <ul>
        <li><a href="distributed.html#module-torch.distributed.rpc.options">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.rpc.rref_proxy

      <ul>
        <li><a href="distributed.html#module-torch.distributed.rpc.rref_proxy">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.rpc.server_process_global_profiler

      <ul>
        <li><a href="distributed.html#module-torch.distributed.rpc.server_process_global_profiler">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.run

      <ul>
        <li><a href="elastic/run.html#module-torch.distributed.run">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.tensor

      <ul>
        <li><a href="distributed.tensor.html#module-torch.distributed.tensor">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.tensor.debug

      <ul>
        <li><a href="distributed.tensor.html#module-torch.distributed.tensor.debug">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.tensor.device_mesh

      <ul>
        <li><a href="distributed.tensor.html#module-torch.distributed.tensor.device_mesh">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.tensor.experimental

      <ul>
        <li><a href="distributed.tensor.html#module-torch.distributed.tensor.experimental">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.tensor.parallel

      <ul>
        <li><a href="distributed.tensor.parallel.html#module-torch.distributed.tensor.parallel">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.tensor.parallel.api

      <ul>
        <li><a href="distributed.html#module-torch.distributed.tensor.parallel.api">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.tensor.parallel.ddp

      <ul>
        <li><a href="distributed.html#module-torch.distributed.tensor.parallel.ddp">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.tensor.parallel.fsdp

      <ul>
        <li><a href="distributed.html#module-torch.distributed.tensor.parallel.fsdp">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.tensor.parallel.input_reshard

      <ul>
        <li><a href="distributed.html#module-torch.distributed.tensor.parallel.input_reshard">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.tensor.parallel.loss

      <ul>
        <li><a href="distributed.html#module-torch.distributed.tensor.parallel.loss">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.tensor.parallel.style

      <ul>
        <li><a href="distributed.html#module-torch.distributed.tensor.parallel.style">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.tensor.placement_types

      <ul>
        <li><a href="distributed.tensor.html#module-torch.distributed.tensor.placement_types">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributed.utils

      <ul>
        <li><a href="distributed.html#module-torch.distributed.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions

      <ul>
        <li><a href="distributions.html#module-torch.distributions">模块</a>
</li>
      </ul></li>
      <li>torch.distributions.伯努利<ul>
        <li><a href="distributions.html#module-torch.distributions.bernoulli">模块</a>
</li>
      </ul></li>
      <li>torch.distributions.贝塔<ul>
        <li><a href="distributions.html#module-torch.distributions.beta">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.binomial

      <ul>
        <li><a href="distributions.html#module-torch.distributions.binomial">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.categorical

      <ul>
        <li><a href="distributions.html#module-torch.distributions.categorical">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.cauchy

      <ul>
        <li><a href="distributions.html#module-torch.distributions.cauchy">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.chi2

      <ul>
        <li><a href="distributions.html#module-torch.distributions.chi2">模块</a>
</li>
      </ul></li>
      <li>torch.distributions 约束注册表<ul>
        <li><a href="distributions.html#module-torch.distributions.constraint_registry">模块</a>
</li>
      </ul></li>
      <li>torch.distributions 约束<ul>
        <li><a href="distributions.html#module-torch.distributions.constraints">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.continuous_bernoulli

      <ul>
        <li><a href="distributions.html#module-torch.distributions.continuous_bernoulli">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.dirichlet

      <ul>
        <li><a href="distributions.html#module-torch.distributions.dirichlet">模块</a>
</li>
      </ul></li>
      <li><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">torch.distributions.distribution

分布库</font></font></font><ul>
        <li><a href="distributions.html#module-torch.distributions.distribution">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.exp_family

      <ul>
        <li><a href="distributions.html#module-torch.distributions.exp_family">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.exponential

      <ul>
        <li><a href="distributions.html#module-torch.distributions.exponential">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.fishersnedecor

      <ul>
        <li><a href="distributions.html#module-torch.distributions.fishersnedecor">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.gamma

      <ul>
        <li><a href="distributions.html#module-torch.distributions.gamma">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.geometric

      <ul>
        <li><a href="distributions.html#module-torch.distributions.geometric">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.gumbel

      <ul>
        <li><a href="distributions.html#module-torch.distributions.gumbel">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.half_cauchy

      <ul>
        <li><a href="distributions.html#module-torch.distributions.half_cauchy">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.half_normal

      <ul>
        <li><a href="distributions.html#module-torch.distributions.half_normal">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.independent

      <ul>
        <li><a href="distributions.html#module-torch.distributions.independent">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.inverse_gamma

      <ul>
        <li><a href="distributions.html#module-torch.distributions.inverse_gamma">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.kl

      <ul>
        <li><a href="distributions.html#module-torch.distributions.kl">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.kumaraswamy

      <ul>
        <li><a href="distributions.html#module-torch.distributions.kumaraswamy">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.laplace

      <ul>
        <li><a href="distributions.html#module-torch.distributions.laplace">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.lkj_cholesky

      <ul>
        <li><a href="distributions.html#module-torch.distributions.lkj_cholesky">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.log_normal

      <ul>
        <li><a href="distributions.html#module-torch.distributions.log_normal">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.logistic_normal

      <ul>
        <li><a href="distributions.html#module-torch.distributions.logistic_normal">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.lowrank_multivariate_normal

      <ul>
        <li><a href="distributions.html#module-torch.distributions.lowrank_multivariate_normal">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.mixture_same_family

      <ul>
        <li><a href="distributions.html#module-torch.distributions.mixture_same_family">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.multinomial

      <ul>
        <li><a href="distributions.html#module-torch.distributions.multinomial">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.multivariate_normal

      <ul>
        <li><a href="distributions.html#module-torch.distributions.multivariate_normal">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.negative_binomial

      <ul>
        <li><a href="distributions.html#module-torch.distributions.negative_binomial">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.normal

      <ul>
        <li><a href="distributions.html#module-torch.distributions.normal">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.one_hot_categorical

      <ul>
        <li><a href="distributions.html#module-torch.distributions.one_hot_categorical">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.pareto

      <ul>
        <li><a href="distributions.html#module-torch.distributions.pareto">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.poisson

      <ul>
        <li><a href="distributions.html#module-torch.distributions.poisson">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.relaxed_bernoulli

      <ul>
        <li><a href="distributions.html#module-torch.distributions.relaxed_bernoulli">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.relaxed_categorical

      <ul>
        <li><a href="distributions.html#module-torch.distributions.relaxed_categorical">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.studentT

      <ul>
        <li><a href="distributions.html#module-torch.distributions.studentT">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.transformed_distribution

      <ul>
        <li><a href="distributions.html#module-torch.distributions.transformed_distribution">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.transforms

      <ul>
        <li><a href="distributions.html#module-torch.distributions.transforms">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.uniform

      <ul>
        <li><a href="distributions.html#module-torch.distributions.uniform">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.utils

      <ul>
        <li><a href="distributions.html#module-torch.distributions.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.von_mises

      <ul>
        <li><a href="distributions.html#module-torch.distributions.von_mises">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.weibull

      <ul>
        <li><a href="distributions.html#module-torch.distributions.weibull">模块</a>
</li>
      </ul></li>
      <li>
    torch.distributions.wishart

      <ul>
        <li><a href="distributions.html#module-torch.distributions.wishart">模块</a>
</li>
      </ul></li>
      <li>
    torch.export

      <ul>
        <li><a href="export.html#module-torch.export">模块</a>
</li>
      </ul></li>
      <li>
    torch.export.custom_obj

      <ul>
        <li><a href="export.html#module-torch.export.custom_obj">模块</a>
</li>
      </ul></li>
      <li>
    torch.export.custom_ops

      <ul>
        <li><a href="export.html#module-torch.export.custom_ops">模块</a>
</li>
      </ul></li>
      <li>
    torch.export.decomp_utils

      <ul>
        <li><a href="export.html#module-torch.export.decomp_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.export.dynamic_shapes

      <ul>
        <li><a href="export.html#module-torch.export.dynamic_shapes">模块</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li>
    torch.export.experimental

      <ul>
        <li><a href="export.html#module-torch.export.experimental">模块</a>
</li>
      </ul></li>
      <li>
    torch.export.exported_program

      <ul>
        <li><a href="export.html#module-torch.export.exported_program">模块</a>
</li>
      </ul></li>
      <li>
    torch.export.graph_signature

      <ul>
        <li><a href="export.html#module-torch.export.graph_signature">模块</a>
</li>
      </ul></li>
      <li>
    torch.export.passes

      <ul>
        <li><a href="export.html#module-torch.export.passes">模块</a>
</li>
      </ul></li>
      <li>
    torch.export.unflatten

      <ul>
        <li><a href="export.html#module-torch.export.unflatten">模块</a>
</li>
      </ul></li>
      <li>
    torch.fft

      <ul>
        <li><a href="fft.html#module-torch.fft">模块</a>
</li>
      </ul></li>
      <li><a href="type_info.html#torch.torch.finfo">torch.finfo（torch 中的类）</a>
</li>
      <li>
    torch.func

      <ul>
        <li><a href="func.api.html#module-torch.func">模块</a>
</li>
      </ul></li>
      <li>
    torch.functional

      <ul>
        <li><a href="torch.html#module-torch.functional">模块</a>
</li>
      </ul></li>
      <li>
    torch.futures

      <ul>
        <li><a href="futures.html#module-torch.futures">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx

      <ul>
        <li><a href="fx.html#module-torch.fx">模块</a>
</li>
      </ul></li>
      <li>torch.fx 注解<ul>
        <li><a href="fx.html#module-torch.fx.annotate">模块</a>
</li>
      </ul></li>
      <li>torch.fx 配置<ul>
        <li><a href="fx.html#module-torch.fx.config">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.accelerator_partitioner

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.accelerator_partitioner">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.const_fold

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.const_fold">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.debug

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.debug">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.graph_gradual_typechecker

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.graph_gradual_typechecker">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.merge_matmul

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.merge_matmul">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.meta_tracer

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.meta_tracer">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.migrate_gradual_types

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.migrate_gradual_types.constraint

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.migrate_gradual_types.constraint_generator

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint_generator">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.migrate_gradual_types.constraint_transformation

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.constraint_transformation">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.migrate_gradual_types.operation

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.operation">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.migrate_gradual_types.transform_to_z3

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.transform_to_z3">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.migrate_gradual_types.util

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.util">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.migrate_gradual_types.z3_types

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.migrate_gradual_types.z3_types">模块</a>
</li>
      </ul></li>
      <li>torch.fx 实验性的规范化<ul>
        <li><a href="fx.html#module-torch.fx.experimental.normalize">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.optimization

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.optimization">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.partitioner_utils

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.partitioner_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.proxy_tensor

      <ul>
        <li><a href="fx.experimental.html#module-torch.fx.experimental.proxy_tensor">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.recording

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.recording">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.refinement_types

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.refinement_types">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.rewriter

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.rewriter">模块</a>
</li>
      </ul></li>
      <li>torch.fx 实验性模式类型注解<ul>
        <li><a href="fx.html#module-torch.fx.experimental.schema_type_annotation">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.sym_node

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.sym_node">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.symbolic_shapes

      <ul>
        <li><a href="fx.experimental.html#module-torch.fx.experimental.symbolic_shapes">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification.core

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification.core">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification.dispatch

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification.dispatch">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification.match

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification.match">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification.more

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification.more">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification.multipledispatch

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification.multipledispatch">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification.multipledispatch.conflict

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification.multipledispatch.conflict">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification.multipledispatch.core

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification.multipledispatch.core">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification.multipledispatch.dispatcher

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification.multipledispatch.dispatcher">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification.multipledispatch.utils

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification.multipledispatch.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification.multipledispatch.variadic

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification.multipledispatch.variadic">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification.unification_tools

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification.unification_tools">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification.utils

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unification.variable

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unification.variable">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.unify_refinements

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.unify_refinements">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.experimental.validator

      <ul>
        <li><a href="fx.html#module-torch.fx.experimental.validator">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.graph

      <ul>
        <li><a href="fx.html#module-torch.fx.graph">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.graph_module

      <ul>
        <li><a href="fx.html#module-torch.fx.graph_module">模块</a>
</li>
      </ul></li>
      <li>torch.fx 不可变集合<ul>
        <li><a href="fx.html#module-torch.fx.immutable_collections">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.interpreter

      <ul>
        <li><a href="fx.html#module-torch.fx.interpreter">模块</a>
</li>
      </ul></li>
      <li>torch.fx 节点<ul>
        <li><a href="fx.html#module-torch.fx.node">模块</a>
</li>
      </ul></li>
      <li>torch.fx 算子模式<ul>
        <li><a href="fx.html#module-torch.fx.operator_schemas">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes

      <ul>
        <li><a href="fx.html#module-torch.fx.passes">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.annotate_getitem_nodes

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.annotate_getitem_nodes">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.backends

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.backends">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.backends.cudagraphs

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.backends.cudagraphs">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.dialect

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.dialect">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.dialect.common

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.dialect.common">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.dialect.common.cse_pass

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.dialect.common.cse_pass">模块</a>
</li>
      </ul></li>
      <li>torch.fx.passes.伪造张量属性<ul>
        <li><a href="fx.html#module-torch.fx.passes.fake_tensor_prop">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.graph_drawer

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.graph_drawer">模块</a>
</li>
      </ul></li>
      <li>torch.fx.passes 图操作<ul>
        <li><a href="fx.html#module-torch.fx.passes.graph_manipulation">模块</a>
</li>
      </ul></li>
      <li>torch.fx.passes 图转换观察者<ul>
        <li><a href="fx.html#module-torch.fx.passes.graph_transform_observer">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.infra

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.infra">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.infra.partitioner

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.infra.partitioner">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.infra.pass_base

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.infra.pass_base">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.infra.pass_manager

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.infra.pass_manager">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.net_min_base

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.net_min_base">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.operator_support

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.operator_support">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.param_fetch

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.param_fetch">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.pass_manager

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.pass_manager">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.reinplace

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.reinplace">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.runtime_assert

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.runtime_assert">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.shape_prop

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.shape_prop">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.split_module

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.split_module">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.split_utils

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.split_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.splitter_base

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.splitter_base">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.tests

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.tests">模块</a>
</li>
      </ul></li>
      <li>torch.fx.passes 单元测试.test_pass_manager<ul>
        <li><a href="fx.html#module-torch.fx.passes.tests.test_pass_manager">模块</a>
</li>
      </ul></li>
      <li>torch.fx.passes 工具.common<ul>
        <li><a href="fx.html#module-torch.fx.passes.tools_common">模块</a>
</li>
      </ul></li>
      <li>torch.fx.passes 工具.utils<ul>
        <li><a href="fx.html#module-torch.fx.passes.utils">模块</a>
</li>
      </ul></li>
      <li>torch.fx.passes 工具.common<ul>
        <li><a href="fx.html#module-torch.fx.passes.utils.common">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.utils.fuser_utils

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.utils.fuser_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.utils.matcher_utils

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.utils.matcher_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.utils.matcher_with_name_node_map_utils

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.utils.matcher_with_name_node_map_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.passes.utils.source_matcher_utils

      <ul>
        <li><a href="fx.html#module-torch.fx.passes.utils.source_matcher_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.proxy

      <ul>
        <li><a href="fx.html#module-torch.fx.proxy">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.subgraph_rewriter

      <ul>
        <li><a href="fx.html#module-torch.fx.subgraph_rewriter">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.tensor_type

      <ul>
        <li><a href="fx.html#module-torch.fx.tensor_type">模块</a>
</li>
      </ul></li>
      <li>
    torch.fx.traceback

      <ul>
        <li><a href="fx.html#module-torch.fx.traceback">模块</a>
</li>
      </ul></li>
      <li>
    torch.hub

      <ul>
        <li><a href="hub.html#module-torch.hub">模块</a>
</li>
      </ul></li>
      <li><a href="type_info.html#torch.torch.iinfo">torch.iinfo（torch 类）</a>
</li>
      <li>
    torch.jit

      <ul>
        <li><a href="jit.html#module-torch.jit">模块</a>
</li>
      </ul></li>
      <li>
    torch.jit.annotations

      <ul>
        <li><a href="jit.html#module-torch.jit.annotations">模块</a>
</li>
      </ul></li>
      <li>
    torch.jit.frontend

      <ul>
        <li><a href="jit.html#module-torch.jit.frontend">模块</a>
</li>
      </ul></li>
      <li>
    torch.jit.generate_bytecode

      <ul>
        <li><a href="jit.html#module-torch.jit.generate_bytecode">模块</a>
</li>
      </ul></li>
      <li>
    torch.jit.mobile

      <ul>
        <li><a href="jit.html#module-torch.jit.mobile">模块</a>
</li>
      </ul></li>
      <li>
    torch.jit.quantized

      <ul>
        <li><a href="jit.html#module-torch.jit.quantized">模块</a>
</li>
      </ul></li>
      <li>
    torch.jit.supported_ops

      <ul>
        <li><a href="jit_builtin_functions.html#module-torch.jit.supported_ops">模块</a>
</li>
      </ul></li>
      <li>
    torch.jit.unsupported_tensor_ops

      <ul>
        <li><a href="jit_unsupported.html#module-torch.jit.unsupported_tensor_ops">模块</a>
</li>
      </ul></li>
      <li>
    torch.library

      <ul>
        <li><a href="library.html#module-torch.library">模块</a>
</li>
      </ul></li>
      <li>
    torch.linalg

      <ul>
        <li><a href="linalg.html#module-torch.linalg">模块</a>
</li>
      </ul></li>
      <li>
    torch.masked

      <ul>
        <li><a href="masked.html#module-torch.masked">模块</a>
</li>
      </ul></li>
      <li>
    torch.masked.maskedtensor

      <ul>
        <li><a href="masked.html#module-torch.masked.maskedtensor">模块</a>
</li>
      </ul></li>
      <li>
    torch.masked.maskedtensor.binary

      <ul>
        <li><a href="masked.html#module-torch.masked.maskedtensor.binary">模块</a>
</li>
      </ul></li>
      <li>
    torch.masked.maskedtensor.core

      <ul>
        <li><a href="masked.html#module-torch.masked.maskedtensor.core">模块</a>
</li>
      </ul></li>
      <li>
    torch.masked.maskedtensor.creation

      <ul>
        <li><a href="masked.html#module-torch.masked.maskedtensor.creation">模块</a>
</li>
      </ul></li>
      <li>
    torch.masked.maskedtensor.passthrough

      <ul>
        <li><a href="masked.html#module-torch.masked.maskedtensor.passthrough">模块</a>
</li>
      </ul></li>
      <li>
    torch.masked.maskedtensor.reductions

      <ul>
        <li><a href="masked.html#module-torch.masked.maskedtensor.reductions">模块</a>
</li>
      </ul></li>
      <li>
    torch.masked.maskedtensor.unary

      <ul>
        <li><a href="masked.html#module-torch.masked.maskedtensor.unary">模块</a>
</li>
      </ul></li>
      <li>
    torch.monitor

      <ul>
        <li><a href="monitor.html#module-torch.monitor">模块</a>
</li>
      </ul></li>
      <li>
    torch.mps

      <ul>
        <li><a href="mps.html#module-torch.mps">模块</a>
</li>
      </ul></li>
      <li>
    torch.mps.event

      <ul>
        <li><a href="mps.html#module-torch.mps.event">模块</a>
</li>
      </ul></li>
      <li>
    torch.mps.profiler

      <ul>
        <li><a href="mps.html#module-torch.mps.profiler">模块</a>
</li>
      </ul></li>
      <li>
    torch.mtia

      <ul>
        <li><a href="mtia.html#module-torch.mtia">模块</a>
</li>
      </ul></li>
      <li>
    torch.mtia.memory

      <ul>
        <li><a href="mtia.memory.html#module-torch.mtia.memory">模块</a>
</li>
      </ul></li>
      <li>
    torch.multiprocessing

      <ul>
        <li><a href="multiprocessing.html#module-torch.multiprocessing">模块</a>
</li>
      </ul></li>
      <li>
    torch.multiprocessing.pool

      <ul>
        <li><a href="multiprocessing.html#module-torch.multiprocessing.pool">模块</a>
</li>
      </ul></li>
      <li>
    torch.multiprocessing.queue

      <ul>
        <li><a href="multiprocessing.html#module-torch.multiprocessing.queue">模块</a>
</li>
      </ul></li>
      <li>
    torch.multiprocessing.reductions

      <ul>
        <li><a href="multiprocessing.html#module-torch.multiprocessing.reductions">模块</a>
</li>
      </ul></li>
      <li>
    torch.multiprocessing.spawn

      <ul>
        <li><a href="multiprocessing.html#module-torch.multiprocessing.spawn">模块</a>
</li>
      </ul></li>
      <li>
    torch.nested

      <ul>
        <li><a href="nested.html#module-torch.nested">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn

      <ul>
        <li><a href="nn.html#module-torch.nn">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.attention

      <ul>
        <li><a href="nn.attention.html#module-torch.nn.attention">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.attention.bias

      <ul>
        <li><a href="nn.attention.bias.html#module-torch.nn.attention.bias">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.attention.experimental

      <ul>
        <li><a href="nn.attention.experimental.html#module-torch.nn.attention.experimental">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.attention.flex_attention

      <ul>
        <li><a href="nn.attention.flex_attention.html#module-torch.nn.attention.flex_attention">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.backends

      <ul>
        <li><a href="nn.html#module-torch.nn.backends">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.backends.thnn

      <ul>
        <li><a href="nn.html#module-torch.nn.backends.thnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.common_types

      <ul>
        <li><a href="nn.html#module-torch.nn.common_types">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.cpp

      <ul>
        <li><a href="nn.html#module-torch.nn.cpp">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.functional

      <ul>
        <li><a href="nn.html#module-torch.nn.functional">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.grad

      <ul>
        <li><a href="nn.html#module-torch.nn.grad">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.init

      <ul>
        <li><a href="nn.html#module-torch.nn.init">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.modules.fused

      <ul>
        <li><a href="quantization.html#module-torch.nn.intrinsic.modules.fused">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.qat

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.qat">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.qat.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.qat.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.qat.modules.conv_fused

      <ul>
        <li><a href="quantization.html#module-torch.nn.intrinsic.qat.modules.conv_fused">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.qat.modules.linear_fused

      <ul>
        <li><a href="quantization.html#module-torch.nn.intrinsic.qat.modules.linear_fused">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.qat.modules.linear_relu

      <ul>
        <li><a href="quantization.html#module-torch.nn.intrinsic.qat.modules.linear_relu">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.quantized

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.quantized">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.quantized.dynamic

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.quantized.dynamic">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.quantized.dynamic.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.quantized.dynamic.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.quantized.dynamic.modules.linear_relu

      <ul>
        <li><a href="quantization.html#module-torch.nn.intrinsic.quantized.dynamic.modules.linear_relu">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.quantized.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.intrinsic.quantized.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.quantized.modules.bn_relu

      <ul>
        <li><a href="quantization.html#module-torch.nn.intrinsic.quantized.modules.bn_relu">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.quantized.modules.conv_relu

      <ul>
        <li><a href="quantization.html#module-torch.nn.intrinsic.quantized.modules.conv_relu">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.intrinsic.quantized.modules.linear_relu

      <ul>
        <li><a href="quantization.html#module-torch.nn.intrinsic.quantized.modules.linear_relu">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules

      <ul>
        <li><a href="nn.html#module-torch.nn.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.activation

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.activation">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.adaptive

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.adaptive">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.batchnorm

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.batchnorm">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.channelshuffle

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.channelshuffle">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.container

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.container">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.conv

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.conv">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.distance

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.distance">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.dropout

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.dropout">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.flatten

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.flatten">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.fold

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.fold">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.instancenorm

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.instancenorm">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.lazy

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.lazy">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.linear

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.linear">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.loss

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.loss">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.module

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.module">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.normalization

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.normalization">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.padding

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.padding">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.pixelshuffle

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.pixelshuffle">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.pooling

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.pooling">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.rnn

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.rnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.sparse

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.sparse">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.transformer

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.transformer">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.upsampling

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.upsampling">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.modules.utils

      <ul>
        <li><a href="nn.html#module-torch.nn.modules.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.parallel

      <ul>
        <li><a href="nn.html#module-torch.nn.parallel">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.parallel.comm

      <ul>
        <li><a href="nn.html#module-torch.nn.parallel.comm">模块</a>
</li>
      </ul></li>
      <li>torch.nn.parallel.distributed 分布式并行<ul>
        <li><a href="nn.html#module-torch.nn.parallel.distributed">模块</a>
</li>
      </ul></li>
      <li>torch.nn.parallel.parallel_apply 并行应用<ul>
        <li><a href="nn.html#module-torch.nn.parallel.parallel_apply">模块</a>
</li>
      </ul></li>
      <li>torch.nn.parallel.replicate 复制并行<ul>
        <li><a href="nn.html#module-torch.nn.parallel.replicate">模块</a>
</li>
      </ul></li>
      <li>torch.nn.parallel.scatter_gather 扩散收集<ul>
        <li><a href="nn.html#module-torch.nn.parallel.scatter_gather">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.parameter

      <ul>
        <li><a href="nn.html#module-torch.nn.parameter">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.qat

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.qat">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.qat.dynamic

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.qat.dynamic">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.qat.dynamic.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.qat.dynamic.modules">模块</a>
</li>
      </ul></li>
      <li>torch.nn.qat 动态模块线性<ul>
        <li><a href="quantization.html#module-torch.nn.qat.dynamic.modules.linear">模块</a>
</li>
      </ul></li>
      <li>torch.nn.qat 模块<ul>
        <li><a href="quantization-support.html#module-torch.nn.qat.modules">模块</a>
</li>
      </ul></li>
      <li>torch.nn.qat 模块卷积<ul>
        <li><a href="quantization.html#module-torch.nn.qat.modules.conv">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.qat.modules.embedding_ops

      <ul>
        <li><a href="quantization.html#module-torch.nn.qat.modules.embedding_ops">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.qat.modules.linear

      <ul>
        <li><a href="quantization.html#module-torch.nn.qat.modules.linear">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantizable

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.quantizable">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantizable.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.quantizable.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantizable.modules.activation

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantizable.modules.activation">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantizable.modules.rnn

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantizable.modules.rnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.quantized">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.dynamic

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.quantized.dynamic">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.dynamic.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.quantized.dynamic.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.dynamic.modules.conv

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.dynamic.modules.conv">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.dynamic.modules.linear

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.dynamic.modules.linear">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.dynamic.modules.rnn

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.dynamic.modules.rnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.functional

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.functional">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.modules

      <ul>
        <li><a href="quantization-support.html#module-torch.nn.quantized.modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.modules.activation

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.activation">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.modules.batchnorm

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.batchnorm">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.modules.conv

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.conv">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.modules.dropout

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.dropout">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.modules.embedding_ops

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.embedding_ops">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.modules.functional_modules

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.functional_modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.modules.linear

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.linear">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.modules.normalization

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.normalization">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.modules.rnn

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.rnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.quantized.modules.utils

      <ul>
        <li><a href="quantization.html#module-torch.nn.quantized.modules.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.utils

      <ul>
        <li><a href="nn.html#module-torch.nn.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.utils.clip_grad

      <ul>
        <li><a href="nn.html#module-torch.nn.utils.clip_grad">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.utils.convert_parameters

      <ul>
        <li><a href="nn.html#module-torch.nn.utils.convert_parameters">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.utils.fusion

      <ul>
        <li><a href="nn.html#module-torch.nn.utils.fusion">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.utils.init

      <ul>
        <li><a href="nn.html#module-torch.nn.utils.init">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.utils.memory_format

      <ul>
        <li><a href="nn.html#module-torch.nn.utils.memory_format">模块</a>
</li>
      </ul></li>
      <li><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">torch.nn.utils.parametrizations
torch.nn.utils 参数化</font></font></font><ul>
        <li><a href="nn.html#module-torch.nn.utils.parametrizations">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.utils.parametrize

      <ul>
        <li><a href="nn.html#module-torch.nn.utils.parametrize">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.utils.prune

      <ul>
        <li><a href="nn.html#module-torch.nn.utils.prune">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.utils.rnn

      <ul>
        <li><a href="nn.html#module-torch.nn.utils.rnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.nn.utils.stateless

      <ul>
        <li><a href="nn.html#module-torch.nn.utils.stateless">模块</a>
</li>
      </ul></li>
      <li>
    torch.onnx

      <ul>
        <li><a href="onnx_torchscript.html#module-torch.onnx">模块</a>
</li>
      </ul></li>
      <li>
    torch.onnx.errors

      <ul>
        <li><a href="onnx.html#module-torch.onnx.errors">模块</a>
</li>
      </ul></li>
      <li>
    torch.onnx.operators

      <ul>
        <li><a href="onnx.html#module-torch.onnx.operators">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号化 caffe2<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_caffe2">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号化助手<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_helper">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号操作集 10<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset10">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号操作集 11<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset11">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号操作集 12<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset12">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号算子集 13<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset13">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号算子集 14<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset14">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号算子集 15<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset15">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号算子集 16<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset16">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号算子集 17<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset17">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号算子集 18<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset18">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号算子集 19<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset19">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号操作集 20<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset20">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号操作集 7<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset7">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号操作集 8<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset8">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 符号操作集 9<ul>
        <li><a href="onnx.html#module-torch.onnx.symbolic_opset9">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 工具<ul>
        <li><a href="onnx.html#module-torch.onnx.utils">模块</a>
</li>
      </ul></li>
      <li>torch.onnx 验证<ul>
        <li><a href="onnx_verification.html#module-torch.onnx.verification">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim

      <ul>
        <li><a href="optim.html#module-torch.optim">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.adadelta

      <ul>
        <li><a href="optim.html#module-torch.optim.adadelta">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.adagrad

      <ul>
        <li><a href="optim.html#module-torch.optim.adagrad">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.adam

      <ul>
        <li><a href="optim.html#module-torch.optim.adam">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.adamax

      <ul>
        <li><a href="optim.html#module-torch.optim.adamax">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.adamw

      <ul>
        <li><a href="optim.html#module-torch.optim.adamw">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.asgd

      <ul>
        <li><a href="optim.html#module-torch.optim.asgd">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.lbfgs

      <ul>
        <li><a href="optim.html#module-torch.optim.lbfgs">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.lr_scheduler

      <ul>
        <li><a href="optim.html#module-torch.optim.lr_scheduler">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.nadam

      <ul>
        <li><a href="optim.html#module-torch.optim.nadam">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.optimizer

      <ul>
        <li><a href="optim.html#module-torch.optim.optimizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.radam

      <ul>
        <li><a href="optim.html#module-torch.optim.radam">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.rmsprop

      <ul>
        <li><a href="optim.html#module-torch.optim.rmsprop">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.rprop

      <ul>
        <li><a href="optim.html#module-torch.optim.rprop">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.sgd

      <ul>
        <li><a href="optim.html#module-torch.optim.sgd">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.sparse_adam

      <ul>
        <li><a href="optim.html#module-torch.optim.sparse_adam">模块</a>
</li>
      </ul></li>
      <li>
    torch.optim.swa_utils

      <ul>
        <li><a href="optim.html#module-torch.optim.swa_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.overrides

      <ul>
        <li><a href="torch.overrides.html#module-torch.overrides">模块</a>
</li>
      </ul></li>
      <li>
    torch.package

      <ul>
        <li><a href="package.html#module-torch.package">模块</a>
</li>
      </ul></li>
      <li>
    torch.package.analyze

      <ul>
        <li><a href="package.html#module-torch.package.analyze">模块</a>
</li>
      </ul></li>
      <li>
    torch.package.analyze.find_first_use_of_broken_modules

      <ul>
        <li><a href="package.html#module-torch.package.analyze.find_first_use_of_broken_modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.package.analyze.is_from_package

      <ul>
        <li><a href="package.html#module-torch.package.analyze.is_from_package">模块</a>
</li>
      </ul></li>
      <li>
    torch.package.analyze.trace_dependencies

      <ul>
        <li><a href="package.html#module-torch.package.analyze.trace_dependencies">模块</a>
</li>
      </ul></li>
      <li>
    torch.package.file_structure_representation

      <ul>
        <li><a href="package.html#module-torch.package.file_structure_representation">模块</a>
</li>
      </ul></li>
      <li>
    torch.package.find_file_dependencies

      <ul>
        <li><a href="package.html#module-torch.package.find_file_dependencies">模块</a>
</li>
      </ul></li>
      <li>
    torch.package.glob_group

      <ul>
        <li><a href="package.html#module-torch.package.glob_group">模块</a>
</li>
      </ul></li>
      <li>
    torch.package.importer

      <ul>
        <li><a href="package.html#module-torch.package.importer">模块</a>
</li>
      </ul></li>
      <li>
    torch.package.package_exporter

      <ul>
        <li><a href="package.html#module-torch.package.package_exporter">模块</a>
</li>
      </ul></li>
      <li>
    torch.package.package_importer

      <ul>
        <li><a href="package.html#module-torch.package.package_importer">模块</a>
</li>
      </ul></li>
      <li>
    torch.profiler

      <ul>
        <li><a href="profiler.html#module-torch.profiler">模块</a>
</li>
      </ul></li>
      <li>
    torch.profiler.itt

      <ul>
        <li><a href="profiler.html#module-torch.profiler.itt">模块</a>
</li>
      </ul></li>
      <li>
    torch.profiler.profiler

      <ul>
        <li><a href="profiler.html#module-torch.profiler.profiler">模块</a>
</li>
      </ul></li>
      <li>
    torch.profiler.python_tracer

      <ul>
        <li><a href="profiler.html#module-torch.profiler.python_tracer">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization

      <ul>
        <li><a href="quantization-support.html#module-torch.quantization">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.fake_quantize

      <ul>
        <li><a href="quantization.html#module-torch.quantization.fake_quantize">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.fuse_modules

      <ul>
        <li><a href="quantization.html#module-torch.quantization.fuse_modules">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.fuser_method_mappings

      <ul>
        <li><a href="quantization.html#module-torch.quantization.fuser_method_mappings">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.fx

      <ul>
        <li><a href="quantization-support.html#module-torch.quantization.fx">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.fx.convert

      <ul>
        <li><a href="quantization.html#module-torch.quantization.fx.convert">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.fx.fuse

      <ul>
        <li><a href="quantization.html#module-torch.quantization.fx.fuse">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.fx.fusion_patterns

      <ul>
        <li><a href="quantization.html#module-torch.quantization.fx.fusion_patterns">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.fx.graph_module

      <ul>
        <li><a href="quantization.html#module-torch.quantization.fx.graph_module">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.fx.match_utils

      <ul>
        <li><a href="quantization.html#module-torch.quantization.fx.match_utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.fx.pattern_utils

      <ul>
        <li><a href="quantization.html#module-torch.quantization.fx.pattern_utils">模块</a>
</li>
      </ul></li>
      <li>torch.quantization.fx 准备<ul>
        <li><a href="quantization.html#module-torch.quantization.fx.prepare">模块</a>
</li>
      </ul></li>
      <li>torch.quantization.fx 量化模式<ul>
        <li><a href="quantization.html#module-torch.quantization.fx.quantization_patterns">模块</a>
</li>
      </ul></li>
      <li>torch.quantization.fx 量化类型<ul>
        <li><a href="quantization.html#module-torch.quantization.fx.quantization_types">模块</a>
</li>
      </ul></li>
      <li>torch.quantization.fx 工具<ul>
        <li><a href="quantization.html#module-torch.quantization.fx.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.observer

      <ul>
        <li><a href="quantization.html#module-torch.quantization.observer">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.qconfig

      <ul>
        <li><a href="quantization.html#module-torch.quantization.qconfig">模块</a>
</li>
      </ul></li>
      <li>torch 量化量化类型<ul>
        <li><a href="quantization.html#module-torch.quantization.quant_type">模块</a>
</li>
      </ul></li>
      <li>torch 量化量化映射<ul>
        <li><a href="quantization.html#module-torch.quantization.quantization_mappings">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.quantize

      <ul>
        <li><a href="quantization.html#module-torch.quantization.quantize">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.quantize_fx

      <ul>
        <li><a href="quantization.html#module-torch.quantization.quantize_fx">模块</a>
</li>
      </ul></li>
      <li>
    torch.quantization.quantize_jit

      <ul>
        <li><a href="quantization.html#module-torch.quantization.quantize_jit">模块</a>
</li>
      </ul></li>
      <li>torch 量化存根<ul>
        <li><a href="quantization.html#module-torch.quantization.stubs">模块</a>
</li>
      </ul></li>
      <li>torch 量化工具<ul>
        <li><a href="quantization.html#module-torch.quantization.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.quasirandom

      <ul>
        <li><a href="torch.html#module-torch.quasirandom">模块</a>
</li>
      </ul></li>
      <li>
    torch.random

      <ul>
        <li><a href="random.html#module-torch.random">模块</a>
</li>
      </ul></li>
      <li>
    torch.return_types

      <ul>
        <li><a href="torch.html#module-torch.return_types">模块</a>
</li>
      </ul></li>
      <li>
    torch.serialization

      <ul>
        <li><a href="torch.html#module-torch.serialization">模块</a>
</li>
      </ul></li>
      <li>
    torch.signal

      <ul>
        <li><a href="signal.html#module-torch.signal">模块</a>
</li>
      </ul></li>
      <li>
    torch.signal.windows

      <ul>
        <li><a href="signal.html#module-torch.signal.windows">模块</a>
</li>
      </ul></li>
      <li>
    torch.signal.windows.windows

      <ul>
        <li><a href="torch.html#module-torch.signal.windows.windows">模块</a>
</li>
      </ul></li>
      <li>
    torch.sparse

      <ul>
        <li><a href="sparse.html#module-torch.sparse">模块</a>
</li>
      </ul></li>
      <li>
    torch.sparse.semi_structured

      <ul>
        <li><a href="torch.html#module-torch.sparse.semi_structured">模块</a>
</li>
      </ul></li>
      <li>
    torch.special

      <ul>
        <li><a href="special.html#module-torch.special">模块</a>
</li>
      </ul></li>
      <li>
    torch.storage

      <ul>
        <li><a href="torch.html#module-torch.storage">模块</a>
</li>
      </ul></li>
      <li>
    torch.testing

      <ul>
        <li><a href="testing.html#module-torch.testing">模块</a>
</li>
      </ul></li>
      <li>torch 版本<ul>
        <li><a href="torch.html#module-torch.torch_version">模块</a>
</li>
      </ul></li>
      <li>torch 类型<ul>
        <li><a href="torch.html#module-torch.types">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils

      <ul>
        <li><a href="utils.html#module-torch.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.backcompat

      <ul>
        <li><a href="torch.html#module-torch.utils.backcompat">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.backend_registration

      <ul>
        <li><a href="utils.html#module-torch.utils.backend_registration">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark

      <ul>
        <li><a href="benchmark_utils.html#module-torch.utils.benchmark">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.examples

      <ul>
        <li><a href="benchmark_utils.html#module-torch.utils.benchmark.examples">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.examples.blas_compare_setup

      <ul>
        <li><a href="utils.html#module-torch.utils.benchmark.examples.blas_compare_setup">模块</a>
</li>
      </ul></li>
      <li>torch.utils.benchmark.example 比较<ul>
        <li><a href="utils.html#module-torch.utils.benchmark.examples.compare">模块</a>
</li>
      </ul></li>
      <li>torch.utils.benchmark.example.fuzzer<ul>
        <li><a href="utils.html#module-torch.utils.benchmark.examples.fuzzer">模块</a>
</li>
      </ul></li>
      <li>torch.utils.benchmark 示例.op_benchmark<ul>
        <li><a href="utils.html#module-torch.utils.benchmark.examples.op_benchmark">模块</a>
</li>
      </ul></li>
      <li>torch.utils.benchmark 示例.simple_timeit<ul>
        <li><a href="utils.html#module-torch.utils.benchmark.examples.simple_timeit">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.examples.spectral_ops_fuzz_test

      <ul>
        <li><a href="utils.html#module-torch.utils.benchmark.examples.spectral_ops_fuzz_test">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.op_fuzzers

      <ul>
        <li><a href="benchmark_utils.html#module-torch.utils.benchmark.op_fuzzers">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.op_fuzzers.binary

      <ul>
        <li><a href="utils.html#module-torch.utils.benchmark.op_fuzzers.binary">模块</a>
</li>
      </ul></li>
      <li>torch.utils.benchmark 操作模糊器.sparse_binary<ul>
        <li><a href="utils.html#module-torch.utils.benchmark.op_fuzzers.sparse_binary">模块</a>
</li>
      </ul></li>
      <li>torch.utils.benchmark 操作模糊器.sparse_unary<ul>
        <li><a href="utils.html#module-torch.utils.benchmark.op_fuzzers.sparse_unary">模块</a>
</li>
      </ul></li>
      <li><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">torch.utils.benchmark.op_fuzzers.spectral

torch.utils.benchmark.op_fuzzers.频谱</font></font></font><ul>
        <li><a href="utils.html#module-torch.utils.benchmark.op_fuzzers.spectral">模块</a>
</li>
      </ul></li>
      <li>torch.utils.benchmark.op_fuzzers.一元操作模糊器<ul>
        <li><a href="utils.html#module-torch.utils.benchmark.op_fuzzers.unary">模块</a>
</li>
      </ul></li>
      <li>torch.utils.benchmark.utils 工具<ul>
        <li><a href="benchmark_utils.html#module-torch.utils.benchmark.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.utils.common

      <ul>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.common">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.utils.compare

      <ul>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.compare">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.utils.compile

      <ul>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.compile">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.utils.cpp_jit

      <ul>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.cpp_jit">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.utils.fuzzer

      <ul>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.fuzzer">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.utils.sparse_fuzzer

      <ul>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.sparse_fuzzer">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.utils.timer

      <ul>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.timer">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.utils.valgrind_wrapper

      <ul>
        <li><a href="benchmark_utils.html#module-torch.utils.benchmark.utils.valgrind_wrapper">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.benchmark.utils.valgrind_wrapper.timer_interface

      <ul>
        <li><a href="utils.html#module-torch.utils.benchmark.utils.valgrind_wrapper.timer_interface">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.bottleneck

      <ul>
        <li><a href="bottleneck.html#module-torch.utils.bottleneck">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.bundled_inputs

      <ul>
        <li><a href="utils.html#module-torch.utils.bundled_inputs">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.checkpoint

      <ul>
        <li><a href="utils.html#module-torch.utils.checkpoint">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.collect_env

      <ul>
        <li><a href="utils.html#module-torch.utils.collect_env">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.cpp_backtrace

      <ul>
        <li><a href="utils.html#module-torch.utils.cpp_backtrace">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.cpp_extension

      <ul>
        <li><a href="utils.html#module-torch.utils.cpp_extension">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data

      <ul>
        <li><a href="data.html#module-torch.utils.data">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.backward_compatibility

      <ul>
        <li><a href="utils.html#module-torch.utils.data.backward_compatibility">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.dataloader

      <ul>
        <li><a href="utils.html#module-torch.utils.data.dataloader">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes

      <ul>
        <li><a href="data.html#module-torch.utils.data.datapipes">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.dataframe

      <ul>
        <li><a href="data.html#module-torch.utils.data.datapipes.dataframe">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.dataframe.dataframe_wrapper

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.dataframe.dataframe_wrapper">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.dataframe.dataframes

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.dataframe.dataframes">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.dataframe.datapipes

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.dataframe.datapipes">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.dataframe.structures

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.dataframe.structures">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.datapipe

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.datapipe">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.gen_pyi

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.gen_pyi">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.iter

      <ul>
        <li><a href="data.html#module-torch.utils.data.datapipes.iter">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.iter.callable

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.callable">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.iter.combinatorics

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.combinatorics">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.iter.combining

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.combining">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.iter.filelister

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.filelister">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.iter.fileopener

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.fileopener">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.iter.grouping

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.grouping">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.iter.routeddecoder

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.routeddecoder">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.iter.selecting

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.selecting">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.iter.sharding

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.sharding">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.iter.streamreader

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.streamreader">模块</a>
</li>
      </ul></li>
      <li><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">torch.utils.data.datapipes.iter.utils
torch.utils.data.datapipes.iter.utils</font></font></font><ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.iter.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.map

      <ul>
        <li><a href="data.html#module-torch.utils.data.datapipes.map">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.map.callable

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.map.callable">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.map.combinatorics

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.map.combinatorics">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.map.combining

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.map.combining">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.map.grouping

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.map.grouping">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.map.utils

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.map.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.utils

      <ul>
        <li><a href="data.html#module-torch.utils.data.datapipes.utils">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.datapipes.utils.common

      <ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.utils.common">模块</a>
</li>
      </ul></li>
      <li>torch.utils.data.datapipes.utils 解码器<ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.utils.decoder">模块</a>
</li>
      </ul></li>
      <li>torch.utils.data.datapipes.utils 快照<ul>
        <li><a href="utils.html#module-torch.utils.data.datapipes.utils.snapshot">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.dataset

      <ul>
        <li><a href="utils.html#module-torch.utils.data.dataset">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.distributed

      <ul>
        <li><a href="utils.html#module-torch.utils.data.distributed">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.graph

      <ul>
        <li><a href="utils.html#module-torch.utils.data.graph">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.graph_settings

      <ul>
        <li><a href="utils.html#module-torch.utils.data.graph_settings">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.data.sampler

      <ul>
        <li><a href="utils.html#module-torch.utils.data.sampler">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.deterministic

      <ul>
        <li><a href="deterministic.html#module-torch.utils.deterministic">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.dlpack

      <ul>
        <li><a href="utils.html#module-torch.utils.dlpack">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.file_baton

      <ul>
        <li><a href="utils.html#module-torch.utils.file_baton">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.flop_counter

      <ul>
        <li><a href="utils.html#module-torch.utils.flop_counter">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.hipify

      <ul>
        <li><a href="torch.html#module-torch.utils.hipify">模块</a>
</li>
      </ul></li>
      <li>torch.utils.hipify.常量<ul>
        <li><a href="utils.html#module-torch.utils.hipify.constants">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.hipify.cuda_to_hip_mappings

      <ul>
        <li><a href="utils.html#module-torch.utils.hipify.cuda_to_hip_mappings">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.hipify.hipify_python

      <ul>
        <li><a href="utils.html#module-torch.utils.hipify.hipify_python">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.hipify.version

      <ul>
        <li><a href="utils.html#module-torch.utils.hipify.version">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.hooks

      <ul>
        <li><a href="utils.html#module-torch.utils.hooks">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.jit

      <ul>
        <li><a href="jit_utils.html#module-torch.utils.jit">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.jit.log_extract

      <ul>
        <li><a href="utils.html#module-torch.utils.jit.log_extract">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.mkldnn

      <ul>
        <li><a href="utils.html#module-torch.utils.mkldnn">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.mobile_optimizer

      <ul>
        <li><a href="utils.html#module-torch.utils.mobile_optimizer">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.model_dump

      <ul>
        <li><a href="torch.html#module-torch.utils.model_dump">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.model_zoo

      <ul>
        <li><a href="model_zoo.html#module-torch.utils.model_zoo">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.module_tracker

      <ul>
        <li><a href="module_tracker.html#module-torch.utils.module_tracker">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.serialization

      <ul>
        <li><a href="notes/serialization.html#module-torch.utils.serialization">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.serialization.config

      <ul>
        <li><a href="notes/serialization.html#module-torch.utils.serialization.config">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.show_pickle

      <ul>
        <li><a href="utils.html#module-torch.utils.show_pickle">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.tensorboard

      <ul>
        <li><a href="tensorboard.html#module-torch.utils.tensorboard">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.tensorboard.summary

      <ul>
        <li><a href="utils.html#module-torch.utils.tensorboard.summary">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.tensorboard.writer

      <ul>
        <li><a href="utils.html#module-torch.utils.tensorboard.writer">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.throughput_benchmark

      <ul>
        <li><a href="utils.html#module-torch.utils.throughput_benchmark">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.viz

      <ul>
        <li><a href="torch.html#module-torch.utils.viz">模块</a>
</li>
      </ul></li>
      <li>
    torch.utils.weak

      <ul>
        <li><a href="utils.html#module-torch.utils.weak">模块</a>
</li>
      </ul></li>
      <li>torch 版本<ul>
        <li><a href="torch.html#module-torch.version">模块</a>
</li>
      </ul></li>
      <li>
    torch.xpu

      <ul>
        <li><a href="xpu.html#module-torch.xpu">模块</a>
</li>
      </ul></li>
      <li>
    torch.xpu.memory

      <ul>
        <li><a href="xpu.html#module-torch.xpu.memory">模块</a>
</li>
      </ul></li>
      <li>
    torch.xpu.random

      <ul>
        <li><a href="xpu.html#module-torch.xpu.random">模块</a>
</li>
      </ul></li>
      <li>
    torch.xpu.streams

      <ul>
        <li><a href="xpu.html#module-torch.xpu.streams">模块</a>
</li>
      </ul></li>
      <li><a href="torch.compiler.config.html#index-0">TORCH_COMPILE_JOB_ID</a>
</li>
      <li><a href="generated/torch.onnx.JitScalarType.html#torch.onnx.JitScalarType.torch_name">torch_name() (torch.onnx.JitScalarType 方法)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.torch_save_to_dcp">torch_save_to_dcp() (在模块 torch.distributed.checkpoint.format_utils 中)</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.TorchAODType.html#torch.ao.quantization.observer.TorchAODType">TorchAODType (torch.ao.quantization.observer 类)</a>
</li>
      <li><a href="generated/torch.autograd.profiler.profile.total_average.html#torch.autograd.profiler.profile.total_average">total_average() (torch.autograd.profiler.profile 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.total_count">total_count (torch.distributions.multinomial.Multinomial 属性)</a>
</li>
      <li><a href="generated/torch.trace.html#torch.trace">trace() (在模块 torch 中)</a>

      <ul>
        <li><a href="generated/torch.jit.trace.html#torch.jit.trace">(在 torch.jit 模块中)</a>
</li>
        <li><a href="fx.html#torch.fx.Tracer.trace">(torch.fx.Tracer 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.trace.html#torch.Tensor.trace">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.trace_module.html#torch.jit.trace_module">trace_module() (在模块 torch.jit 中)</a>
</li>
      <li><a href="fx.html#torch.fx.Tracer">Tracer (torch.fx 中的类)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.train">train() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.train">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.train">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.transforms.Transform">Transform (torch.distributions.transforms 中的类)</a>
</li>
      <li><a href="fx.html#torch.fx.Transformer.transform">transform() (torch.fx.Transformer 方法)</a>

      <ul>
        <li><a href="benchmark_utils.html#torch.utils.benchmark.FunctionCounts.transform">(torch.utils.benchmark.FunctionCounts 方法)</a>
</li>
      </ul></li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.DefaultSavePlanner.transform_object">transform_object() (torch.distributed.checkpoint.DefaultSavePlanner 方法)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.DefaultLoadPlanner.transform_tensor">transform_tensor() (torch.distributed.checkpoint.DefaultLoadPlanner 方法)</a>
</li>
      <li><a href="distributions.html#torch.distributions.transformed_distribution.TransformedDistribution">torch.distributions.transformed_distribution 中的 TransformedDistribution 类</a>
</li>
      <li><a href="fx.html#torch.fx.Transformer">torch.fx 中的 Transformer 类</a>

      <ul>
        <li><a href="generated/torch.nn.Transformer.html#torch.nn.Transformer">（torch.nn 中的类）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder">torch.nn 中的 TransformerDecoder 类</a>
</li>
      <li><a href="generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer">torch.nn 中的 TransformerDecoderLayer 类</a>
</li>
      <li><a href="generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder">TransformerEncoder (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer">TransformerEncoderLayer (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.transpose.html#torch.transpose">transpose() (torch 模块中的方法)</a>

      <ul>
        <li><a href="generated/torch.Tensor.transpose.html#torch.Tensor.transpose">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_">transpose_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.trapezoid.html#torch.trapezoid">torch 模块中的 trapezoid()</a>
</li>
      <li><a href="generated/torch.trapz.html#torch.trapz">torch 模块中的 trapz()</a>
</li>
      <li><a href="generated/torch.triangular_solve.html#torch.triangular_solve">torch 模块中的 triangular_solve()</a>

      <ul>
        <li><a href="generated/torch.Tensor.triangular_solve.html#torch.Tensor.triangular_solve">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.tril.html#torch.tril">torch 模块中的 tril()</a>

      <ul>
        <li><a href="generated/torch.Tensor.tril.html#torch.Tensor.tril">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.tril_.html#torch.Tensor.tril_">tril_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.tril_indices.html#torch.tril_indices">tril_indices() (在模块 torch 中)</a>
</li>
      <li><a href="benchmark_utils.html#torch.utils.benchmark.Compare.trim_significant_figures">trim_significant_figures() (torch.utils.benchmark.Compare 方法)</a>
</li>
      <li><a href="generated/torch.nn.functional.triplet_margin_loss.html#torch.nn.functional.triplet_margin_loss">triplet_margin_loss() (在模块 torch.nn.functional 中)</a>
</li>
      <li><a href="generated/torch.nn.functional.triplet_margin_with_distance_loss.html#torch.nn.functional.triplet_margin_with_distance_loss">triplet_margin_with_distance_loss() (在模块 torch.nn.functional 中)</a>
</li>
      <li><a href="generated/torch.nn.TripletMarginLoss.html#torch.nn.TripletMarginLoss">TripletMarginLoss (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss">TripletMarginWithDistanceLoss (torch.nn 中的类)</a>
</li>
      <li><a href="library.html#torch.library.triton_op">triton_op() (在模块 torch.library 中)</a>
</li>
      <li><a href="generated/torch.triu.html#torch.triu">triu()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.triu.html#torch.Tensor.triu">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.triu_.html#torch.Tensor.triu_">triu_()（torch.Tensor 方法）</a>
</li>
      <li><a href="generated/torch.triu_indices.html#torch.triu_indices">triu_indices()（在 torch 模块中）</a>
</li>
      <li><a href="generated/torch.true_divide.html#torch.true_divide">true_divide()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.true_divide.html#torch.Tensor.true_divide">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_">true_divide_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.trunc.html#torch.trunc">trunc() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.trunc.html#torch.Tensor.trunc">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.trunc_.html#torch.Tensor.trunc_">trunc_() (torch.Tensor 方法)</a>
</li>
      <li><a href="nn.init.html#torch.nn.init.trunc_normal_">trunc_normal_() (在 torch.nn.init 模块中)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.tune_gemm_in_file">tune_gemm_in_file() (在模块 torch.cuda.tunable 中)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.tuning_enable">tuning_enable() (在模块 torch.cuda.tunable 中)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.tuning_is_enabled">tuning_is_enabled() (在模块 torch.cuda.tunable 中)</a>
</li>
      <li><a href="generated/torch.jit.Attribute.html#torch.jit.Attribute.type">type (torch.jit.Attribute 属性)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.type">type() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.type">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.type">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.type.html#torch.Tensor.type">torch.Tensor 方法</a>
</li>
        <li><a href="storage.html#torch.TypedStorage.type">(torch.TypedStorage 方法)</a>
</li>
        <li><a href="storage.html#torch.UntypedStorage.type">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.type_as.html#torch.Tensor.type_as">type_as() (torch.Tensor 方法)</a>
</li>
      <li><a href="storage.html#torch.TypedStorage">TypedStorage (torch 中的类)</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="U">U</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.unbind.html#torch.unbind">unbind() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.unbind.html#torch.Tensor.unbind">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributed.html#torch.distributed.Work.unbox">torch.distributed.Work 静态方法 unbox()</a>
</li>
      <li><a href="distributed.html#torch.distributed.PrefixStore.underlying_store">torch.distributed.PrefixStore 属性 underlying_store</a>
</li>
      <li><a href="generated/torch.nn.Unflatten.html#torch.nn.Unflatten">torch.nn 中的 Unflatten 类</a>
</li>
      <li><a href="generated/torch.unflatten.html#torch.unflatten">torch 模块中的 unflatten()函数</a>

      <ul>
        <li><a href="export.html#torch.export.unflatten.unflatten">(在 torch.export.unflatten 模块中)</a>
</li>
        <li><a href="generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.Unfold.html#torch.nn.Unfold">Unfold（torch.nn 中的类）</a>
</li>
      <li><a href="generated/torch.nn.functional.unfold.html#torch.nn.functional.unfold">unfold()（在 torch.nn.functional 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.unfold.html#torch.Tensor.unfold">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.uniform.Uniform">Uniform（torch.distributions.uniform 中的类）</a>
</li>
      <li><a href="nn.init.html#torch.nn.init.uniform_">torch.nn.init 模块中的 uniform_()</a>

      <ul>
        <li><a href="generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.nn.parameter.UninitializedBuffer.html#torch.nn.parameter.UninitializedBuffer">torch.nn.parameter 中的 UninitializedBuffer 类</a>
</li>
      <li><a href="generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter">torch.nn.parameter 中的 UninitializedParameter 类</a>
</li>
      <li><a href="generated/torch.unique.html#torch.unique">torch 模块中的 unique()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.unique.html#torch.Tensor.unique">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.unique_consecutive.html#torch.unique_consecutive">unique_consecutive() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual">unpack_dual() (在模块 torch.autograd.forward_ad 中)</a>
</li>
      <li><a href="generated/torch.nn.utils.rnn.unpack_sequence.html#torch.nn.utils.rnn.unpack_sequence">unpack_sequence() (在模块 torch.nn.utils.rnn 中)</a>
</li>
      <li><a href="generated/torch.autograd.forward_ad.UnpackedDualTensor.html#torch.autograd.forward_ad.UnpackedDualTensor">UnpackedDualTensor (类，位于 torch.autograd.forward_ad)</a>
</li>
      <li><a href="generated/torch.nn.utils.rnn.unpad_sequence.html#torch.nn.utils.rnn.unpad_sequence">unpad_sequence() (在模块 torch.nn.utils.rnn 中)</a>
</li>
      <li><a href="generated/torch.unravel_index.html#torch.unravel_index">unravel_index()（在 torch 模块中）</a>
</li>
      <li><a href="onnx_torchscript.html#torch.onnx.unregister_custom_op_symbolic">unregister_custom_op_symbolic()（在 torch.onnx 模块中）</a>
</li>
      <li><a href="monitor.html#torch.monitor.unregister_event_handler">unregister_event_handler()（在 torch.monitor 模块中）</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.unshard">unshard()（torch.distributed.fsdp.FSDPModule 方法）</a>
</li>
      <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.UnshardHandle">torch.distributed.fsdp 中的 UnshardHandle（类）</a>
</li>
      <li><a href="generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence.unsorted_indices">torch.nn.utils.rnn.PackedSequence 属性中的 unsorted_indices</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.unsqueeze.html#torch.unsqueeze">torch 模块中的 unsqueeze()函数</a>

      <ul>
        <li><a href="generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.unsqueeze_.html#torch.Tensor.unsqueeze_">torch.Tensor 方法中的 unsqueeze_()</a>
</li>
      <li><a href="storage.html#torch.TypedStorage.untyped">untyped() (torch.TypedStorage 方法)</a>

      <ul>
        <li><a href="storage.html#torch.UntypedStorage.untyped">(torch.UntypedStorage 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.Tensor.untyped_storage.html#torch.Tensor.untyped_storage">untyped_storage() (torch.Tensor 方法)</a>
</li>
      <li><a href="storage.html#torch.UntypedStorage">UntypedStorage (torch 中的类)</a>
</li>
      <li><a href="generated/torch.jit.unused.html#torch.jit.unused">unused() (在 torch.jit 模块中)</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.update">update() (torch.autograd.profiler_util.StringTable 方法)</a>

      <ul>
        <li><a href="export.html#torch.export.decomp_utils.CustomDecompTable.update">(torch.export.decomp_utils.CustomDecompTable 方法)</a>
</li>
        <li><a href="generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.update">(torch.nn.ModuleDict 方法)</a>
</li>
        <li><a href="generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.update">(torch.nn.ParameterDict 方法)</a>
</li>
      </ul></li>
      <li><a href="fx.html#torch.fx.Node.update_arg">update_arg() (torch.fx.Node 方法)</a>
</li>
      <li><a href="optim.html#torch.optim.swa_utils.update_bn">update_bn() (在模块 torch.optim.swa_utils 中)</a>
</li>
      <li><a href="generated/torch.ao.nn.intrinsic.qat.update_bn_stats.html#torch.ao.nn.intrinsic.qat.update_bn_stats">update_bn_stats (torch.ao.nn.intrinsic.qat 类)</a>
</li>
      <li><a href="fx.html#torch.fx.Node.update_kwarg">update_kwarg() (torch.fx.Node 方法)</a>
</li>
      <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.update_parameters">update_parameters() (torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.upsample.html#torch.ao.nn.quantized.functional.upsample">upsample (torch.ao.nn.quantized.functional 中的类)</a>
</li>
      <li><a href="generated/torch.nn.Upsample.html#torch.nn.Upsample">Upsample (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.functional.upsample.html#torch.nn.functional.upsample">torch.nn.functional 模块中的 upsample()函数</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.upsample_bilinear.html#torch.ao.nn.quantized.functional.upsample_bilinear">torch.ao.nn.quantized.functional 类中的 upsample_bilinear</a>
</li>
      <li><a href="generated/torch.nn.functional.upsample_bilinear.html#torch.nn.functional.upsample_bilinear">torch.nn.functional 模块中的 upsample_bilinear()函数</a>
</li>
      <li><a href="generated/torch.ao.nn.quantized.functional.upsample_nearest.html#torch.ao.nn.quantized.functional.upsample_nearest">torch.ao.nn.quantized.functional 类中的 upsample_nearest</a>
</li>
      <li><a href="generated/torch.nn.functional.upsample_nearest.html#torch.nn.functional.upsample_nearest">torch.nn.functional 模块中的 upsample_nearest()</a>
</li>
      <li><a href="generated/torch.nn.UpsamplingBilinear2d.html#torch.nn.UpsamplingBilinear2d">torch.nn 中的 UpsamplingBilinear2d 类</a>
</li>
      <li><a href="generated/torch.nn.UpsamplingNearest2d.html#torch.nn.UpsamplingNearest2d">torch.nn 中的 UpsamplingNearest2d 类</a>
</li>
      <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.RendezvousHandler.use_agent_store">torch.distributed.elastic.rendezvous.RendezvousHandler 属性中的 use_agent_store</a>
</li>
      <li><a href="generated/torch.cuda.MemPool.html#torch.cuda.MemPool.use_count">torch.cuda.MemPool 方法 use_count()</a>
</li>
      <li><a href="generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms">torch 模块中的 use_deterministic_algorithms()</a>
</li>
      <li><a href="cuda.html#torch.cuda.use_mem_pool">torch.cuda 中的 use_mem_pool 类</a>
</li>
      <li><a href="generated/torch.cuda.utilization.html#torch.cuda.utilization">torch.cuda 模块中的 utilization()</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="V">V</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader.validate_checkpoint_id">验证检查点 ID() (torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader 类方法)</a>

      <ul>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageReader.validate_checkpoint_id">(torch.distributed.checkpoint.StorageReader 类方法)</a>
</li>
        <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.validate_checkpoint_id">(torch.distributed.checkpoint.StorageWriter 类方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.jit.Attribute.html#torch.jit.Attribute.value">value (torch.jit.Attribute 属性)</a>
</li>
      <li><a href="futures.html#torch.futures.Future.value">value() (torch.futures.Future 方法)</a>
</li>
      <li><a href="generated/torch.autograd.profiler_util.StringTable.html#torch.autograd.profiler_util.StringTable.values">values() (torch.autograd.profiler_util.StringTable 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict.values">(torch.nn.ModuleDict 方法)</a>
</li>
        <li><a href="generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict.values">(torch.nn.ParameterDict 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.values.html#torch.Tensor.values">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.vander.html#torch.vander">vander() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.linalg.vander.html#torch.linalg.vander">(在 torch.linalg 模块中)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.var.html#torch.var">var() (在 torch 模块中)</a>

      <ul>
        <li><a href="generated/torch.Tensor.var.html#torch.Tensor.var">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.var_mean.html#torch.var_mean">var_mean() (在 torch 模块中)</a>
</li>
      <li><a href="distributions.html#torch.distributions.bernoulli.Bernoulli.variance">伯努利方差（torch.distributions.bernoulli.Bernoulli 属性）</a>

      <ul>
        <li><a href="distributions.html#torch.distributions.beta.Beta.variance">(torch.distributions.beta.Beta 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.binomial.Binomial.variance">(torch.distributions.binomial.Binomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.categorical.Categorical.variance">(torch.distributions.categorical.Categorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.cauchy.Cauchy.variance">(torch.distributions.cauchy.Cauchy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance">(torch.distributions.continuous_bernoulli.ContinuousBernoulli 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.dirichlet.Dirichlet.variance">(torch.distributions.dirichlet.Dirichlet 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.distribution.Distribution.variance">(torch.distributions.distribution.Distribution 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.exponential.Exponential.variance">(torch.distributions.exponential.Exponential 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.fishersnedecor.FisherSnedecor.variance">(torch.distributions.fishersnedecor.FisherSnedecor 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gamma.Gamma.variance">(torch.distributions.gamma.Gamma 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.geometric.Geometric.variance">(torch.distributions.geometric.Geoemtric 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.gumbel.Gumbel.variance">(torch.distributions.gumbel.Gumbel 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_cauchy.HalfCauchy.variance">(torch.distributions.half_cauchy.HalfCauchy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.half_normal.HalfNormal.variance">(torch.distributions.half_normal.HalfNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.independent.Independent.variance">(torch.distributions.independent.Independent 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.inverse_gamma.InverseGamma.variance">(torch.distributions.inverse_gamma.InverseGamma 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.kumaraswamy.Kumaraswamy.variance">(torch.distributions.kumaraswamy.Kumaraswamy 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.laplace.Laplace.variance">(torch.distributions.laplace.Laplace 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.log_normal.LogNormal.variance">(torch.distributions.log_normal.LogNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance">(torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.mixture_same_family.MixtureSameFamily.variance">(torch.distributions.mixture_same_family.MixtureSameFamily 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multinomial.Multinomial.variance">(torch.distributions.multinomial.Multinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.multivariate_normal.MultivariateNormal.variance">(torch.distributions.multivariate_normal.MultivariateNormal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.negative_binomial.NegativeBinomial.variance">(torch.distributions.negative_binomial.NegativeBinomial 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.normal.Normal.variance">(torch.distributions.normal.Normal 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.one_hot_categorical.OneHotCategorical.variance">(torch.distributions.one_hot_categorical.OneHotCategorical 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.pareto.Pareto.variance">(torch.distributions.pareto.Pareto 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.poisson.Poisson.variance">(torch.distributions.poisson.Poisson 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.studentT.StudentT.variance">(torch.distributions.studentT.StudentT 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.uniform.Uniform.variance">(torch.distributions.uniform.Uniform 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.von_mises.VonMises.variance">(torch.distributions.von_mises.VonMises 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.weibull.Weibull.variance">(torch.distributions.weibull.Weibull 属性)</a>
</li>
        <li><a href="distributions.html#torch.distributions.wishart.Wishart.variance">(torch.distributions.wishart.Wishart 属性)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.vdot.html#torch.vdot">vdot()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.Tensor.vdot.html#torch.Tensor.vdot">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.linalg.vecdot.html#torch.linalg.vecdot">vecdot()（在 torch.linalg 模块中）</a>
</li>
      <li><a href="generated/torch.linalg.vector_norm.html#torch.linalg.vector_norm">vector_norm()（在 torch.linalg 模块中）</a>
</li>
      <li><a href="generated/torch.nn.utils.vector_to_parameters.html#torch.nn.utils.vector_to_parameters">torch.nn.utils 模块中的 vector_to_parameters()</a>
</li>
      <li><a href="backends.html#torch.backends.mkl.verbose">torch.backends.mkl 中的 verbose 类</a>

      <ul>
        <li><a href="backends.html#torch.backends.mkldnn.verbose">torch.backends.mkldnn 中的类</a>
</li>
      </ul></li>
      <li><a href="onnx_verification.html#torch.onnx.verification.VerificationInfo">torch.onnx.verification 中的 VerificationInfo 类</a>
</li>
      <li><a href="onnx_verification.html#torch.onnx.verification.VerificationOptions">torch.onnx.verification 中的 VerificationOptions（类）</a>
</li>
      <li><a href="onnx_verification.html#torch.onnx.verification.verify">torch.onnx.verification 模块中的 verify()函数</a>
</li>
      <li><a href="onnx_verification.html#torch.onnx.verification.verify_aten_graph">torch.onnx.verification 模块中的 verify_aten_graph()函数</a>
</li>
      <li><a href="cpp_extension.html#torch.utils.cpp_extension.verify_ninja_availability">torch.utils.cpp_extension 模块中的 verify_ninja_availability()函数</a>
</li>
      <li><a href="onnx_verification.html#torch.onnx.verification.verify_onnx_program">verify_onnx_program()（在 torch.onnx.verification 模块中）</a>
</li>
      <li><a href="backends.html#torch.backends.cudnn.version">version()（在 torch.backends.cudnn 模块中）</a>

      <ul>
        <li><a href="backends.html#torch.backends.cusparselt.version">（在 torch.backends.cusparselt 模块中）</a>
</li>
      </ul></li>
      <li><a href="generated/torch.autograd.functional.vhp.html#torch.autograd.functional.vhp">vhp()（在 torch.autograd.functional 模块中）</a>
</li>
      <li><a href="generated/torch.Tensor.view.html#torch.Tensor.view">view() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.Tensor.view_as.html#torch.Tensor.view_as">view_as() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.view_as_complex.html#torch.view_as_complex">view_as_complex() (在模块 torch 中)</a>
</li>
      <li><a href="generated/torch.view_as_real.html#torch.view_as_real">view_as_real() (在模块 torch 中)</a>
</li>
      <li><a href="distributed.tensor.html#torch.distributed.tensor.debug.visualize_sharding">visualize_sharding()（在 torch.distributed.tensor.debug 模块中）</a>
</li>
      <li><a href="generated/torch.autograd.functional.vjp.html#torch.autograd.functional.vjp">vjp()（在 torch.autograd.functional 模块中）</a>

      <ul>
        <li><a href="generated/torch.func.vjp.html#torch.func.vjp">(在 torch.func 模块中)</a>
</li>
        <li><a href="generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.vjp">(torch.autograd.function.内联函数静态方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.vjp">(torch.autograd.function.NestedIOFunction 静态方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.vmap.html#torch.vmap">vmap()（在 torch 模块中）</a>

      <ul>
        <li><a href="generated/torch.func.vmap.html#torch.func.vmap">(在 torch.func 模块中)</a>
</li>
        <li><a href="generated/torch.autograd.Function.vmap.html#torch.autograd.Function.vmap">(torch.autograd.函数静态方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.InplaceFunction.html#torch.autograd.function.InplaceFunction.vmap">(torch.autograd.function.内联函数静态方法)</a>
</li>
        <li><a href="generated/torch.autograd.function.NestedIOFunction.html#torch.autograd.function.NestedIOFunction.vmap">(torch.autograd.function.NestedIOFunction 静态方法)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.von_mises.VonMises">冯·米塞斯（torch.distributions.von_mises 中的类）</a>
</li>
      <li><a href="generated/torch.vsplit.html#torch.vsplit">torch 模块中的 vsplit()</a>

      <ul>
        <li><a href="generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.vstack.html#torch.vstack">torch 模块中的 vstack()</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="W">W</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.jit.wait.html#torch.jit.wait">torch.jit 模块中的 wait()</a>

      <ul>
        <li><a href="generated/torch.cuda.Event.html#torch.cuda.Event.wait">（torch.cuda.Event 方法）</a>
</li>
        <li><a href="elastic/rendezvous.html#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.wait">(torch.distributed.elastic.rendezvous.etcd_store.EtcdStore 方法)</a>
</li>
        <li><a href="distributed.fsdp.fully_shard.html#torch.distributed.fsdp.UnshardHandle.wait">torch.distributed.fsdp.UnshardHandle 方法</a>
</li>
        <li><a href="distributed.html#torch.distributed.Store.wait">(torch.distributed.Store 方法)</a>
</li>
        <li><a href="distributed.html#torch.distributed.Work.wait">torch.distributed.Work 方法</a>
</li>
        <li><a href="generated/torch.Event.html#torch.Event.wait">torch.Event 方法</a>
</li>
        <li><a href="futures.html#torch.futures.Future.wait">(torch.futures.Future 方法)</a>
</li>
        <li><a href="generated/torch.mps.event.Event.html#torch.mps.event.Event.wait">torch.mps.event.Event 方法</a>
</li>
        <li><a href="generated/torch.mtia.Event.html#torch.mtia.Event.wait">(torch.mtia.Event 方法)</a>
</li>
        <li><a href="generated/torch.xpu.Event.html#torch.xpu.Event.wait">(torch.xpu.Event 方法)</a>
</li>
      </ul></li>
      <li><a href="futures.html#torch.futures.wait_all">wait_all()（在 torch.futures 模块中）</a>
</li>
      <li><a href="generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream.wait_event">wait_event()（torch.cuda.ExternalStream 方法）</a>

      <ul>
        <li><a href="generated/torch.cuda.Stream.html#torch.cuda.Stream.wait_event">(torch.cuda.Stream 方法)</a>
</li>
        <li><a href="generated/torch.mtia.Stream.html#torch.mtia.Stream.wait_event">(torch.mtia.Stream 方法)</a>
</li>
        <li><a href="generated/torch.Stream.html#torch.Stream.wait_event">(torch.Stream 方法)</a>
</li>
        <li><a href="generated/torch.xpu.Stream.html#torch.xpu.Stream.wait_event">(torch.xpu.Stream 方法)</a>
</li>
      </ul></li>
      <li><a href="generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream.wait_stream">wait_stream()（torch.cuda.ExternalStream 方法）</a>

      <ul>
        <li><a href="generated/torch.cuda.Stream.html#torch.cuda.Stream.wait_stream">(torch.cuda.Stream 方法)</a>
</li>
        <li><a href="generated/torch.mtia.Stream.html#torch.mtia.Stream.wait_stream">(torch.mtia.Stream 方法)</a>
</li>
        <li><a href="generated/torch.Stream.html#torch.Stream.wait_stream">(torch.Stream 方法)</a>
</li>
        <li><a href="generated/torch.xpu.Stream.html#torch.xpu.Stream.wait_stream">(torch.xpu.Stream 方法)</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.weibull.Weibull">torch.distributions.weibull 中的 Weibull 类</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.nn.utils.weight_norm.html#torch.nn.utils.weight_norm">torch.nn.utils 模块中的 weight_norm()</a>

      <ul>
        <li><a href="generated/torch.nn.utils.parametrizations.weight_norm.html#torch.nn.utils.parametrizations.weight_norm">（在 torch.nn.utils.parametrizations 模块中）</a>
</li>
      </ul></li>
      <li><a href="data.html#torch.utils.data.WeightedRandomSampler">torch.utils.data 中的 WeightedRandomSampler 类</a>
</li>
      <li><a href="generated/torch.where.html#torch.where">torch 模块中的 where()</a>

      <ul>
        <li><a href="generated/torch.Tensor.where.html#torch.Tensor.where">torch.Tensor 方法</a>
</li>
      </ul></li>
      <li><a href="distributions.html#torch.distributions.wishart.Wishart">torch.distributions.wishart 中的 Wishart 类</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.AffineQuantizedObserverBase.html#torch.ao.quantization.observer.AffineQuantizedObserverBase.with_args">torch.ao.quantization.observer.AffineQuantizedObserverBase 类的 with_args() 方法</a>

      <ul>
        <li><a href="generated/torch.ao.quantization.observer.ObserverBase.html#torch.ao.quantization.observer.ObserverBase.with_args">torch.ao.quantization.observer.ObserverBase 类的方法</a>
</li>
      </ul></li>
      <li><a href="generated/torch.ao.quantization.observer.ObserverBase.html#torch.ao.quantization.observer.ObserverBase.with_callable_args">torch.ao.quantization.observer.ObserverBase 类的 with_callable_args() 方法</a>
</li>
      <li><a href="distributed.html#torch.distributed.Work">torch.distributed 中的工作（类）</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.Worker">torch.distributed.elastic.agent.server 中的 Worker（类）</a>
</li>
      <li><a href="elastic/control_plane.html#torch.distributed.elastic.control_plane.worker_main">torch.distributed.elastic.control_plane 模块中的 worker_main()函数</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.WorkerGroup">torch.distributed.elastic.agent.server 中的 WorkerGroup（类）</a>
</li>
      <li><a href="rpc.html#torch.distributed.rpc.WorkerInfo">torch.distributed.rpc 中的 WorkerInfo（类）</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.WorkerSpec">torch.distributed.elastic.agent.server 中的 WorkerSpec（类）</a>
</li>
      <li><a href="elastic/agent.html#torch.distributed.elastic.agent.server.WorkerState">torch.distributed.elastic.agent.server 中的 WorkerState（类）</a>
</li>
      <li><a href="fx.html#torch.fx.wrap">torch.fx 模块中的 wrap()函数</a>
</li>
      <li><a href="torch.overrides.html#torch.overrides.wrap_torch_function">wrap_torch_function() (在模块 torch.overrides 中)</a>
</li>
      <li><a href="library.html#torch.library.wrap_triton">wrap_triton() (在模块 torch.library 中)</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.StorageWriter.write_data">write_data() (torch.distributed.checkpoint.StorageWriter 方法)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.write_file">write_file() (在模块 torch.cuda.tunable 中)</a>
</li>
      <li><a href="cuda.tunable.html#torch.cuda.tunable.write_file_on_exit">torch.cuda.tunable 模块中的 write_file_on_exit()</a>
</li>
      <li><a href="distributed.checkpoint.html#torch.distributed.checkpoint.planner.WriteItem">torch.distributed.checkpoint.planner 类中的 WriteItem</a>
</li>
  </ul></td>
</tr></tbody></table>

<h2 id="X">X</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="nn.init.html#torch.nn.init.xavier_normal_">torch.nn.init 模块中的 xavier_normal_()</a>
</li>
      <li><a href="nn.init.html#torch.nn.init.xavier_uniform_">torch.nn.init 模块中的 xavier_uniform_()</a>
</li>
      <li><a href="special.html#torch.special.xlog1py">xlog1py()（在 torch.special 模块中）</a>
</li>
      <li><a href="generated/torch.xlogy.html#torch.xlogy">xlogy()（在 torch 模块中）</a>

      <ul>
        <li><a href="special.html#torch.special.xlogy">torch.special 模块中</a>
</li>
        <li><a href="generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy">torch.Tensor 方法</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.Tensor.xlogy_.html#torch.Tensor.xlogy_">xlogy_()（torch.Tensor 方法）</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.xpu">xpu()（torch.jit.ScriptModule 方法）</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.xpu">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.xpu">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
        <li><a href="generated/torch.Tensor.xpu.html#torch.Tensor.xpu">torch.Tensor 方法</a>
</li>
      </ul></li>
  </ul></td>
</tr></tbody></table>

<h2 id="Z">Z</h2>
<table style="width: 100%" class="indextable genindextable"><tbody><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.Tensor.zero_.html#torch.Tensor.zero_">zero_() (torch.Tensor 方法)</a>
</li>
      <li><a href="generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule.zero_grad">zero_grad() (torch.jit.ScriptModule 方法)</a>

      <ul>
        <li><a href="generated/torch.nn.Module.html#torch.nn.Module.zero_grad">(torch.nn.Module 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adadelta.html#torch.optim.Adadelta.zero_grad">(torch.optim.Adadelta 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adafactor.html#torch.optim.Adafactor.zero_grad">(torch.optim.Adafactor 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adagrad.html#torch.optim.Adagrad.zero_grad">(torch.optim.Adagrad 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adam.html#torch.optim.Adam.zero_grad">(torch.optim.Adam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Adamax.html#torch.optim.Adamax.zero_grad">(torch.optim.Adamax 方法)</a>
</li>
        <li><a href="generated/torch.optim.AdamW.html#torch.optim.AdamW.zero_grad">(torch.optim.AdamW 方法)</a>
</li>
        <li><a href="generated/torch.optim.ASGD.html#torch.optim.ASGD.zero_grad">(torch.optim.ASGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.LBFGS.html#torch.optim.LBFGS.zero_grad">(torch.optim.LBFGS 方法)</a>
</li>
        <li><a href="generated/torch.optim.NAdam.html#torch.optim.NAdam.zero_grad">(torch.optim.NAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad">(torch.optim.Optimizer 方法)</a>
</li>
        <li><a href="generated/torch.optim.RAdam.html#torch.optim.RAdam.zero_grad">(torch.optim.RAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.RMSprop.html#torch.optim.RMSprop.zero_grad">(torch.optim.RMSprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.Rprop.html#torch.optim.Rprop.zero_grad">(torch.optim.Rprop 方法)</a>
</li>
        <li><a href="generated/torch.optim.SGD.html#torch.optim.SGD.zero_grad">(torch.optim.SGD 方法)</a>
</li>
        <li><a href="generated/torch.optim.SparseAdam.html#torch.optim.SparseAdam.zero_grad">(torch.optim.SparseAdam 方法)</a>
</li>
        <li><a href="generated/torch.optim.swa_utils.AveragedModel.html#torch.optim.swa_utils.AveragedModel.zero_grad">(torch.optim.swa_utils.AveragedModel 方法)</a>
</li>
      </ul></li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="generated/torch.nn.ZeroPad1d.html#torch.nn.ZeroPad1d">ZeroPad1d (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.ZeroPad2d.html#torch.nn.ZeroPad2d">ZeroPad2d (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.nn.ZeroPad3d.html#torch.nn.ZeroPad3d">ZeroPad3d (torch.nn 中的类)</a>
</li>
      <li><a href="generated/torch.ao.quantization.observer.ZeroPointDomain.html#torch.ao.quantization.observer.ZeroPointDomain">torch.ao.quantization.observer 中的 ZeroPointDomain（类）</a>
</li>
      <li><a href="distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer">torch.distributed.optim 中的 ZeroRedundancyOptimizer（类）</a>
</li>
      <li><a href="generated/torch.zeros.html#torch.zeros">torch 模块中的 zeros()</a>

      <ul>
        <li><a href="distributed.tensor.html#torch.distributed.tensor.zeros">torch.distributed.tensor 模块中的()</a>
</li>
      </ul></li>
      <li><a href="nn.init.html#torch.nn.init.zeros_">torch.nn.init 模块中的 zeros_()</a>
</li>
      <li><a href="generated/torch.zeros_like.html#torch.zeros_like">torch 模块中的 zeros_like()</a>
</li>
      <li><a href="special.html#torch.special.zeta">torch.special 模块中的 zeta()</a>
</li>
  </ul></td>
</tr></tbody></table>



             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>© 版权所有 PyTorch 贡献者。</p>
  </div>
    
      <div>使用 Sphinx 构建，主题由 Read the Docs 提供。</div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script="" type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0">


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>文档</h2>
          <p>PyTorch 开发者文档全面访问</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">查看文档</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>教程</h2>
          <p>获取初学者和高级开发者的深入教程</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">查看教程</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>资源</h2>
          <p>查找开发资源并获得您的疑问解答</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">查看资源</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">开始使用</a></li>
            <li><a href="https://pytorch.org/features">功能</a></li>
            <li><a href="https://pytorch.org/ecosystem">生态系统</a></li>
            <li><a href="https://pytorch.org/blog/">博客</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">贡献</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">资源</a></li>
            <li><a href="https://pytorch.org/tutorials">教程</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">文档</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">讨论</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub 问题和任务</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">品牌指南</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">保持更新</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">推特</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">领英</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch 播客</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">苹果</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">谷歌</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">亚马逊</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">条款</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">隐私</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© 版权所有 Linux 基金会。PyTorch 基金会是 Linux 基金会的一个项目。有关本网站的使用条款、商标政策以及其他适用于 PyTorch 基金会的政策，请参阅 www.linuxfoundation.org/policies/。PyTorch 基金会支持 PyTorch 开源项目，该项目已被确立为 LF Projects, LLC 的 PyTorch 项目系列。有关适用于 PyTorch 项目系列 LF Projects, LLC 的政策，请参阅 www.lfprojects.org/policies/。</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">为分析流量并优化您的体验，我们在本网站上提供 cookies。通过点击或导航，您同意允许我们使用 cookies。作为本站点的当前维护者，Facebook 的 Cookies 政策适用。了解更多信息，包括可用的控制选项：Cookies 政策。</p>
    <img class="close-button" src="_static/images/pytorch-x.svg" width="16" height="16">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>学习</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">开始学习</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">教程</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">学习基础知识</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch 菜谱</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">PyTorch 入门 - YouTube 系列</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>生态系统</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">工具</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">社区</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">论坛</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">开发者资源</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">贡献者奖项 - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">关于 PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch 文档</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>文档</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch 领域</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>博客 &amp; 新闻</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch 博客</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">社区博客</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">视频</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">社区故事</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">活动</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">通讯</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>关于</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch 基金会</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">治理委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">云信用计划</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">技术顾问委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">员工</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">联系我们</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

</body></html>