<!DOCTYPE html>
<html lang="zh_CN">
<head>
  <meta charset="UTF-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.functional — PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/nn.functional.html">
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css">
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css">
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css">
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css">
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css">
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css">
    <link rel="index" title="Index" href="genindex.html">
    <link rel="search" title="Search" href="search.html">
    <link rel="next" title="torch.nn.functional.conv1d" href="generated/torch.nn.functional.conv1d.html">
    <link rel="prev" title="RMSNorm" href="generated/torch.nn.modules.normalization.RMSNorm.html">

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head><body class="pytorch-body"><div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">学习</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class="dropdown-title">开始使用</span>
                  <p>在本地运行 PyTorch 或快速开始使用支持的云平台之一</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">教程</span><p></p>
                  <p>PyTorch 教程中的新内容</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">学习基础知识</span><p></p>
                  <p>熟悉 PyTorch 的概念和模块</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch 食谱</span><p></p>
                  <p>精简版、可直接部署的 PyTorch 代码示例</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">PyTorch 入门 - YouTube 系列</span><p></p>
                  <p>通过我们引人入胜的 YouTube 教程系列掌握 PyTorch 基础知识</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">生态系统</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">工具</span><p></p>
                  <p>了解 PyTorch 生态系统中的工具和框架</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">社区</span>
                  <p>加入 PyTorch 开发者社区，贡献、学习并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">论坛</span>
                  <p>讨论 PyTorch 代码、问题、安装、研究的地方</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">开发者资源</span>
                  <p>查找资源并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">贡献者奖项 - 2024</span><p></p>
                  <p>本届 PyTorch 会议揭晓获奖者</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">关于 PyTorch Edge</span><p></p>
                  <p>为边缘设备构建创新和隐私感知的 AI 体验</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span><p></p>
                  <p>基于移动和边缘设备的端到端推理能力解决方案</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch 文档</span><p></p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">文档</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span><p></p>
                  <p>探索文档以获取全面指导，了解如何使用 PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch 领域</span><p></p>
                  <p>阅读 PyTorch 领域的文档，了解更多关于特定领域库的信息</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">博客与新闻</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch 博客</span><p></p>
                  <p>捕捉最新的技术新闻和事件</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">社区博客</span><p></p>
                  <p>PyTorch 生态系统故事</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">视频</span><p></p>
                  <p>了解最新的 PyTorch 教程、新内容等</p>
                </a><a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">社区故事</span><p></p>
                  <p>学习如何我们的社区使用 PyTorch 解决真实、日常的机器学习问题</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">活动</span><p></p>
                  <p>查找活动、网络研讨会和播客</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">通讯</span><p></p>
                  <p>跟踪最新更新</p>
                </a>
            </div>
          </div></li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">关于</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch 基金会</span><p></p>
                  <p>了解更多关于 PyTorch 基金会的信息</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">管理委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">云信用计划</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">技术顾问委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">员工</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">联系我们</span><p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">成为会员</a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>



   

    

    <div class="table-of-contents-link-wrapper">
      <span>目录</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="https://pytorch.org/docs/versions.html">主程序 (2.7.0+cpu ) ▼</a>
    </div>
    <div id="searchBox">
    <div class="searchbox" id="googleSearchBox">
      <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
      <div class="gcse-search"></div>
    </div>
    <div id="sphinxSearchBox" style="display: none;">
      <div role="search">
        <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
          <input type="text" name="q" placeholder="Search Docs">
          <input type="hidden" name="check_keywords" value="yes">
          <input type="hidden" name="area" value="default">
        </form>
      </div>
    </div>
  </div>
  <form id="searchForm">
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="google" checked="">谷歌搜索</label>
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="sphinx">经典搜索</label>
  </form>

  <script>
     document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
  </script>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/nn.functional.html">您正在查看不稳定开发者预览文档。请点击此处查看最新稳定版本的文档。</a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">社区</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch 治理 | 构建 + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch 贡献指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch 设计哲学</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch 治理 | 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch 治理 | 维护者</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者笔记</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">自动混合精度示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">广播语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU 多线程和 TorchScript 推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA 语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/custom_operators.html">PyTorch 自定义算子页面</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">分布式数据并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">扩展 PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">使用 autograd.Function 扩展 torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">常见问题解答</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/fsdp.html">FSDP 笔记</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/get_start_xpu.html">在 Intel GPU 上入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm)语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">大规模部署功能</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/libtorch_stable_abi.html">LibTorch 稳定 ABI</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">模块</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS 后端</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">多进程最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">数值精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">可重现性</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">序列化语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows 常见问题解答</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语言绑定</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">张量属性</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">张量视图</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="accelerator.html">torch.accelerator</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">理解 CUDA 内存使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">生成快照</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">使用可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">快照 API 参考</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.memory.html">torch.mtia.memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">元设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.html">torch.distributed.tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.fsdp.fully_shard.html">torch.distributed.fsdp.fully_shard</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">复数</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP 通信钩子</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">分布式 RPC 框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">类型信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">命名张量</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">命名张量操作覆盖率</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_environment_variables.html">火炬环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">库</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">火炬推荐</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch 在 XLA 设备上</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/ao">torchao</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        文档 &gt;</li>

        
      <li>torch.nn.functional</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/nn.functional.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg" width="16" height="16"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">快捷键</div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="torch-nn-functional">
<h1>torch.nn.functional<a class="headerlink" href="#torch-nn-functional" title="Permalink to this heading">¶</a></h1>
<section id="convolution-functions">
<h2>卷积函数</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.conv1d"><a class="reference internal" href="generated/torch.nn.functional.conv1d.html#torch.nn.functional.conv1d" title="torch.nn.functional.conv1d"><code class="xref py py-obj docutils literal "><span class="pre">conv1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 1D 卷积。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.conv2d"><a class="reference internal" href="generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d" title="torch.nn.functional.conv2d"><code class="xref py py-obj docutils literal "><span class="pre">conv2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入图像应用 2D 卷积。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.conv3d"><a class="reference internal" href="generated/torch.nn.functional.conv3d.html#torch.nn.functional.conv3d" title="torch.nn.functional.conv3d"><code class="xref py py-obj docutils literal "><span class="pre">conv3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入图像应用 3D 卷积。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.conv_transpose1d"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose1d.html#torch.nn.functional.conv_transpose1d" title="torch.nn.functional.conv_transpose1d"><code class="xref py py-obj docutils literal "><span class="pre">conv_transpose1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 1D 转置卷积算子，有时也称为“反卷积”。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.conv_transpose2d"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose2d.html#torch.nn.functional.conv_transpose2d" title="torch.nn.functional.conv_transpose2d"><code class="xref py py-obj docutils literal "><span class="pre">conv_transpose2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入图像应用 2D 转置卷积算子，有时也称为“反卷积”。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.conv_transpose3d"><a class="reference internal" href="generated/torch.nn.functional.conv_transpose3d.html#torch.nn.functional.conv_transpose3d" title="torch.nn.functional.conv_transpose3d"><code class="xref py py-obj docutils literal "><span class="pre">conv_transpose3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入图像应用 3D 转置卷积算子，有时也称为“反卷积”</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.unfold"><a class="reference internal" href="generated/torch.nn.functional.unfold.html#torch.nn.functional.unfold" title="torch.nn.functional.unfold"><code class="xref py py-obj docutils literal "><span class="pre">unfold</span></code></a></p></td>
<td><p>从批处理的输入张量中提取滑动局部块。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.fold"><a class="reference internal" href="generated/torch.nn.functional.fold.html#torch.nn.functional.fold" title="torch.nn.functional.fold"><code class="xref py py-obj docutils literal "><span class="pre">fold</span></code></a></p></td>
<td><p>将一系列滑动局部块组合成一个大型包含张量。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="pooling-functions">
<h2>池化函数</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.avg_pool1d"><a class="reference internal" href="generated/torch.nn.functional.avg_pool1d.html#torch.nn.functional.avg_pool1d" title="torch.nn.functional.avg_pool1d"><code class="xref py py-obj docutils literal "><span class="pre">avg_pool1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 1D 平均池化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.avg_pool2d"><a class="reference internal" href="generated/torch.nn.functional.avg_pool2d.html#torch.nn.functional.avg_pool2d" title="torch.nn.functional.avg_pool2d"><code class="xref py py-obj docutils literal "><span class="pre">avg_pool2d</span></code></a></p></td>
<td><p>在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">kH \times kW</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.77777em;vertical-align:-0.08333em;" class="strut"></span><span style="margin-right:0.03148em;" class="mord mathnormal">k</span><span style="margin-right:0.08125em;" class="mord mathnormal">H</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">×</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span style="margin-right:0.13889em;" class="mord mathnormal">kW</span></span></span></span> 区域通过步长 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">sH \times sW</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.76666em;vertical-align:-0.08333em;" class="strut"></span><span style="margin-right:0.08125em;" class="mord mathnormal">sH</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">×</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.68333em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">s</span><span style="margin-right:0.13889em;" class="mord mathnormal">W</span></span></span></span> 步应用 2D 平均池化操作。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.avg_pool3d"><a class="reference internal" href="generated/torch.nn.functional.avg_pool3d.html#torch.nn.functional.avg_pool3d" title="torch.nn.functional.avg_pool3d"><code class="xref py py-obj docutils literal "><span class="pre">avg_pool3d</span></code></a></p></td>
<td><p>在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>T</mi><mo>×</mo><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">kT \times kH \times kW</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.77777em;vertical-align:-0.08333em;" class="strut"></span><span style="margin-right:0.03148em;" class="mord mathnormal">k</span><span style="margin-right:0.13889em;" class="mord mathnormal">T</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">×</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.77777em;vertical-align:-0.08333em;" class="strut"></span><span style="margin-right:0.03148em;" class="mord mathnormal">k</span><span style="margin-right:0.08125em;" class="mord mathnormal">H</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">×</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span style="margin-right:0.13889em;" class="mord mathnormal">kW</span></span></span></span> 区域通过步长 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>T</mi><mo>×</mo><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">sT \times sH \times sW</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.76666em;vertical-align:-0.08333em;" class="strut"></span><span class="mord mathnormal">s</span><span style="margin-right:0.13889em;" class="mord mathnormal">T</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">×</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.76666em;vertical-align:-0.08333em;" class="strut"></span><span style="margin-right:0.08125em;" class="mord mathnormal">sH</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">×</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.68333em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">s</span><span style="margin-right:0.13889em;" class="mord mathnormal">W</span></span></span></span> 步应用 3D 平均池化操作。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.max_pool1d"><a class="reference internal" href="generated/torch.nn.functional.max_pool1d.html#torch.nn.functional.max_pool1d" title="torch.nn.functional.max_pool1d"><code class="xref py py-obj docutils literal "><span class="pre">max_pool1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 1D 最大池化。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.max_pool2d"><a class="reference internal" href="generated/torch.nn.functional.max_pool2d.html#torch.nn.functional.max_pool2d" title="torch.nn.functional.max_pool2d"><code class="xref py py-obj docutils literal "><span class="pre">max_pool2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 2D 最大池化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.max_pool3d"><a class="reference internal" href="generated/torch.nn.functional.max_pool3d.html#torch.nn.functional.max_pool3d" title="torch.nn.functional.max_pool3d"><code class="xref py py-obj docutils literal "><span class="pre">max_pool3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 3D 最大池化。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.max_unpool1d"><a class="reference internal" href="generated/torch.nn.functional.max_unpool1d.html#torch.nn.functional.max_unpool1d" title="torch.nn.functional.max_unpool1d"><code class="xref py py-obj docutils literal "><span class="pre">max_unpool1d</span></code></a></p></td>
<td><p>计算部分逆元 <code class="xref py py-class docutils literal "><span class="pre">MaxPool1d</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.max_unpool2d"><a class="reference internal" href="generated/torch.nn.functional.max_unpool2d.html#torch.nn.functional.max_unpool2d" title="torch.nn.functional.max_unpool2d"><code class="xref py py-obj docutils literal "><span class="pre">max_unpool2d</span></code></a></p></td>
<td><p>计算部分逆元 <code class="xref py py-class docutils literal "><span class="pre">MaxPool2d</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.max_unpool3d"><a class="reference internal" href="generated/torch.nn.functional.max_unpool3d.html#torch.nn.functional.max_unpool3d" title="torch.nn.functional.max_unpool3d"><code class="xref py py-obj docutils literal "><span class="pre">max_unpool3d</span></code></a></p></td>
<td><p>计算部分逆元 <code class="xref py py-class docutils literal "><span class="pre">MaxPool3d</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.lp_pool1d"><a class="reference internal" href="generated/torch.nn.functional.lp_pool1d.html#torch.nn.functional.lp_pool1d" title="torch.nn.functional.lp_pool1d"><code class="xref py py-obj docutils literal "><span class="pre">lp_pool1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 1D 功率平均池化。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.lp_pool2d"><a class="reference internal" href="generated/torch.nn.functional.lp_pool2d.html#torch.nn.functional.lp_pool2d" title="torch.nn.functional.lp_pool2d"><code class="xref py py-obj docutils literal "><span class="pre">lp_pool2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 2D 功率平均池化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.lp_pool3d"><a class="reference internal" href="generated/torch.nn.functional.lp_pool3d.html#torch.nn.functional.lp_pool3d" title="torch.nn.functional.lp_pool3d"><code class="xref py py-obj docutils literal "><span class="pre">lp_pool3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 3D 功率平均池化。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.adaptive_max_pool1d"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool1d.html#torch.nn.functional.adaptive_max_pool1d" title="torch.nn.functional.adaptive_max_pool1d"><code class="xref py py-obj docutils literal "><span class="pre">adaptive_max_pool1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 1D 自适应最大池化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.adaptive_max_pool2d"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool2d.html#torch.nn.functional.adaptive_max_pool2d" title="torch.nn.functional.adaptive_max_pool2d"><code class="xref py py-obj docutils literal "><span class="pre">adaptive_max_pool2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 2D 自适应最大池化。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.adaptive_max_pool3d"><a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool3d.html#torch.nn.functional.adaptive_max_pool3d" title="torch.nn.functional.adaptive_max_pool3d"><code class="xref py py-obj docutils literal "><span class="pre">adaptive_max_pool3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 3D 自适应最大池化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.adaptive_avg_pool1d"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool1d.html#torch.nn.functional.adaptive_avg_pool1d" title="torch.nn.functional.adaptive_avg_pool1d"><code class="xref py py-obj docutils literal "><span class="pre">adaptive_avg_pool1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 1D 自适应平均池化。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.adaptive_avg_pool2d"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool2d.html#torch.nn.functional.adaptive_avg_pool2d" title="torch.nn.functional.adaptive_avg_pool2d"><code class="xref py py-obj docutils literal "><span class="pre">adaptive_avg_pool2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 2D 自适应平均池化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.adaptive_avg_pool3d"><a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool3d.html#torch.nn.functional.adaptive_avg_pool3d" title="torch.nn.functional.adaptive_avg_pool3d"><code class="xref py py-obj docutils literal "><span class="pre">adaptive_avg_pool3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 3D 自适应平均池化。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.fractional_max_pool2d"><a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool2d.html#torch.nn.functional.fractional_max_pool2d" title="torch.nn.functional.fractional_max_pool2d"><code class="xref py py-obj docutils literal "><span class="pre">fractional_max_pool2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 2D 分数最大池化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.fractional_max_pool3d"><a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool3d.html#torch.nn.functional.fractional_max_pool3d" title="torch.nn.functional.fractional_max_pool3d"><code class="xref py py-obj docutils literal "><span class="pre">fractional_max_pool3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 3D 分数最大池化。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="attention-mechanisms">
<h2>注意力机制</h2>
<p>The <code class="xref py py-mod docutils literal "><span class="pre">torch.nn.attention.bias</span></code> 模块包含为与 scaled_dot_product_attention 一起使用而设计的 attention_biases。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.scaled_dot_product_attention"><a class="reference internal" href="generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention" title="torch.nn.functional.scaled_dot_product_attention"><code class="xref py py-obj docutils literal "><span class="pre">scaled_dot_product_attention</span></code></a></p></td>
<td><p>scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,</p></td>
</tr>
</tbody>
</table>
</section>
<section id="non-linear-activation-functions">
<h2>非线性激活函数 §</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.threshold"><a class="reference internal" href="generated/torch.nn.functional.threshold.html#torch.nn.functional.threshold" title="torch.nn.functional.threshold"><code class="xref py py-obj docutils literal "><span class="pre">threshold</span></code></a></p></td>
<td><p>将阈值应用于输入 Tensor 的每个元素。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.threshold_"><a class="reference internal" href="generated/torch.nn.functional.threshold_.html#torch.nn.functional.threshold_" title="torch.nn.functional.threshold_"><code class="xref py py-obj docutils literal "><span class="pre">threshold_</span></code></a></p></td>
<td><p>现场版本 <code class="xref py py-func docutils literal "><span class="pre">threshold()</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.relu"><a class="reference internal" href="generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu"><code class="xref py py-obj docutils literal "><span class="pre">relu</span></code></a></p></td>
<td><p>元素级应用修正线性单元函数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.relu_"><a class="reference internal" href="generated/torch.nn.functional.relu_.html#torch.nn.functional.relu_" title="torch.nn.functional.relu_"><code class="xref py py-obj docutils literal "><span class="pre">relu_</span></code></a></p></td>
<td><p>现场版本 <code class="xref py py-func docutils literal "><span class="pre">relu()</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.hardtanh"><a class="reference internal" href="generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh" title="torch.nn.functional.hardtanh"><code class="xref py py-obj docutils literal "><span class="pre">hardtanh</span></code></a></p></td>
<td><p>元素级应用 HardTanh 函数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.hardtanh_"><a class="reference internal" href="generated/torch.nn.functional.hardtanh_.html#torch.nn.functional.hardtanh_" title="torch.nn.functional.hardtanh_"><code class="xref py py-obj docutils literal "><span class="pre">hardtanh_</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">hardtanh()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.hardswish"><a class="reference internal" href="generated/torch.nn.functional.hardswish.html#torch.nn.functional.hardswish" title="torch.nn.functional.hardswish"><code class="xref py py-obj docutils literal "><span class="pre">hardswish</span></code></a></p></td>
<td><p>应用逐元素硬 swish 函数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.relu6"><a class="reference internal" href="generated/torch.nn.functional.relu6.html#torch.nn.functional.relu6" title="torch.nn.functional.relu6"><code class="xref py py-obj docutils literal "><span class="pre">relu6</span></code></a></p></td>
<td><p>应用逐元素的函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ReLU6</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mn>6</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{ReLU6}(x) = \min(\max(0,x), 6)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">ReLU6</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mop">min</span><span class="mopen">(</span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span class="mord">6</span><span class="mclose">)</span></span></span></span> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.elu"><a class="reference internal" href="generated/torch.nn.functional.elu.html#torch.nn.functional.elu" title="torch.nn.functional.elu"><code class="xref py py-obj docutils literal "><span class="pre">elu</span></code></a></p></td>
<td><p>应用逐元素的指数线性单元（ELU）函数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.elu_"><a class="reference internal" href="generated/torch.nn.functional.elu_.html#torch.nn.functional.elu_" title="torch.nn.functional.elu_"><code class="xref py py-obj docutils literal "><span class="pre">elu_</span></code></a></p></td>
<td><p>原位版本 <code class="xref py py-func docutils literal "><span class="pre">elu()</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.selu"><a class="reference internal" href="generated/torch.nn.functional.selu.html#torch.nn.functional.selu" title="torch.nn.functional.selu"><code class="xref py py-obj docutils literal "><span class="pre">selu</span></code></a></p></td>
<td><p>元素级应用， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>SELU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mo>∗</mo><mo stretchy="false">(</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>α</mi><mo>∗</mo><mo stretchy="false">(</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">SELU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">sc</span><span class="mord mathnormal">a</span><span style="margin-right:0.01968em;" class="mord mathnormal">l</span><span class="mord mathnormal">e</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">∗</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mopen">(</span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">+</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.0037em;" class="mord mathnormal">α</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">∗</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mopen">(</span><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">−</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord">1</span><span class="mclose">)))</span></span></span></span> ，与 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1.6732632423543772848170429916717</mn></mrow><annotation encoding="application/x-tex">\alpha=1.6732632423543772848170429916717</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.43056em;vertical-align:0em;" class="strut"></span><span style="margin-right:0.0037em;" class="mord mathnormal">α</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.64444em;vertical-align:0em;" class="strut"></span><span class="mord">1.6732632423543772848170429916717</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mo>=</mo><mn>1.0507009873554804934193349852946</mn></mrow><annotation encoding="application/x-tex">scale=1.0507009873554804934193349852946</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">sc</span><span class="mord mathnormal">a</span><span style="margin-right:0.01968em;" class="mord mathnormal">l</span><span class="mord mathnormal">e</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.64444em;vertical-align:0em;" class="strut"></span><span class="mord">1.0507009873554804934193349852946</span></span></span></span> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.celu"><a class="reference internal" href="generated/torch.nn.functional.celu.html#torch.nn.functional.celu" title="torch.nn.functional.celu"><code class="xref py py-obj docutils literal "><span class="pre">celu</span></code></a></p></td>
<td><p>元素级应用， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>CELU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>α</mi><mo>∗</mo><mo stretchy="false">(</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">/</mi><mi>α</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">CELU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">+</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.0037em;" class="mord mathnormal">α</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">∗</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mopen">(</span><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">/</span><span style="margin-right:0.0037em;" class="mord mathnormal">α</span><span class="mclose">)</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">−</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord">1</span><span class="mclose">))</span></span></span></span> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.leaky_relu"><a class="reference internal" href="generated/torch.nn.functional.leaky_relu.html#torch.nn.functional.leaky_relu" title="torch.nn.functional.leaky_relu"><code class="xref py py-obj docutils literal "><span class="pre">leaky_relu</span></code></a></p></td>
<td><p>元素级应用， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>LeakyReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mtext>negative_slope</mtext><mo>∗</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">LeakyReLU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">+</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1.00444em;vertical-align:-0.31em;" class="strut"></span><span class="mord text"><span class="mord">negative_slope</span></span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">∗</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.leaky_relu_"><a class="reference internal" href="generated/torch.nn.functional.leaky_relu_.html#torch.nn.functional.leaky_relu_" title="torch.nn.functional.leaky_relu_"><code class="xref py py-obj docutils literal "><span class="pre">leaky_relu_</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">leaky_relu()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.prelu"><a class="reference internal" href="generated/torch.nn.functional.prelu.html#torch.nn.functional.prelu" title="torch.nn.functional.prelu"><code class="xref py py-obj docutils literal "><span class="pre">prelu</span></code></a></p></td>
<td><p>将函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>PReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mtext>weight</mtext><mo>∗</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">PReLU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">+</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.8888799999999999em;vertical-align:-0.19444em;" class="strut"></span><span class="mord text"><span class="mord">weight</span></span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">∗</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 应用到元素级别，其中权重是一个可学习的参数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.rrelu"><a class="reference internal" href="generated/torch.nn.functional.rrelu.html#torch.nn.functional.rrelu" title="torch.nn.functional.rrelu"><code class="xref py py-obj docutils literal "><span class="pre">rrelu</span></code></a></p></td>
<td><p>随机化漏斗型 ReLU。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.rrelu_"><a class="reference internal" href="generated/torch.nn.functional.rrelu_.html#torch.nn.functional.rrelu_" title="torch.nn.functional.rrelu_"><code class="xref py py-obj docutils literal "><span class="pre">rrelu_</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">rrelu()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.glu"><a class="reference internal" href="generated/torch.nn.functional.glu.html#torch.nn.functional.glu" title="torch.nn.functional.glu"><code class="xref py py-obj docutils literal "><span class="pre">glu</span></code></a></p></td>
<td><p>门控线性单元。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.gelu"><a class="reference internal" href="generated/torch.nn.functional.gelu.html#torch.nn.functional.gelu" title="torch.nn.functional.gelu"><code class="xref py py-obj docutils literal "><span class="pre">gelu</span></code></a></p></td>
<td><p>当近似参数为 'none' 时，逐元素应用函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>GELU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mo>∗</mo><mi mathvariant="normal">Φ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{GELU}(x) = x * \Phi(x)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">GELU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.46528em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">x</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">∗</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord">Φ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.logsigmoid"><a class="reference internal" href="generated/torch.nn.functional.logsigmoid.html#torch.nn.functional.logsigmoid" title="torch.nn.functional.logsigmoid"><code class="xref py py-obj docutils literal "><span class="pre">logsigmoid</span></code></a></p></td>
<td><p>逐元素应用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>LogSigmoid</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">LogSigmoid</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.31166399999999994em;" class="vlist"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose">)</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1.80002em;vertical-align:-0.65002em;" class="strut"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span class="minner"><span style="top:0em;" class="mopen delimcenter"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.845108em;" class="vlist"><span style="top:-2.655em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.3280857142857143em;" class="vlist"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span style="height:2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.143em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span style="height:3em;" class="pstrut"></span><span style="border-bottom-width:0.04em;" class="frac-line"></span></span><span style="top:-3.394em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.52em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span style="top:0em;" class="mclose delimcenter"><span class="delimsizing size2">)</span></span></span></span></span></span> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.hardshrink"><a class="reference internal" href="generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink" title="torch.nn.functional.hardshrink"><code class="xref py py-obj docutils literal "><span class="pre">hardshrink</span></code></a></p></td>
<td><p>逐元素应用硬收缩函数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.tanhshrink"><a class="reference internal" href="generated/torch.nn.functional.tanhshrink.html#torch.nn.functional.tanhshrink" title="torch.nn.functional.tanhshrink"><code class="xref py py-obj docutils literal "><span class="pre">tanhshrink</span></code></a></p></td>
<td><p>元素级应用， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Tanhshrink</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mo>−</mo><mtext>Tanh</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Tanhshrink}(x) = x - \text{Tanh}(x)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">Tanhshrink</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.66666em;vertical-align:-0.08333em;" class="strut"></span><span class="mord mathnormal">x</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">−</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">Tanh</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> </p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.softsign"><a class="reference internal" href="generated/torch.nn.functional.softsign.html#torch.nn.functional.softsign" title="torch.nn.functional.softsign"><code class="xref py py-obj docutils literal "><span class="pre">softsign</span></code></a></p></td>
<td><p>元素级应用，函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>SoftSign</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>x</mi><mrow><mn>1</mn><mo>+</mo><mi mathvariant="normal">∣</mi><mi>x</mi><mi mathvariant="normal">∣</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{SoftSign}(x) = \frac{x}{1 + |x|}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">SoftSign</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1.215392em;vertical-align:-0.52em;" class="strut"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.695392em;" class="vlist"><span style="top:-2.655em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">x</span><span class="mord mtight">∣</span></span></span></span><span style="top:-3.23em;"><span style="height:3em;" class="pstrut"></span><span style="border-bottom-width:0.04em;" class="frac-line"></span></span><span style="top:-3.394em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.52em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> </p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.softplus"><a class="reference internal" href="generated/torch.nn.functional.softplus.html#torch.nn.functional.softplus" title="torch.nn.functional.softplus"><code class="xref py py-obj docutils literal "><span class="pre">softplus</span></code></a></p></td>
<td><p>元素级应用，函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Softplus</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>β</mi></mfrac><mo>∗</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>β</mi><mo>∗</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">Softplus</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1.326216em;vertical-align:-0.481108em;" class="strut"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.845108em;" class="vlist"><span style="top:-2.6550000000000002em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span style="margin-right:0.05278em;" class="mord mathnormal mtight">β</span></span></span></span><span style="top:-3.23em;"><span style="height:3em;" class="pstrut"></span><span style="border-bottom-width:0.04em;" class="frac-line"></span></span><span style="top:-3.394em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.481108em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">∗</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">1</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">+</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mop">exp</span><span class="mopen">(</span><span style="margin-right:0.05278em;" class="mord mathnormal">β</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">∗</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.softmin"><a class="reference internal" href="generated/torch.nn.functional.softmin.html#torch.nn.functional.softmin" title="torch.nn.functional.softmin"><code class="xref py py-obj docutils literal "><span class="pre">softmin</span></code></a></p></td>
<td><p>应用软最大函数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.softmax"><a class="reference internal" href="generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax" title="torch.nn.functional.softmax"><code class="xref py py-obj docutils literal "><span class="pre">softmax</span></code></a></p></td>
<td><p>应用 softmax 函数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.softshrink"><a class="reference internal" href="generated/torch.nn.functional.softshrink.html#torch.nn.functional.softshrink" title="torch.nn.functional.softshrink"><code class="xref py py-obj docutils literal "><span class="pre">softshrink</span></code></a></p></td>
<td><p>元素级应用软收缩函数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.gumbel_softmax"><a class="reference internal" href="generated/torch.nn.functional.gumbel_softmax.html#torch.nn.functional.gumbel_softmax" title="torch.nn.functional.gumbel_softmax"><code class="xref py py-obj docutils literal "><span class="pre">gumbel_softmax</span></code></a></p></td>
<td><p>从 Gumbel-Softmax 分布中采样（链接 1 链接 2）并可选择进行离散化。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.log_softmax"><a class="reference internal" href="generated/torch.nn.functional.log_softmax.html#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><code class="xref py py-obj docutils literal "><span class="pre">log_softmax</span></code></a></p></td>
<td><p>应用 softmax 后跟对数运算。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.tanh"><a class="reference internal" href="generated/torch.nn.functional.tanh.html#torch.nn.functional.tanh" title="torch.nn.functional.tanh"><code class="xref py py-obj docutils literal "><span class="pre">tanh</span></code></a></p></td>
<td><p>元素级应用， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Tanh</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">Tanh</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1.53em;vertical-align:-0.52em;" class="strut"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:1.01em;" class="vlist"><span style="top:-2.655em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span><span class="mbin mtight">+</span><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span style="height:3em;" class="pstrut"></span><span style="border-bottom-width:0.04em;" class="frac-line"></span></span><span style="top:-3.485em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span><span class="mbin mtight">−</span><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.52em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> </p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.sigmoid"><a class="reference internal" href="generated/torch.nn.functional.sigmoid.html#torch.nn.functional.sigmoid" title="torch.nn.functional.sigmoid"><code class="xref py py-obj docutils literal "><span class="pre">sigmoid</span></code></a></p></td>
<td><p>应用元素级函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Sigmoid</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">Sigmoid</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1.365108em;vertical-align:-0.52em;" class="strut"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.845108em;" class="vlist"><span style="top:-2.655em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span style="height:3em;" class="pstrut"></span><span style="border-bottom-width:0.04em;" class="frac-line"></span></span><span style="top:-3.394em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.52em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> </p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.hardsigmoid"><a class="reference internal" href="generated/torch.nn.functional.hardsigmoid.html#torch.nn.functional.hardsigmoid" title="torch.nn.functional.hardsigmoid"><code class="xref py py-obj docutils literal "><span class="pre">hardsigmoid</span></code></a></p></td>
<td><p>元素级应用 Hardsigmoid 函数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.silu"><a class="reference internal" href="generated/torch.nn.functional.silu.html#torch.nn.functional.silu" title="torch.nn.functional.silu"><code class="xref py py-obj docutils literal "><span class="pre">silu</span></code></a></p></td>
<td><p>应用 Sigmoid 线性单元（SiLU）函数，元素级。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.mish"><a class="reference internal" href="generated/torch.nn.functional.mish.html#torch.nn.functional.mish" title="torch.nn.functional.mish"><code class="xref py py-obj docutils literal "><span class="pre">mish</span></code></a></p></td>
<td><p>应用 Mish 函数，逐元素应用。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.batch_norm"><a class="reference internal" href="generated/torch.nn.functional.batch_norm.html#torch.nn.functional.batch_norm" title="torch.nn.functional.batch_norm"><code class="xref py py-obj docutils literal "><span class="pre">batch_norm</span></code></a></p></td>
<td><p>对每个通道在整个数据批次上应用批量归一化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.group_norm"><a class="reference internal" href="generated/torch.nn.functional.group_norm.html#torch.nn.functional.group_norm" title="torch.nn.functional.group_norm"><code class="xref py py-obj docutils literal "><span class="pre">group_norm</span></code></a></p></td>
<td><p>对最后若干维度应用组归一化。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.instance_norm"><a class="reference internal" href="generated/torch.nn.functional.instance_norm.html#torch.nn.functional.instance_norm" title="torch.nn.functional.instance_norm"><code class="xref py py-obj docutils literal "><span class="pre">instance_norm</span></code></a></p></td>
<td><p>对每个数据样本中的每个通道独立应用实例归一化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.layer_norm"><a class="reference internal" href="generated/torch.nn.functional.layer_norm.html#torch.nn.functional.layer_norm" title="torch.nn.functional.layer_norm"><code class="xref py py-obj docutils literal "><span class="pre">layer_norm</span></code></a></p></td>
<td><p>应用层归一化于最后若干维度。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.local_response_norm"><a class="reference internal" href="generated/torch.nn.functional.local_response_norm.html#torch.nn.functional.local_response_norm" title="torch.nn.functional.local_response_norm"><code class="xref py py-obj docutils literal "><span class="pre">local_response_norm</span></code></a></p></td>
<td><p>在输入信号上应用局部响应归一化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.rms_norm"><a class="reference internal" href="generated/torch.nn.functional.rms_norm.html#torch.nn.functional.rms_norm" title="torch.nn.functional.rms_norm"><code class="xref py py-obj docutils literal "><span class="pre">rms_norm</span></code></a></p></td>
<td><p>应用根均方层归一化。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.normalize"><a class="reference internal" href="generated/torch.nn.functional.normalize.html#torch.nn.functional.normalize" title="torch.nn.functional.normalize"><code class="xref py py-obj docutils literal "><span class="pre">normalize</span></code></a></p></td>
<td><p>在指定维度上对输入执行 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.969438em;vertical-align:-0.286108em;" class="strut"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.15139200000000003em;" class="vlist"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.286108em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> 归一化。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="linear-functions">
<h2>线性函数 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.linear"><a class="reference internal" href="generated/torch.nn.functional.linear.html#torch.nn.functional.linear" title="torch.nn.functional.linear"><code class="xref py py-obj docutils literal "><span class="pre">linear</span></code></a></p></td>
<td><p>对传入数据进行线性变换： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = xA^T + b</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.924661em;vertical-align:-0.08333em;" class="strut"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height:0.8413309999999999em;" class="vlist"><span style="top:-3.063em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span style="margin-right:0.13889em;" class="mord mathnormal mtight">T</span></span></span></span></span></span></span></span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">+</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">b</span></span></span></span> .</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.bilinear"><a class="reference internal" href="generated/torch.nn.functional.bilinear.html#torch.nn.functional.bilinear" title="torch.nn.functional.bilinear"><code class="xref py py-obj docutils literal "><span class="pre">bilinear</span></code></a></p></td>
<td><p>对传入数据进行双线性变换： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msubsup><mi>x</mi><mn>1</mn><mi>T</mi></msubsup><mi>A</mi><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = x_1^T A x_2 + b</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1.0894389999999998em;vertical-align:-0.24810799999999997em;" class="strut"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.8413309999999999em;" class="vlist"><span style="top:-2.4518920000000004em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span style="margin-right:0.13889em;" class="mord mathnormal mtight">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.24810799999999997em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.30110799999999993em;" class="vlist"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.15em;" class="vlist"><span></span></span></span></span></span></span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">+</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">b</span></span></span></span> </p></td>
</tr>
</tbody>
</table>
</section>
<section id="dropout-functions">
<h2>Dropout 函数 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.dropout"><a class="reference internal" href="generated/torch.nn.functional.dropout.html#torch.nn.functional.dropout" title="torch.nn.functional.dropout"><code class="xref py py-obj docutils literal "><span class="pre">dropout</span></code></a></p></td>
<td><p>在训练过程中，以概率 <code class="xref py py-attr docutils literal "><span class="pre">p</span></code> 随机将输入张量的一些元素置为零。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.alpha_dropout"><a class="reference internal" href="generated/torch.nn.functional.alpha_dropout.html#torch.nn.functional.alpha_dropout" title="torch.nn.functional.alpha_dropout"><code class="xref py py-obj docutils literal "><span class="pre">alpha_dropout</span></code></a></p></td>
<td><p>对输入应用 alpha dropout。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.feature_alpha_dropout"><a class="reference internal" href="generated/torch.nn.functional.feature_alpha_dropout.html#torch.nn.functional.feature_alpha_dropout" title="torch.nn.functional.feature_alpha_dropout"><code class="xref py py-obj docutils literal "><span class="pre">feature_alpha_dropout</span></code></a></p></td>
<td><p>随机屏蔽整个通道（通道是一个特征图）。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.dropout1d"><a class="reference internal" href="generated/torch.nn.functional.dropout1d.html#torch.nn.functional.dropout1d" title="torch.nn.functional.dropout1d"><code class="xref py py-obj docutils literal "><span class="pre">dropout1d</span></code></a></p></td>
<td><p>随机将整个通道置为零（通道是一个 1D 特征图）。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.dropout2d"><a class="reference internal" href="generated/torch.nn.functional.dropout2d.html#torch.nn.functional.dropout2d" title="torch.nn.functional.dropout2d"><code class="xref py py-obj docutils literal "><span class="pre">dropout2d</span></code></a></p></td>
<td><p>随机将整个通道置零（通道是一个二维特征图）。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.dropout3d"><a class="reference internal" href="generated/torch.nn.functional.dropout3d.html#torch.nn.functional.dropout3d" title="torch.nn.functional.dropout3d"><code class="xref py py-obj docutils literal "><span class="pre">dropout3d</span></code></a></p></td>
<td><p>随机将整个通道置零（通道是一个三维特征图）。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="sparse-functions">
<h2>稀疏函数 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.embedding"><a class="reference internal" href="generated/torch.nn.functional.embedding.html#torch.nn.functional.embedding" title="torch.nn.functional.embedding"><code class="xref py py-obj docutils literal "><span class="pre">embedding</span></code></a></p></td>
<td><p>生成一个简单的查找表，用于在固定字典和大小中查找嵌入。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.embedding_bag"><a class="reference internal" href="generated/torch.nn.functional.embedding_bag.html#torch.nn.functional.embedding_bag" title="torch.nn.functional.embedding_bag"><code class="xref py py-obj docutils literal "><span class="pre">embedding_bag</span></code></a></p></td>
<td><p>计算嵌入包的总和、平均值或最大值。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.one_hot"><a class="reference internal" href="generated/torch.nn.functional.one_hot.html#torch.nn.functional.one_hot" title="torch.nn.functional.one_hot"><code class="xref py py-obj docutils literal "><span class="pre">one_hot</span></code></a></p></td>
<td><p>接受形状为 <code class="docutils literal "><span class="pre">(*)</span></code> 的 LongTensor，并返回形状为 <code class="docutils literal "><span class="pre">(*,</span> <span class="pre">num_classes)</span></code> 的张量，其中除最后一个维度的索引与输入张量的对应值匹配的地方为 1，其他地方为 0。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="distance-functions">
<h2>距离函数</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.pairwise_distance"><a class="reference internal" href="generated/torch.nn.functional.pairwise_distance.html#torch.nn.functional.pairwise_distance" title="torch.nn.functional.pairwise_distance"><code class="xref py py-obj docutils literal "><span class="pre">pairwise_distance</span></code></a></p></td>
<td><p>详细信息请见 <code class="xref py py-class docutils literal "><span class="pre">torch.nn.PairwiseDistance</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.cosine_similarity"><a class="reference internal" href="generated/torch.nn.functional.cosine_similarity.html#torch.nn.functional.cosine_similarity" title="torch.nn.functional.cosine_similarity"><code class="xref py py-obj docutils literal "><span class="pre">cosine_similarity</span></code></a></p></td>
<td><p>在 dim 维度上计算 <code class="docutils literal "><span class="pre">x1</span></code> 和 <code class="docutils literal "><span class="pre">x2</span></code> 之间的余弦相似度。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.pdist"><a class="reference internal" href="generated/torch.nn.functional.pdist.html#torch.nn.functional.pdist" title="torch.nn.functional.pdist"><code class="xref py py-obj docutils literal "><span class="pre">pdist</span></code></a></p></td>
<td><p>计算输入中每一对行向量的 p 范数距离。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="loss-functions">
<h2>损失函数</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.binary_cross_entropy"><a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy.html#torch.nn.functional.binary_cross_entropy" title="torch.nn.functional.binary_cross_entropy"><code class="xref py py-obj docutils literal "><span class="pre">binary_cross_entropy</span></code></a></p></td>
<td><p>测量目标概率和输入概率之间的二元交叉熵。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.binary_cross_entropy_with_logits"><a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy_with_logits.html#torch.nn.functional.binary_cross_entropy_with_logits" title="torch.nn.functional.binary_cross_entropy_with_logits"><code class="xref py py-obj docutils literal "><span class="pre">binary_cross_entropy_with_logits</span></code></a></p></td>
<td><p>计算目标与输入 logits 之间的二元交叉熵。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.poisson_nll_loss"><a class="reference internal" href="generated/torch.nn.functional.poisson_nll_loss.html#torch.nn.functional.poisson_nll_loss" title="torch.nn.functional.poisson_nll_loss"><code class="xref py py-obj docutils literal "><span class="pre">poisson_nll_loss</span></code></a></p></td>
<td><p>泊松负对数似然损失。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.cosine_embedding_loss"><a class="reference internal" href="generated/torch.nn.functional.cosine_embedding_loss.html#torch.nn.functional.cosine_embedding_loss" title="torch.nn.functional.cosine_embedding_loss"><code class="xref py py-obj docutils literal "><span class="pre">cosine_embedding_loss</span></code></a></p></td>
<td><p>详细内容请见 <code class="xref py py-class docutils literal "><span class="pre">CosineEmbeddingLoss</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.cross_entropy"><a class="reference internal" href="generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy" title="torch.nn.functional.cross_entropy"><code class="xref py py-obj docutils literal "><span class="pre">cross_entropy</span></code></a></p></td>
<td><p>计算输入 logits 与目标之间的交叉熵损失。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.ctc_loss"><a class="reference internal" href="generated/torch.nn.functional.ctc_loss.html#torch.nn.functional.ctc_loss" title="torch.nn.functional.ctc_loss"><code class="xref py py-obj docutils literal "><span class="pre">ctc_loss</span></code></a></p></td>
<td><p>应用连接主义时序分类损失。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.gaussian_nll_loss"><a class="reference internal" href="generated/torch.nn.functional.gaussian_nll_loss.html#torch.nn.functional.gaussian_nll_loss" title="torch.nn.functional.gaussian_nll_loss"><code class="xref py py-obj docutils literal "><span class="pre">gaussian_nll_loss</span></code></a></p></td>
<td><p>高斯负对数似然损失。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.hinge_embedding_loss"><a class="reference internal" href="generated/torch.nn.functional.hinge_embedding_loss.html#torch.nn.functional.hinge_embedding_loss" title="torch.nn.functional.hinge_embedding_loss"><code class="xref py py-obj docutils literal "><span class="pre">hinge_embedding_loss</span></code></a></p></td>
<td><p>详细内容请见 <code class="xref py py-class docutils literal "><span class="pre">HingeEmbeddingLoss</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.kl_div"><a class="reference internal" href="generated/torch.nn.functional.kl_div.html#torch.nn.functional.kl_div" title="torch.nn.functional.kl_div"><code class="xref py py-obj docutils literal "><span class="pre">kl_div</span></code></a></p></td>
<td><p>计算 KL 散度损失。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.l1_loss"><a class="reference internal" href="generated/torch.nn.functional.l1_loss.html#torch.nn.functional.l1_loss" title="torch.nn.functional.l1_loss"><code class="xref py py-obj docutils literal "><span class="pre">l1_loss</span></code></a></p></td>
<td><p>取元素间平均绝对值差的函数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.mse_loss"><a class="reference internal" href="generated/torch.nn.functional.mse_loss.html#torch.nn.functional.mse_loss" title="torch.nn.functional.mse_loss"><code class="xref py py-obj docutils literal "><span class="pre">mse_loss</span></code></a></p></td>
<td><p>测量元素间的平均平方误差，可选加权。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.margin_ranking_loss"><a class="reference internal" href="generated/torch.nn.functional.margin_ranking_loss.html#torch.nn.functional.margin_ranking_loss" title="torch.nn.functional.margin_ranking_loss"><code class="xref py py-obj docutils literal "><span class="pre">margin_ranking_loss</span></code></a></p></td>
<td><p>详细信息请见 <code class="xref py py-class docutils literal "><span class="pre">MarginRankingLoss</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.multilabel_margin_loss"><a class="reference internal" href="generated/torch.nn.functional.multilabel_margin_loss.html#torch.nn.functional.multilabel_margin_loss" title="torch.nn.functional.multilabel_margin_loss"><code class="xref py py-obj docutils literal "><span class="pre">multilabel_margin_loss</span></code></a></p></td>
<td><p>详细信息请见 <code class="xref py py-class docutils literal "><span class="pre">MultiLabelMarginLoss</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.multilabel_soft_margin_loss"><a class="reference internal" href="generated/torch.nn.functional.multilabel_soft_margin_loss.html#torch.nn.functional.multilabel_soft_margin_loss" title="torch.nn.functional.multilabel_soft_margin_loss"><code class="xref py py-obj docutils literal "><span class="pre">multilabel_soft_margin_loss</span></code></a></p></td>
<td><p>详细信息请见 <code class="xref py py-class docutils literal "><span class="pre">MultiLabelSoftMarginLoss</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.multi_margin_loss"><a class="reference internal" href="generated/torch.nn.functional.multi_margin_loss.html#torch.nn.functional.multi_margin_loss" title="torch.nn.functional.multi_margin_loss"><code class="xref py py-obj docutils literal "><span class="pre">multi_margin_loss</span></code></a></p></td>
<td><p>详细信息请见 <code class="xref py py-class docutils literal "><span class="pre">MultiMarginLoss</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.nll_loss"><a class="reference internal" href="generated/torch.nn.functional.nll_loss.html#torch.nn.functional.nll_loss" title="torch.nn.functional.nll_loss"><code class="xref py py-obj docutils literal "><span class="pre">nll_loss</span></code></a></p></td>
<td><p>计算负对数似然损失。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.huber_loss"><a class="reference internal" href="generated/torch.nn.functional.huber_loss.html#torch.nn.functional.huber_loss" title="torch.nn.functional.huber_loss"><code class="xref py py-obj docutils literal "><span class="pre">huber_loss</span></code></a></p></td>
<td><p>计算 Huber 损失，可选加权。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.smooth_l1_loss"><a class="reference internal" href="generated/torch.nn.functional.smooth_l1_loss.html#torch.nn.functional.smooth_l1_loss" title="torch.nn.functional.smooth_l1_loss"><code class="xref py py-obj docutils literal "><span class="pre">smooth_l1_loss</span></code></a></p></td>
<td><p>计算平滑 L1 损失。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.soft_margin_loss"><a class="reference internal" href="generated/torch.nn.functional.soft_margin_loss.html#torch.nn.functional.soft_margin_loss" title="torch.nn.functional.soft_margin_loss"><code class="xref py py-obj docutils literal "><span class="pre">soft_margin_loss</span></code></a></p></td>
<td><p>详细内容请见 <code class="xref py py-class docutils literal "><span class="pre">SoftMarginLoss</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.triplet_margin_loss"><a class="reference internal" href="generated/torch.nn.functional.triplet_margin_loss.html#torch.nn.functional.triplet_margin_loss" title="torch.nn.functional.triplet_margin_loss"><code class="xref py py-obj docutils literal "><span class="pre">triplet_margin_loss</span></code></a></p></td>
<td><p>计算给定输入张量之间的三元组损失，并设置一个大于 0 的间隔。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.triplet_margin_with_distance_loss"><a class="reference internal" href="generated/torch.nn.functional.triplet_margin_with_distance_loss.html#torch.nn.functional.triplet_margin_with_distance_loss" title="torch.nn.functional.triplet_margin_with_distance_loss"><code class="xref py py-obj docutils literal "><span class="pre">triplet_margin_with_distance_loss</span></code></a></p></td>
<td><p>使用自定义距离函数计算输入张量的三元组间隔损失。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="vision-functions">
<h2>沉浸式翻译功能</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.pixel_shuffle"><a class="reference internal" href="generated/torch.nn.functional.pixel_shuffle.html#torch.nn.functional.pixel_shuffle" title="torch.nn.functional.pixel_shuffle"><code class="xref py py-obj docutils literal "><span class="pre">pixel_shuffle</span></code></a></p></td>
<td><p>将形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>C</mi><mo>×</mo><msup><mi>r</mi><mn>2</mn></msup><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, C \times r^2, H, W)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mopen">(</span><span class="mord">∗</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.07153em;" class="mord mathnormal">C</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">×</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1.064108em;vertical-align:-0.25em;" class="strut"></span><span class="mord"><span style="margin-right:0.02778em;" class="mord mathnormal">r</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height:0.8141079999999999em;" class="vlist"><span style="top:-3.063em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.08125em;" class="mord mathnormal">H</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.13889em;" class="mord mathnormal">W</span><span class="mclose">)</span></span></span></span> 的张量元素重新排列为形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo>×</mo><mi>r</mi><mo separator="true">,</mo><mi>W</mi><mo>×</mo><mi>r</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, C, H \times r, W \times r)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mopen">(</span><span class="mord">∗</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.07153em;" class="mord mathnormal">C</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.08125em;" class="mord mathnormal">H</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">×</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.8777699999999999em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.02778em;" class="mord mathnormal">r</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.13889em;" class="mord mathnormal">W</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">×</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span style="margin-right:0.02778em;" class="mord mathnormal">r</span><span class="mclose">)</span></span></span></span> 的张量，其中 r 是 <code class="xref py py-attr docutils literal "><span class="pre">upscale_factor</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.pixel_unshuffle"><a class="reference internal" href="generated/torch.nn.functional.pixel_unshuffle.html#torch.nn.functional.pixel_unshuffle" title="torch.nn.functional.pixel_unshuffle"><code class="xref py py-obj docutils literal "><span class="pre">pixel_unshuffle</span></code></a></p></td>
<td><p>通过将形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo>×</mo><mi>r</mi><mo separator="true">,</mo><mi>W</mi><mo>×</mo><mi>r</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, C, H \times r, W \times r)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mopen">(</span><span class="mord">∗</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.07153em;" class="mord mathnormal">C</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.08125em;" class="mord mathnormal">H</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">×</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.8777699999999999em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.02778em;" class="mord mathnormal">r</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.13889em;" class="mord mathnormal">W</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">×</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span style="margin-right:0.02778em;" class="mord mathnormal">r</span><span class="mclose">)</span></span></span></span> 的张量元素重新排列为形状为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>C</mi><mo>×</mo><msup><mi>r</mi><mn>2</mn></msup><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, C \times r^2, H, W)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mopen">(</span><span class="mord">∗</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.07153em;" class="mord mathnormal">C</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">×</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:1.064108em;vertical-align:-0.25em;" class="strut"></span><span class="mord"><span style="margin-right:0.02778em;" class="mord mathnormal">r</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height:0.8141079999999999em;" class="vlist"><span style="top:-3.063em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.08125em;" class="mord mathnormal">H</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.13889em;" class="mord mathnormal">W</span><span class="mclose">)</span></span></span></span> 的张量来反转 <code class="xref py py-class docutils literal "><span class="pre">PixelShuffle</span></code> 操作，其中 r 是 <code class="xref py py-attr docutils literal "><span class="pre">downscale_factor</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.pad"><a class="reference internal" href="generated/torch.nn.functional.pad.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-obj docutils literal "><span class="pre">pad</span></code></a></p></td>
<td><p>填充张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.interpolate"><a class="reference internal" href="generated/torch.nn.functional.interpolate.html#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code class="xref py py-obj docutils literal "><span class="pre">interpolate</span></code></a></p></td>
<td><p>对输入进行下/上采样。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.upsample"><a class="reference internal" href="generated/torch.nn.functional.upsample.html#torch.nn.functional.upsample" title="torch.nn.functional.upsample"><code class="xref py py-obj docutils literal "><span class="pre">upsample</span></code></a></p></td>
<td><p>上采样输入。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.upsample_nearest"><a class="reference internal" href="generated/torch.nn.functional.upsample_nearest.html#torch.nn.functional.upsample_nearest" title="torch.nn.functional.upsample_nearest"><code class="xref py py-obj docutils literal "><span class="pre">upsample_nearest</span></code></a></p></td>
<td><p>使用最近邻像素值对输入进行上采样。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.upsample_bilinear"><a class="reference internal" href="generated/torch.nn.functional.upsample_bilinear.html#torch.nn.functional.upsample_bilinear" title="torch.nn.functional.upsample_bilinear"><code class="xref py py-obj docutils literal "><span class="pre">upsample_bilinear</span></code></a></p></td>
<td><p>使用双线性上采样对输入进行上采样。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.functional.grid_sample"><a class="reference internal" href="generated/torch.nn.functional.grid_sample.html#torch.nn.functional.grid_sample" title="torch.nn.functional.grid_sample"><code class="xref py py-obj docutils literal "><span class="pre">grid_sample</span></code></a></p></td>
<td><p>计算网格样本。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.functional.affine_grid"><a class="reference internal" href="generated/torch.nn.functional.affine_grid.html#torch.nn.functional.affine_grid" title="torch.nn.functional.affine_grid"><code class="xref py py-obj docutils literal "><span class="pre">affine_grid</span></code></a></p></td>
<td><p>根据一批仿射矩阵生成 2D 或 3D 流场（采样网格） <code class="xref py py-attr docutils literal "><span class="pre">theta</span></code> 。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="dataparallel-functions-multi-gpu-distributed">
<h2>DataParallel 函数（多 GPU、分布式）¶</h2>
<section id="data-parallel">
<h3><span class="hidden-section">data_parallel</span><a class="headerlink" href="#data-parallel" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal "><span class="pre">torch.nn.parallel.data_parallel</span></code></p></td>
<td><p>在给定的 device_ids 中并行评估 module(input)模块。</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        下一个 <img height="16" width="16" class="next-page" src="_static/images/chevron-right-orange.svg"> <img height="16" width="16" class="previous-page" src="_static/images/chevron-right-orange.svg"> 上一个
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>© 版权所有 PyTorch 贡献者。</p>
  </div>
    
      <div>使用 Sphinx 构建，主题由 Read the Docs 提供。</div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.nn.functional</a><ul>
<li><a class="reference internal" href="#convolution-functions">卷积函数</a></li>
<li><a class="reference internal" href="#pooling-functions">池化函数</a></li>
<li><a class="reference internal" href="#attention-mechanisms">注意力机制</a></li>
<li><a class="reference internal" href="#non-linear-activation-functions">非线性激活函数</a></li>
<li><a class="reference internal" href="#linear-functions">线性函数</a></li>
<li><a class="reference internal" href="#dropout-functions">Dropout 函数</a></li>
<li><a class="reference internal" href="#sparse-functions">稀疏函数</a></li>
<li><a class="reference internal" href="#distance-functions">距离函数</a></li>
<li><a class="reference internal" href="#loss-functions">损失函数</a></li>
<li><a class="reference internal" href="#vision-functions">视觉函数</a></li>
<li><a class="reference internal" href="#dataparallel-functions-multi-gpu-distributed">DataParallel 函数（多 GPU、分布式）</a><ul>
<li><a class="reference internal" href="#data-parallel"><span class="hidden-section">数据并行</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script="" type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0">


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>文档</h2>
          <p>PyTorch 开发者文档全面访问</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">查看文档</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>教程</h2>
          <p>获取初学者和高级开发者的深入教程</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">查看教程</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>资源</h2>
          <p>查找开发资源并获得您的疑问解答</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">查看资源</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">开始使用</a></li>
            <li><a href="https://pytorch.org/features">功能</a></li>
            <li><a href="https://pytorch.org/ecosystem">生态系统</a></li>
            <li><a href="https://pytorch.org/blog/">博客</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">贡献</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">资源</a></li>
            <li><a href="https://pytorch.org/tutorials">教程</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">文档</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">讨论</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub 问题和任务</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">品牌指南</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">保持更新</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">推特</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">领英</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch 播客</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">苹果</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">谷歌</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">亚马逊</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">条款</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">隐私</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© 版权所有 Linux 基金会。PyTorch 基金会是 Linux 基金会的一个项目。有关本网站的使用条款、商标政策以及其他适用于 PyTorch 基金会的政策，请参阅 www.linuxfoundation.org/policies/。PyTorch 基金会支持 PyTorch 开源项目，该项目已被确立为 LF Projects, LLC 的 PyTorch 项目系列。有关适用于 PyTorch 项目系列 LF Projects, LLC 的政策，请参阅 www.lfprojects.org/policies/。</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">为分析流量并优化您的体验，我们在本网站上提供 cookies。通过点击或导航，您同意允许我们使用 cookies。作为本站点的当前维护者，Facebook 的 Cookies 政策适用。了解更多信息，包括可用的控制选项：Cookies 政策。</p>
    <img class="close-button" src="_static/images/pytorch-x.svg" width="16" height="16">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>学习</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">开始学习</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">教程</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">学习基础知识</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch 菜谱</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">PyTorch 入门 - YouTube 系列</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>生态系统</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">工具</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">社区</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">论坛</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">开发者资源</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">贡献者奖项 - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">关于 PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch 文档</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>文档</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch 领域</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>博客 &amp; 新闻</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch 博客</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">社区博客</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">视频</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">社区故事</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">活动</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">通讯</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>关于</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch 基金会</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">治理委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">云信用计划</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">技术顾问委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">员工</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">联系我们</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

</body></html>