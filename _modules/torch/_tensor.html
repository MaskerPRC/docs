<!DOCTYPE html>
<html lang="zh_CN">
<head>
  <meta charset="UTF-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch._tensor — PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/_tensor.html">
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css">
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css">
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css">
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="../../_static/katex-math.css" type="text/css">
  <link rel="stylesheet" href="../../_static/sphinx-dropdown.css" type="text/css">
  <link rel="stylesheet" href="../../_static/panels-bootstrap.min.css" type="text/css">
  <link rel="stylesheet" href="../../_static/css/jit.css" type="text/css">
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css">
    <link rel="index" title="Index" href="../../genindex.html">
    <link rel="search" title="Search" href="../../search.html">

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head><body class="pytorch-body"><div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">学习</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class="dropdown-title">开始使用</span>
                  <p>在本地运行 PyTorch 或快速开始使用支持的云平台之一</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">教程</span><p></p>
                  <p>PyTorch 教程中的新内容</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">学习基础知识</span><p></p>
                  <p>熟悉 PyTorch 概念和模块</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch 烹饪技巧</span><p></p>
                  <p>精简型、可直接部署的 PyTorch 代码示例</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">PyTorch 入门 - YouTube 系列</span><p></p>
                  <p>通过我们引人入胜的 YouTube 教程系列掌握 PyTorch 基础知识</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">生态系统</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">工具</span><p></p>
                  <p>了解 PyTorch 生态系统中的工具和框架</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">社区</span>
                  <p>加入 PyTorch 开发者社区，贡献、学习并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">论坛</span>
                  <p>讨论 PyTorch 代码、问题、安装、研究的地方</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">开发者资源</span>
                  <p>查找资源并解答问题</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">2024 年度贡献者奖项</span><p></p>
                  <p>本届 PyTorch 会议揭晓获奖者</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">关于 PyTorch Edge</span><p></p>
                  <p>为边缘设备构建创新和隐私感知的 AI 体验</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span><p></p>
                  <p>针对移动和边缘设备实现设备端推理能力的端到端解决方案</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch 文档</span><p></p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">文档</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span><p></p>
                  <p>探索文档以获取如何使用 PyTorch 的全面指南</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch 领域</span><p></p>
                  <p>阅读 PyTorch 领域的文档以了解更多关于特定领域的库</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">博客与新闻</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch 博客</span><p></p>
                  <p>跟上最新的技术新闻和动态</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">社区博客</span><p></p>
                  <p>PyTorch 生态系统故事</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">视频</span><p></p>
                  <p>了解最新的 PyTorch 教程、新内容等</p>
                </a><a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">社区故事</span><p></p>
                  <p>了解我们的社区如何使用 PyTorch 解决真实、日常的机器学习问题</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">活动</span><p></p>
                  <p>查找活动、网络研讨会和播客</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">通讯</span><p></p>
                  <p>保持最新更新</p>
                </a>
            </div>
          </div></li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">关于</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch 基金会</span><p></p>
                  <p>了解更多关于 PyTorch 基金会的信息</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">管理委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">云信用计划</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">技术顾问委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">员工</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">联系我们</span><p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">成为会员</a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>



   

    

    <div class="table-of-contents-link-wrapper">
      <span>目录</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="https://pytorch.org/docs/versions.html">主页 (2.7.0+cpu ) ▼</a>
    </div>
    <div id="searchBox">
    <div class="searchbox" id="googleSearchBox">
      <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
      <div class="gcse-search"></div>
    </div>
    <div id="sphinxSearchBox" style="display: none;">
      <div role="search">
        <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
          <input type="text" name="q" placeholder="Search Docs">
          <input type="hidden" name="check_keywords" value="yes">
          <input type="hidden" name="area" value="default">
        </form>
      </div>
    </div>
  </div>
  <form id="searchForm">
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="google" checked="">Google 搜索</label>
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="sphinx">经典搜索</label>
  </form>

  <script>
     document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
  </script>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/_tensor.html">您正在查看不稳定开发者预览文档。请点击此处查看最新稳定版本的文档。</a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">社区</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../community/build_ci_governance.html">PyTorch 治理 | 构建 + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/contribution_guide.html">PyTorch 贡献指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/design.html">PyTorch 设计哲学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/governance.html">PyTorch 治理 | 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/persons_of_interest.html">PyTorch 治理 | 维护者</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者笔记</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../notes/amp_examples.html">自动混合精度示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/autograd.html">Autograd 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/broadcasting.html">广播语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cpu_threading_torchscript_inference.html">CPU 线程和 TorchScript 推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cuda.html">CUDA 语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/custom_operators.html">PyTorch 自定义操作着陆页面</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/ddp.html">分布式数据并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.html">扩展 PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.func.html">扩展 torch.func 与 autograd.Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/faq.html">常见问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/fsdp.html">FSDP 笔记</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/get_start_xpu.html">在英特尔 GPU 上入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/gradcheck.html">毕业审核力学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/hip.html">HIP (ROCm) 语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/large_scale_deployments.html">大规模部署功能</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/libtorch_stable_abi.html">LibTorch 稳定 ABI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/modules.html">模块</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/mps.html">MPS 后端</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/multiprocessing.html">多进程最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/numerical_accuracy.html">数值精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/randomness.html">可复现性</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/serialization.html">序列化语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/windows.html">Windows 常见问题解答</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语言绑定</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_attributes.html">Tensor 属性</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_view.html">张量视图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../accelerator.html">torch 加速器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_cuda_memory.html">理解 CUDA 内存使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_cuda_memory.html#generating-a-snapshot">生成快照</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_cuda_memory.html#using-the-visualizer">使用可视化器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_cuda_memory.html#snapshot-api-reference">快照 API 参考</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mtia.memory.html">torch.mtia.memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../meta.html">元设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.tensor.html">torch.distributed.tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.fsdp.fully_shard.html">torch.distributed.fsdp.fully_shard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../complex_numbers.html">复数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ddp_comm_hooks.html">DDP 通信钩子</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quantization.html">量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rpc.html">分布式 RPC 框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../storage.html">torch 存储</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../type_info.html">类型信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../named_tensor.html">命名张量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../name_inference.html">命名张量操作覆盖率</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../torch_environment_variables.html">火炬环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">图书馆</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">火炬推荐</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch 在 XLA 设备上</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/ao">torchao</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        文档 &gt;</li>

        
          <li>模块代码 &gt;</li>
        
          <li><a href="../torch.html">torch</a> &gt;</li>
        
      <li>torch._tensor</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">快捷键</div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>torch._tensor 的源代码</font></font></font></h1><div class="highlight"><pre><span></span><span class="c1"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "># mypy: 允许未类型化定义</span>
<span class="kn">导入</span> <span class="nn">copyreg</span>
<span class="kn">导入</font></font></font></span> <span class="nn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">枚举</span>
<span class="kn">导入</span> <span class="nn">functools</span>
<span class="kn">导入</font></font></font></span> <span class="nn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">警告</span>
<span class="kn">来自</font></font></font></span> <span class="nn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">集合</font></font></font></span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</span> <span class="n">OrderedDict</span>
<span class="kn">来自</font></font></font></span> <span class="nn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">复制</font></font></font></span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">深拷贝</span>
<span class="kn">来自</font></font></font></span> <span class="nn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数字</font></font></font></span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数</span>
<span class="kn">来自</font></font></font></span> <span class="nn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">打字</font></font></font></span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">任何</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">可调用</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">角色</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">可选</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">联合</span>

<span class="kn">导入</font></font></font></span> <span class="nn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">火炬</span>
<span class="kn">导入</span> <span class="nn">torch._C</span> <span class="k">as</span> <span class="nn">_C</span>
<span class="kn">来自</font></font></font></span> <span class="nn">torch._namedtensor_internals</span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</span> <span class="p">(</span>
    <span class="n">检查序列化命名张量</span><span class="p">,</span>
    <span class="n">是否省略号</span><span class="p">,</span>
    <span class="n">解析省略号</span><span class="p">,</span>
    <span class="n">单个省略号索引</span><span class="p">,</span>
    <span class="n">解压命名形状</span><span class="p">,</span>
    <span class="n">更新名称</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">来自</font></font></font></span> <span class="nn">torch.overrides</span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</span> <span class="p">(</span>
    <span class="n">获取默认不换行函数</span><span class="p">,</span>
    <span class="n">handle_torch_function</span><span class="p">,</span>
    <span class="n">有 torch 功能</span><span class="p">,</span>
    <span class="n">has_torch_function_unary</span><span class="p">,</span>
    <span class="n">has_torch_function_variadic</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">处理 torch 函数并将类型错误包装为未实现</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="n">分配</font></font></font></span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">包装任务</span>

    <span class="nd">@functools</span><span class="o">.</span><span class="n">包装</font></font></font></span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">分配</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">分配</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">包装</font></font></font></span><span class="p">(</span><span class="o">*</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">尝试</span><span class="p">:</span>
            <span class="c1"># See https://github.com/pytorch/pytorch/issues/75462</span>
            <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">有 torch 功能</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</span><span class="p">):</span>
                <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">包装</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</font></font></font></span><span class="p">,</span> <span class="o">*</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">返回</font></font></font></span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">除了</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型错误</span><span class="p">:</span>
            <span class="k">返回</span> <span class="bp">NotImplemented</span>

    <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">包装</span>


<span class="c1"># 应该不使用，仅为了向后兼容加载旧的序列化张量子类</span>
<span class="k">def</span> <span class="nf">从类型重建</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">函数</font></font></font></span><span class="p">,</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</font></font></font></span><span class="p">,</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">字典</span><span class="p">):</span>
    <span class="k">如果</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="p">:</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">函数</font></font></font></span><span class="p">(</span><span class="o">*</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</span><span class="p">)</span>

    <span class="n">返回</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">函数</font></font></font></span><span class="p">(</span><span class="o">*</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</font></font></font></span><span class="p">)</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">作为子类</font></font></font></span><span class="p">(</span><span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</span><span class="p">)</span>
    <span class="n">返回</font></font></font></span><span class="o">.</span><span class="vm"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">字典</font></font></font></span> <span class="o">=</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">字典</span>
    <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</span>


<span class="k">def</span> <span class="nf">从类型重建_v2</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">函数</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">新类型</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</span><span class="p">):</span>
    <span class="n">返回</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">函数</font></font></font></span><span class="p">(</span><span class="o">*</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</span><span class="p">)</span>
    <span class="k">如果</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</font></font></font></span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">新类型</span><span class="p">:</span>
        <span class="n">返回</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">作为子类</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">新类型</span><span class="p">)</span>
    <span class="c1"># 张量即使没有定义 __setstate__，也定义了 __getstate__。所以只有当它不是在 Tensor 上定义的时才使用 __setstate__</span>
    <span class="c1"># __getstate__。所以只有使用 __setstate__ 如果它不是 Tensor 上定义的那个</span>
    <span class="c1"># 的</span>
    <span class="k">如果</span> <span class="p">(</span>
        <span class="nb">getattr</span><span class="p">(</span><span class="n">返回</font></font></font></span><span class="o">.</span><span class="vm"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类</font></font></font></span><span class="p">,</span> <span class="s2">"__setstate__"</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">)</span>
        <span class="ow">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="o">.</span><span class="n">__setstate__</span>
    <span class="p">):</span>
        <span class="n">返回</font></font></font></span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</span><span class="p">)</span>
    <span class="k">否则</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_set_obj_state</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</span><span class="p">)</span>
    <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</span>


<span class="k">def</span> <span class="nf">_dtype_to_typestr</span><span class="p">(</span><span class="n">数据类型</span><span class="p">):</span>
    <span class="c1"># CUDA 设备为小端序，张量以本地字节顺序存储</span>
    <span class="c1"># 1 字节条目是端序无关的。</span>
    <span class="k">返回</span> <span class="p">{</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span> <span class="s2">"&lt;c8"</span><span class="p">,</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span> <span class="s2">"&lt;c16"</span><span class="p">,</span>
        <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span> <span class="s2">"&lt;V2"</span><span class="p">,</span>  <span class="c1"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "># 相同于 ml_dtypes.bfloat16.dtype.str.</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span> <span class="s2">"&lt;f2"</span><span class="p">,</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span> <span class="s2">"&lt;f4"</span><span class="p">,</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span> <span class="s2">"&lt;f8"</span><span class="p">,</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span> <span class="s2">"|u1"</span><span class="p">,</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="s2">"|i1"</span><span class="p">,</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">uint16</span><span class="p">:</span> <span class="s2">"&lt;u2"</span><span class="p">,</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="s2">"&lt;i2"</span><span class="p">,</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">uint32</span><span class="p">:</span> <span class="s2">"&lt;u4"</span><span class="p">,</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span> <span class="s2">"&lt;i4"</span><span class="p">,</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">uint64</span><span class="p">:</span> <span class="s2">"&lt;u8"</span><span class="p">,</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span> <span class="s2">"&lt;i8"</span><span class="p">,</span>
        <span class="n">PyTorch</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span> <span class="s2">"|b1"</span><span class="p">,</span>
    <span class="p">}</font></font></font></span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据类型</span><span class="p">]</span>


<span class="c1"># 如果您想子类化 Tensor，并且想要跨进程共享子类化的类</span>
<span class="c1"># 您还必须更新 torch/multiprocessing/reductions.py</span>
<span class="c1"># 为该类定义一个 ForkingPickler 序列化模式。</span>
<span class="c1">#</span>
<span class="c1"># NB: 如果您向 Tensor 添加新方法，您必须更新</span>
<span class="c1"># torch/_C/__init__.pyi.in 以添加您的方法的类型注解；</span>
<span class="c1"># 否则，它将不会显示在自动完成中。</span>
<span class="k">类</font></font></font></span> <span class="nc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="p">):</span>
    <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_is_param
是参数</font></font></font></span><span class="p">:</span> <span class="nb">布尔值</span>

    <span class="k">def</span> <span class="nf">清除非序列化缓存数据</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">清除张量`__dict__`中任何可能阻止张量</span>
<span class="sd">从序列化中。</span>

<span class="sd">例如，具有自定义分派大小/步长的子类将此信息缓存在</span>
<span class="sd">``__dict__`` 中的非序列化 PyCapsules 中，并且必须在序列化到函数之前清除。</span>
<span class="sd">任何覆盖此方法的子类都必须调用 ``super()._clear_non_serializable_cached_data().``</span>

<span class="sd">任何覆盖此方法的子类都必须调用 ``super()._clear_non_serializable_cached_data().``</span>
<span class="sd">在覆盖中清除的附加数据必须能够透明地重新缓存</span>
<span class="sd">以避免破坏子类功能。</span>
<span class="sd">        """</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_清除非序列化缓存数据</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span>
            <span class="p">)</span>
        <span class="c1"># 注意：实现自定义分派大小/步长缓存的包装子类</span>
        <span class="c1"># 此信息通过非序列化 PyCapsules 获取。</span>
        <span class="n">缓存的大小、步长和键</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">_sym_sizes_capsule</span><span class="p">,</span>
            <span class="s2">_sym_sizes_capsule_len</span><span class="p">,</span>
            <span class="s2">“对称步长胶囊”</span><span class="p">,</span>
            <span class="s2">“对称步长胶囊长度”</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">键</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">“缓存的大小、步长和键”</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="vm">字典</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">流行</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">键</font></font></font></span><span class="p">,</span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">深拷贝</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">描述</span><span class="p">):</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">深拷贝</font></font></font></span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">描述</span><span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是否为叶子节点</span><span class="p">:</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                <span class="s2">“仅由用户显式创建的张量”</span>
                <span class="s2">"(图叶子)目前支持 deepcopy 协议。"</span>
                <span class="s2">"如果你尝试 deepcopy 一个模块，这可能是因为"</span>
                <span class="s2">"torch.nn.utils.weight_norm 的使用，"</span>
                <span class="s2">"请参阅 https://github.com/pytorch/pytorch/pull/103001"</span>
            <span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">描述</span><span class="p">:</span>
            <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">描述</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span>
        <span class="k">替换为</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不梯度</span><span class="p">():</span>
            <span class="c1"># TODO: 跳过存储副本是元数据错误，因为元数据</span>
            <span class="c1"># 精确的别名跟踪；然而，下面的代码</span>
            <span class="c1">因为 # 不起作用</span>
            <span class="c1"># https://github.com/pytorch/pytorch/issues/47442</span>
            <span class="c1"># 如果您从这里删除 'meta'，请更新 test_serialization 中的测试</span>
            <span class="k">如果</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span>
                <span class="ow">或者</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</span>
                <span class="ow">在</font></font></font></span> <span class="p">[</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">懒惰</font></font></font></span><span class="p">,</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">轻快</font></font></font></span><span class="p">,</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">翻译</font></font></font></span><span class="p">,</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">国会议员</font></font></font></span><span class="p">,</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">美亚</font></font></font></span><span class="p">,</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元数据</font></font></font></span><span class="p">,</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">知识产权</span><span class="p">]</span>
                <span class="ow">或者</span> <span class="p">(</span>
                    <span class="ow">不</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">有存储</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
                    <span class="ow">和</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span> <span class="o">==</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="ow">或者</font></font></font></span> <span class="p">(</span><span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">和</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据指针</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">新张量</font></font></font></span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">克隆</span><span class="p">()</span>
                <span class="k">如果</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">新张量</font></font></font></span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                    <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                        <span class="s2">"__deepcopy__()"的默认实现仅适用于包装子类的子类</span>
                        <span class="s2">"实现 clone()并且克隆返回同一子类实例的类型"</span>
                        <span class="s2">"您应该为您的子类正确实现 clone()或重写__deepcopy__()"</span>
                        <span class="s2">"__deepcopy__()"方法"</span>
                        <span class="s2">"如果 clone()返回不同类型的实例是预期行为，那么"</span>
                        <span class="s2">"就是这样的。"</span>
                    <span class="p">)</span>
            <span class="k">否则</span><span class="p">:</span>
                <span class="n">新存储</font></font></font></span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">深拷贝</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">描述</span><span class="p">)</span>
                <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是否量化</span><span class="p">:</span>
                    <span class="c1"># quantizer_params 的类型可以根据 torch 属性的不同而不同</span>
                    <span class="n">量化器参数</font></font></font></span><span class="p">:</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">联合</span><span class="p">[</span>
                        <span class="nb">元组</font></font></font></span><span class="p">[</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">q 方案</font></font></font></span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">]</span>
                        <span class="nb">元组</font></font></font></span><span class="p">[</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">q 方案</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="p">,</span> <span class="nb">int</span><span class="p"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">]</span>
                    <span class="p">]</span>
                    <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">q 方案</font></font></font></span><span class="p">()</span> <span class="o">==</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">每张张量仿射</span><span class="p">:</span>
                        <span class="n">量化器参数</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q 方案</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_scale</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_zero_point</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="k">如果...否则</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">q 方案</font></font></font></span><span class="p">()</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</span> <span class="p">(</span>
                        <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">单通道仿射变换</span><span class="p">,</span>
                        <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">单通道仿射变换浮点 Q 参数</span><span class="p">,</span>
                    <span class="p">):</span>
                        <span class="n">量化器参数</span> <span class="o">=</span> <span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">q 方案</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">每通道缩放的 q</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">q_per_channel_zero_points
q_per_channel_零点</font></font></font></span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">每通道轴的 q</span><span class="p">(),</span>
                        <span class="p">)</span>
                    <span class="k">否则</span><span class="p">:</span>
                        <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">不支持 Q 方案</font></font></font></span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">q 方案</font></font></font></span><span class="p">()</span><span class="si">}</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在 deepcopy"中</span>
                        <span class="p">)</span>
                    <span class="c1"># TODO: 一旦我们决定中断序列化 FC，就不再</span>
                    <span class="c1"># 需要使用 TypedStorage 包装</span>
                    <span class="n">新张量</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">重建 qtensor</span><span class="p">(</span>
                        <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">存储</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型化存储</span><span class="p">(</span>
                            <span class="n">包装存储</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">新存储</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">未类型化存储</span><span class="p">,</span>
                            <span class="n">数据类型</font></font></font></span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据类型</span><span class="p">,</span>
                            <span class="n">内部</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="p">),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">存储偏移</span><span class="p">(),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">尺寸</span><span class="p">(),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">步长</span><span class="p">(),</span>
                        <span class="n">量化器参数</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">需要梯度</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="k">如果</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">新张量</font></font></font></span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                            <span class="s2">"__deepcopy__()"的默认实现，用于量化张量</span>
                            <span class="s2">"期望 torch._utils._rebuild_qtensor()返回的 tensor 与被复制的实例类型匹配。如果您遇到这种情况，"</span>
                            <span class="s2">"请在 PyTorch 的 GitHub 上提交一个 issue。"</span>
                            <span class="s2">"请打开 PyTorch 的 GitHub 上的一个 issue。"</span>
                        <span class="p">)</span>
                <span class="k">否则</span><span class="p">:</span>
                    <span class="n">新 tensor</font></font></font></span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">新空</span><span class="p">([])</span>
                    <span class="k">如果</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">新张量</font></font></font></span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                            <span class="s2">"__deepcopy__()"的默认实现仅适用于非包装子类</span>
                            <span class="s2">"且这些子类实现了 new_empty()方法，并且该方法返回同一子类的另一个实例。你应该"</span>
                            <span class="s2">"为此实现自己的__deepcopy__()方法。"</span>
                            <span class="s2">"正确实现子类中的 new_empty() 方法，或者覆盖 "</span>
                            <span class="s2">"__deepcopy__()，如果 new_empty() 返回不同类型的实例是预期行为的话，"</span>
                            <span class="s2">"则返回一个不同类型的实例。"</span>
                        <span class="p">)</span>
                    <span class="n">新张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">集合</span><span class="p">(</span>
                        <span class="n">新存储</font></font></font></span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">存储偏移</font></font></font></span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">尺寸</font></font></font></span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">步长</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是连词</span><span class="p">():</span>
                        <span class="n">新张量</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">新张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">物理连词</span><span class="p">()</span>
                    <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是否定</span><span class="p">():</span>
                        <span class="n">新张量</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">新张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">否定</span><span class="p">()</span>
            <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">需要梯度</span><span class="p">:</span>
                <span class="n">新张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">需要梯度_</span><span class="p">()</span>
            <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">梯度</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span><span class="p">:</span>
                <span class="n">新张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">梯度</font></font></font></span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">研究生</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">深拷贝</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">描述</span><span class="p">)</span>

            <span class="k">如果</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="p">:</span>
                <span class="k">如果</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">新张量</font></font></font></span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                    <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                        <span class="s2">"深度拷贝的结果类型与源张量类型不匹配。"</span>
                        <span class="s2">"如果您遇到这种情况，请在 PyTorch 的 GitHub 上提交一个 issue。"</span>
                    <span class="p">)</span>

                <span class="c1"># 纯张量没有槽位</span>
                <span class="n">slots_to_save</span> <span class="o">=</span> <span class="n">复制注册</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_槽位名称</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类</font></font></font></span><span class="p">)</span>  <span class="c1"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "># 类型：忽略[已定义]</span>
                <span class="k">for</span> <span class="n">槽</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">要保存的槽位</span><span class="p">:</span>
                    <span class="k">如果</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">有属性</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">槽</span><span class="p">):</span>
                        <span class="nb">setattr</span><span class="p">(</span><span class="n">新张量</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">槽</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">深拷贝</font></font></font></span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">槽</font></font></font></span><span class="p">),</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">描述</span><span class="p">))</span>

            <span class="c1"># 不要尝试深拷贝非序列化缓存数据</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_清除非序列化缓存数据</span><span class="p">()</span>
            <span class="n">新张量</font></font></font></span><span class="o">.</span><span class="vm"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">字典</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">深拷贝</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">字典</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">描述</span><span class="p">)</span>

            <span class="n">描述</font></font></font></span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">新张量</span>
            <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">新张量</span>

    <span class="k">def</span> <span class="nf">__reduce_ex__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">proto</span><span class="p">):</span>
        <span class="n">materialize_fake_tensors</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">序列化</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">序列化 TLS</span><span class="o">.</span><span class="n">materialize_fake_tensors</span>
        <span class="p">)</span>
        <span class="n">状态</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_获取对象状态</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="c1">使用 FakeTensor 时，由于 FakeTensor 具有无法序列化的状态，请忽略所有状态，使用 skip_data(materialize_fake_tensors)</span>
        <span class="c1"># 一些无法序列化的状态</span>
        <span class="k">如果</span> <span class="p">(</span>
            <span class="c1"># TODO: 删除 hasattr，这是一个为了支持 torch 的某些版本而采用的 hack 方法</span>
            <span class="c1">没有子类</span>
            <span class="nb">有属性</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="p">,</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">"_子类"</span><span class="p">)</span>
            <span class="ow">和</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">子类</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">模拟张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">模拟 Tensor</span>
            <span class="ow">和</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">实现伪造张量</span>
        <span class="p">)</span> <span class="ow">或者</font></font></font></span> <span class="p">(</span><span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">和</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</span><span class="p">):</span>
            <span class="c1">定制路径用于没有 Python 状态的常规张量。</span>
            <span class="k">返回</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_ex_internal</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">协议</span><span class="p">)</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="n">__reduce_ex__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">协议</span><span class="p">)</span>
        <span class="n">函数</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_ex_internal</span><span class="p">(</span><span class="n">proto</span><span class="p">)</span>
        <span class="c1"># 需要在这里清除大小/步长缓存，因为它将被重新缓存</span>
        <span class="c1"># 如果提前清除。注意，状态引用的是实际的张量字典。</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_清除非序列化缓存数据</span><span class="p">()</span>
        <span class="k">返回</font></font></font></span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">从类型重建_v2</font></font></font></span><span class="p">,</span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">函数</font></font></font></span><span class="p">,</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</span><span class="p">))</span>

<div class="viewcode-block" id="Tensor.storage"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def storage(self):
r"""
storage() -&gt; torch.TypedStorage

返回底层的 :class:`TypedStorage`。

.. 警告::

class:`TypedStorage` 已弃用。它将在未来被移除，而
class:`UntypedStorage` 将成为唯一的存储类。要访问
直接使用 :class:`UntypedStorage`，请使用 :attr:`Tensor.untyped_storage()`。
"""
如果有 torch_function_unary(self)：
返回 handle_torch_function(Tensor.storage, (self,), self)

torch.storage._warn_typed_storage_removal(stacklevel=2)
return self._typed_storage()</font></font></font></div>

    <span class="c1">仅限内部使用，避免引发弃用警告</span>
    <span class="k">def</span> <span class="nf">_typed_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">未类型化存储</font></font></font></span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">未类型化存储</span><span class="p">()</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型化存储</span><span class="p">(</span>
            <span class="n">包装存储</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">未类型化存储</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据类型</font></font></font></span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据类型</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">内部</font></font></font></span><span class="o">=</span><span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">真实</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_reduce_ex_internal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">原型</span><span class="p">):</span>
        <span class="n">检查序列化命名张量</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="kn">来自</font></font></font></span> <span class="nn">torch.utils.hooks</span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">如果有钩子则警告</span>

        <span class="c1"># 请参阅注释[不要序列化钩子]</span>
        <span class="n">警告如果存在钩子</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">向后钩子</font></font></font></span><span class="p">:</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">字典</font></font></font></span><span class="p">[</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">任何</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">任何</font></font></font></span><span class="p">]</span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">有序字典</span><span class="p">()</span>

        <span class="n">跳过数据</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">序列化</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">序列化 TLS</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">跳过数据</span>
        <span class="n">材料化假张量</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">序列化</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">序列化 TLS</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">实现伪造张量</span>
        <span class="p">)</span>

        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="p">[</span><span class="s2">"xla"</span><span class="p">,</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">"玛雅"</font></font></font></span><span class="p">]</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">或者</span> <span class="p">(</span>
            <span class="ow">不</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_has_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="ow">和</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span> <span class="o">==</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">跳过数据</span><span class="p">:</span>
                <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                    <span class="s2">无法在跳过数据上下文管理器下将张量序列化到没有存储的后端</span>
                <span class="p">)</span>
            <span class="n">CPU 张量</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="k">返回</span> <span class="p">(</span>
                <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_工具</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">从 CPU 张量重建设备张量</span><span class="p">,</span>
                <span class="p">(</span><span class="n">cpu_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">数据类型</font></font></font></span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">需要梯度</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="c1"># 旧评论已不再适用。</span>
        <span class="c1"># 注意：Numpy 数组被选为 XLA、MTIA、MAIA 张量的重建组件。</span>
        <span class="c1"># 我们考虑了几种选择：</span>
        <span class="c1"># 1. 这里不能使用 CPU 张量。</span>
        <span class="c1">否则在 torch.load 中，CPU 存储以随机方式重建</span>
        <span class="c1">初始化数据，移动到后端设备，然后更新存储</span>
        <span class="c1">将序列化内容添加到其中。这对于 CPU/CUDA 来说效果很好，但不适用于这些后端。</span>
        <span class="c1">他们的张量与存储断开连接，因此不会收到更新。</span>
        <span class="c1">由于性能原因，Python 列表不是一个好的选择。</span>
        <span class="c1">`tolist()`会将张量中的每个元素都转换为 Python 对象。</span>
        <span class="c1">然后将它们一个接一个地序列化。</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="p">[</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">翻译</font></font></font></span><span class="p"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">]</span>
            <span class="c1">在将 BFloat16 张量转换为 numpy 之前，请将其转换为 Float32，因为 numpy 不支持 BFloat16。</span>
            <span class="c1">由于 numpy 不支持 BFloat16，重建张量时将接受原始 self.dtype，从而从 numpy 中重建 BFloat16 张量。</span>
            <span class="c1">这将重建 numpy 中的 BFloat16 张量。</span>
            <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">跳过数据</span><span class="p">:</span>
                <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                    <span class="s2">在 skip_data 上下文管理器下，无法序列化没有存储的 backend 上的张量。</span>
                <span class="p">)</span>
            <span class="n">numpy 张量</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</span><span class="o">.</span><span class="n">bfloat16</span>
                <span class="k">否则</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">到</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">返回</span> <span class="p">(</span>
                <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_工具</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">从 NumPy 重建设备张量</span><span class="p">,</span>
                <span class="p">(</span><span class="n">numpy 张量</font></font></font></span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据类型</font></font></font></span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">需要梯度</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span> <span class="o">==</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元数据</span><span class="p">:</span>
            <span class="c1"># 注意：此实现会破坏存储共享。当前</span>
            <span class="c1">没有人关心元张量。</span>
            <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">跳过数据</span><span class="p">:</span>
                <span class="n">警告</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">警告</span><span class="p">(</span>
                    <span class="s2">在跳过数据上下文管理器下，将张量序列化到元设备上是一个无操作。</span>
                <span class="p">)</span>
            <span class="n">arg_meta</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">数据类型</span><span class="p">,</span>
                <span class="nb">元组</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">尺寸</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">步长</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">需要梯度</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">返回</font></font></font></span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_工具</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">重建元张量不存储</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数元</span><span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是否量化</span><span class="p">:</span>
            <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">跳过数据</span><span class="p">:</span>
                <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                    <span class="s2">"无法在跳过数据上下文管理器下序列化 qtensor，如需此功能请提交问题"</span>
                <span class="p">)</span>
            <span class="c1"># quantizer_params 的类型可以根据 torch 属性的不同而不同</span>
            <span class="n">量化器参数</font></font></font></span><span class="p">:</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">联合</span><span class="p">[</span>
                <span class="nb">元组</font></font></font></span><span class="p">[</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">q 方案</font></font></font></span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">]</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元组</font></font></font></span><span class="p">[</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">任何</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
            <span class="p">]</span>
            <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">q 方案</font></font></font></span><span class="p">()</span> <span class="o">==</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">每张张量仿射</span><span class="p">:</span>
                <span class="n">量化器参数</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">每张张量仿射</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_scale</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">q_zero_point</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">如果...否则</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">q 方案</font></font></font></span><span class="p">()</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</span> <span class="p">(</span>
                <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">每通道仿射</span><span class="p">,</span>
                <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">每通道仿射浮点量化参数</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="c1"># 将尺度因子和零点转换为元组以避免递归调用</span>
                <span class="c1"># 当/如果未来我们得到多轴量化张量，形状</span>
                <span class="c1"># 可以从主张量形状中恢复</span>
                <span class="n">量化参数</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">每通道仿射变换</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">每通道缩放的 q</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">q_per_channel_zero_points
q_per_channel_零点</font></font></font></span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">每通道轴的 q</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">否则</span><span class="p">:</span>
                <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"不支持序列化类型为张量的序列化"</font></font></font></span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">q 方案</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span>
                <span class="p">)</span>
            <span class="c1"># TODO: 一旦我们决定在断开序列化 FC 后不再</span>
            <span class="c1">需要使用 TypedStorage 包装</span>
            <span class="n">args_qtensor</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">存储</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型化存储</span><span class="p">(</span>
                    <span class="n">包装存储</font></font></font></span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">未类型化存储</span><span class="p">,</span>
                    <span class="n">数据类型</font></font></font></span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据类型</span><span class="p">,</span>
                    <span class="n">内部</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                <span class="nb">元组</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">尺寸</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">步长</span><span class="p">(),</span>
                <span class="n">量化器参数</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">需要梯度</span><span class="p">,</span>
                <span class="n">后向钩子</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">返回</font></font></font></span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_工具</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">重建_qtensor</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">args_qtensor 参数</span><span class="p">)</span>
        <span class="k">如果...否则</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">布局</font></font></font></span> <span class="o">==</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">稀疏 COO</span><span class="p">:</span>
                <span class="n">args_sparse 稀疏参数</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">布局</span><span class="p">,</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">索引</font></font></font></span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">值</font></font></font></span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">尺寸</font></font></font></span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是否合并</span><span class="p">()),</span>
                <span class="p">)</span>
            <span class="k">否则</span><span class="p">:</span>
                <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不支持的操作异常</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">稀疏张量 __reduce_ex__ 用于布局 `</font></font></font></span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">布局</font></font></font></span><span class="si">}</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "> </span>
                <span class="p">)</span>
            <span class="k">返回</font></font></font></span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_工具</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">重建稀疏张量</span><span class="p">,</span> <span class="n">args_sparse</span><span class="p">)</span>
        <span class="k">如果...否则</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">布局</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</span> <span class="p">{</span>
            <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">稀疏压缩存储格式</span><span class="p">,</span>
            <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">稀疏压缩存储格式</span><span class="p">,</span>
            <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">稀疏_bsr</span><span class="p">,</span>
            <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">稀疏矩阵</span><span class="p">,</span>
        <span class="p">}:</span>
            <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">布局</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="p">{</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">稀疏压缩存储格式</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">稀疏_bsr</span><span class="p">}:</span>
                <span class="n">压缩索引</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">平凡索引</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">鸡索引</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">列索引</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="k">否则</span><span class="p">:</span>
                <span class="n">压缩索引</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">平凡索引</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">c 列索引</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">行索引</span><span class="p">(),</span>
                <span class="p">)</span>
            <span class="n">稀疏压缩参数</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">布局</span><span class="p">,</span>
                <span class="p">(</span>
                    <span class="n">压缩索引</span><span class="p">,</span>
                    <span class="n">平凡索引</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">值</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">尺寸</span><span class="p">(),</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="k">返回</font></font></font></span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_工具</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">重建稀疏张量</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">args 稀疏压缩</span><span class="p">)</span>
        <span class="k">如果...否则</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是嵌套的</span><span class="p">:</span>
            <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">跳过数据</span><span class="p">:</span>
                <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                    <span class="s2">在 skip_data 上下文管理器下无法序列化嵌套张量，如需此功能请提交问题</span>
                <span class="p">)</span>
            <span class="n">args 嵌套</span> <span class="o">=</span> <span class="p">(</span>
                <span class="c1"># NB: values() 当前以不安全的方式返回存储为缓冲区。</span>
                <span class="c1"># 理想情况下，我们会使用私有 API 来完成这项工作。TODO: 如果可行，切换到这个 API。</span>
                <span class="c1"># 如果我们最终添加了它，我们将解决这个问题。</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">值</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_嵌套张量大小</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_嵌套张量步长</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_嵌套张量存储偏移量</span><span class="p">(),</span>
            <span class="p">)</span>
            <span class="k">返回</font></font></font></span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_工具</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_重建嵌套张量</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">args 嵌套</span><span class="p">)</span>
        <span class="k">如果...否则</span> <span class="p">(</span>
            <span class="nb">类型</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span>
            <span class="ow">和</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">__torch 分派__</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="o">.</span><span class="n">__torch_dispatch__</span>
            <span class="ow">和</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">子类</span><span class="o">.</span><span class="n">functional_tensor</span><span class="o">.</span><span class="n">FunctionalTensor</span><span class="p">)</span>
                <span class="ow">或者</span> <span class="p">(</span>
                    <span class="ow">不</font></font></font></span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">子类</font></font></font></span><span class="o">.</span><span class="n">fake_tensor</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">假 Tensor</span><span class="p">)</span>
                    <span class="ow">和</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据指针</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">参数包装子类</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">类型</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">数据类型</span><span class="p">,</span>
                <span class="nb">元组</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">尺寸</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">步长</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">存储偏移</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">布局</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">设备</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">需要梯度</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">返回</font></font></font></span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_工具</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">重建包装子类</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数包装子类</span><span class="p">)</span>
        <span class="k">如果...否则</span> <span class="p">(</span>
            <span class="nb">类型</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span>
            <span class="ow">和</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__torch_dispatch__</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="o">.</span><span class="n">__torch_dispatch__</span>
            <span class="ow">和</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">子类</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">模拟张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">模拟 Tensor</span><span class="p">)</span>
                <span class="ow">和</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">跳过数据</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">和</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">材料化伪造张量</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="n">参数包装子类</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">类型</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">数据类型</span><span class="p">,</span>
                <span class="nb">元组</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">尺寸</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">步长</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">存储偏移</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">布局</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">设备</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">需要梯度</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">返回</font></font></font></span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_工具</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">重建包装子类</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数包装子类</span><span class="p">)</span>
        <span class="k">否则</span><span class="p">:</span>
            <span class="n">v3 数据类型</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">存储</span><span class="o">.</span><span class="n">_new_dtypes</span><span class="p">()</span>
            <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">v3 数据类型</span><span class="p">:</span>
                <span class="n">重建函数</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_工具</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_重建_v3 张量</span>
                <span class="n">存储</font></font></font></span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">未类型化存储</span><span class="p">()</span>
            <span class="k">否则</span><span class="p">:</span>
                <span class="c1"># TODO：一旦我们决定要中断序列化 FC，就不再</span>
                <span class="c1">需要使用 TypedStorage 包装</span>
                <span class="n">重建函数</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_工具</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_重建张量_v2</font></font></font></span>  <span class="c1"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "># 类型：忽略[赋值]</span>
                <span class="n">存储</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">存储</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型化存储</span><span class="p">(</span>
                    <span class="n">包装存储</font></font></font></span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">未类型化存储</span><span class="p">,</span>
                    <span class="n">数据类型</font></font></font></span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据类型</span><span class="p">,</span>
                    <span class="n">内部</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>  <span class="c1"># 类型：忽略[赋值]</span>

            <span class="c1"># TODO: 移除 hasattr，这是一个为了支持旧版 torch 而采用的 hack</span>
            <span class="c1"># 不要有_subclasses</span>
            <span class="k">如果</span> <span class="p">(</span>
                <span class="nb">有属性</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="p">,</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_subclasses</span><span class="p">)</span>
                <span class="ow">和</font></font></font></span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">子类</font></font></font></span><span class="o">.</span><span class="n">fake_tensor</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">伪造张量</span><span class="p">)</span>
                <span class="ow">和</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">跳过数据</span>
            <span class="p">):</span>
                <span class="n">存储</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_假设备</font></font></font></span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</span>

            <span class="n">args</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">存储</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">存储偏移</span><span class="p">(),</span>
                <span class="nb">元组</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">尺寸</span><span class="p">()),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">步长</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">需要梯度</span><span class="p">,</span>
                <span class="n">后退钩子</span><span class="p">,</span>
            <span class="p">)</span>  <span class="c1">#之前是 self._backward_hooks</span>

            <span class="k">如果</font></font></font></span> <span class="nb">isinstance</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">存储</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">存储</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">未类型化存储</span><span class="p">):</span>
                <span class="n">args</span> <span class="o">=</span> <span class="n">args</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">数据类型</font></font></font></span><span class="p">,)</span>  <span class="c1"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "># 类型：忽略[赋值]</span>

            <span class="n">元数据</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_工具</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">获取张量元数据</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元数据</span><span class="p">:</span>
                <span class="n">args</span> <span class="o">=</span> <span class="n">args</span> <span class="o">+</span> <span class="p">(</span><span class="n">元数据</font></font></font></span><span class="p">,)</span>  <span class="c1"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "># 类型：忽略[赋值]</span>

            <span class="k">返回</font></font></font></span> <span class="p">(</span><span class="n">rebuild_func</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">状态</span><span class="p">):</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="n">__setstate__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</span><span class="p">)</span>
        <span class="c1"># 警告：当你使用 torch.load()加载张量时，此方法不会被调用；</span>
        <span class="c1"># 这是由_rebuild_tensor_v2 管理的</span>
        <span class="k">如果</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是否为叶子节点</span><span class="p">:</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</font></font></font></span><span class="p">(</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">"__setstate__只能在叶子张量上调用"</span><span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">长度</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="c1"># 线张量序列化</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">集合</font></font></font></span><span class="p">(</span><span class="o">*</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</span><span class="p">)</span>
            <span class="k">返回</span>
        <span class="k">如果...否则</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">长度</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="c1"># 变量序列化</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">数据</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">状态</font></font></font></span> <span class="o">=</span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</font></font></font></span><span class="p">[</span><span class="mi">3</span><span class="p"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">]</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</font></font></font></span><span class="p">[</span><span class="mi">4</span><span class="p"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">]</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="c1"># _backward_hooks 的设置预期为空操作。</span>
        <span class="c1"># 参考注释 [不要序列化钩子]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">需要梯度</font></font></font></span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">状态</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">张量内容</font></font></font></span><span class="o">=</span><span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span><span class="p">):</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">张量</font></font></font></span><span class="o">.</span><span class="fm">__repr__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量内容</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量内容</span>
            <span class="p">)</span>
        <span class="c1"># 所有字符串在 Python 3 中都是 Unicode。</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n">_tensor_str</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">字符串</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量内容</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量内容</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.backward"><a class="viewcode-back" href="../../generated/torch.Tensor.backward.html#torch.Tensor.backward">[文档]</font></font></font></a>    <span class="k">def</span> <span class="nf"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">反向</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">渐变</font></font></font></span><span class="o">=</span><span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</font></font></font></span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">创建图</font></font></font></span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">输入</font></font></font></span><span class="o">=</span><span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">计算当前张量相对于图叶的梯度。</span>

<span class="sd">使用链式法则对图进行微分。如果张量不是标量（即其数据包含多个元素）且需要梯度，则函数还需要指定一个“梯度”。</span>
<span class="sd">如果张量非标量（即其数据包含多个元素）且需要计算梯度，则该函数还需要额外指定一个“梯度”。</span>
<span class="sd">如果张量非标量（即其数据包含多个元素）且需要梯度，则函数还需要指定一个“梯度”。</span>
<span class="sd">应该是一个类型和形状匹配的张量，表示对“self”的微分函数的梯度。</span>
<span class="sd">表示对“self”的微分函数的梯度。</span>

<span class="sd">此函数在叶子节点中累积梯度 - 你可能需要将其清零。</span>
<span class="sd">在调用之前，请查看 `.grad` 属性或将其设置为 `None`。</span>
<span class="sd">请参阅：:ref:`默认梯度布局`。</span>
<span class="sd">关于累积梯度的内存布局详情。</span>

<span class="sd">.. 注意::</span>

<span class="sd">如果运行任何前向操作，创建“gradient”，以及/或调用“backward”。</span>
<span class="sd">在用户指定的 CUDA 流上下文中，请参阅</span>
<span class="sd">ref:`反向传播的流语义`.</span>

<span class="sd">.. 注意::</span>

<span class="sd">当提供 `inputs` 并给定输入不是叶子节点时，</span>
<span class="sd">当前实现将调用其 grad_fn（尽管获取这些梯度并非绝对必要）。</span>
<span class="sd">这是一个实现细节，用户不应依赖。</span>
<span class="sd">更多详情请见 https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780。</span>

<span class="sd">参数：</span>
<span class="sd">梯度（张量，可选）：函数相对于 ``self`` 的梯度</span>
<span class="sd">的微分</span>
<span class="sd">此参数可以省略，如果 ``self`` 是标量</span>
<span class="sd">retain_graph（布尔值，可选）：如果 ``False``，则用于计算</span>
<span class="sd">毕业生将被释放。请注意，在几乎所有情况下设置</span>
<span class="sd">这个选项设置为 True 是不必要的，通常可以绕过</span>
<span class="sd">以更高效的方式。默认值为</span>
<span class="sd">``创建图``。</span>
<span class="sd">创建图（布尔值，可选）：如果 ``True``，则创建导数的图</span>
<span class="sd">可以构建，允许计算高阶导数</span>
<span class="sd">products。默认为 ``False``。</span>
<span class="sd">输入（Tensor 序列，可选）：相对于该梯度将被</span>
<span class="sd">累积到 ``.grad`` 的。所有其他张量将被忽略。如果没有提供，</span>
<span class="sd">则梯度将累积到所有叶张量中。</span>
<span class="sd">用于计算 :attr:`张量`。</span>
<span class="sd">        """</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">反向</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">渐变</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">渐变</span><span class="p">,</span>
                <span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">,</span>
                <span class="n">创建图</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">创建图</span><span class="p">,</span>
                <span class="n">输入</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">输入</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">自动微分</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">反向</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">渐变</font></font></font></span><span class="p">,</span> <span class="n">retain_graph</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">创建图</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">输入</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">输入</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.register_hook"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def register_hook(self, hook):
注册反向钩子。

每次计算相对于梯度的梯度时，都会调用此钩子。
张量已计算。钩子应具有以下签名::

hook(grad) -&gt; Tensor 或 None


钩子不应修改其参数，但可以可选地返回
一个新的梯度，该梯度将用于替换 :attr:`grad`。

该函数返回一个带有方法 `handle.remove()` 的句柄
该方法从模块中移除钩子

.. 注意::
有关此钩子何时执行的更多信息，请参阅 :ref:`backward-hooks-execution`
执行过程及其相对于其他钩子的执行顺序。

示例::

&gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)
&gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2)  # 将梯度加倍
&gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))
&gt;&gt;&gt; v.grad

             2
             4
             6
[torch.FloatTensor of size (3,)]

&gt;&gt;&gt; h.remove()  # 移除钩子
"""
if has_torch_function_unary(self):
return handle_torch_function(Tensor.register_hook, (self,), self, hook)
if not self.requires_grad:
raise RuntimeError(
无法在不需要梯度的张量上注册钩子
            )
如果 self._backward_hooks 是 None：
self._backward_hooks = 有序字典()
如果 self.grad_fn 不为 None：
self.grad_fn._register_hook_dict(self)

从 torch.utils.hooks 导入 RemovableHandle

handle = RemovableHandle(self._backward_hooks)
self._backward_hooks[handle.id] = hook
return handle</font></font></font></div>

<div class="viewcode-block" id="Tensor.register_post_accumulate_grad_hook"><a class="viewcode-back" href="../../generated/torch.Tensor.register_post_accumulate_grad_hook.html#torch.Tensor.register_post_accumulate_grad_hook">[文档]</a>    <span class="k">def</span> <span class="nf">register_post_accumulate_grad_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">注册一个在梯度累积之后运行的回退钩子。</span>

<span class="sd">钩子将在张量所有梯度累积后调用</span>
<span class="sd">这意味着该张量上的 .grad 字段已被更新。该帖子</span>
<span class="sd">累积梯度钩子仅适用于叶张量（无子张量的张量）</span>
<span class="sd">在非叶子张量上注册此钩子将报错！</span>

<span class="sd">该钩子应具有以下签名：</span>

<span class="sd">            hook(param: Tensor) -&gt; None</span>

<span class="sd">注意，与其它自动微分钩子不同，此钩子作用于张量</span>
<span class="sd">这需要梯度而不是梯度本身。钩子可以就地修改</span>
<span class="sd">并访问其 Tensor 参数，包括其.grad 字段。</span>

<span class="sd">此函数返回一个带有方法`handle.remove()`的句柄</span>
<span class="sd">该方法从模块中移除钩子。</span>

<span class="sd">.. 注意::</span>
<span class="sd">查看：:ref:`backward-hooks-execution` 了解有关此钩子何时执行的更多信息</span>
<span class="sd">执行方式及其相对于其他钩子的执行顺序。由于</span>
<span class="sd">这个钩子在反向传播期间运行，它将以 no_grad 模式运行（除非</span>
<span class="sd">创建图是 True）。您可以使用 torch.enable_grad()重新启用自动微分</span>
<span class="sd">在钩子中，如果您需要的话。</span>

<span class="sd">示例::</span>

<span class="sd">            &gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)</span>
<span class="sd">            &gt;&gt;&gt; lr = 0.01</span>
<span class="sd">&gt;&gt;&gt; # 模拟简单的 SGD 更新</span>
<span class="sd">            &gt;&gt;&gt; h = v.register_post_accumulate_grad_hook(lambda p: p.add_(p.grad, alpha=-lr))</span>
<span class="sd">            &gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))</span>
<span class="sd">            &gt;&gt;&gt; v</span>
<span class="sd">            tensor([-0.0100, -0.0200, -0.0300], requires_grad=True)</span>

<span class="sd">&gt;&gt;&gt; h.remove()  # 移除钩子</span>
<span class="sd">        """</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">张量</font></font></font></span><span class="o">.</span><span class="n">register_post_accumulate_grad_hook</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">钩子</span>
            <span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">需要梯度</span><span class="p">:</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                <span class="s2">"无法在不需要梯度的张量上注册钩子"</span>
            <span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span><span class="p">:</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                <span class="s2">"非叶子张量上无法注册后累加梯度钩子"</span>
            <span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_accumulate_grad_hooks</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_accumulate_grad_hooks</span><span class="p">:</span> <span class="nb">字典</font></font></font></span><span class="p">[</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">任何</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">任何</font></font></font></span><span class="p">]</span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">有序字典</span><span class="p">()</span>

        <span class="kn">来自</font></font></font></span> <span class="nn">torch.utils.hooks</span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">可移除句柄</span>

        <span class="n">handle</span> <span class="o">=</span> <span class="n">可移除句柄</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_post_accumulate_grad_hooks</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_accumulate_grad_hooks</span><span class="p">[</span><span class="n">处理</font></font></font></span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">钩子</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">处理</span></div>

    <span class="k">def</span> <span class="nf">加强</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">奖励</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">剪辑</span><span class="p">(</span><span class="nb">str</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="s2">"</span><span class="se"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">输入文本翻译为简体中文为：\n</font></font></font></span><span class="s2">"</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">连接</font></font></font></span><span class="p"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">[</font></font></font></span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">行</font></font></font></span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">行</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="nb">str</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">分割</font></font></font></span><span class="p">(</span><span class="s2">"</span><span class="se"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">输入文本翻译为简体中文为：\n</font></font></font></span><span class="s2">"</span><span class="p"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">)))</span>

        <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
            <span class="n">去除</span><span class="p">(</span>
<span class="w">                </span><span class="sa">r</span><span class="sd">"reinforce() 已移除。</span>
<span class="sd">使用 torch.distributions 代替。</span>
<span class="sd">请参阅 https://pytorch.org/docs/main/distributions.html</span>

<span class="sd">取代：</span>

<span class="sd">            probs = policy_network(state)</span>
<span class="sd">            action = probs.multinomial()</span>
<span class="sd">            next_state, reward = env.step(action)</span>
<span class="sd">            action.reinforce(reward)</span>
<span class="sd">            action.backward()</span>

<span class="sd">使用：</span>

<span class="sd">            probs = policy_network(state)</span>
<span class="sd"># NOTE: categorical 是以前所说的 multinomial 的等价物</span>
<span class="sd">            m = torch.distributions.Categorical(probs)</span>
<span class="sd">            action = m.sample()</span>
<span class="sd">            next_state, reward = env.step(action)</span>
<span class="sd">            loss = -m.log_prob(action) * reward</span>
<span class="sd">            loss.backward()</span>
<span class="sd">        """</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="n">detach</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_添加文档字符串</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">detach</span><span class="p">,</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">""</span>
<span class="sd">返回一个新的 Tensor，与当前图分离。</span>

<span class="sd">结果永远不会需要梯度。</span>

<span class="sd">此方法还会影响前向模式 AD 梯度，并且结果将永远不会</span>
<span class="sd">具有前向模式的 AD 梯度。</span>

<span class="sd">.. 注意::</span>

<span class="sd">返回的 Tensor 与原始 Tensor 共享相同的存储空间。</span>
<span class="sd">对任一 Tensor 的就地修改都将被看到，并可能触发</span>
<span class="sd">正确性检查中的错误。</span>
<span class="sd">"沉浸式翻译"</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">断开连接</font></font></font></span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_添加文档字符串</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">断开连接</span><span class="p">,</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">""</span>
<span class="sd">从创建它的图中分离张量，使其成为叶子节点。</span>
<span class="sd">视图不能就地分离。</span>

<span class="sd">此方法还会影响前向模式 AD 梯度，结果将不会有前向模式 AD 梯度。</span>
<span class="sd">将不会有前向模式 AD 梯度。</span>
<span class="sd">    """</span><span class="p">,</span>
    <span class="p">)</span>

<div class="viewcode-block" id="Tensor.is_shared"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def is_shared(self):
r"""检查张量是否在共享内存中。

对于 CUDA 张量，这始终是 ``True``。
"""
如果有 torch_function_unary(self):
返回 handle_torch_function(Tensor.is_shared, (self,), self)
返回 self._typed_storage()._is_shared()</font></font></font></div>

<div class="viewcode-block" id="Tensor.share_memory_"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def share_memory_(self):
r"""将底层存储移动到共享内存。

如果底层存储已经位于共享内存中，则此操作不执行任何操作。
对于 CUDA 张量也是如此。共享内存中的张量不能调整大小。

查看更多详情，请参阅：:meth:`torch.UntypedStorage.share_memory_`。
"""
如果 has_torch_function_unary(self) 为真：
返回 handle_torch_function(Tensor.share_memory_, (self,), self)
self._typed_storage()._share_memory_()
return self</font></font></font></div>

<div class="viewcode-block" id="Tensor.module_load"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def module_load(self, other, assign=False):
r"""定义了在 :meth:`~nn.Module.load_state_dict` 中将 ``other`` 载入 ``self`` 时如何转换。"""

当 `:func:`~torch.__future__.get_swap_module_params_on_conversion` 为 `True` 时使用。

预期 `self` 是 `nn.Module` 中的一个参数或缓冲区，而 `other` 是具有相应键的状态字典中的值。
此方法定义了在通过 `other` 与 `self` 交换之前，`other` 将如何重新映射。
通过 `other` 与 `self` 交换之前，`other` 将如何重新映射。
在 :meth:`~nn.Module.load_state_dict` 中的 :func:`~torch.utils.swap_tensors`。

.. 注意::
此方法应始终返回一个新对象，该对象不是 ``self`` 或 ``other``。
例如，默认实现返回 ``self.copy_(other).detach()``。
如果 `assign` 为 `False` 或 `assign` 为 `True` 时调用 `other.detach()`。

参数：
other (Tensor)：与 `self` 对应键的 state dict 中的值
assign (bool)：传递给 :meth:`nn.Module.load_state_dict` 的 assign 参数

"""
如果有 torch_function_variadic(self, other)函数：
返回 handle_torch_function(
Tensor.module_load, (self, other), self, other, assign=assign
            )

如果分配：
返回 other.detach()
否则：
返回 self.copy_(other).detach()</font></font></font></div>

    <span class="k">def</span> <span class="fm">__倒序__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">将张量沿维度 0 反转。</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="fm"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">__倒序__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">维度</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">返回</span> <span class="bp">self</span>
        <span class="k">否则</span><span class="p">:</span>
            <span class="k">返回</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">翻转</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.norm"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def norm(
self,
p: Optional[Union[float, str]] = "fro",
dim=None,
keepdim=False,
dtype=None,
    ):
r"""参见 :func:`torch.norm`"""
if has_torch_function_unary(self):
return handle_torch_function(
Tensor.norm, (self,), self, p=p, dim=dim, keepdim=keepdim, dtype=dtype
            )
return torch.norm(self, p, dim, keepdim, dtype=dtype)</font></font></font></div>

    <span class="k">def</span> <span class="nf">求解</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="kn">来自</font></font></font></span> <span class="nn">torch._linalg_utils</span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">解决</span>

        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">求解</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">最小二乘法</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="kn">来自</font></font></font></span> <span class="nn">torch._linalg_utils</span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</span> <span class="n">lstsq</span>

        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">最小二乘法</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">特征向量</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">特征向量</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="kn">来自</font></font></font></span> <span class="nn">torch._linalg_utils</span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">特征值</span>

        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">特征向量</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">特征向量</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">特征向量</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">特征值</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">特征向量</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="kn">来自</font></font></font></span> <span class="nn">torch._linalg_utils</span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</span> <span class="n">_symeig</span>

        <span class="k">返回</font></font></font></span> <span class="n">_symeig</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">特征向量</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">特征向量</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.lu"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def lu(self, pivot=True, get_infos=False):
查看函数：torch.lu
如果 get_infos 为 True，则不需要检查错误，反之亦然
如果 has_torch_function_unary(self)为真：
返回 handle_torch_function(
Tensor.lu, (self,), self, pivot=pivot, get_infos=get_infos
            )

LU, pivots, infos = torch._lu_with_info(
self, pivot=pivot, check_errors=(not get_infos)
        )
如果获取信息：
返回 LU、pivots、infos
否则：
返回 LU，转置</font></font></font></div>

<div class="viewcode-block" id="Tensor.stft"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档] 定义 stft(
self，
n_fft: 整数,
hop_length: 可选[int] = None,
win_length: 可选[int] = None,
window: "可选[Tensor]" = None,
center: bool = True,
pad_mode: str = "reflect",
normalized: bool = False,
onesided: Optional[bool] = None,
return_complex: Optional[bool] = None,
align_to_window: 可选[bool] = None,
    ):
r"""参见 :func:`torch.stft`

.. 警告::
这个函数在版本 0.4.1 中改变了签名。使用之前的签名调用可能会导致错误或返回不正确的结果。
使用之前的签名调用可能会导致错误或返回不正确的结果。
"""
如果 has_torch_function_unary(self):
return handle_torch_function(
Tensor.stft,
(self,),
self,
n_fft
hop_length=hop_length
win_length=win_length
window=window
center=居中,
pad_mode=填充模式,
normalized=归一化,
onesided=单边,
return_complex=return_complex,
align_to_window=align_to_window,
            )
return torch.stft(
self,
n_fft,
hop_length,
win_length,
窗口,
居中,
填充模式,
标准化,
单方面,
return_complex=return_complex,
align_to_window=align_to_window,
        )</font></font></font></div>

<div class="viewcode-block" id="Tensor.istft"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def istft(
self,
n_fft: int,
hop_length: Optional[int] = None,
win_length: 可选[int] = None,
window: "Optional[Tensor]" = None,
center: bool = True,
normalized: bool = False,
onesided: 可选[bool] = None,
length: 可选[int] = None,
return_complex: bool = False,
    ):
查看函数：`torch.istft`
如果有 torch 函数一元运算符(self):
返回 handle_torch_function(
Tensor.istft,
self,
self,
n_fft,
hop_length=hop_length,
win_length=win_length,
window=window,
center=center,
normalized=normalized,
onesided=onesided,
length=length,
return_complex=return_complex,
            )
return torch.istft(
self,
n_fft,
hop_length,
win_length,
window,
center,
normalized,
单方面的,
长度,
return_complex=return_complex,
        )</font></font></font></div>

    <span class="k">def</span> <span class="nf">调整大小</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">尺寸</span><span class="p">):</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">调整大小</font></font></font></span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">尺寸</span><span class="p">)</span>
        <span class="n">警告</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">警告</font></font></font></span><span class="p">(</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">"非原地调整大小已弃用"</span><span class="p">)</span>
        <span class="kn">来自</font></font></font></span> <span class="nn">torch.autograd._functions</span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">调整大小</span>

        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">调整大小</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">应用</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">尺寸</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">调整为</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="p">):</span>
        <span class="k">如果</font></font></font></span> <span class="n">has_torch_function_variadic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">调整为</font></font></font></span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="p">),</span> <span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="p">)</span>
        <span class="n">警告</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">警告</font></font></font></span><span class="p">(</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">"原地 resize_as 已弃用"</span><span class="p">)</span>
        <span class="kn">来自</font></font></font></span> <span class="nn">torch.autograd._functions</span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">调整大小</span>

        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">调整大小</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">应用</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">尺寸</span><span class="p">())</span>

<div class="viewcode-block" id="Tensor.split"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def split(self, split_size, dim=0):
查看函数：:func:`torch.split`
if has_torch_function_unary(self):
return handle_torch_function(
Tensor.split, (self,), self, split_size, dim=dim
            )
如果 split_size 是 Tensor 类型：
尝试：
split_size = int(split_size)
except ValueError:
pass

if isinstance(split_size, (int, torch.SymInt)):
return torch._VF.split(self, split_size, dim)  # 忽略未定义的属性
else:
return torch._VF.split_with_sizes(self, split_size, dim)</font></font></font></div>

<div class="viewcode-block" id="Tensor.unique"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None):
返回输入张量的唯一元素。

查看 :func:`torch.unique`
""
如果 has_torch_function_unary(self):
return handle_torch_function(
Tensor.unique
(self,)
self
sorted=sorted,
return_inverse=return_inverse,
return_counts=return_counts,
dim=dim,
            )
return torch.unique(
self,
sorted=sorted,
return_inverse=return_inverse,
return_counts=return_counts,
dim=dim,
        )</font></font></font></div>

<div class="viewcode-block" id="Tensor.unique_consecutive"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def unique_consecutive(self, return_inverse=False, return_counts=False, dim=None):
移除每个等效元素连续组中除了第一个元素之外的所有元素。

查看 :func:`torch.unique_consecutive`
"""
if has_torch_function_unary(self):
return handle_torch_function(
Tensor.unique_consecutive,
(self,)
self,
return_inverse=return_inverse,
return_counts=return_counts,
dim=dim,
            )
return torch.unique_consecutive(
self, return_inverse=return_inverse, return_counts=return_counts, dim=dim
        )</font></font></font></div>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">返回</font></font></font></span> <span class="n">_C</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_变量函数</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">反向子集</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="nf">__rdiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">返回</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">相互</font></font></font></span><span class="p">()</span> <span class="o">*</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">其他</span>

    <span class="fm">__rtruediv__</span> <span class="o">=</span> <span class="n">__rdiv__</span>
    <span class="fm">__itruediv__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">__idiv__</span>

    <span class="fm">__pow__</span> <span class="o">=</span> <span class="n">角色</span><span class="p">(</span>
        <span class="n">可调用</span><span class="p">[</span>
            <span class="p">[</span><span class="s2">"torch._C.TensorBase"</span><span class="p">,</span> <span class="n">联合</font></font></font></span><span class="p">[</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">"张量"</font></font></font></span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">复杂</span><span class="p">]],</span>
            <span class="s2">"张量"</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="n">处理 torch 函数并将类型错误包装为未实现</span><span class="p">(</span>
            <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">幂</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="fm">__ipow__</span> <span class="o">=</span> <span class="n">处理 torch 函数并将类型错误包装为未实现</span><span class="p">(</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">pow_</span>
    <span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rmod__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">剩余</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">格式化</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">格式说明</span><span class="p">):</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="fm"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">格式化</font></font></font></span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">格式规范</span><span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">维度</font></font></font></span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">和</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是否元数据</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">和</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="p">:</span>
            <span class="k">返回</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">项目</font></font></font></span><span class="p">()</span><span class="o">.</span><span class="fm"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">格式化</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">格式规范</span><span class="p">)</span>
        <span class="k">返回</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">对象</font></font></font></span><span class="o">.</span><span class="fm"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">格式化</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">格式规范</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rpow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__floordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">向下取整除</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rfloordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">向下取整除</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rlshift__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">按位左移</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rrshift__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">位右移</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="nd">@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rmatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">矩阵乘法</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="fm">__pos__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n">正向</span>
    <span class="fm">负</font></font></font></span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">负</span>
    <span class="fm">绝对值</font></font></font></span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">TensorBase</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">绝对值</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="o">.</span><span class="fm">__len__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">维度</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型错误</font></font></font></span><span class="p">(</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">"0 维张量的 len()"</span><span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">获取跟踪状态</span><span class="p">():</span>
            <span class="n">警告</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">警告</span><span class="p">(</span>
                <span class="s2">使用 len 获取张量形状可能会导致跟踪不正确。</span>
                <span class="s2">建议的使用方法是 tensor.shape[0]。</span>
                <span class="s2">传递不同形状的张量可能会导致错误或静默给出。</span>
                <span class="s2">"结果不正确。"</span><span class="p">,</span>
                <span class="n">分类</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">算子</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">跟踪警告</span><span class="p">,</span>
                <span class="n">栈级别</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">返回</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">形状</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># NB: 我们在这里使用 'imap' 而不是 'map'，这样在 Python 2 中我们得到一个</span>
        <span class="c1"># 生成器，并且不会立即执行所有索引。这可以</span>
        <span class="c1"># 节省我们的工作，并且也有助于保持跟踪顺序的确定性</span>
        <span class="c1"># (例如，如果您对 hiddens 进行 zip(*hiddens)，急切映射将强制所有</span>
        <span class="c1"># 隐藏层[0]的索引在隐藏层[1]之前，而生成器</span>
        <span class="c1"># 地图将交错它们。</span>
        <span class="c1"># 注意：我们故意在这里跳过了 __torch_function__ 分发。</span>
        <span class="c1"># See gh-54457</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">维度</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型错误</font></font></font></span><span class="p">(</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">"迭代一个 0 维张量"</span><span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">_获取追踪状态</span><span class="p">():</span>
            <span class="n">警告</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">警告</span><span class="p">(</span>
                <span class="s2">"迭代张量可能会导致追踪结果不正确。"</span>
                <span class="s2">"传递不同形状的张量不会改变执行的迭代次数（可能会导致错误或静默地给出错误的结果）。"</span>
                <span class="s2">"迭代次数执行（可能会导致错误或静默地给出错误的结果）。"</span>
                <span class="s2">"请在此处不要处理 __torch_function__，因为这是用户的默认行为。"</span><span class="p">,</span>
                <span class="n">分类</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">算子</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">跟踪警告</span><span class="p">,</span>
                <span class="n">栈级别</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">返回</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">迭代</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">解绑</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__哈希__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># 请在此处不要处理 __torch_function__，因为这是用户的默认行为。</span>
        <span class="c1">实现处理大多数功能的机制很可能会出错。</span>
        <span class="c1">可以通过定义此方法在用户处轻松覆盖。</span>
        <span class="c1">如需，可进行子类化。</span>
        <span class="k">返回</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__dir__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="o">.</span><span class="fm">__dir__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">张量方法</font></font></font></span> <span class="o">=</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">目录</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类</span><span class="p">)</span>
        <span class="n">张量方法</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">删除</font></font></font></span><span class="p">(</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">易挥发的</font></font></font></span><span class="p">)</span>  <span class="c1"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "># 已弃用</span>
        <span class="n">属性</font></font></font></span> <span class="o">=</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">列表</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">字典</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">键</span><span class="p">())</span>
        <span class="n">键</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量方法</span> <span class="o">+</span> <span class="n">attrs</span>

        <span class="c1"># 仅在密集、CUDA 张量中可用的属性</span>
        <span class="k">如果</font></font></font></span> <span class="p">(</span><span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是否为 CUDA</font></font></font></span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">或者</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="n">键</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">删除</span><span class="p">(</span><span class="s2">"__cuda_array_interface__"</span><span class="p">)</span>

        <span class="k">返回</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">排序</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">键</span><span class="p">)</span>

    <span class="c1"># Numpy 数组接口，以支持 `numpy.asarray(tensor) -&gt; ndarray`</span>
    <span class="n">__数组优先级__</font></font></font></span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "># 优先使用 Tensor 操作而非 NumPy</span>

    <span class="k">def</span> <span class="nf">__数组__</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据类型</font></font></font></span><span class="o">=</span><span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span><span class="p">):</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">__数组__</font></font></font></span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据类型</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据类型</span><span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="n">dtype</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span><span class="p">:</span>
            <span class="k">返回</span> <span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">否则</span><span class="p">:</span>
            <span class="k">返回</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据类型</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">复制</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># 将完成的 Numpy 数组再次包裹在合适的张量中，以支持例如</span>
    <span class="c1">`numpy.sin(tensor) -&gt; tensor` 或 `numpy.greater(tensor, 0) -&gt; ByteTensor`</span>
    <span class="k">def</span> <span class="nf">__数组包装__</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数组</span><span class="p">):</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">__数组包装__</font></font></font></span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数组</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数组</span>
            <span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数组</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">bool</span><span class="p">:</span>
            <span class="c1">torch 没有内置的 bool 张量</span>
            <span class="n">数组</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数组</font></font></font></span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">uint8</span><span class="p">)</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">从 NumPy 转换</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数组</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">包含</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元素</font></font></font></span><span class="p">:</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">任何</span><span class="p">,</span> <span class="o">/</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">检查 `元素` 是否存在于张量中</span>

<span class="sd">参数：</span>
<span class="sd">元素（张量或标量）：要检查的元素</span>
<span class="sd">检查是否存在于当前张量中</span>
<span class="sd">        """</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="fm"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">包含</font></font></font></span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元素</span><span class="p">)</span>
        <span class="k">如果</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">元素</font></font></font></span><span class="p">,</span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数字</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n">SymInt</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n">SymFloat</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">符号布尔值</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="c1">类型提示不理解 __contains__ 结果数组</span>
            <span class="k">返回</font></font></font></span> <span class="nb">bool</span><span class="p">((</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元素</font></font></font></span> <span class="o">==</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">任何</font></font></font></span><span class="p">()</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">项目</font></font></font></span><span class="p">())</span>  <span class="c1"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "># 类型：忽略[联合属性]</span>

        <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">"Tensor.__contains__ 只支持 Tensor 或标量，但你传递了"</font></font></font></span><span class="si">{</span><span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元素</font></font></font></span><span class="p">)</span><span class="si">}</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">。</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">__cuda_array_interface__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">CUDA 张量的数组视图描述。</span>

<span class="sd">参考：</span>
<span class="sd">        https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html</span>
<span class="sd">        """</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="c1"># TODO mypy 不支持@property 属性，参考：https://github.com/python/mypy/issues/6185</span>
            <span class="k">返回</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">张量</font></font></font></span><span class="o">.</span><span class="n">__cuda_array_interface__</span><span class="o">.</span><span class="fm">__get__</span><span class="p">,</span>  <span class="c1"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "># 类型：忽略[已定义]</span>
                <span class="p">(</span><span class="bp">self</span><span class="p">,),</span>
                <span class="bp">self</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1">对于不支持的张量，将引发 AttributeError 异常</span>
        <span class="c1">如果没有 "__cuda_array_interface__" 属性，则表示不是 CUDA 张量类型。</span>
        <span class="k">如果</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是否为 CUDA</span><span class="p">:</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">属性错误</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">无法在非 CUDA 张量类型上获取 "__cuda_array_interface__"：</font></font></font></span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</span><span class="p">()</span><span class="si">}</span><span class="s2"> "</span>
                <span class="s2">如果需要 CUDA 数据，请使用 tensor.cuda() 将张量复制到设备内存。</span>
            <span class="p">)</span>

        <span class="k">如果</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">属性错误</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"无法在稀疏类型上获取 __cuda_array_interface__："</font></font></font></span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</span><span class="p">()</span><span class="si">}</span><span class="s2"> "</span>
                <span class="s2">"请使用 Tensor.to_dense() 首先将张量转换为稠密张量。"</span>
            <span class="p">)</span>

        <span class="c1">运行时错误，匹配 tensor.__array__() 的行为。</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">需要梯度</span><span class="p">:</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                <span class="s2">"无法在需要求导的 Variable 上获取 __cuda_array_interface__。"</span>
                <span class="s2">"如果不需要梯度，请使用 var.detach() 获取不需要求导的 Variable。"</span>
            <span class="p">)</span>

        <span class="n">类型字符串</font></font></font></span> <span class="o">=</span> <span class="n">_dtype_to_typestr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据类型</span><span class="p">)</span>
        <span class="n">元素大小</font></font></font></span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元素大小</span><span class="p">()</span>
        <span class="n">形状</font></font></font></span> <span class="o">=</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元组</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">形状</span><span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">连续的</span><span class="p">():</span>
            <span class="c1"># __cuda_array_interface__ v2 要求省略 strides</span>
            <span class="c1"># （未设置或设置为 None）对于 C-连续数组。</span>
            <span class="n">步长</font></font></font></span> <span class="o">=</span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span>
        <span class="k">否则</span><span class="p">:</span>
            <span class="n">步长</font></font></font></span> <span class="o">=</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元组</font></font></font></span><span class="p">(</span><span class="n">s</span> <span class="o">*</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元素大小</font></font></font></span> <span class="k">for</span> <span class="n">s</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">步长</span><span class="p">())</span>
        <span class="n">数据指针</font></font></font></span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据指针</font></font></font></span><span class="p">()</span> <span class="k"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元素数量</font></font></font></span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">否则</span> <span class="mi">0</span>
        <span class="n">数据</font></font></font></span> <span class="o">=</span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据指针</font></font></font></span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>  <span class="c1"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">只读为假</span>

        <span class="k">返回</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">字典</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型字符串</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型字符串</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">形状</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">形状</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">步长</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">步伐</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">数据</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">版本</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.storage_type"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def 存储类型(self):
r"""存储类型() -&gt; 类型

返回底层存储的类型。

"""
如果有 torch_function_unary(self):
返回 handle_torch_function(Tensor.storage_type, (self,), self)

torch.storage._warn_typed_storage_removal()

return self._typed_storage()._get_legacy_storage_class()</font></font></font></div>

<div class="viewcode-block" id="Tensor.refine_names"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档] def refine_names(self, *names):
r"""根据 :attr:`names` 优化 :attr:`self` 的维度名称。

优化是一种特殊的重命名方式，可以将未命名的维度“提升”。
一个“无名称”维度可以被细化为任何名称；一个已命名的维度只能被
细化为相同的名称。

因为命名张量可以与未命名的张量共存，所以细化名称
为编写既适用于命名张量又适用于未命名张量的命名张量感知代码提供了一种
命名和未命名的张量。

attr:`names` 可能包含最多一个省略号（``...``）。
省略号贪婪地展开；它就地展开以填充
attr:`names` 到与 `self.dim()` 相同的长度，使用名称从
对应于 `self.names` 的索引。

Python 2 不支持省略号，但可以使用字符串字面量
（`'...'`）代替。

参数：
输出张量的名称（字符串的可迭代对象）：期望的输出张量名称。最多可以包含一个省略号。
可以包含一个省略号。

示例::

&gt;&gt;&gt; imgs = torch.randn(32, 3, 128, 128)
&gt;&gt;&gt; named_imgs = imgs.refine_names('N', 'C', 'H', 'W')
&gt;&gt;&gt; named_imgs.names
('N', 'C', 'H', 'W')

&gt;&gt;&gt; tensor = torch.randn(2, 3, 5, 7, 11)
&gt;&gt;&gt; tensor = tensor.refine_names('A', ..., 'B', 'C')
&gt;&gt;&gt; tensor.names
('A', None, None, 'B', 'C')

.. 警告::
命名张量 API 是实验性的，可能会发生变化。

"""
如果有 torch_function_unary(self)：
返回 handle_torch_function(Tensor.refine_names, (self,), self, *names)
names = resolve_ellipsis(names, self.names, "refine_names")
return super().refine_names(names)</font></font></font></div>

<div class="viewcode-block" id="Tensor.align_to"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def align_to(self, *names):
r"""调整 :attr:`self` 张量的维度以匹配顺序
在 :attr:`names` 中指定的，为任何新的名称添加一维。

使用此方法之前，:attr:`self` 的所有维度都必须命名。
结果张量是原始张量上的视图。

attr:`self` 的所有维度名称都必须存在于 :attr:`names` 中。
attr:`names` 可能包含不在 ``self.names`` 中的额外名称；
输出张量对于每个新名称都有一个大小为 1 的维度。

attr:`names` 可能最多包含一个省略号（``...``）。
省略号展开后等于 :attr:`self` 的所有维度名称。
那些在 :attr:`names` 中未提及的，按照它们出现的顺序
在 :attr:`self` 中。

Python 2 不支持省略号，但可以使用字符串字面量
（``'...'``）代替。

Args:
names (字符串的可迭代对象): 输出张量的期望维度顺序。最多可包含一个省略号，该省略号将展开为所有未提及的维度名称，即：attr:`self`。
输出张量的期望维度顺序。最多可包含一个省略号，该省略号将展开为所有未提及的维度名称，即：attr:`self`。
输出张量的期望维度顺序。最多可包含一个省略号，该省略号将展开为所有未提及的维度名称，即：attr:`self`。

示例::

&gt;&gt;&gt; tensor = torch.randn(2, 2, 2, 2, 2, 2)
&gt;&gt;&gt; named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')

# 将 F 和 E 维度移到前面，其余维度保持顺序
&gt;&gt;&gt; 命名张量对齐到('F', 'E', ...)

.. 警告::
命名张量 API 是实验性的，可能会更改。

"""
if has_torch_function_unary(self):
return handle_torch_function(Tensor.align_to, (self,), self, *names)
ellipsis_idx = single_ellipsis_index(names, "align_to")
if ellipsis_idx is None:
return super().align_to(names)
return super().align_to(
[name for name in names if not is_ellipsis(name)], ellipsis_idx
        )</font></font></font></div>

<div class="viewcode-block" id="Tensor.unflatten"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def unflatten(self, dim, sizes):  # type: ignore[override]
r"""
unflatten(dim, sizes) -&gt; Tensor

查看 :func:`torch.unflatten`.

"""
如果有 torch_function_unary(self)函数
返回 handle_torch_function(Tensor.unflatten, (self,), self, dim, sizes)

如果 sizes 为空
raise RuntimeError("unflatten: 大小必须非空")

names = None
if isinstance(sizes, OrderedDict) or (
isinstance(sizes, (tuple, list)) and isinstance(sizes[0], (tuple, list))
        ):
names, sizes = unzip_namedshape(sizes)
return super().unflatten(dim, sizes, names)
else:
return super().unflatten(dim, sizes)</font></font></font></div>

<div class="viewcode-block" id="Tensor.rename_"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def rename_(self, *names, **rename_map):
"""原地版本 of :meth:`~Tensor.rename`."""

if has_torch_function_unary(self):
return handle_torch_function(
Tensor.rename_(self,), self, *names, **rename_map
            )

# Note [rename_ / rename API]
Python API 与 C++ API 的实现方式不同。在 Python 中：
1) tensor.rename(*names) 接受一个可变参数列表的名称。
2) tensor.rename(**rename_map) 接受一个名称到重命名的映射。
C++ 是静态的，这使得实现类似的行为变得困难。
return update_names(self, names, rename_map, inplace=True)</font></font></font></div>

<div class="viewcode-block" id="Tensor.rename"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def rename(self, *names, **rename_map):
"""重命名 :attr:`self` 的维度名称。

有两种主要用法：

`self.rename(**rename_map) 返回一个具有维度的张量视图`
重命名为映射中指定的：`rename_map`。

返回一个张量视图，重命名所有
使用 `:attr:`names` 定位维度。
使用 `self.rename(None)` 来删除张量上的名称。

不能同时指定位置参数 :attr:`names` 和关键字参数 :attr:`rename_map`。
attr:`rename_map`。

示例::

&gt;&gt;&gt; imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))
&gt;&gt;&gt; renamed_imgs = imgs.rename(N='batch', C='channels')
&gt;&gt;&gt; renamed_imgs.names
('batch', 'channels', 'H', 'W')

&gt;&gt;&gt; renamed_imgs = imgs.rename(None)
&gt;&gt;&gt; renamed_imgs.names
(None, None, None, None)

&gt;&gt;&gt; renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')
&gt;&gt;&gt; 重命名图片文件名
('批次', '通道', '高度', '宽度')

.. 警告::
命名张量 API 是实验性的，可能会发生变化。

"""
如果有 torch_function_unary(self):
返回 handle_torch_function(
Tensor.rename, (self,), self, *names, **rename_map
            )

# 请见笔记 [重命名_ / 重命名 API]
返回 update_names(self, names, rename_map, inplace=False)</font></font></font></div>

<div class="viewcode-block" id="Tensor.to_sparse_coo"><font class=" " lang="zh-CN"><br hidden=""><font class="    "><font class="  ">[文档]    def to_sparse_coo(self):
将张量转换为：ref:`坐标格式 `.

示例::

&gt;&gt;&gt; dense = torch.randn(5, 5)
&gt;&gt;&gt; 稀疏矩阵 = 稠密矩阵.to_sparse_coo()
&gt;&gt;&gt; 稀疏矩阵._nnz()
             25

"""
返回 self.to_sparse()</font></font></font></div>

<div class="viewcode-block" id="Tensor.dim_order"><a class="viewcode-back" href="../../generated/torch.Tensor.dim_order.html#torch.Tensor.dim_order">[文档]</font></font></font></a>    <span class="k">def</span> <span class="nf"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">维度排序</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">模糊性检查</font></font></font></span><span class="p">:</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">联合</font></font></font></span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">列表</font></font></font></span><span class="p">[</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">内存格式</font></font></font></span><span class="p">]]</span> <span class="o">=</span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">假</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">""</span>
<span class="sd">        dim_order(ambiguity_check=False) -&gt; tuple</span>

<span class="sd">返回唯一确定的描述维度排序的 int 元组</span>
<span class="sd">自身的物理布局。</span>

<span class="sd">维度顺序表示稠密张量在内存中的布局方式，</span>
<span class="sd">从最外层维度到最内层维度。</span>

<span class="sd">注意，维度顺序不一定总是唯一确定的。</span>
<span class="sd">如果 `ambiguity_check` 为 True，当维度顺序无法唯一确定时，此函数将引发 RuntimeError；</span>
<span class="sd">如果 `ambiguity_check` 是一个内存格式的列表，当张量无法解释为给定的内存格式之一，或者无法唯一确定时，此函数将引发 RuntimeError；</span>
<span class="sd">如果 `ambiguity_check` 为 False，它将返回一个合法的维度顺序（s）而无需检查其唯一性。</span>
<span class="sd">如果 `ambiguity_check` 为 False，它将返回一个合法的维度顺序（s）而无需检查其唯一性。</span>
<span class="sd">否则将引发 TypeError。</span>

<span class="sd">参数：</span>
<span class="sd">ambiguity_check (bool 或 List[torch.memory_format])：维度顺序模糊性检查方法。</span>

<span class="sd">示例：</span>

<span class="sd">            &gt;&gt;&gt; torch.empty((2, 3, 5, 7)).dim_order()</span>
<span class="sd">            (0, 1, 2, 3)</span>
<span class="sd">            &gt;&gt;&gt; torch.empty((2, 3, 5, 7)).transpose(1, 2).dim_order()</span>
<span class="sd">            (0, 2, 1, 3)</span>
<span class="sd">            &gt;&gt;&gt; torch.empty((2, 3, 5, 7), memory_format=torch.channels_last).dim_order()</span>
<span class="sd">            (0, 2, 3, 1)</span>
<span class="sd">            &gt;&gt;&gt; torch.empty((1, 2, 3, 4)).dim_order()</span>
<span class="sd">            (0, 1, 2, 3)</span>
<span class="sd">&gt;&gt;&gt; 尝试：</span>
<span class="sd">            ...     torch.empty((1, 2, 3, 4)).dim_order(ambiguity_check=True)</span>
<span class="sd">...     except RuntimeError as e:</span>
<span class="sd">            ...     print(e)</span>
<span class="sd">张量维度顺序不唯一，或无法映射到给定的内存格式之一。</span>
<span class="sd">            &gt;&gt;&gt; torch.empty((1, 2, 3, 4)).dim_order(</span>
<span class="sd">...     模糊性检查=[torch.contiguous_format, torch.channels_last]</span>
<span class="sd">... )  # 可以映射到连续格式</span>
<span class="sd">            (0, 1, 2, 3)</span>
<span class="sd">&gt;&gt;&gt; 尝试：</span>
<span class="sd">            ...     torch.empty((1, 2, 3, 4)).dim_order(ambiguity_check="ILLEGAL")</span>
<span class="sd">...     except TypeError as e:</span>
<span class="sd">            ...     print(e)</span>
<span class="sd">ambiguity_check 参数必须是布尔值或内存格式的列表。</span>

<span class="sd">..警告::</span>
<span class="sd">dim_order 矩阵 API 是实验性的，可能会更改。</span>
<span class="sd">        """</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="o">.</span><span class="n">dim_order</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>

        <span class="k">如果</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">属性错误</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"无法在稀疏类型上获取 dim_order："</font></font></font></span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</span><span class="p">()</span><span class="si">}</span><span class="s2"> "</span>
                <span class="s2">"首先使用 Tensor.to_dense()转换为稠密张量。"</span>
            <span class="p">)</span>

        <span class="c1"># 稳定性检查数据类型</span>
        <span class="k">如果</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="nb">isinstance</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">稳定性检查</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">如果</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="nb">isinstance</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">稳定性检查</font></font></font></span><span class="p">,</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">列表</span><span class="p">):</span>
                <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型错误</span><span class="p">(</span>
                    <span class="s2">"ambiguity_check 参数必须是布尔值或内存格式的列表。"</span>
                <span class="p">)</span>
            <span class="k">for</span> <span class="n">内存格式</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</span> <span class="n">ambiguity_check</span><span class="p">:</span>
                <span class="k">如果</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="nb">isinstance</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">内存格式</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">内存格式</span><span class="p">):</span>
                    <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型错误</span><span class="p">(</span>
                        <span class="s2">"ambiguity_check 参数必须是布尔值或内存格式的列表。"</span>
                    <span class="p">)</span>

        <span class="k">def</span> <span class="nf">无效的唯一内存格式</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">有效的内存格式</span><span class="p">):</span>
<span class="w">            </span><span class="sd">""</span>
<span class="sd">如果张量无法唯一映射到给定的任何内存格式，则返回 True，否则返回 False。</span>
<span class="sd">            """</span>

            <span class="n">合法性</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">for</span> <span class="n">内存格式</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">有效的内存格式</span><span class="p">:</span>
                <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是否连续</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">内存格式</font></font></font></span><span class="o">=</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">内存格式</span><span class="p">):</span>
                    <span class="n">合法性数量</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">返回</span> <span class="n">n_legality</span> <span class="o">!=</span> <span class="mi">1</span>

        <span class="k">def</span> <span class="nf">has_multiple_dim_order</span><span class="p">(</span><span class="n">张量</span><span class="p">):</span>
<span class="w">            </span><span class="sd">""</span>
<span class="sd">如果给定的张量存在多个合法的维度顺序，则返回 True，否则返回 False。</span>

<span class="sd">当满足以下任一条件时，张量被认为具有多个合法的维度顺序：</span>

<span class="sd">单例维度：张量中至少有一个单例维度。</span>
<span class="sd">由于它们的大小为 1，它们不会影响内存偏移（步长 * 索引）。</span>
<span class="sd">这为零，因为索引始终为零）。因此，它们可以放置在维度顺序中的任何位置，而不会改变数据访问方式。</span>
<span class="sd">因此，它们可以放置在维度顺序中的任何位置，而不会改变数据访问方式。</span>
<span class="sd">* 相同步长：步长反映了张量在内存中的存储方式。</span>
<span class="sd">如果两个维度具有相同的步长，交换这两个维度不会</span>
<span class="sd">改变数据访问方式，从而导致多个正确的维度顺序。</span>
<span class="sd">            """</span>

            <span class="n">大小</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">尺寸</span><span class="p">()</span>
            <span class="n">步伐</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">步长</span><span class="p">()</span>

            <span class="c1"># 检查是否有重复的步伐</span>
            <span class="n">存在重复步伐</font></font></font></span> <span class="o">=</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">任何</span><span class="p">(</span>
                <span class="n">早期</font></font></font></span> <span class="o">==</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">后来</font></font></font></span> <span class="k">for</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">更早</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">后来</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="nb">zip</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">步伐</font></font></font></span><span class="p">,</span> <span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">])</span>
            <span class="p">)</span>

            <span class="c1"># 检查是否存在任何单例维度</span>
            <span class="n">has_singleton_dims</span> <span class="o">=</span> <span class="nb">任何</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">大小</font></font></font></span> <span class="o">==</span> <span class="mi">1</span> <span class="k">for</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">大小</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">尺寸</span><span class="p">)</span>

            <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">具有重复步长</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">或者</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">具有单例维度</span>

        <span class="n">有效的内存格式</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">模糊性检查</font></font></font></span> <span class="k"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">如果</font></font></font></span> <span class="nb">isinstance</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">模糊性检查</font></font></font></span><span class="p">,</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">列表</font></font></font></span><span class="p">)</span> <span class="k"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">否则</font></font></font></span> <span class="p"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">输入文本为空，请提供需要翻译的文本</span>
        <span class="p">)</span>
        <span class="n">检查多个维度顺序</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">模糊性检查</font></font></font></span> <span class="k"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">如果</font></font></font></span> <span class="nb">isinstance</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">模糊性检查</font></font></font></span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span> <span class="k"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">否则</font></font></font></span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">真实</span>
        <span class="p">)</span>

        <span class="k">如果</span> <span class="p">(</span>
            <span class="n">检查多维度顺序</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">和</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">具有多维度顺序</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="p">)</span> <span class="ow">和</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无效的唯一内存格式</font></font></font></span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">有效的内存格式</span><span class="p">):</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                <span class="s2">张量维度顺序不唯一，或无法映射到给定的内存格式之一。</span>
            <span class="p">)</span>

        <span class="kn">导入</font></font></font></span> <span class="nn">torch._prims_common</span> <span class="k">as</span> <span class="nn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">工具</span>

        <span class="k">返回</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元组</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">工具</span><span class="o">.</span><span class="n">compute_elementwise_output_logical_to_physical_perm</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span></div>

    <span class="k">def</span> <span class="nf">_update_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">原地</span><span class="p">):</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">张量</font></font></font></span><span class="o">.</span><span class="n">_update_names</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">内置</span>
            <span class="p">)</span>

        <span class="c1"># 请参阅注释 [重命名_ / 重命名 API]</span>
        <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">原地</span><span class="p">:</span>
            <span class="k">返回</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">超级</font></font></font></span><span class="p">()</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">重命名_</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
        <span class="k">否则</span><span class="p">:</span>
            <span class="k">返回</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">超级</font></font></font></span><span class="p">()</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">重命名</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__torch_function__</span><span class="p">(</span><span class="bp">类</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">函数</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</font></font></font></span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span><span class="p">):</span>
<span class="w">        </span><span class="sd">""</span>
<span class="sd">这个 __torch_function__ 实现包装了子类，以便</span>
<span class="sd">子类上调用的方法返回子类实例而不是</span>
<span class="sd">一个 `torch.Tensor` 实例。</span>

<span class="sd">这一结果的一个推论是，你需要为 torch.Tensor 提供覆盖</span>
<span class="sd">实现子类 __torch_function__ 的方法。</span>

<span class="sd">我们建议始终调用 `super().__torch_function__` 作为基类</span>
<span class="sd">当执行上述操作时。</span>

<span class="sd">虽然不是强制性的，但我们建议将`__torch_function__`设为类方法。</span>
<span class="sd">        """</span>
        <span class="k">如果</font></font></font></span> <span class="n">kwargs</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span><span class="p">:</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">如果</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">所有</font></font></font></span><span class="p">(</span><span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">派生类</font></font></font></span><span class="p">(</span><span class="bp"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类</font></font></font></span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</span><span class="p">):</span>
            <span class="k">返回</span> <span class="bp">NotImplemented</span>

        <span class="k">替换为</font></font></font></span> <span class="n">_C</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">禁用 TorchFunction 子类</span><span class="p">():</span>
            <span class="n">返回</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">函数</font></font></font></span><span class="p">(</span><span class="o">*</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">参数</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">函数</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">获取默认不换行函数</span><span class="p">():</span>
                <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</span>
            <span class="k">否则</span><span class="p">:</span>
                <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">转换</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</font></font></font></span><span class="p">,</span> <span class="bp"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类</span><span class="p">)</span>

    <span class="n">__torch_dispatch__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">禁用 torch 分发实现</span>

    <span class="k">def</span> <span class="nf">__dlpack__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">流</font></font></font></span><span class="o">=</span><span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span><span class="p">):</span>
<span class="w">        </span><span class="sd">""</span>
<span class="sd">创建一个 DLpack `胶囊 https://data-apis.org/array-api/latest/design_topics/data_interchange.html#data-interchange`</span>
<span class="sd">将当前张量导出到其他库中。</span>

<span class="sd">此函数将从 `from_dlpack` 方法中调用。</span>
<span class="sd">消费胶囊的库将调用此方法。`from_dlpack` 作为规范的一部分将流传递给此方法。</span>
<span class="sd">流作为规范的一部分传递给此方法。</span>

<span class="sd">参数：</span>
<span class="sd">流（整数或 None）：表示 CUDA 流的 Python 整数，可选</span>
<span class="sd">指向 CUDA 流的指针。在创建胶囊之前，当前流与此流同步，并且胶囊与张量共享存储，因此从该流访问是安全的</span>
<span class="sd">这使得从该流访问胶囊是安全的，因为胶囊与张量共享存储</span>
<span class="sd">因此，从该流访问胶囊是安全的</span>
<span class="sd">两个流。如果传入 None 或-1，则不执行同步。</span>
<span class="sd">如果为 1（在 CUDA 上）或 0（在 ROCM 上），则使用默认流。</span>
<span class="sd">用于同步。</span>
<span class="sd">        """</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">dlpack</font></font></font></span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">流</span><span class="p">)</span>

        <span class="c1">DLPack 胶囊无法捕获 PyTorch 的所有语义</span>
        <span class="c1">因此，我们禁止导出会失去其属性的张量</span>
        <span class="c1"># 需要梯度并且设置了共轭位。</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">需要梯度</span><span class="p">:</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                <span class="s2">"无法导出需要梯度的张量，请使用 tensor.detach()"</span>
            <span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是连词</span><span class="p">():</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</font></font></font></span><span class="p">(</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">"无法导出设置了共轭位的张量"</span><span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">布局</font></font></font></span> <span class="o">!=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</span><span class="o">.</span><span class="n">strided</span><span class="p">:</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                <span class="s2">"无法导出布局不是 torch.strided 的张量"</span>
            <span class="p">)</span>

        <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">流</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">和</font></font></font></span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">流</font></font></font></span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</span> <span class="nb">int</span><span class="p">:</span>
            <span class="c1">CUDA/ROCm 中的流指针是唯一编号的，并且可以</span>
            <span class="c1">从它们的整数值中检索。</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型错误</font></font></font></span><span class="p">(</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">流必须是 ``int`` 或 ``none``</span><span class="p">)</span>
        <span class="k">如果...否则</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">流</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">和</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">流</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span> <span class="o">==</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">cuda</span><span class="p">:</span>
                <span class="c1"># 注意：此逻辑处理默认值的特殊情况</span>
                <span class="c1">流必须与 from_dlpack 保持同步</span>
                <span class="c1">torch/utils/dlpack.py</span>
                <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">流</font></font></font></span> <span class="o">==</span> <span class="mi">1</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">和</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">版本</font></font></font></span><span class="o">.</span><span class="n">hip</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span><span class="p">:</span>
                    <span class="n">流</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">默认流</span><span class="p">()</span>
                <span class="k">如果...否则</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">流</font></font></font></span> <span class="o">==</span> <span class="mi">0</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">和</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">版本</font></font></font></span><span class="o">.</span><span class="n">hip</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span><span class="p">:</span>
                    <span class="n">流</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">默认流</span><span class="p">()</span>
                <span class="k">否则</span><span class="p">:</span>
                    <span class="n">流</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">外部流</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">流</span><span class="p">)</span>
                <span class="c1"># 仅在不同流上同步</span>
                <span class="n">同步流</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
                <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">流</font></font></font></span> <span class="o">!=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">同步流</span><span class="p">:</span>
                    <span class="n">事件</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">活动</span><span class="p">()</span>
                    <span class="n">事件</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">记录</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">同步流</span><span class="p">)</span>
                    <span class="n">流</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">等待事件</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">事件</span><span class="p">)</span>
        <span class="k">如果</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</span> <span class="o">==</span> <span class="s2">"xla"</span><span class="p">:</span>
            <span class="kn">导入</span> <span class="nn">torch_xla</span>
            <span class="kn">导入</span> <span class="nn">torch_xla.utils.dlpack</span> <span class="k">as</span> <span class="nn">xla_dlpack</span>

            <span class="k">如果</span> <span class="p">(</span>
                <span class="nb">长度</span><span class="p">(</span><span class="n">torch_xla</span><span class="o">.</span><span class="n">real_devices</span><span class="p">())</span> <span class="o">&lt;=</span> <span class="mi">0</span>
                <span class="ow">或者</font></font></font></span> <span class="s2">"cuda"</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="n">torch_xla</span><span class="o">.</span><span class="n">real_devices</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">小写</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">运行时错误</span><span class="p">(</span>
                    <span class="s2">无法导出非 CUDA 上的 XLA 张量到 dlpack。</span>
                <span class="p">)</span>
            <span class="k">返回</font></font></font></span> <span class="n">xla_dlpack</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">转换为 dlpack</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">转换为 dlpack</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__dlpack_device__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">元组</font></font></font></span><span class="p">[</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">枚举</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">整数枚举</font></font></font></span><span class="p">,</span> <span class="nb">int</span><span class="p"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">]</span>
        <span class="k">如果</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">返回</font></font></font></span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="o">.</span><span class="n">__dlpack_device__</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">,),</span> <span class="bp">self</span><span class="p">)</span>

        <span class="kn">来自</font></font></font></span> <span class="nn">torch.utils.dlpack</span> <span class="kn"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">导入</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">DL 设备类型</span>

        <span class="n">设备</font></font></font></span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</span>
        <span class="n">索引</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">索引</font></font></font></span> <span class="k"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">索引</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</font></font></font></span> <span class="k"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">否则</span> <span class="mi">0</span>
        <span class="n">火炬设备类型</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</span>
        <span class="k">如果</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">火炬设备类型</font></font></font></span> <span class="o">==</span> <span class="s2">"cuda"</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">和</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">PyTorch</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">版本</font></font></font></span><span class="o">.</span><span class="n">hip</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="kc"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">无</span><span class="p">:</span>
            <span class="n">设备类型</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">深度学习设备类型</span><span class="o">.</span><span class="n">kDLROCM</span>
        <span class="k">如果...否则</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">火炬设备类型</font></font></font></span> <span class="o">==</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">cpu</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">和</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_pinned</span><span class="p">():</span>
            <span class="n">设备类型</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">DL 设备类型</span><span class="o">.</span><span class="n">kDLCPUPinned</span>
        <span class="k">如果...否则</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">火炬设备类型</font></font></font></span> <span class="o">==</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">cuda</span><span class="p">:</span>
            <span class="n">设备类型</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">DL 设备类型</span><span class="o">.</span><span class="n">kDLGPU</span>
        <span class="k">如果...否则</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">火炬设备类型</span> <span class="o">==</span> <span class="s2">"cpu"</span><span class="p">:</span>
            <span class="n">设备类型</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">DL 设备类型</span><span class="o">.</span><span class="n">kDLCPU</span>
        <span class="k">如果...否则</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">torch 设备类型</font></font></font></span> <span class="o">==</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">XPU</span><span class="p">:</span>
            <span class="n">设备类型</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">DL 设备类型</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">kDL 单 API</span>
        <span class="k">如果...否则</font></font></font></span> <span class="bp">self</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span> <span class="o">==</span> <span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">privateuse1</span><span class="p">:</span>
            <span class="n">设备类型</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">DL 设备类型</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">kDL 扩展设备</span>
        <span class="k">如果...否则</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">火炬设备类型</span> <span class="o">==</span> <span class="s2">"xla"</span><span class="p">:</span>
            <span class="kn">导入</span> <span class="nn">torch_xla</span>

            <span class="k">如果</span> <span class="p">(</span>
                <span class="nb">长度</font></font></font></span><span class="p">(</span><span class="n">torch_xla</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">真实设备</span><span class="p">())</span> <span class="o">&lt;=</span> <span class="mi">0</span>
                <span class="ow">或者</font></font></font></span> <span class="s2">"cuda"</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="n">torch_xla</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">真实设备</font></font></font></span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">小写</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">值错误</font></font></font></span><span class="p">(</span><span class="sa">f</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">"未知设备类型"</font></font></font></span><span class="si">{</span><span class="n">torch_device_type</span><span class="si">}</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">用于 Dlpack</span><span class="p">)</span>

            <span class="n">设备类型</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">DL 设备类型</span><span class="o">.</span><span class="n">kDLGPU</span>
        <span class="k">否则</span><span class="p">:</span>
            <span class="k">抛出</font></font></font></span> <span class="ne"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">值错误</font></font></font></span><span class="p">(</span><span class="sa">f</span><span class="s2"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">"未知设备类型"</font></font></font></span><span class="si">{</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">火炬设备类型</span><span class="si">}</span><span class="s2"> for Dlpack"</span><span class="p">)</span>
        <span class="k">返回</font></font></font></span> <span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">设备类型</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">索引</span><span class="p">)</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">torch</span>


<span class="k">def</span> <span class="nf">_convert</span><span class="p">(</span><span class="n">返回</font></font></font></span><span class="p">,</span> <span class="bp"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类</span><span class="p">):</span>
    <span class="k">如果</font></font></font></span> <span class="bp"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">是</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</span><span class="p">:</span>
        <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</span>

    <span class="k">如果</font></font></font></span> <span class="nb">isinstance</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</font></font></font></span><span class="p">,</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">张量</font></font></font></span><span class="p">)</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">和</font></font></font></span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">不</font></font></font></span> <span class="nb">isinstance</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</font></font></font></span><span class="p">,</span> <span class="bp"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类</span><span class="p">):</span>
        <span class="n">返回</font></font></font></span> <span class="o">=</span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</font></font></font></span><span class="o">.</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">作为子类</font></font></font></span><span class="p">(</span><span class="bp"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类</span><span class="p">)</span>

    <span class="k">如果</font></font></font></span> <span class="nb">isinstance</span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</font></font></font></span><span class="p">,</span> <span class="p">(</span><span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">元组</font></font></font></span><span class="p">,</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">列表</span><span class="p">)):</span>
        <span class="c1">也处理像 namedtuple 这样的东西</span>
        <span class="n">返回</font></font></font></span> <span class="o">=</span> <span class="nb"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类型</font></font></font></span><span class="p">(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</font></font></font></span><span class="p">)(</span><span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">转换</font></font></font></span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="bp"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">类</font></font></font></span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">在</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</span><span class="p">)</span>

    <span class="k">返回</font></font></font></span> <span class="n"><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">返回</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>© 版权所有 PyTorch 贡献者。</p>
  </div>
    
      <div>使用 Sphinx 构建，并使用 Read the Docs 提供的主题。</div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/clipboard.min.js"></script>
         <script src="../../_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script="" type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0">


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>文档</h2>
          <p>查看 PyTorch 的全面开发者文档</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">查看文档</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>教程</h2>
          <p>深入了解初学者和高级开发者的教程</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">查看教程</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>资源</h2>
          <p>查找开发资源，获取您的疑问解答</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">查看资源</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">开始使用</a></li>
            <li><a href="https://pytorch.org/features">功能</a></li>
            <li><a href="https://pytorch.org/ecosystem">生态系统</a></li>
            <li><a href="https://pytorch.org/blog/">博客</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">贡献</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">资源</a></li>
            <li><a href="https://pytorch.org/tutorials">教程</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">文档</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">讨论</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github 问题</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">品牌指南</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">保持更新</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">推特</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">领英</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch 播客</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">苹果</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">谷歌</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">亚马逊</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">条款</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">隐私</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© 版权所有 Linux 基金会。PyTorch 基金会是 Linux 基金会的一个项目。有关 PyTorch 基金会的网站使用条款、商标政策以及其他适用政策，请参阅 www.linuxfoundation.org/policies/。PyTorch 基金会支持 PyTorch 开源项目，该项目已被确立为 LF Projects, LLC 的 PyTorch 项目系列。有关适用于 PyTorch 项目系列 LF Projects, LLC 的政策，请参阅 www.lfprojects.org/policies/。</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">为了分析流量并优化您的体验，我们在本网站上使用 cookies。通过点击或导航，您同意允许我们使用 cookies。作为本网站的当前维护者，适用 Facebook 的 Cookies 政策。了解更多信息，包括关于可用控制的信息：Cookies 政策。</p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg" width="16" height="16">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>学习</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">开始使用</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">教程</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">学习基础知识</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch 食谱</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">PyTorch 入门 - YouTube 系列</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>生态系统</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">工具</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">社区</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">论坛</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">开发者资源</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">贡献者奖项 - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">关于 PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">执行火炬</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch 文档</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>文档</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch 领域</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>博客 &amp; 新闻</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch 博客</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">社区博客</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">视频</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">社区故事</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">活动</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">新闻简报</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>关于</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch 基金会</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">治理委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">云信用计划</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">技术咨询委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">员工</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">联系我们</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

</body></html>