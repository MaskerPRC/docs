<!DOCTYPE html>
<html lang="zh_CN">
<head>
  <meta charset="UTF-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.Tensor — PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/tensors.html">
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css">
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css">
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css">
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css">
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css">
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css">
    <link rel="index" title="Index" href="genindex.html">
    <link rel="search" title="Search" href="search.html">
    <link rel="next" title="torch.Tensor.new_tensor" href="generated/torch.Tensor.new_tensor.html">
    <link rel="prev" title="torch.nn.functional.torch.nn.parallel.data_parallel" href="generated/torch.nn.functional.torch.nn.parallel.data_parallel.html">

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head><body class="pytorch-body"><div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">学习</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class="dropdown-title">开始使用</span>
                  <p>在本地运行 PyTorch 或快速开始使用支持的云平台之一</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">教程</span><p></p>
                  <p>PyTorch 教程中的新内容</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">学习基础知识</span><p></p>
                  <p>熟悉 PyTorch 的概念和模块</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch 食谱</span><p></p>
                  <p>精简版、可直接部署的 PyTorch 代码示例</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">PyTorch 入门 - YouTube 系列</span><p></p>
                  <p>通过我们引人入胜的 YouTube 教程系列掌握 PyTorch 基础知识</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">生态系统</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">工具</span><p></p>
                  <p>了解 PyTorch 生态系统中的工具和框架</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">社区</span>
                  <p>加入 PyTorch 开发者社区，贡献、学习并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">论坛</span>
                  <p>讨论 PyTorch 代码、问题、安装、研究的地方</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">开发者资源</span>
                  <p>查找资源并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">贡献者奖项 - 2024</span><p></p>
                  <p>本届 PyTorch 会议揭晓获奖者</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">关于 PyTorch Edge</span><p></p>
                  <p>为边缘设备构建创新和隐私感知的 AI 体验</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span><p></p>
                  <p>基于移动和边缘设备的端到端推理能力解决方案</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch 文档</span><p></p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">文档</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span><p></p>
                  <p>探索文档以获取全面指导，了解如何使用 PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch 领域</span><p></p>
                  <p>阅读 PyTorch 领域的文档，了解更多关于特定领域库的信息</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">博客与新闻</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch 博客</span><p></p>
                  <p>捕捉最新的技术新闻和事件</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">社区博客</span><p></p>
                  <p>PyTorch 生态系统故事</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">视频</span><p></p>
                  <p>了解最新的 PyTorch 教程、新内容等</p>
                </a><a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">社区故事</span><p></p>
                  <p>学习如何我们的社区使用 PyTorch 解决真实、日常的机器学习问题</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">活动</span><p></p>
                  <p>查找活动、网络研讨会和播客</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">通讯</span><p></p>
                  <p>跟踪最新更新</p>
                </a>
            </div>
          </div></li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">关于</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch 基金会</span><p></p>
                  <p>了解更多关于 PyTorch 基金会的信息</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">管理委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">云信用计划</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">技术顾问委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">员工</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">联系我们</span><p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">成为会员</a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>



   

    

    <div class="table-of-contents-link-wrapper">
      <span>目录</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="https://pytorch.org/docs/versions.html">主程序 (2.7.0+cpu ) ▼</a>
    </div>
    <div id="searchBox">
    <div class="searchbox" id="googleSearchBox">
      <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
      <div class="gcse-search"></div>
    </div>
    <div id="sphinxSearchBox" style="display: none;">
      <div role="search">
        <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
          <input type="text" name="q" placeholder="Search Docs">
          <input type="hidden" name="check_keywords" value="yes">
          <input type="hidden" name="area" value="default">
        </form>
      </div>
    </div>
  </div>
  <form id="searchForm">
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="google" checked="">谷歌搜索</label>
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="sphinx">经典搜索</label>
  </form>

  <script>
     document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
  </script>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/tensors.html">您正在查看不稳定开发者预览文档。请点击此处查看最新稳定版本的文档。</a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">社区</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch 治理 | 构建 + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch 贡献指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch 设计哲学</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch 治理 | 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch 治理 | 维护者</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者笔记</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">自动混合精度示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">广播语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU 多线程和 TorchScript 推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA 语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/custom_operators.html">PyTorch 自定义算子页面</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">分布式数据并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">扩展 PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">使用 autograd.Function 扩展 torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">常见问题解答</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/fsdp.html">FSDP 笔记</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/get_start_xpu.html">在 Intel GPU 上入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm)语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">大规模部署功能</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/libtorch_stable_abi.html">LibTorch 稳定 ABI</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">模块</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS 后端</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">多进程最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">数值精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">可重现性</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">序列化语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows 常见问题解答</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语言绑定</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">张量属性</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">索引视图</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.自动混合精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="accelerator.html">torch.accelerator</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">理解 CUDA 内存使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">生成快照</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">使用可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">快照 API 参考</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.memory.html">torch.mtia.memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">元设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.html">torch.distributed.tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.fsdp.fully_shard.html">torch.distributed.fsdp.fully_shard</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">复数</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP 通信钩子</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">分布式 RPC 框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">类型信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">命名张量</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">命名张量操作覆盖率</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_environment_variables.html">火炬环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">库</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">火炬推荐</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch 在 XLA 设备上</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/ao">torchao</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        文档 &gt;</li>

        
      <li>torch.Tensor</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/tensors.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg" width="16" height="16"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">快捷键</div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="torch-tensor">
<span id="tensor-doc"></span><h1>torch.Tensor</h1>
<p>A <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> 是一个包含单一数据类型元素的多元矩阵。</p>
<section id="data-types">
<h2>数据类型</h2>
<p>火炬使用以下数据类型定义张量类型：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>数据类型</p></th>
<th class="head"><p>dtype</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>32 位浮点数</p></td>
<td><p> <code class="docutils literal "><span class="pre">torch.float32</span></code> 或 <code class="docutils literal "><span class="pre">torch.float</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p>64 位浮点数</p></td>
<td><p> <code class="docutils literal "><span class="pre">torch.float64</span></code> 或 <code class="docutils literal "><span class="pre">torch.double</span></code> </p></td>
</tr>
<tr class="row-even"><td><p>16 位浮点数 [1]</p></td>
<td><p> <code class="docutils literal "><span class="pre">torch.float16</span></code> 或 <code class="docutils literal "><span class="pre">torch.half</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p>16 位浮点数 [2]</p></td>
<td><p><code class="docutils literal "><span class="pre">torch.bfloat16</span></code></p></td>
</tr>
<tr class="row-even"><td><p>32 位复数</p></td>
<td><p> <code class="docutils literal "><span class="pre">torch.complex32</span></code> 或 <code class="docutils literal "><span class="pre">torch.chalf</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p>64 位复数</p></td>
<td><p> <code class="docutils literal "><span class="pre">torch.complex64</span></code> 或 <code class="docutils literal "><span class="pre">torch.cfloat</span></code> </p></td>
</tr>
<tr class="row-even"><td><p>128 位复数</p></td>
<td><p> <code class="docutils literal "><span class="pre">torch.complex128</span></code> 或 <code class="docutils literal "><span class="pre">torch.cdouble</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p>8 位无符号整数</p></td>
<td><p><code class="docutils literal "><span class="pre">torch.uint8</span></code></p></td>
</tr>
<tr class="row-even"><td><p>16 位无符号整数</p></td>
<td><p> <code class="docutils literal "><span class="pre">torch.uint16</span></code> （有限支持）[4]</p></td>
</tr>
<tr class="row-odd"><td><p>32 位整数（无符号）</p></td>
<td><p> <code class="docutils literal "><span class="pre">torch.uint32</span></code> （有限支持）[4]</p></td>
</tr>
<tr class="row-even"><td><p>64 位整数（无符号）</p></td>
<td><p>（有限支持）[4]</p></td>
</tr>
<tr class="row-odd"><td><p>8 位整数（有符号）</p></td>
<td><p><code class="docutils literal "><span class="pre">torch.int8</span></code></p></td>
</tr>
<tr class="row-even"><td><p>16 位整数（有符号）</p></td>
<td><p> <code class="docutils literal "><span class="pre">torch.int16</span></code> 或 <code class="docutils literal "><span class="pre">torch.short</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p>32 位整数（有符号）</p></td>
<td><p> <code class="docutils literal "><span class="pre">torch.int32</span></code> 或 <code class="docutils literal "><span class="pre">torch.int</span></code> </p></td>
</tr>
<tr class="row-even"><td><p>64 位整数（有符号）</p></td>
<td><p> <code class="docutils literal "><span class="pre">torch.int64</span></code> 或 <code class="docutils literal "><span class="pre">torch.long</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p>布尔值</p></td>
<td><p><code class="docutils literal "><span class="pre">torch.bool</span></code></p></td>
</tr>
<tr class="row-even"><td><p>定量 8 位整数（无符号）</p></td>
<td><p><code class="docutils literal "><span class="pre">torch.quint8</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>定量 8 位整数（有符号）</p></td>
<td><p><code class="docutils literal "><span class="pre">torch.qint8</span></code></p></td>
</tr>
<tr class="row-even"><td><p>量化 32 位整数（有符号）</p></td>
<td><p><code class="docutils literal "><span class="pre">torch.qint32</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>量化 4 位整数（无符号）[3]</p></td>
<td><p><code class="docutils literal "><span class="pre">torch.quint4x2</span></code></p></td>
</tr>
<tr class="row-even"><td><p>8 位浮点数，e4m3 [5]</p></td>
<td><p> <code class="docutils literal "><span class="pre">torch.float8_e4m3fn</span></code> （有限支持）</p></td>
</tr>
<tr class="row-odd"><td><p>8 位浮点数，e5m2 [5]</p></td>
<td><p> <code class="docutils literal "><span class="pre">torch.float8_e5m2</span></code> （有限支持）</p></td>
</tr>
</tbody>
</table>
<aside class="footnote brackets" id="id9" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>有时被称为二进制 16：使用 1 个符号位、5 个指数位和 10 个尾数位。当精度重要而范围牺牲时很有用。</p>
</aside>
<aside class="footnote brackets" id="id10" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>有时被称为脑浮点数：使用 1 个符号位、8 个指数位和 7 个尾数位。当范围很重要时很有用，因为它与 <code class="docutils literal "><span class="pre">float32</span></code> 具有相同的指数位数量。</p>
</aside>
<aside class="footnote brackets" id="id11" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">3</a><span class="fn-bracket">]</span></span>
<p>量化 4 位整数存储为 8 位有符号整数。目前仅在 EmbeddingBag 操作符中支持。</p>
</aside>
<aside class="footnote brackets" id="id12" role="note">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id4">2</a>,<a role="doc-backlink" href="#id5">3</a>)</span>
<p>除了 <code class="docutils literal "><span class="pre">uint8</span></code> 之外，未签名的类型目前仅计划在急切模式下提供有限支持（它们主要存在是为了辅助使用 torch.compile）；如果您需要急切支持且不需要额外的范围，我们建议使用它们的签名变体。有关更多详细信息，请参阅 https://github.com/pytorch/pytorch/issues/58734。</p>
</aside>
<aside class="footnote brackets" id="id13" role="note">
<span class="label"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id7">1</a>,<a role="doc-backlink" href="#id8">2</a>)</span>
<p> <code class="docutils literal "><span class="pre">torch.float8_e4m3fn</span></code> 和 <code class="docutils literal "><span class="pre">torch.float8_e5m2</span></code> 实现了来自 https://arxiv.org/abs/2209.05433 的 8 位浮点类型规范。操作支持非常有限。</p>
</aside>
<p>为了向后兼容，我们支持以下这些数据类型的替代类名：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>数据类型</p></th>
<th class="head"><p>CPU 张量</p></th>
<th class="head"><p>GPU 张量</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>32 位浮点数</p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.FloatTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.cuda.FloatTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>64 位浮点数</p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.DoubleTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.cuda.DoubleTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>16 位浮点数</p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.HalfTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.cuda.HalfTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>16 位浮点数</p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.BFloat16Tensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.cuda.BFloat16Tensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>8 位无符号整数</p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.ByteTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.cuda.ByteTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>8 位有符号整数</p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.CharTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.cuda.CharTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>16 位有符号整数</p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.ShortTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.cuda.ShortTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>32 位有符号整数</p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.IntTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.cuda.IntTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>64 位整数（有符号）</p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.LongTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.cuda.LongTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>布尔值</p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.BoolTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal "><span class="pre">torch.cuda.BoolTensor</span></code></p></td>
</tr>
</tbody>
</table>
<p>然而，为了构建张量，我们建议使用工厂函数，例如使用 <code class="xref py py-func docutils literal "><span class="pre">torch.empty()</span></code> 函数，并传入 <code class="docutils literal "><span class="pre">dtype</span></code> 参数。 <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> 构造函数是默认张量类型（ <code class="xref py py-class docutils literal "><span class="pre">torch.FloatTensor</span></code> ）的别名。</p>
</section>
<section id="initializing-and-basic-operations">
<h2>初始化和基本操作</h2>
<p>张量可以通过使用 <code class="xref py py-class docutils literal "><span class="pre">list</span></code> 或序列通过 <code class="xref py py-func docutils literal "><span class="pre">torch.tensor()</span></code> 构造器来构建：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]])</span>
<span class="go">tensor([[ 1.0000, -1.0000],</span>
<span class="go">        [ 1.0000, -1.0000]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]))</span>
<span class="go">tensor([[ 1,  2,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p> <code class="xref py py-func docutils literal "><span class="pre">torch.tensor()</span></code> 总是复制 <code class="xref py py-attr docutils literal "><span class="pre">data</span></code> 。如果你有一个张量 <code class="xref py py-attr docutils literal "><span class="pre">data</span></code> 并且只想更改其 <code class="docutils literal "><span class="pre">requires_grad</span></code> 标志，请使用 <code class="xref py py-meth docutils literal "><span class="pre">requires_grad_()</span></code> 或 <code class="xref py py-meth docutils literal "><span class="pre">detach()</span></code> 以避免复制。如果你有一个 NumPy 数组并且想避免复制，请使用 <code class="xref py py-func docutils literal "><span class="pre">torch.as_tensor()</span></code> 。</p>
</div>
<p>可以通过传递一个 <code class="xref py py-class docutils literal "><span class="pre">torch.dtype</span></code> 和/或一个 <code class="xref py py-class docutils literal "><span class="pre">torch.device</span></code> 给构造器或张量创建操作来构建特定数据类型的张量：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="go">tensor([[ 0,  0,  0,  0],</span>
<span class="go">        [ 0,  0,  0,  0]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda:0'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda0</span><span class="p">)</span>
<span class="go">tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],</span>
<span class="go">        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')</span>
</pre></div>
</div>
<p>有关构建张量的更多信息，请参阅创建操作</p>
<p>张量的内容可以使用 Python 的索引和切片表示法进行访问和修改：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="go">tensor(6)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor([[ 1,  8,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
</pre></div>
</div>
<p>使用 <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.item()</span></code> 从包含单个值的张量中获取 Python 数字：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor(2.5000)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">2.5</span>
</pre></div>
</div>
<p>更多关于索引的信息，请参阅索引、切片、连接、可变操作</p>
<p>可以使用 <code class="xref py py-attr docutils literal "><span class="pre">requires_grad=True</span></code> 创建张量，以便 <code class="xref py py-mod docutils literal "><span class="pre">torch.autograd</span></code> 记录对它们的操作，以实现自动微分。</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[ 2.0000, -2.0000],</span>
<span class="go">        [ 2.0000,  2.0000]])</span>
</pre></div>
</div>
<p>每个张量都有一个关联的 <code class="xref py py-class docutils literal "><span class="pre">torch.Storage</span></code> ，用于存储其数据。张量类还提供了对存储的多维、步长视图，并定义了对其上的数值操作。</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>关于张量视图的更多信息，请参阅张量视图。</p>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>关于张量的 <code class="xref py py-class docutils literal "><span class="pre">torch.dtype</span></code> 、 <code class="xref py py-class docutils literal "><span class="pre">torch.device</span></code> 和 <code class="xref py py-class docutils literal "><span class="pre">torch.layout</span></code> 属性的相关信息，请参阅张量属性。</p>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>修改张量的方法带有下划线后缀。例如， <code class="xref py py-func docutils literal "><span class="pre">torch.FloatTensor.abs_()</span></code> 在原地计算绝对值并返回修改后的张量，而 <code class="xref py py-func docutils literal "><span class="pre">torch.FloatTensor.abs()</span></code> 在新的张量中计算结果。</p>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>要更改现有张量的 <code class="xref py py-class docutils literal "><span class="pre">torch.device</span></code> 和/或 <code class="xref py py-class docutils literal "><span class="pre">torch.dtype</span></code> ，请考虑使用张量上的 <code class="xref py py-meth docutils literal "><span class="pre">to()</span></code> 方法。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>当前 <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> 的实现引入了内存开销，因此它可能导致在包含许多小张量的应用程序中意外地使用大量内存。如果这是您的情况，请考虑使用一个大结构。</p>
</div>
</section>
<section id="tensor-class-reference">
<h2>张量类参考</h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.Tensor">
class torch.Tensor</dt>
<dd><p>根据您的使用场景，创建张量主要有几种方法。</p>
<ul class="simple">
<li><p>要使用现有数据创建张量，请使用 <code class="xref py py-func docutils literal "><span class="pre">torch.tensor()</span></code> 。</p></li>
<li><p>要创建具有特定大小的张量，请使用 <code class="docutils literal "><span class="pre">torch.*</span></code> 张量创建操作（参见创建操作）。</p></li>
<li><p>要创建与另一个张量具有相同大小（和类似类型）的张量，请使用 <code class="docutils literal "><span class="pre">torch.*_like</span></code> 张量创建操作（参见创建操作）。</p></li>
<li><p>要创建与另一个张量类型相似但大小不同的张量，请使用 <code class="docutils literal "><span class="pre">tensor.new_*</span></code> 创建操作。</p></li>
<li><p>存在一个不推荐的遗留构造函数 <code class="docutils literal "><span class="pre">torch.Tensor</span></code> ，请使用 <code class="xref py py-func docutils literal "><span class="pre">torch.tensor()</span></code> 代替。</p></li>
</ul>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.Tensor.__init__">
<span class="sig-prename descclassname"><span class="pre">Tensor.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>此构造函数已弃用，我们建议使用 <code class="xref py py-func docutils literal "><span class="pre">torch.tensor()</span></code> 代替。此构造函数执行的操作取决于 <code class="docutils literal "><span class="pre">data</span></code> 的类型。</p>
<ul class="simple">
<li><p>如果 <code class="docutils literal "><span class="pre">data</span></code> 是一个 Tensor，则返回原始 Tensor 的别名。与 <code class="xref py py-func docutils literal "><span class="pre">torch.tensor()</span></code> 不同，此操作会跟踪 autograd 并将梯度传播到原始 Tensor。 <code class="docutils literal "><span class="pre">device</span></code> 关键字参数不支持此 <code class="docutils literal "><span class="pre">data</span></code> 类型。</p></li>
<li><p>如果 <code class="docutils literal "><span class="pre">data</span></code> 是一个序列或嵌套序列，则创建一个默认数据类型（通常是 <code class="docutils literal "><span class="pre">torch.float32</span></code> ）的张量，其数据是序列中的值，必要时进行强制转换。值得注意的是，这与 <code class="xref py py-func docutils literal "><span class="pre">torch.tensor()</span></code> 不同，因为此构造函数始终构造一个浮点张量，即使输入都是整数。</p></li>
<li><p>如果 <code class="docutils literal "><span class="pre">data</span></code> 是一个 <code class="xref py py-class docutils literal "><span class="pre">torch.Size</span></code> ，则返回该大小的空张量。</p></li>
</ul>
<p>此构造函数不支持显式指定返回张量的 <code class="docutils literal "><span class="pre">dtype</span></code> 或 <code class="docutils literal "><span class="pre">device</span></code> 。我们建议使用 <code class="xref py py-func docutils literal "><span class="pre">torch.tensor()</span></code> ，它提供了此功能。</p>
<dl class="simple">
<dt>Args:</dt><dd><p>data (array_like): 从中构建的张量。</p>
</dd>
<dt>关键字参数:</dt><dd><dl class="simple">
<dt>device ( <code class="xref py py-class docutils literal "><span class="pre">torch.device</span></code> , 可选): 返回张量的期望设备。</font></font></font></dt><dd><p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">默认：如果为空，则与该张量相同的 <code class="xref py py-class docutils literal "><span class="pre">torch.device</span></code> 。</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.Tensor.T">
张量.T ¶</dt>
<dd><p>返回一个反转维度的该张量视图。</p>
<p>如果 <code class="docutils literal "><span class="pre">n</span></code> 是 <code class="docutils literal "><span class="pre">x</span></code> 的维度数，则 <code class="docutils literal "><span class="pre">x.T</span></code> 等价于 <code class="docutils literal "><span class="pre">x.permute(n-1,</span> <span class="pre">n-2,</span> <span class="pre">...,</span> <span class="pre">0)</span></code> 。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>在维度不是 2 的张量上使用 <code class="xref py py-func docutils literal "><span class="pre">Tensor.T()</span></code> 来反转其形状已被弃用，并在未来的版本中会抛出错误。请考虑使用 <code class="xref py py-attr docutils literal "><span class="pre">mT</span></code> 来转置矩阵批次或 x.permute(*torch.arange(x.ndim - 1, -1, -1))来反转张量的维度。</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.Tensor.H">
<span class="sig-prename descclassname"><span class="pre">Tensor.</span></span><span class="sig-name descname"><span class="pre">H</span></span><a class="headerlink" href="#torch.Tensor.H" title="Permalink to this definition">¶</a></dt>
<dd><p>返回一个矩阵（二维张量）的共轭转置视图。</p>
<p> <code class="docutils literal "><span class="pre">x.H</span></code> 对于复数矩阵等同于 <code class="docutils literal "><span class="pre">x.transpose(0,</span> <span class="pre">1).conj()</span></code> ，对于实数矩阵等同于 <code class="docutils literal "><span class="pre">x.transpose(0,</span> <span class="pre">1)</span></code> 。</p>
<div class="admonition seealso">
<p class="admonition-title">参见</p>
<p> <code class="xref py py-attr docutils literal "><span class="pre">mH</span></code> ：一个同时适用于矩阵批次的属性。</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.Tensor.mT">
<span class="sig-prename descclassname"><span class="pre">Tensor.</span></span><span class="sig-name descname"><span class="pre">mT</span></span><a class="headerlink" href="#torch.Tensor.mT" title="Permalink to this definition">¶</a></dt>
<dd><p>返回一个视图，该视图将此张量的最后两个维度交换。</p>
<p> <code class="docutils literal "><span class="pre">x.mT</span></code> 等价于 <code class="docutils literal "><span class="pre">x.transpose(-2,</span> <span class="pre">-1)</span></code> 。</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="torch.Tensor.mH">
<span class="sig-prename descclassname"><span class="pre">Tensor.</span></span><span class="sig-name descname"><span class="pre">mH</span></span><a class="headerlink" href="#torch.Tensor.mH" title="Permalink to this definition">¶</a></dt>
<dd><p>访问此属性等同于调用 <code class="xref py py-func docutils literal "><span class="pre">adjoint()</span></code> 。</p>
</dd></dl>

<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.new_tensor.html#torch.Tensor.new_tensor" title="torch.Tensor.new_tensor"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.new_tensor</span></code></a></p></td>
<td><p>返回一个新的 Tensor，其数据为 <code class="xref py py-attr docutils literal "><span class="pre">data</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.new_full.html#torch.Tensor.new_full" title="torch.Tensor.new_full"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.new_full</span></code></a></p></td>
<td><p>返回一个大小为 <code class="xref py py-attr docutils literal "><span class="pre">size</span></code> ，填充为 <code class="xref py py-attr docutils literal "><span class="pre">fill_value</span></code> 的 Tensor 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.new_empty.html#torch.Tensor.new_empty" title="torch.Tensor.new_empty"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.new_empty</span></code></a></p></td>
<td><p>返回一个大小为 <code class="xref py py-attr docutils literal "><span class="pre">size</span></code> 的 Tensor，其中包含未初始化的数据。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.new_ones.html#torch.Tensor.new_ones" title="torch.Tensor.new_ones"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.new_ones</span></code></a></p></td>
<td><p>返回一个大小为 <code class="xref py py-attr docutils literal "><span class="pre">size</span></code> 的 Tensor，其中填充了 <code class="docutils literal "><span class="pre">1</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.new_zeros.html#torch.Tensor.new_zeros" title="torch.Tensor.new_zeros"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.new_zeros</span></code></a></p></td>
<td><p>返回一个大小为 <code class="xref py py-attr docutils literal "><span class="pre">size</span></code> 的 Tensor，其中填充了 <code class="docutils literal "><span class="pre">0</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_cuda.html#torch.Tensor.is_cuda" title="torch.Tensor.is_cuda"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_cuda</span></code></a></p></td>
<td><p>如果 Tensor 存储在 GPU 上，则为 <code class="docutils literal "><span class="pre">True</span></code> ，否则为 <code class="docutils literal "><span class="pre">False</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_quantized.html#torch.Tensor.is_quantized" title="torch.Tensor.is_quantized"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_quantized</span></code></a></p></td>
<td><p>如果张量是量化的，则为 <code class="docutils literal "><span class="pre">True</span></code> ；否则为 <code class="docutils literal "><span class="pre">False</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_meta.html#torch.Tensor.is_meta" title="torch.Tensor.is_meta"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_meta</span></code></a></p></td>
<td><p>如果张量是元张量，则为 <code class="docutils literal "><span class="pre">True</span></code> ；否则为 <code class="docutils literal "><span class="pre">False</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.device.html#torch.Tensor.device" title="torch.Tensor.device"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.device</span></code></a></p></td>
<td><p>这是该张量的 <code class="xref py py-class docutils literal "><span class="pre">torch.device</span></code> 位置。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.grad.html#torch.Tensor.grad" title="torch.Tensor.grad"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.grad</span></code></a></p></td>
<td><p>此属性默认为 <code class="docutils literal "><span class="pre">None</span></code> ，并在第一次调用 <code class="xref py py-func docutils literal "><span class="pre">backward()</span></code> 计算 <code class="docutils literal "><span class="pre">self</span></code> 的梯度时变为 Tensor。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.ndim.html#torch.Tensor.ndim" title="torch.Tensor.ndim"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ndim</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">dim()</span></code> 的别名。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.real.html#torch.Tensor.real" title="torch.Tensor.real"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.real</span></code></a></p></td>
<td><p>返回一个新的张量，其中包含复数值输入张量的 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量的实数值。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.imag.html#torch.Tensor.imag" title="torch.Tensor.imag"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.imag</span></code></a></p></td>
<td><p>返回一个包含 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量虚部的新张量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.nbytes.html#torch.Tensor.nbytes" title="torch.Tensor.nbytes"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.nbytes</span></code></a></p></td>
<td><p>如果张量不使用稀疏存储布局，则返回元素 "view" 消耗的字节数。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.itemsize.html#torch.Tensor.itemsize" title="torch.Tensor.itemsize"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.itemsize</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">element_size()</span></code> 的别名。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.abs.html#torch.Tensor.abs" title="torch.Tensor.abs"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.abs</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.abs()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.abs_.html#torch.Tensor.abs_" title="torch.Tensor.abs_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.abs_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">abs()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.absolute.html#torch.Tensor.absolute" title="torch.Tensor.absolute"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.absolute</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">abs()</span></code> 的别名。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.absolute_.html#torch.Tensor.absolute_" title="torch.Tensor.absolute_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.absolute_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">absolute()</span></code> 的就地版本。 <code class="xref py py-func docutils literal "><span class="pre">abs_()</span></code> 的别名。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.acos.html#torch.Tensor.acos" title="torch.Tensor.acos"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.acos</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.acos()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.acos_.html#torch.Tensor.acos_" title="torch.Tensor.acos_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.acos_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">acos()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arccos.html#torch.Tensor.arccos" title="torch.Tensor.arccos"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arccos</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.arccos()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arccos_.html#torch.Tensor.arccos_" title="torch.Tensor.arccos_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arccos_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">arccos()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.add.html#torch.Tensor.add" title="torch.Tensor.add"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.add</span></code></a></p></td>
<td><p>向 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量添加一个标量或张量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.add_.html#torch.Tensor.add_" title="torch.Tensor.add_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.add_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">add()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm" title="torch.Tensor.addbmm"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.addbmm</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.addbmm()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.addbmm_.html#torch.Tensor.addbmm_" title="torch.Tensor.addbmm_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.addbmm_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">addbmm()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv" title="torch.Tensor.addcdiv"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.addcdiv</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.addcdiv()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.addcdiv_.html#torch.Tensor.addcdiv_" title="torch.Tensor.addcdiv_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.addcdiv_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">addcdiv()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul" title="torch.Tensor.addcmul"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.addcmul</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.addcmul()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.addcmul_.html#torch.Tensor.addcmul_" title="torch.Tensor.addcmul_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.addcmul_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">addcmul()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.addmm.html#torch.Tensor.addmm" title="torch.Tensor.addmm"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.addmm</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.addmm()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_" title="torch.Tensor.addmm_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.addmm_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">addmm()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm" title="torch.Tensor.sspaddmm"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sspaddmm</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.sspaddmm()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.addmv.html#torch.Tensor.addmv" title="torch.Tensor.addmv"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.addmv</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.addmv()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.addmv_.html#torch.Tensor.addmv_" title="torch.Tensor.addmv_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.addmv_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">addmv()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.addr.html#torch.Tensor.addr" title="torch.Tensor.addr"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.addr</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.addr()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.addr_.html#torch.Tensor.addr_" title="torch.Tensor.addr_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.addr_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">addr()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.adjoint.html#torch.Tensor.adjoint" title="torch.Tensor.adjoint"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.adjoint</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">adjoint()</span></code> 的别名。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.allclose.html#torch.Tensor.allclose" title="torch.Tensor.allclose"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.allclose</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.allclose()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.amax.html#torch.Tensor.amax" title="torch.Tensor.amax"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.amax</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.amax()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.amin.html#torch.Tensor.amin" title="torch.Tensor.amin"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.amin</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.amin()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.aminmax.html#torch.Tensor.aminmax" title="torch.Tensor.aminmax"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.aminmax</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.aminmax()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.angle.html#torch.Tensor.angle" title="torch.Tensor.angle"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.angle</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.angle()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.apply_.html#torch.Tensor.apply_" title="torch.Tensor.apply_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.apply_</span></code></a></p></td>
<td><p>将函数 <code class="xref py py-attr docutils literal "><span class="pre">callable</span></code> 应用到张量的每个元素上，用 <code class="xref py py-attr docutils literal "><span class="pre">callable</span></code> 返回的值替换每个元素。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.argmax.html#torch.Tensor.argmax" title="torch.Tensor.argmax"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.argmax</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.argmax()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.argmin.html#torch.Tensor.argmin" title="torch.Tensor.argmin"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.argmin</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.argmin()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.argsort.html#torch.Tensor.argsort" title="torch.Tensor.argsort"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.argsort</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.argsort()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.argwhere.html#torch.Tensor.argwhere" title="torch.Tensor.argwhere"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.argwhere</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.argwhere()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.asin.html#torch.Tensor.asin" title="torch.Tensor.asin"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.asin</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.asin()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.asin_.html#torch.Tensor.asin_" title="torch.Tensor.asin_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.asin_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">asin()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin" title="torch.Tensor.arcsin"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arcsin</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.arcsin()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_" title="torch.Tensor.arcsin_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arcsin_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">arcsin()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided" title="torch.Tensor.as_strided"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.as_strided</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.as_strided()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.atan.html#torch.Tensor.atan" title="torch.Tensor.atan"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.atan</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.atan()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.atan_.html#torch.Tensor.atan_" title="torch.Tensor.atan_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.atan_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">atan()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arctan.html#torch.Tensor.arctan" title="torch.Tensor.arctan"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arctan</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.arctan()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arctan_.html#torch.Tensor.arctan_" title="torch.Tensor.arctan_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arctan_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">arctan()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.atan2.html#torch.Tensor.atan2" title="torch.Tensor.atan2"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.atan2</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.atan2()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.atan2_.html#torch.Tensor.atan2_" title="torch.Tensor.atan2_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.atan2_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">atan2()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arctan2.html#torch.Tensor.arctan2" title="torch.Tensor.arctan2"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arctan2</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.arctan2()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arctan2_.html#torch.Tensor.arctan2_" title="torch.Tensor.arctan2_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arctan2_</span></code></a></p></td>
<td><p>atan2_(other) -&gt; 张量</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.all.html#torch.Tensor.all" title="torch.Tensor.all"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.all</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.all()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.any.html#torch.Tensor.any" title="torch.Tensor.any"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.any</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.any()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.backward.html#torch.Tensor.backward" title="torch.Tensor.backward"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.backward</span></code></a></p></td>
<td><p>计算当前张量相对于图叶子的梯度。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm" title="torch.Tensor.baddbmm"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.baddbmm</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.baddbmm()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.baddbmm_.html#torch.Tensor.baddbmm_" title="torch.Tensor.baddbmm_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.baddbmm_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">baddbmm()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bernoulli.html#torch.Tensor.bernoulli" title="torch.Tensor.bernoulli"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bernoulli</span></code></a></p></td>
<td><p>返回一个结果张量，其中每个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">result[i]</mtext></mrow><annotation encoding="application/x-tex">\texttt{result[i]}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.77777em;vertical-align:-0.08333em;" class="strut"></span><span class="mord text"><span class="mord texttt">result[i]</span></span></span></span></span> 都独立地从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Bernoulli</mtext><mo stretchy="false">(</mo><mtext mathvariant="monospace">self[i]</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Bernoulli}(\texttt{self[i]})</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">Bernoulli</span></span><span class="mopen">(</span><span class="mord text"><span class="mord texttt">self[i]</span></span><span class="mclose">)</span></span></span></span> 中采样。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_" title="torch.Tensor.bernoulli_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bernoulli_</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 的每个位置填充为从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Bernoulli</mtext><mo stretchy="false">(</mo><mtext mathvariant="monospace">p</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Bernoulli}(\texttt{p})</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">Bernoulli</span></span><span class="mopen">(</span><span class="mord text"><span class="mord texttt">p</span></span><span class="mclose">)</span></span></span></span> 中独立采样的样本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bfloat16.html#torch.Tensor.bfloat16" title="torch.Tensor.bfloat16"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bfloat16</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.bfloat16()</span></code> 等同于 <code class="docutils literal "><span class="pre">self.to(torch.bfloat16)</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bincount.html#torch.Tensor.bincount" title="torch.Tensor.bincount"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bincount</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.bincount()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not" title="torch.Tensor.bitwise_not"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bitwise_not</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.bitwise_not()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_not_.html#torch.Tensor.bitwise_not_" title="torch.Tensor.bitwise_not_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bitwise_not_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">bitwise_not()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and" title="torch.Tensor.bitwise_and"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bitwise_and</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.bitwise_and()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_and_.html#torch.Tensor.bitwise_and_" title="torch.Tensor.bitwise_and_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bitwise_and_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">bitwise_and()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or" title="torch.Tensor.bitwise_or"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bitwise_or</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.bitwise_or()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_or_.html#torch.Tensor.bitwise_or_" title="torch.Tensor.bitwise_or_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bitwise_or_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">bitwise_or()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor" title="torch.Tensor.bitwise_xor"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bitwise_xor</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.bitwise_xor()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_xor_.html#torch.Tensor.bitwise_xor_" title="torch.Tensor.bitwise_xor_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bitwise_xor_</span></code></a></p></td>
<td><p>原位版本 <code class="xref py py-meth docutils literal "><span class="pre">bitwise_xor()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift" title="torch.Tensor.bitwise_left_shift"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bitwise_left_shift</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.bitwise_left_shift()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_left_shift_.html#torch.Tensor.bitwise_left_shift_" title="torch.Tensor.bitwise_left_shift_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bitwise_left_shift_</span></code></a></p></td>
<td><p>原位版本 <code class="xref py py-meth docutils literal "><span class="pre">bitwise_left_shift()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift" title="torch.Tensor.bitwise_right_shift"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bitwise_right_shift</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.bitwise_right_shift()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bitwise_right_shift_.html#torch.Tensor.bitwise_right_shift_" title="torch.Tensor.bitwise_right_shift_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bitwise_right_shift_</span></code></a></p></td>
<td><p>原位版本 <code class="xref py py-meth docutils literal "><span class="pre">bitwise_right_shift()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.bmm.html#torch.Tensor.bmm" title="torch.Tensor.bmm"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bmm</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.bmm()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.bool.html#torch.Tensor.bool" title="torch.Tensor.bool"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.bool</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.bool()</span></code> 等价于 <code class="docutils literal "><span class="pre">self.to(torch.bool)</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.byte.html#torch.Tensor.byte" title="torch.Tensor.byte"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.byte</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.byte()</span></code> 等同于 <code class="docutils literal "><span class="pre">self.to(torch.uint8)</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.broadcast_to.html#torch.Tensor.broadcast_to" title="torch.Tensor.broadcast_to"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.broadcast_to</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.broadcast_to()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_" title="torch.Tensor.cauchy_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cauchy_</span></code></a></p></td>
<td><p>填充张量，其中的数字来自柯西分布：</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.ceil.html#torch.Tensor.ceil" title="torch.Tensor.ceil"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ceil</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.ceil()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.ceil_.html#torch.Tensor.ceil_" title="torch.Tensor.ceil_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ceil_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">ceil()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.char.html#torch.Tensor.char" title="torch.Tensor.char"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.char</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.char()</span></code> 等同于 <code class="docutils literal "><span class="pre">self.to(torch.int8)</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cholesky.html#torch.Tensor.cholesky" title="torch.Tensor.cholesky"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cholesky</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.cholesky()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cholesky_inverse.html#torch.Tensor.cholesky_inverse" title="torch.Tensor.cholesky_inverse"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cholesky_inverse</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.cholesky_inverse()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cholesky_solve.html#torch.Tensor.cholesky_solve" title="torch.Tensor.cholesky_solve"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cholesky_solve</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.cholesky_solve()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.chunk.html#torch.Tensor.chunk" title="torch.Tensor.chunk"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.chunk</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.chunk()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.clamp.html#torch.Tensor.clamp" title="torch.Tensor.clamp"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.clamp</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.clamp()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_" title="torch.Tensor.clamp_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.clamp_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">clamp()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.clip.html#torch.Tensor.clip" title="torch.Tensor.clip"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.clip</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">clamp()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.clip_.html#torch.Tensor.clip_" title="torch.Tensor.clip_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.clip_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">clamp_()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.clone.html#torch.Tensor.clone" title="torch.Tensor.clone"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.clone</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.clone()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.contiguous</span></code></a></p></td>
<td><p>返回一个包含与 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量相同数据的连续内存张量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.copy_.html#torch.Tensor.copy_" title="torch.Tensor.copy_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.copy_</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">src</span></code> 中的元素复制到 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量中，并返回 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.conj.html#torch.Tensor.conj" title="torch.Tensor.conj"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.conj</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.conj()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical" title="torch.Tensor.conj_physical"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.conj_physical</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.conj_physical()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.conj_physical_.html#torch.Tensor.conj_physical_" title="torch.Tensor.conj_physical_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.conj_physical_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">conj_physical()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.resolve_conj.html#torch.Tensor.resolve_conj" title="torch.Tensor.resolve_conj"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.resolve_conj</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.resolve_conj()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.resolve_neg.html#torch.Tensor.resolve_neg" title="torch.Tensor.resolve_neg"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.resolve_neg</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.resolve_neg()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.copysign.html#torch.Tensor.copysign" title="torch.Tensor.copysign"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.copysign</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.copysign()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.copysign_.html#torch.Tensor.copysign_" title="torch.Tensor.copysign_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.copysign_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">copysign()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cos.html#torch.Tensor.cos" title="torch.Tensor.cos"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cos</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.cos()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cos_.html#torch.Tensor.cos_" title="torch.Tensor.cos_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cos_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">cos()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cosh.html#torch.Tensor.cosh" title="torch.Tensor.cosh"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cosh</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.cosh()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cosh_.html#torch.Tensor.cosh_" title="torch.Tensor.cosh_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cosh_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">cosh()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.corrcoef.html#torch.Tensor.corrcoef" title="torch.Tensor.corrcoef"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.corrcoef</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.corrcoef()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.count_nonzero.html#torch.Tensor.count_nonzero" title="torch.Tensor.count_nonzero"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.count_nonzero</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.count_nonzero()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cov.html#torch.Tensor.cov" title="torch.Tensor.cov"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cov</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.cov()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.acosh.html#torch.Tensor.acosh" title="torch.Tensor.acosh"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.acosh</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.acosh()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.acosh_.html#torch.Tensor.acosh_" title="torch.Tensor.acosh_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.acosh_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">acosh()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arccosh.html#torch.Tensor.arccosh" title="torch.Tensor.arccosh"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arccosh</span></code></a></p></td>
<td><p>acosh() -&gt; 张量</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arccosh_.html#torch.Tensor.arccosh_" title="torch.Tensor.arccosh_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arccosh_</span></code></a></p></td>
<td><p>acosh_() -&gt; 索引</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cpu.html#torch.Tensor.cpu" title="torch.Tensor.cpu"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cpu</span></code></a></p></td>
<td><p>返回此对象在 CPU 内存中的副本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cross.html#torch.Tensor.cross" title="torch.Tensor.cross"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cross</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.cross()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cuda.html#torch.Tensor.cuda" title="torch.Tensor.cuda"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cuda</span></code></a></p></td>
<td><p>返回此对象在 CUDA 内存中的副本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logcumsumexp.html#torch.Tensor.logcumsumexp" title="torch.Tensor.logcumsumexp"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logcumsumexp</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.logcumsumexp()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cummax.html#torch.Tensor.cummax" title="torch.Tensor.cummax"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cummax</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.cummax()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cummin.html#torch.Tensor.cummin" title="torch.Tensor.cummin"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cummin</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.cummin()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod" title="torch.Tensor.cumprod"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cumprod</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.cumprod()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cumprod_.html#torch.Tensor.cumprod_" title="torch.Tensor.cumprod_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cumprod_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">cumprod()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum" title="torch.Tensor.cumsum"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cumsum</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.cumsum()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cumsum_.html#torch.Tensor.cumsum_" title="torch.Tensor.cumsum_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cumsum_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">cumsum()</span></code> 的原地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.chalf.html#torch.Tensor.chalf" title="torch.Tensor.chalf"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.chalf</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.chalf()</span></code> 等价于 <code class="docutils literal "><span class="pre">self.to(torch.complex32)</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.cfloat.html#torch.Tensor.cfloat" title="torch.Tensor.cfloat"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cfloat</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.cfloat()</span></code> 等价于 <code class="docutils literal "><span class="pre">self.to(torch.complex64)</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.cdouble.html#torch.Tensor.cdouble" title="torch.Tensor.cdouble"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.cdouble</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.cdouble()</span></code> 等价于 <code class="docutils literal "><span class="pre">self.to(torch.complex128)</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.data_ptr.html#torch.Tensor.data_ptr" title="torch.Tensor.data_ptr"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.data_ptr</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量的第一个元素的地址。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad" title="torch.Tensor.deg2rad"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.deg2rad</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.deg2rad()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize" title="torch.Tensor.dequantize"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.dequantize</span></code></a></p></td>
<td><p>给定一个量化张量，对其进行去量化并返回去量化的浮点张量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.det.html#torch.Tensor.det" title="torch.Tensor.det"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.det</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.det()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim" title="torch.Tensor.dense_dim"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.dense_dim</span></code></a></p></td>
<td><p>返回稀疏张量 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 的稠密维度数量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.detach.html#torch.Tensor.detach" title="torch.Tensor.detach"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.detach</span></code></a></p></td>
<td><p>返回一个新的 Tensor，与当前图分离。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.detach_.html#torch.Tensor.detach_" title="torch.Tensor.detach_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.detach_</span></code></a></p></td>
<td><p>从创建它的图中分离张量，使其成为叶子节点。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.diag.html#torch.Tensor.diag" title="torch.Tensor.diag"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.diag</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.diag()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.diag_embed.html#torch.Tensor.diag_embed" title="torch.Tensor.diag_embed"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.diag_embed</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.diag_embed()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.diagflat.html#torch.Tensor.diagflat" title="torch.Tensor.diagflat"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.diagflat</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.diagflat()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal" title="torch.Tensor.diagonal"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.diagonal</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.diagonal()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.diagonal_scatter.html#torch.Tensor.diagonal_scatter" title="torch.Tensor.diagonal_scatter"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.diagonal_scatter</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.diagonal_scatter()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.fill_diagonal_.html#torch.Tensor.fill_diagonal_" title="torch.Tensor.fill_diagonal_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.fill_diagonal_</span></code></a></p></td>
<td><p>填充至少有 2 维度的张量的主对角线。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.fmax.html#torch.Tensor.fmax" title="torch.Tensor.fmax"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.fmax</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.fmax()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.fmin.html#torch.Tensor.fmin" title="torch.Tensor.fmin"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.fmin</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.fmin()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.diff.html#torch.Tensor.diff" title="torch.Tensor.diff"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.diff</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.diff()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.digamma.html#torch.Tensor.digamma" title="torch.Tensor.digamma"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.digamma</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.digamma()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.digamma_.html#torch.Tensor.digamma_" title="torch.Tensor.digamma_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.digamma_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">digamma()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.dim.html#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.dim</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量的维度数。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.dim_order.html#torch.Tensor.dim_order" title="torch.Tensor.dim_order"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.dim_order</span></code></a></p></td>
<td><p>返回唯一确定的描述维度顺序或物理布局的 int 元组。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.dist.html#torch.Tensor.dist" title="torch.Tensor.dist"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.dist</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.dist()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.div.html#torch.Tensor.div" title="torch.Tensor.div"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.div</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.div()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.div_.html#torch.Tensor.div_" title="torch.Tensor.div_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.div_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">div()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.divide.html#torch.Tensor.divide" title="torch.Tensor.divide"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.divide</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.divide()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.divide_.html#torch.Tensor.divide_" title="torch.Tensor.divide_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.divide_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">divide()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.dot.html#torch.Tensor.dot" title="torch.Tensor.dot"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.dot</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.dot()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.double.html#torch.Tensor.double" title="torch.Tensor.double"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.double</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.double()</span></code> 等价于 <code class="docutils literal "><span class="pre">self.to(torch.float64)</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.dsplit.html#torch.Tensor.dsplit" title="torch.Tensor.dsplit"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.dsplit</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.dsplit()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.element_size.html#torch.Tensor.element_size" title="torch.Tensor.element_size"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.element_size</span></code></a></p></td>
<td><p>返回单个元素的字节大小。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.eq.html#torch.Tensor.eq" title="torch.Tensor.eq"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.eq</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.eq()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.eq_.html#torch.Tensor.eq_" title="torch.Tensor.eq_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.eq_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">eq()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.equal.html#torch.Tensor.equal" title="torch.Tensor.equal"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.equal</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.equal()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.erf.html#torch.Tensor.erf" title="torch.Tensor.erf"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.erf</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.erf()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.erf_.html#torch.Tensor.erf_" title="torch.Tensor.erf_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.erf_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">erf()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.erfc.html#torch.Tensor.erfc" title="torch.Tensor.erfc"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.erfc</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.erfc()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.erfc_.html#torch.Tensor.erfc_" title="torch.Tensor.erfc_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.erfc_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">erfc()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv" title="torch.Tensor.erfinv"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.erfinv</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.erfinv()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.erfinv_.html#torch.Tensor.erfinv_" title="torch.Tensor.erfinv_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.erfinv_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">erfinv()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.exp.html#torch.Tensor.exp" title="torch.Tensor.exp"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.exp</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.exp()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.exp_.html#torch.Tensor.exp_" title="torch.Tensor.exp_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.exp_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">exp()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.expm1.html#torch.Tensor.expm1" title="torch.Tensor.expm1"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.expm1</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.expm1()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.expm1_.html#torch.Tensor.expm1_" title="torch.Tensor.expm1_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.expm1_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">expm1()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.expand.html#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.expand</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量的新视图，将单例维度扩展到更大的大小。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as" title="torch.Tensor.expand_as"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.expand_as</span></code></a></p></td>
<td><p>将此张量扩展到与 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 相同的大小。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_" title="torch.Tensor.exponential_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.exponential_</span></code></a></p></td>
<td><p>用 PDF（概率密度函数）中的元素填充 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量：</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.fix.html#torch.Tensor.fix" title="torch.Tensor.fix"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.fix</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.fix()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.fix_.html#torch.Tensor.fix_" title="torch.Tensor.fix_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.fix_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">fix()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.fill_.html#torch.Tensor.fill_" title="torch.Tensor.fill_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.fill_</span></code></a></p></td>
<td><p>用指定的值填充 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.flatten.html#torch.Tensor.flatten" title="torch.Tensor.flatten"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.flatten</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.flatten()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.flip.html#torch.Tensor.flip" title="torch.Tensor.flip"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.flip</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.flip()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.fliplr.html#torch.Tensor.fliplr" title="torch.Tensor.fliplr"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.fliplr</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.fliplr()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.flipud.html#torch.Tensor.flipud" title="torch.Tensor.flipud"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.flipud</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.flipud()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.float.html#torch.Tensor.float" title="torch.Tensor.float"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.float</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.float()</span></code> 与 <code class="docutils literal "><span class="pre">self.to(torch.float32)</span></code> 等价。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.float_power.html#torch.Tensor.float_power" title="torch.Tensor.float_power"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.float_power</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.float_power()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.float_power_.html#torch.Tensor.float_power_" title="torch.Tensor.float_power_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.float_power_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">float_power()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.floor.html#torch.Tensor.floor" title="torch.Tensor.floor"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.floor</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.floor()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.floor_.html#torch.Tensor.floor_" title="torch.Tensor.floor_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.floor_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">floor()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide" title="torch.Tensor.floor_divide"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.floor_divide</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.floor_divide()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_" title="torch.Tensor.floor_divide_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.floor_divide_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">floor_divide()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.fmod.html#torch.Tensor.fmod" title="torch.Tensor.fmod"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.fmod</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.fmod()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.fmod_.html#torch.Tensor.fmod_" title="torch.Tensor.fmod_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.fmod_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">fmod()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.frac.html#torch.Tensor.frac" title="torch.Tensor.frac"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.frac</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.frac()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.frac_.html#torch.Tensor.frac_" title="torch.Tensor.frac_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.frac_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">frac()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.frexp.html#torch.Tensor.frexp" title="torch.Tensor.frexp"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.frexp</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.frexp()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.gather.html#torch.Tensor.gather" title="torch.Tensor.gather"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.gather</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.gather()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.gcd.html#torch.Tensor.gcd" title="torch.Tensor.gcd"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.gcd</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.gcd()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.gcd_.html#torch.Tensor.gcd_" title="torch.Tensor.gcd_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.gcd_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">gcd()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.ge.html#torch.Tensor.ge" title="torch.Tensor.ge"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ge</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.ge()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.ge_.html#torch.Tensor.ge_" title="torch.Tensor.ge_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ge_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">ge()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal" title="torch.Tensor.greater_equal"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.greater_equal</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.greater_equal()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.greater_equal_.html#torch.Tensor.greater_equal_" title="torch.Tensor.greater_equal_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.greater_equal_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">greater_equal()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_" title="torch.Tensor.geometric_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.geometric_</span></code></a></p></td>
<td><p>用几何分布的元素填充 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量：</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.geqrf.html#torch.Tensor.geqrf" title="torch.Tensor.geqrf"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.geqrf</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.geqrf()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.ger.html#torch.Tensor.ger" title="torch.Tensor.ger"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ger</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.ger()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.get_device.html#torch.Tensor.get_device" title="torch.Tensor.get_device"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.get_device</span></code></a></p></td>
<td><p>对于 CUDA 张量，此函数返回张量所在的 GPU 的设备序号。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.gt.html#torch.Tensor.gt" title="torch.Tensor.gt"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.gt</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.gt()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.gt_.html#torch.Tensor.gt_" title="torch.Tensor.gt_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.gt_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">gt()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.greater.html#torch.Tensor.greater" title="torch.Tensor.greater"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.greater</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.greater()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.greater_.html#torch.Tensor.greater_" title="torch.Tensor.greater_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.greater_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">greater()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.half.html#torch.Tensor.half" title="torch.Tensor.half"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.half</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.half()</span></code> 等同于 <code class="docutils literal "><span class="pre">self.to(torch.float16)</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.hardshrink.html#torch.Tensor.hardshrink" title="torch.Tensor.hardshrink"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.hardshrink</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.nn.functional.hardshrink()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.heaviside.html#torch.Tensor.heaviside" title="torch.Tensor.heaviside"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.heaviside</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.heaviside()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.histc.html#torch.Tensor.histc" title="torch.Tensor.histc"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.histc</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.histc()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.histogram.html#torch.Tensor.histogram" title="torch.Tensor.histogram"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.histogram</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.histogram()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit" title="torch.Tensor.hsplit"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.hsplit</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.hsplit()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.hypot.html#torch.Tensor.hypot" title="torch.Tensor.hypot"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.hypot</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.hypot()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.hypot_.html#torch.Tensor.hypot_" title="torch.Tensor.hypot_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.hypot_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">hypot()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.i0.html#torch.Tensor.i0" title="torch.Tensor.i0"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.i0</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.i0()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.i0_.html#torch.Tensor.i0_" title="torch.Tensor.i0_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.i0_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">i0()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.igamma.html#torch.Tensor.igamma" title="torch.Tensor.igamma"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.igamma</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.igamma()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.igamma_.html#torch.Tensor.igamma_" title="torch.Tensor.igamma_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.igamma_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">igamma()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.igammac.html#torch.Tensor.igammac" title="torch.Tensor.igammac"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.igammac</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.igammac()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.igammac_.html#torch.Tensor.igammac_" title="torch.Tensor.igammac_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.igammac_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">igammac()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_" title="torch.Tensor.index_add_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.index_add_</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">alpha</span></code> 的元素累加到 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量中，累加次数为 <code class="docutils literal "><span class="pre">source</span></code> ，并按照 <code class="xref py py-attr docutils literal "><span class="pre">index</span></code> 中给出的顺序添加到索引。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.index_add.html#torch.Tensor.index_add" title="torch.Tensor.index_add"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.index_add</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.index_add_()</span></code> 的非就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_" title="torch.Tensor.index_copy_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.index_copy_</span></code></a></p></td>
<td><p>通过选择 <code class="xref py py-attr docutils literal "><span class="pre">index</span></code> 中给出的顺序，将 <code class="xref py py-attr docutils literal "><span class="pre">tensor</span></code> 的元素复制到 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量中。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.index_copy.html#torch.Tensor.index_copy" title="torch.Tensor.index_copy"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.index_copy</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.index_copy_()</span></code> 的不合适版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_" title="torch.Tensor.index_fill_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.index_fill_</span></code></a></p></td>
<td><p>通过选择在 <code class="xref py py-attr docutils literal "><span class="pre">index</span></code> 中给出的顺序，将值 <code class="xref py py-attr docutils literal "><span class="pre">value</span></code> 填充到 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量的元素中。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.index_fill.html#torch.Tensor.index_fill" title="torch.Tensor.index_fill"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.index_fill</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.index_fill_()</span></code> 的不合适版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_" title="torch.Tensor.index_put_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.index_put_</span></code></a></p></td>
<td><p>使用指定的索引（索引是一个张量元组）将张量 <code class="xref py py-attr docutils literal "><span class="pre">values</span></code> 的值放入张量 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 中。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.index_put.html#torch.Tensor.index_put" title="torch.Tensor.index_put"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.index_put</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">index_put_()</span></code> 的错位版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_" title="torch.Tensor.index_reduce_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.index_reduce_</span></code></a></p></td>
<td><p>将 <code class="docutils literal "><span class="pre">source</span></code> 的元素累积到 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量中，按照 <code class="xref py py-attr docutils literal "><span class="pre">index</span></code> 中给出的顺序累积到索引，使用 <code class="docutils literal "><span class="pre">reduce</span></code> 参数指定的累加方式。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.index_reduce.html#torch.Tensor.index_reduce" title="torch.Tensor.index_reduce"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.index_reduce</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.index_select.html#torch.Tensor.index_select" title="torch.Tensor.index_select"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.index_select</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.index_select()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.indices.html#torch.Tensor.indices" title="torch.Tensor.indices"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.indices</span></code></a></p></td>
<td><p>返回稀疏 COO 张量的索引张量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.inner.html#torch.Tensor.inner" title="torch.Tensor.inner"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.inner</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.inner()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.int.html#torch.Tensor.int" title="torch.Tensor.int"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.int</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.int()</span></code> 等价于 <code class="docutils literal "><span class="pre">self.to(torch.int32)</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr" title="torch.Tensor.int_repr"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.int_repr</span></code></a></p></td>
<td><p>给定一个量化张量， <code class="docutils literal "><span class="pre">self.int_repr()</span></code> 返回一个以 uint8_t 作为数据类型的 CPU 张量，该张量存储了给定张量的底层 uint8_t 值。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.inverse.html#torch.Tensor.inverse" title="torch.Tensor.inverse"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.inverse</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.inverse()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.isclose.html#torch.Tensor.isclose" title="torch.Tensor.isclose"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.isclose</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.isclose()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.isfinite.html#torch.Tensor.isfinite" title="torch.Tensor.isfinite"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.isfinite</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.isfinite()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.isinf.html#torch.Tensor.isinf" title="torch.Tensor.isinf"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.isinf</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.isinf()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.isposinf.html#torch.Tensor.isposinf" title="torch.Tensor.isposinf"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.isposinf</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.isposinf()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.isneginf.html#torch.Tensor.isneginf" title="torch.Tensor.isneginf"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.isneginf</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.isneginf()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.isnan.html#torch.Tensor.isnan" title="torch.Tensor.isnan"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.isnan</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.isnan()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_contiguous.html#torch.Tensor.is_contiguous" title="torch.Tensor.is_contiguous"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_contiguous</span></code></a></p></td>
<td><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量在内存中按指定的内存格式连续，则返回 True。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_complex.html#torch.Tensor.is_complex" title="torch.Tensor.is_complex"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_complex</span></code></a></p></td>
<td><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 的数据类型是复杂数据类型，则返回 True。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_conj.html#torch.Tensor.is_conj" title="torch.Tensor.is_conj"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_conj</span></code></a></p></td>
<td><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 的共轭位设置为 true，则返回 True。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_floating_point.html#torch.Tensor.is_floating_point" title="torch.Tensor.is_floating_point"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_floating_point</span></code></a></p></td>
<td><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 的数据类型是浮点数据类型，则返回 True。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_inference.html#torch.Tensor.is_inference" title="torch.Tensor.is_inference"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_inference</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.is_inference()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf" title="torch.Tensor.is_leaf"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_leaf</span></code></a></p></td>
<td><p>所有具有 <code class="xref py py-attr docutils literal "><span class="pre">requires_grad</span></code> 且为 <code class="docutils literal "><span class="pre">False</span></code> 的 Tensor 都将按照惯例成为叶子 Tensor。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_pinned.html#torch.Tensor.is_pinned" title="torch.Tensor.is_pinned"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_pinned</span></code></a></p></td>
<td><p>返回此张量是否位于固定内存中。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_set_to.html#torch.Tensor.is_set_to" title="torch.Tensor.is_set_to"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_set_to</span></code></a></p></td>
<td><p>如果两个张量指向完全相同的内存（相同的存储、偏移、大小和步长），则返回 True。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared" title="torch.Tensor.is_shared"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_shared</span></code></a></p></td>
<td><p>检查张量是否在共享内存中。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_signed.html#torch.Tensor.is_signed" title="torch.Tensor.is_signed"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_signed</span></code></a></p></td>
<td><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 的数据类型是有符号数据类型，则返回 True。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse" title="torch.Tensor.is_sparse"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_sparse</span></code></a></p></td>
<td><p>如果张量使用稀疏 COO 存储布局，则为 <code class="docutils literal "><span class="pre">True</span></code> ，否则为 <code class="docutils literal "><span class="pre">False</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.istft.html#torch.Tensor.istft" title="torch.Tensor.istft"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.istft</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.istft()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.isreal.html#torch.Tensor.isreal" title="torch.Tensor.isreal"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.isreal</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.isreal()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.item.html#torch.Tensor.item" title="torch.Tensor.item"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.item</span></code></a></p></td>
<td><p>返回此张量的标准 Python 数值。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.kthvalue.html#torch.Tensor.kthvalue" title="torch.Tensor.kthvalue"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.kthvalue</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.kthvalue()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.lcm.html#torch.Tensor.lcm" title="torch.Tensor.lcm"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.lcm</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.lcm()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.lcm_.html#torch.Tensor.lcm_" title="torch.Tensor.lcm_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.lcm_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">lcm()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp" title="torch.Tensor.ldexp"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ldexp</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.ldexp()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.ldexp_.html#torch.Tensor.ldexp_" title="torch.Tensor.ldexp_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ldexp_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">ldexp()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.le.html#torch.Tensor.le" title="torch.Tensor.le"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.le</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.le()</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.le_.html#torch.Tensor.le_" title="torch.Tensor.le_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.le_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">le()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal" title="torch.Tensor.less_equal"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.less_equal</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.less_equal()</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.less_equal_.html#torch.Tensor.less_equal_" title="torch.Tensor.less_equal_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.less_equal_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">less_equal()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.lerp.html#torch.Tensor.lerp" title="torch.Tensor.lerp"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.lerp</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.lerp()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.lerp_.html#torch.Tensor.lerp_" title="torch.Tensor.lerp_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.lerp_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">lerp()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma" title="torch.Tensor.lgamma"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.lgamma</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.lgamma()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.lgamma_.html#torch.Tensor.lgamma_" title="torch.Tensor.lgamma_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.lgamma_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">lgamma()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.log.html#torch.Tensor.log" title="torch.Tensor.log"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.log</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.log()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.log_.html#torch.Tensor.log_" title="torch.Tensor.log_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.log_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">log()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logdet.html#torch.Tensor.logdet" title="torch.Tensor.logdet"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logdet</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.logdet()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.log10.html#torch.Tensor.log10" title="torch.Tensor.log10"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.log10</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.log10()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.log10_.html#torch.Tensor.log10_" title="torch.Tensor.log10_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.log10_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">log10()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.log1p.html#torch.Tensor.log1p" title="torch.Tensor.log1p"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.log1p</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.log1p()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_" title="torch.Tensor.log1p_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.log1p_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">log1p()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.log2.html#torch.Tensor.log2" title="torch.Tensor.log2"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.log2</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.log2()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.log2_.html#torch.Tensor.log2_" title="torch.Tensor.log2_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.log2_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">log2()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_" title="torch.Tensor.log_normal_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.log_normal_</span></code></a></p></td>
<td><p>使用给定的均值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span class="mord mathnormal">μ</span></span></span></span> 和标准差 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.43056em;vertical-align:0em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">σ</span></span></span></span> 参数化的对数正态分布从日志中抽取样本填充 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logaddexp.html#torch.Tensor.logaddexp" title="torch.Tensor.logaddexp"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logaddexp</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.logaddexp()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.logaddexp2.html#torch.Tensor.logaddexp2" title="torch.Tensor.logaddexp2"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logaddexp2</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.logaddexp2()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logsumexp.html#torch.Tensor.logsumexp" title="torch.Tensor.logsumexp"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logsumexp</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.logsumexp()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and" title="torch.Tensor.logical_and"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logical_and</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.logical_and()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_and_.html#torch.Tensor.logical_and_" title="torch.Tensor.logical_and_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logical_and_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">logical_and()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not" title="torch.Tensor.logical_not"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logical_not</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.logical_not()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_not_.html#torch.Tensor.logical_not_" title="torch.Tensor.logical_not_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logical_not_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">logical_not()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or" title="torch.Tensor.logical_or"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logical_or</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.logical_or()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_or_.html#torch.Tensor.logical_or_" title="torch.Tensor.logical_or_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logical_or_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">logical_or()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor" title="torch.Tensor.logical_xor"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logical_xor</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.logical_xor()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logical_xor_.html#torch.Tensor.logical_xor_" title="torch.Tensor.logical_xor_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logical_xor_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">logical_xor()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.logit.html#torch.Tensor.logit" title="torch.Tensor.logit"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logit</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.logit()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.logit_.html#torch.Tensor.logit_" title="torch.Tensor.logit_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.logit_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">logit()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.long.html#torch.Tensor.long" title="torch.Tensor.long"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.long</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.long()</span></code> 等价于 <code class="docutils literal "><span class="pre">self.to(torch.int64)</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.lt.html#torch.Tensor.lt" title="torch.Tensor.lt"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.lt</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.lt()</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.lt_.html#torch.Tensor.lt_" title="torch.Tensor.lt_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.lt_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">lt()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.less.html#torch.Tensor.less" title="torch.Tensor.less"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.less</span></code></a></p></td>
<td><p>lt(other) -&gt; 张量</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.less_.html#torch.Tensor.less_" title="torch.Tensor.less_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.less_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">less()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.lu.html#torch.Tensor.lu" title="torch.Tensor.lu"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.lu</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.lu()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.lu_solve.html#torch.Tensor.lu_solve" title="torch.Tensor.lu_solve"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.lu_solve</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.lu_solve()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.as_subclass.html#torch.Tensor.as_subclass" title="torch.Tensor.as_subclass"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.as_subclass</span></code></a></p></td>
<td><p>创建一个与 <code class="docutils literal "><span class="pre">self</span></code> 具有相同数据指针的 <code class="docutils literal "><span class="pre">cls</span></code> 实例。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.map_.html#torch.Tensor.map_" title="torch.Tensor.map_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.map_</span></code></a></p></td>
<td><p>对每个元素在 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量中应用 <code class="xref py py-attr docutils literal "><span class="pre">callable</span></code> ，并将结果存储在 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量中。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_" title="torch.Tensor.masked_scatter_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.masked_scatter_</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">source</span></code> 中的元素复制到 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量中，位置由 <code class="xref py py-attr docutils literal "><span class="pre">mask</span></code> 为 True 确定。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.masked_scatter.html#torch.Tensor.masked_scatter" title="torch.Tensor.masked_scatter"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.masked_scatter</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.masked_scatter_()</span></code> 的非原地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_" title="torch.Tensor.masked_fill_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.masked_fill_</span></code></a></p></td>
<td><p>当 <code class="xref py py-attr docutils literal "><span class="pre">mask</span></code> 为 True 时，用 <code class="xref py py-attr docutils literal "><span class="pre">value</span></code> 填充 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量的元素。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.masked_fill.html#torch.Tensor.masked_fill" title="torch.Tensor.masked_fill"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.masked_fill</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.masked_fill_()</span></code> 的错位版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.masked_select.html#torch.Tensor.masked_select" title="torch.Tensor.masked_select"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.masked_select</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.masked_select()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.matmul.html#torch.Tensor.matmul" title="torch.Tensor.matmul"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.matmul</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.matmul()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power" title="torch.Tensor.matrix_power"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.matrix_power</span></code></a></p></td>
<td><p></p><div class="admonition note">
<p class="admonition-title">注意</p>
<p> <code class="xref py py-meth docutils literal "><span class="pre">matrix_power()</span></code> 已弃用，请使用 <code class="xref py py-func docutils literal "><span class="pre">torch.linalg.matrix_power()</span></code> 代替。</p>
</div>
<p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.matrix_exp.html#torch.Tensor.matrix_exp" title="torch.Tensor.matrix_exp"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.matrix_exp</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.matrix_exp()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.max.html#torch.Tensor.max" title="torch.Tensor.max"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.max</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.max()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.maximum.html#torch.Tensor.maximum" title="torch.Tensor.maximum"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.maximum</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.maximum()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.mean.html#torch.Tensor.mean" title="torch.Tensor.mean"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.mean</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.mean()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.module_load.html#torch.Tensor.module_load" title="torch.Tensor.module_load"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.module_load</span></code></a></p></td>
<td><p>定义如何在 <code class="xref py py-meth docutils literal "><span class="pre">load_state_dict()</span></code> 中加载 <code class="docutils literal "><span class="pre">other</span></code> 时将其转换为 <code class="docutils literal "><span class="pre">self</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.nanmean.html#torch.Tensor.nanmean" title="torch.Tensor.nanmean"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.nanmean</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.nanmean()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.median.html#torch.Tensor.median" title="torch.Tensor.median"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.median</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.median()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.nanmedian.html#torch.Tensor.nanmedian" title="torch.Tensor.nanmedian"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.nanmedian</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.nanmedian()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.min.html#torch.Tensor.min" title="torch.Tensor.min"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.min</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.min()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.minimum.html#torch.Tensor.minimum" title="torch.Tensor.minimum"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.minimum</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.minimum()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.mm.html#torch.Tensor.mm" title="torch.Tensor.mm"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.mm</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.mm()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.smm.html#torch.Tensor.smm" title="torch.Tensor.smm"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.smm</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.smm()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.mode.html#torch.Tensor.mode" title="torch.Tensor.mode"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.mode</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.mode()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.movedim.html#torch.Tensor.movedim" title="torch.Tensor.movedim"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.movedim</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.movedim()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.moveaxis.html#torch.Tensor.moveaxis" title="torch.Tensor.moveaxis"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.moveaxis</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.moveaxis()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.msort.html#torch.Tensor.msort" title="torch.Tensor.msort"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.msort</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.msort()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.mul.html#torch.Tensor.mul" title="torch.Tensor.mul"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.mul</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.mul()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.mul_.html#torch.Tensor.mul_" title="torch.Tensor.mul_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.mul_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">mul()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.multiply.html#torch.Tensor.multiply" title="torch.Tensor.multiply"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.multiply</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.multiply()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.multiply_.html#torch.Tensor.multiply_" title="torch.Tensor.multiply_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.multiply_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">multiply()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.multinomial.html#torch.Tensor.multinomial" title="torch.Tensor.multinomial"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.multinomial</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.multinomial()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.mv.html#torch.Tensor.mv" title="torch.Tensor.mv"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.mv</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.mv()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma" title="torch.Tensor.mvlgamma"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.mvlgamma</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.mvlgamma()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.mvlgamma_.html#torch.Tensor.mvlgamma_" title="torch.Tensor.mvlgamma_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.mvlgamma_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">mvlgamma()</span></code> 的本地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.nansum.html#torch.Tensor.nansum" title="torch.Tensor.nansum"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.nansum</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.nansum()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.narrow.html#torch.Tensor.narrow" title="torch.Tensor.narrow"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.narrow</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.narrow()</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy" title="torch.Tensor.narrow_copy"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.narrow_copy</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.narrow_copy()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.ndimension.html#torch.Tensor.ndimension" title="torch.Tensor.ndimension"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ndimension</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">dim()</span></code> 的别名。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num" title="torch.Tensor.nan_to_num"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.nan_to_num</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.nan_to_num()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.nan_to_num_.html#torch.Tensor.nan_to_num_" title="torch.Tensor.nan_to_num_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.nan_to_num_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">nan_to_num()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.ne.html#torch.Tensor.ne" title="torch.Tensor.ne"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ne</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.ne()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.ne_.html#torch.Tensor.ne_" title="torch.Tensor.ne_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ne_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">ne()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal" title="torch.Tensor.not_equal"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.not_equal</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.not_equal()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.not_equal_.html#torch.Tensor.not_equal_" title="torch.Tensor.not_equal_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.not_equal_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">not_equal()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.neg.html#torch.Tensor.neg" title="torch.Tensor.neg"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.neg</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.neg()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.neg_.html#torch.Tensor.neg_" title="torch.Tensor.neg_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.neg_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">neg()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.negative.html#torch.Tensor.negative" title="torch.Tensor.negative"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.negative</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.negative()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.negative_.html#torch.Tensor.negative_" title="torch.Tensor.negative_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.negative_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">negative()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.nelement.html#torch.Tensor.nelement" title="torch.Tensor.nelement"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.nelement</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">numel()</span></code> 的别名。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter" title="torch.Tensor.nextafter"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.nextafter</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.nextafter()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.nextafter_.html#torch.Tensor.nextafter_" title="torch.Tensor.nextafter_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.nextafter_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">nextafter()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.nonzero.html#torch.Tensor.nonzero" title="torch.Tensor.nonzero"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.nonzero</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.nonzero()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.norm.html#torch.Tensor.norm" title="torch.Tensor.norm"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.norm</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.norm()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.normal_.html#torch.Tensor.normal_" title="torch.Tensor.normal_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.normal_</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量填充为从由 <code class="xref py py-attr docutils literal "><span class="pre">mean</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">std</span></code> 参数化的正态分布中抽取的样本元素。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.numel.html#torch.Tensor.numel" title="torch.Tensor.numel"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.numel</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.numel()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.numpy.html#torch.Tensor.numpy" title="torch.Tensor.numpy"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.numpy</span></code></a></p></td>
<td><p>返回张量作为 NumPy <code class="xref py py-class docutils literal "><span class="pre">ndarray</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.orgqr.html#torch.Tensor.orgqr" title="torch.Tensor.orgqr"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.orgqr</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.orgqr()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.ormqr.html#torch.Tensor.ormqr" title="torch.Tensor.ormqr"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ormqr</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.ormqr()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.outer.html#torch.Tensor.outer" title="torch.Tensor.outer"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.outer</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.outer()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.permute.html#torch.Tensor.permute" title="torch.Tensor.permute"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.permute</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.permute()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory" title="torch.Tensor.pin_memory"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.pin_memory</span></code></a></p></td>
<td><p>如果张量尚未固定，则将其复制到固定内存。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.pinverse.html#torch.Tensor.pinverse" title="torch.Tensor.pinverse"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.pinverse</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.pinverse()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma" title="torch.Tensor.polygamma"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.polygamma</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.polygamma()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.polygamma_.html#torch.Tensor.polygamma_" title="torch.Tensor.polygamma_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.polygamma_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">polygamma()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.positive.html#torch.Tensor.positive" title="torch.Tensor.positive"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.positive</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.positive()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.pow.html#torch.Tensor.pow" title="torch.Tensor.pow"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.pow</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.pow()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.pow_.html#torch.Tensor.pow_" title="torch.Tensor.pow_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.pow_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">pow()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.prod.html#torch.Tensor.prod" title="torch.Tensor.prod"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.prod</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.prod()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.put_.html#torch.Tensor.put_" title="torch.Tensor.put_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.put_</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">source</span></code> 中的元素复制到由 <code class="xref py py-attr docutils literal "><span class="pre">index</span></code> 指定的位置。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.qr.html#torch.Tensor.qr" title="torch.Tensor.qr"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.qr</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.qr()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.qscheme.html#torch.Tensor.qscheme" title="torch.Tensor.qscheme"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.qscheme</span></code></a></p></td>
<td><p>返回给定 QTensor 的量化方案。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.quantile.html#torch.Tensor.quantile" title="torch.Tensor.quantile"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.quantile</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.quantile()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.nanquantile.html#torch.Tensor.nanquantile" title="torch.Tensor.nanquantile"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.nanquantile</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.nanquantile()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale" title="torch.Tensor.q_scale"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.q_scale</span></code></a></p></td>
<td><p>给定一个通过线性（仿射）量化量化的张量，返回底层量化器的尺度()。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point" title="torch.Tensor.q_zero_point"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.q_zero_point</span></code></a></p></td>
<td><p>给定一个通过线性（仿射）量化量化的张量，返回底层量化器的零点()。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales" title="torch.Tensor.q_per_channel_scales"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.q_per_channel_scales</span></code></a></p></td>
<td><p>给定一个通过线性（仿射）按通道量化的张量，返回底层量化器的尺度张量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points" title="torch.Tensor.q_per_channel_zero_points"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.q_per_channel_zero_points</span></code></a></p></td>
<td><p>给定一个通过线性（仿射）按通道量化的张量，返回底层量化器的零点张量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis" title="torch.Tensor.q_per_channel_axis"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.q_per_channel_axis</span></code></a></p></td>
<td><p>给定一个通过线性（仿射）按通道量化的张量，返回应用按通道量化的维度索引。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg" title="torch.Tensor.rad2deg"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.rad2deg</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.rad2deg()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.random_.html#torch.Tensor.random_" title="torch.Tensor.random_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.random_</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量填充为从离散均匀分布中采样的数字。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.ravel.html#torch.Tensor.ravel" title="torch.Tensor.ravel"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ravel</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.ravel()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal" title="torch.Tensor.reciprocal"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.reciprocal</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.reciprocal()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.reciprocal_.html#torch.Tensor.reciprocal_" title="torch.Tensor.reciprocal_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.reciprocal_</span></code></a></p></td>
<td><p>原位版本 <code class="xref py py-meth docutils literal "><span class="pre">reciprocal()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream" title="torch.Tensor.record_stream"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.record_stream</span></code></a></p></td>
<td><p>标记张量已被此流使用。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook" title="torch.Tensor.register_hook"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.register_hook</span></code></a></p></td>
<td><p>注册反向钩子。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.register_post_accumulate_grad_hook.html#torch.Tensor.register_post_accumulate_grad_hook" title="torch.Tensor.register_post_accumulate_grad_hook"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.register_post_accumulate_grad_hook</span></code></a></p></td>
<td><p>注册在梯度累积之后运行的回退钩子。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.remainder.html#torch.Tensor.remainder" title="torch.Tensor.remainder"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.remainder</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.remainder()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.remainder_.html#torch.Tensor.remainder_" title="torch.Tensor.remainder_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.remainder_</span></code></a></p></td>
<td><p>原位版本 <code class="xref py py-meth docutils literal "><span class="pre">remainder()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.renorm.html#torch.Tensor.renorm" title="torch.Tensor.renorm"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.renorm</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.renorm()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.renorm_.html#torch.Tensor.renorm_" title="torch.Tensor.renorm_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.renorm_</span></code></a></p></td>
<td><p>原位版本 <code class="xref py py-meth docutils literal "><span class="pre">renorm()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.repeat.html#torch.Tensor.repeat" title="torch.Tensor.repeat"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.repeat</span></code></a></p></td>
<td><p>沿指定维度重复此张量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.repeat_interleave.html#torch.Tensor.repeat_interleave" title="torch.Tensor.repeat_interleave"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.repeat_interleave</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.repeat_interleave()</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad" title="torch.Tensor.requires_grad"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.requires_grad</span></code></a></p></td>
<td><p>如果需要为此张量计算梯度，则使用 <code class="docutils literal "><span class="pre">True</span></code> ；否则使用 <code class="docutils literal "><span class="pre">False</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.requires_grad_</span></code></a></p></td>
<td><p>如果 autograd 应记录此张量的操作：则就地设置此张量的 <code class="xref py py-attr docutils literal "><span class="pre">requires_grad</span></code> 属性。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.reshape.html#torch.Tensor.reshape" title="torch.Tensor.reshape"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.reshape</span></code></a></p></td>
<td><p>返回一个具有与 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 相同数据和元素数量的张量，但具有指定的形状。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as" title="torch.Tensor.reshape_as"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.reshape_as</span></code></a></p></td>
<td><p>以与 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 相同的形状返回此张量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.resize_.html#torch.Tensor.resize_" title="torch.Tensor.resize_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.resize_</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量调整到指定的大小。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_" title="torch.Tensor.resize_as_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.resize_as_</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量调整大小，使其与指定的 <code class="xref py py-attr docutils literal "><span class="pre">tensor</span></code> 大小相同。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.retain_grad.html#torch.Tensor.retain_grad" title="torch.Tensor.retain_grad"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.retain_grad</span></code></a></p></td>
<td><p>允许这个张量在 <code class="xref py py-func docutils literal "><span class="pre">backward()</span></code> 期间填充 <code class="xref py py-attr docutils literal "><span class="pre">grad</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.retains_grad.html#torch.Tensor.retains_grad" title="torch.Tensor.retains_grad"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.retains_grad</span></code></a></p></td>
<td><p>如果此张量是非叶节点且其 <code class="xref py py-attr docutils literal "><span class="pre">grad</span></code> 在 <code class="xref py py-func docutils literal "><span class="pre">backward()</span></code> 时启用以填充，则返回 <code class="docutils literal "><span class="pre">True</span></code> ，否则返回 <code class="docutils literal "><span class="pre">False</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.roll.html#torch.Tensor.roll" title="torch.Tensor.roll"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.roll</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.roll()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.rot90.html#torch.Tensor.rot90" title="torch.Tensor.rot90"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.rot90</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.rot90()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.round.html#torch.Tensor.round" title="torch.Tensor.round"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.round</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.round()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.round_.html#torch.Tensor.round_" title="torch.Tensor.round_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.round_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">round()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt" title="torch.Tensor.rsqrt"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.rsqrt</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.rsqrt()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.rsqrt_.html#torch.Tensor.rsqrt_" title="torch.Tensor.rsqrt_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.rsqrt_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">rsqrt()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.scatter.html#torch.Tensor.scatter" title="torch.Tensor.scatter"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.scatter</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.scatter_()</span></code> 的非就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.scatter_</span></code></a></p></td>
<td><p>将张量 <code class="xref py py-attr docutils literal "><span class="pre">src</span></code> 中所有值写入 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> ，索引由张量 <code class="xref py py-attr docutils literal "><span class="pre">index</span></code> 指定。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_" title="torch.Tensor.scatter_add_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.scatter_add_</span></code></a></p></td>
<td><p>将张量 <code class="xref py py-attr docutils literal "><span class="pre">src</span></code> 中的所有值按 <code class="xref py py-attr docutils literal "><span class="pre">index</span></code> 张量指定的索引添加到 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 中，类似于 <code class="xref py py-meth docutils literal "><span class="pre">scatter_()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.scatter_add.html#torch.Tensor.scatter_add" title="torch.Tensor.scatter_add"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.scatter_add</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.scatter_add_()</span></code> 的非原地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_" title="torch.Tensor.scatter_reduce_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.scatter_reduce_</span></code></a></p></td>
<td><p>使用通过 <code class="xref py py-attr docutils literal "><span class="pre">reduce</span></code> 参数定义的应用的归约操作，将张量 <code class="xref py py-attr docutils literal "><span class="pre">src</span></code> 中的所有值归约到 <code class="xref py py-attr docutils literal "><span class="pre">index</span></code> 张量中指定的索引，结果存储在 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量中（ <code class="xref py py-obj docutils literal "><span class="pre">"sum"</span></code> ， <code class="xref py py-obj docutils literal "><span class="pre">"prod"</span></code> ， <code class="xref py py-obj docutils literal "><span class="pre">"mean"</span></code> ， <code class="xref py py-obj docutils literal "><span class="pre">"amax"</span></code> ， <code class="xref py py-obj docutils literal "><span class="pre">"amin"</span></code> ）。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.scatter_reduce.html#torch.Tensor.scatter_reduce" title="torch.Tensor.scatter_reduce"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.scatter_reduce</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.scatter_reduce_()</span></code> 的非原地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.select.html#torch.Tensor.select" title="torch.Tensor.select"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.select</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.select()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.select_scatter.html#torch.Tensor.select_scatter" title="torch.Tensor.select_scatter"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.select_scatter</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.select_scatter()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.set_.html#torch.Tensor.set_" title="torch.Tensor.set_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.set_</span></code></a></p></td>
<td><p>设置底层存储、大小和步长。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_" title="torch.Tensor.share_memory_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.share_memory_</span></code></a></p></td>
<td><p>将底层存储移动到共享内存。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.short.html#torch.Tensor.short" title="torch.Tensor.short"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.short</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.short()</span></code> 等价于 <code class="docutils literal "><span class="pre">self.to(torch.int16)</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid" title="torch.Tensor.sigmoid"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sigmoid</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.sigmoid()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sigmoid_.html#torch.Tensor.sigmoid_" title="torch.Tensor.sigmoid_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sigmoid_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">sigmoid()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sign.html#torch.Tensor.sign" title="torch.Tensor.sign"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sign</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.sign()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sign_.html#torch.Tensor.sign_" title="torch.Tensor.sign_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sign_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">sign()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.signbit.html#torch.Tensor.signbit" title="torch.Tensor.signbit"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.signbit</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.signbit()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sgn.html#torch.Tensor.sgn" title="torch.Tensor.sgn"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sgn</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.sgn()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sgn_.html#torch.Tensor.sgn_" title="torch.Tensor.sgn_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sgn_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">sgn()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sin.html#torch.Tensor.sin" title="torch.Tensor.sin"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sin</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.sin()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sin_.html#torch.Tensor.sin_" title="torch.Tensor.sin_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sin_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">sin()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sinc.html#torch.Tensor.sinc" title="torch.Tensor.sinc"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sinc</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.sinc()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sinc_.html#torch.Tensor.sinc_" title="torch.Tensor.sinc_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sinc_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">sinc()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sinh.html#torch.Tensor.sinh" title="torch.Tensor.sinh"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sinh</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.sinh()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sinh_.html#torch.Tensor.sinh_" title="torch.Tensor.sinh_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sinh_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">sinh()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.asinh.html#torch.Tensor.asinh" title="torch.Tensor.asinh"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.asinh</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.asinh()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.asinh_.html#torch.Tensor.asinh_" title="torch.Tensor.asinh_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.asinh_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">asinh()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh" title="torch.Tensor.arcsinh"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arcsinh</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.arcsinh()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arcsinh_.html#torch.Tensor.arcsinh_" title="torch.Tensor.arcsinh_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arcsinh_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">arcsinh()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.shape.html#torch.Tensor.shape" title="torch.Tensor.shape"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.shape</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量的尺寸。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.size.html#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.size</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量的尺寸。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.slogdet.html#torch.Tensor.slogdet" title="torch.Tensor.slogdet"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.slogdet</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.slogdet()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.slice_scatter.html#torch.Tensor.slice_scatter" title="torch.Tensor.slice_scatter"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.slice_scatter</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.slice_scatter()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.softmax.html#torch.Tensor.softmax" title="torch.Tensor.softmax"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.softmax</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.nn.functional.softmax()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sort.html#torch.Tensor.sort" title="torch.Tensor.sort"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sort</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.sort()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.split.html#torch.Tensor.split" title="torch.Tensor.split"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.split</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.split()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask" title="torch.Tensor.sparse_mask"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sparse_mask</span></code></a></p></td>
<td><p>返回一个新的稀疏张量，其值来自步长张量 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> ，并通过稀疏张量 <code class="xref py py-attr docutils literal "><span class="pre">mask</span></code> 的索引进行过滤。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim" title="torch.Tensor.sparse_dim"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sparse_dim</span></code></a></p></td>
<td><p>返回稀疏张量 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 中的稀疏维度数量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt" title="torch.Tensor.sqrt"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sqrt</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.sqrt()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sqrt_.html#torch.Tensor.sqrt_" title="torch.Tensor.sqrt_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sqrt_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">sqrt()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.square.html#torch.Tensor.square" title="torch.Tensor.square"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.square</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.square()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.square_.html#torch.Tensor.square_" title="torch.Tensor.square_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.square_</span></code></a></p></td>
<td><p>原位版本 <code class="xref py py-meth docutils literal "><span class="pre">square()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze" title="torch.Tensor.squeeze"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.squeeze</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.squeeze()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.squeeze_.html#torch.Tensor.squeeze_" title="torch.Tensor.squeeze_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.squeeze_</span></code></a></p></td>
<td><p>原位版本 <code class="xref py py-meth docutils literal "><span class="pre">squeeze()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.std.html#torch.Tensor.std" title="torch.Tensor.std"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.std</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.std()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.stft.html#torch.Tensor.stft" title="torch.Tensor.stft"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.stft</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.stft()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.storage.html#torch.Tensor.storage" title="torch.Tensor.storage"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.storage</span></code></a></p></td>
<td><p>返回基础 <code class="xref py py-class docutils literal "><span class="pre">TypedStorage</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.untyped_storage.html#torch.Tensor.untyped_storage" title="torch.Tensor.untyped_storage"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.untyped_storage</span></code></a></p></td>
<td><p>返回基础 <code class="xref py py-class docutils literal "><span class="pre">UntypedStorage</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.storage_offset.html#torch.Tensor.storage_offset" title="torch.Tensor.storage_offset"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.storage_offset</span></code></a></p></td>
<td><p>返回在底层存储中 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量的偏移量，以存储元素数量表示（不是字节数）。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type" title="torch.Tensor.storage_type"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.storage_type</span></code></a></p></td>
<td><p>返回底层存储的类型。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.stride.html#torch.Tensor.stride" title="torch.Tensor.stride"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.stride</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量的步长。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sub.html#torch.Tensor.sub" title="torch.Tensor.sub"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sub</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.sub()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sub_.html#torch.Tensor.sub_" title="torch.Tensor.sub_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sub_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">sub()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.subtract.html#torch.Tensor.subtract" title="torch.Tensor.subtract"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.subtract</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.subtract()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.subtract_.html#torch.Tensor.subtract_" title="torch.Tensor.subtract_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.subtract_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">subtract()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sum.html#torch.Tensor.sum" title="torch.Tensor.sum"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sum</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.sum()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sum_to_size.html#torch.Tensor.sum_to_size" title="torch.Tensor.sum_to_size"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sum_to_size</span></code></a></p></td>
<td><p>将 <code class="docutils literal "><span class="pre">this</span></code> 张量转换为 <code class="xref py py-attr docutils literal "><span class="pre">size</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.svd.html#torch.Tensor.svd" title="torch.Tensor.svd"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.svd</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.svd()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes" title="torch.Tensor.swapaxes"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.swapaxes</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.swapaxes()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims" title="torch.Tensor.swapdims"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.swapdims</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.swapdims()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.t.html#torch.Tensor.t" title="torch.Tensor.t"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.t</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.t()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.t_.html#torch.Tensor.t_" title="torch.Tensor.t_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.t_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">t()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split" title="torch.Tensor.tensor_split"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.tensor_split</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.tensor_split()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.tile.html#torch.Tensor.tile" title="torch.Tensor.tile"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.tile</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.tile()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.to.html#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to</span></code></a></p></td>
<td><p>执行张量数据类型和/或设备转换。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.to_mkldnn.html#torch.Tensor.to_mkldnn" title="torch.Tensor.to_mkldnn"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_mkldnn</span></code></a></p></td>
<td><p>返回 <code class="docutils literal "><span class="pre">torch.mkldnn</span></code> 布局的张量副本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.take.html#torch.Tensor.take" title="torch.Tensor.take"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.take</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.take()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.take_along_dim.html#torch.Tensor.take_along_dim" title="torch.Tensor.take_along_dim"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.take_along_dim</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.take_along_dim()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.tan.html#torch.Tensor.tan" title="torch.Tensor.tan"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.tan</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.tan()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.tan_.html#torch.Tensor.tan_" title="torch.Tensor.tan_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.tan_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">tan()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.tanh.html#torch.Tensor.tanh" title="torch.Tensor.tanh"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.tanh</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.tanh()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.tanh_.html#torch.Tensor.tanh_" title="torch.Tensor.tanh_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.tanh_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">tanh()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.atanh.html#torch.Tensor.atanh" title="torch.Tensor.atanh"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.atanh</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.atanh()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.atanh_.html#torch.Tensor.atanh_" title="torch.Tensor.atanh_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.atanh_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">atanh()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh" title="torch.Tensor.arctanh"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arctanh</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.arctanh()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.arctanh_.html#torch.Tensor.arctanh_" title="torch.Tensor.arctanh_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.arctanh_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">arctanh()</span></code> 的就地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.tolist.html#torch.Tensor.tolist" title="torch.Tensor.tolist"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.tolist</span></code></a></p></td>
<td><p>返回作为（嵌套）列表的张量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.topk.html#torch.Tensor.topk" title="torch.Tensor.topk"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.topk</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.topk()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense" title="torch.Tensor.to_dense"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_dense</span></code></a></p></td>
<td><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 不是一个带偏移量的张量，则创建 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 的带偏移量的副本，否则返回 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse" title="torch.Tensor.to_sparse"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_sparse</span></code></a></p></td>
<td><p>返回张量的稀疏副本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr" title="torch.Tensor.to_sparse_csr"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_sparse_csr</span></code></a></p></td>
<td><p>将张量转换为压缩行存储格式（CSR）。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc" title="torch.Tensor.to_sparse_csc"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_sparse_csc</span></code></a></p></td>
<td><p>将张量转换为压缩列存储（CSC）格式。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr" title="torch.Tensor.to_sparse_bsr"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_sparse_bsr</span></code></a></p></td>
<td><p>将张量转换为给定块大小的块稀疏行（BSR）存储格式。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsc.html#torch.Tensor.to_sparse_bsc" title="torch.Tensor.to_sparse_bsc"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_sparse_bsc</span></code></a></p></td>
<td><p>将张量转换为给定块大小的块稀疏列（BSC）存储格式。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.trace.html#torch.Tensor.trace" title="torch.Tensor.trace"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.trace</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.trace()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.transpose.html#torch.Tensor.transpose" title="torch.Tensor.transpose"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.transpose</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.transpose()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_" title="torch.Tensor.transpose_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.transpose_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">transpose()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.triangular_solve.html#torch.Tensor.triangular_solve" title="torch.Tensor.triangular_solve"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.triangular_solve</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.triangular_solve()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.tril.html#torch.Tensor.tril" title="torch.Tensor.tril"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.tril</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.tril()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.tril_.html#torch.Tensor.tril_" title="torch.Tensor.tril_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.tril_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">tril()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.triu.html#torch.Tensor.triu" title="torch.Tensor.triu"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.triu</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.triu()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.triu_.html#torch.Tensor.triu_" title="torch.Tensor.triu_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.triu_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">triu()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.true_divide.html#torch.Tensor.true_divide" title="torch.Tensor.true_divide"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.true_divide</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.true_divide()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_" title="torch.Tensor.true_divide_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.true_divide_</span></code></a></p></td>
<td><p>原地版本 <code class="xref py py-meth docutils literal "><span class="pre">true_divide_()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.trunc.html#torch.Tensor.trunc" title="torch.Tensor.trunc"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.trunc</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.trunc()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.trunc_.html#torch.Tensor.trunc_" title="torch.Tensor.trunc_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.trunc_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">trunc()</span></code> 的本地版本</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.type.html#torch.Tensor.type" title="torch.Tensor.type"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.type</span></code></a></p></td>
<td><p>如果未提供 dtype，则返回类型，否则将此对象转换为指定的类型。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.type_as.html#torch.Tensor.type_as" title="torch.Tensor.type_as"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.type_as</span></code></a></p></td>
<td><p>将此张量转换为给定张量的类型。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.unbind.html#torch.Tensor.unbind" title="torch.Tensor.unbind"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.unbind</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.unbind()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten" title="torch.Tensor.unflatten"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.unflatten</span></code></a></p></td>
<td><p>见 <code class="xref py py-func docutils literal "><span class="pre">torch.unflatten()</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.unfold.html#torch.Tensor.unfold" title="torch.Tensor.unfold"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.unfold</span></code></a></p></td>
<td><p>返回原始张量的一个视图，该视图包含从 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量在 <code class="xref py py-attr docutils literal "><span class="pre">dimension</span></code> 维度上的所有大小为 <code class="xref py py-attr docutils literal "><span class="pre">size</span></code> 的切片。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_" title="torch.Tensor.uniform_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.uniform_</span></code></a></p></td>
<td><p>使用连续均匀分布的数字填充 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量：</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.unique.html#torch.Tensor.unique" title="torch.Tensor.unique"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.unique</span></code></a></p></td>
<td><p>返回输入张量中的唯一元素。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive" title="torch.Tensor.unique_consecutive"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.unique_consecutive</span></code></a></p></td>
<td><p>删除每个连续等效元素组中除了第一个元素之外的所有元素。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze" title="torch.Tensor.unsqueeze"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.unsqueeze</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.unsqueeze()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.unsqueeze_.html#torch.Tensor.unsqueeze_" title="torch.Tensor.unsqueeze_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.unsqueeze_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">unsqueeze()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.values.html#torch.Tensor.values" title="torch.Tensor.values"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.values</span></code></a></p></td>
<td><p>返回稀疏 COO 张量的值张量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.var.html#torch.Tensor.var" title="torch.Tensor.var"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.var</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.var()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.vdot.html#torch.Tensor.vdot" title="torch.Tensor.vdot"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.vdot</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.vdot()</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.view.html#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.view</span></code></a></p></td>
<td><p>返回一个新张量，其数据与 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量相同，但类型不同 <code class="xref py py-attr docutils literal "><span class="pre">shape</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.view_as.html#torch.Tensor.view_as" title="torch.Tensor.view_as"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.view_as</span></code></a></p></td>
<td><p>将此张量视为与 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 相同大小。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit" title="torch.Tensor.vsplit"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.vsplit</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.vsplit()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.where.html#torch.Tensor.where" title="torch.Tensor.where"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.where</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">self.where(condition,</span> <span class="pre">y)</span></code> 与 <code class="docutils literal "><span class="pre">torch.where(condition,</span> <span class="pre">self,</span> <span class="pre">y)</span></code> 等价。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy" title="torch.Tensor.xlogy"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.xlogy</span></code></a></p></td>
<td><p>查看 <code class="xref py py-func docutils literal "><span class="pre">torch.xlogy()</span></code> </p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.xlogy_.html#torch.Tensor.xlogy_" title="torch.Tensor.xlogy_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.xlogy_</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">xlogy()</span></code> 的就地版本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.xpu.html#torch.Tensor.xpu" title="torch.Tensor.xpu"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.xpu</span></code></a></p></td>
<td><p>返回此对象在 XPU 内存中的副本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.zero_.html#torch.Tensor.zero_" title="torch.Tensor.zero_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.zero_</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量填充为零。</p></td>
</tr>
</tbody>
</table>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        下一个 <img height="16" width="16" class="next-page" src="_static/images/chevron-right-orange.svg"> <img height="16" width="16" class="previous-page" src="_static/images/chevron-right-orange.svg"> 上一个
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>© 版权所有 PyTorch 贡献者。</p>
  </div>
    
      <div>使用 Sphinx 构建，主题由 Read the Docs 提供。</div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.Tensor</a><ul>
<li><a class="reference internal" href="#data-types">数据类型</a></li>
<li><a class="reference internal" href="#initializing-and-basic-operations">初始化和基本操作</a></li>
<li><a class="reference internal" href="#tensor-class-reference">张量类参考</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script="" type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0">


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>文档</h2>
          <p>PyTorch 开发者文档全面访问</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">查看文档</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>教程</h2>
          <p>获取初学者和高级开发者的深入教程</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">查看教程</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>资源</h2>
          <p>查找开发资源并获得您的疑问解答</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">查看资源</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">开始使用</a></li>
            <li><a href="https://pytorch.org/features">功能</a></li>
            <li><a href="https://pytorch.org/ecosystem">生态系统</a></li>
            <li><a href="https://pytorch.org/blog/">博客</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">贡献</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">资源</a></li>
            <li><a href="https://pytorch.org/tutorials">教程</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">文档</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">讨论</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub 问题和任务</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">品牌指南</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">保持更新</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">推特</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">领英</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch 播客</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">苹果</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">谷歌</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">亚马逊</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">条款</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">隐私</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© 版权所有 Linux 基金会。PyTorch 基金会是 Linux 基金会的一个项目。有关本网站的使用条款、商标政策以及其他适用于 PyTorch 基金会的政策，请参阅 www.linuxfoundation.org/policies/。PyTorch 基金会支持 PyTorch 开源项目，该项目已被确立为 LF Projects, LLC 的 PyTorch 项目系列。有关适用于 PyTorch 项目系列 LF Projects, LLC 的政策，请参阅 www.lfprojects.org/policies/。</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">为分析流量并优化您的体验，我们在本网站上提供 cookies。通过点击或导航，您同意允许我们使用 cookies。作为本站点的当前维护者，Facebook 的 Cookies 政策适用。了解更多信息，包括可用的控制选项：Cookies 政策。</p>
    <img class="close-button" src="_static/images/pytorch-x.svg" width="16" height="16">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>学习</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">开始学习</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">教程</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">学习基础知识</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch 菜谱</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">PyTorch 入门 - YouTube 系列</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>生态系统</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">工具</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">社区</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">论坛</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">开发者资源</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">贡献者奖项 - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">关于 PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch 文档</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>文档</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch 领域</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>博客 &amp; 新闻</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch 博客</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">社区博客</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">视频</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">社区故事</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">活动</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">通讯</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>关于</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch 基金会</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">治理委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">云信用计划</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">技术顾问委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">员工</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">联系我们</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

</body></html>