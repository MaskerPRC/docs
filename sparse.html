<!DOCTYPE html>
<html lang="zh_CN">
<head>
  <meta charset="UTF-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.sparse — PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/sparse.html">
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css">
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css">
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css">
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css">
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css">
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css">
    <link rel="index" title="Index" href="genindex.html">
    <link rel="search" title="Search" href="search.html">
    <link rel="next" title="torch.Tensor.is_sparse_csr" href="generated/torch.Tensor.is_sparse_csr.html">
    <link rel="prev" title="torch.Size" href="size.html">

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head><body class="pytorch-body"><div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">学习</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class="dropdown-title">开始使用</span>
                  <p>在本地运行 PyTorch 或快速开始使用支持的云平台之一</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">教程</span><p></p>
                  <p>PyTorch 教程中的新内容</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">学习基础知识</span><p></p>
                  <p>熟悉 PyTorch 的概念和模块</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch 食谱</span><p></p>
                  <p>精简版、可直接部署的 PyTorch 代码示例</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">PyTorch 入门 - YouTube 系列</span><p></p>
                  <p>通过我们引人入胜的 YouTube 教程系列掌握 PyTorch 基础知识</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">生态系统</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">工具</span><p></p>
                  <p>了解 PyTorch 生态系统中的工具和框架</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">社区</span>
                  <p>加入 PyTorch 开发者社区，贡献、学习并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">论坛</span>
                  <p>讨论 PyTorch 代码、问题、安装、研究的地方</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">开发者资源</span>
                  <p>查找资源并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">贡献者奖项 - 2024</span><p></p>
                  <p>本届 PyTorch 会议揭晓获奖者</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">关于 PyTorch Edge</span><p></p>
                  <p>为边缘设备构建创新和隐私感知的 AI 体验</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span><p></p>
                  <p>基于移动和边缘设备的端到端推理能力解决方案</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch 文档</span><p></p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">文档</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span><p></p>
                  <p>探索文档以获取全面指导，了解如何使用 PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch 领域</span><p></p>
                  <p>阅读 PyTorch 领域的文档，了解更多关于特定领域库的信息</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">博客与新闻</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch 博客</span><p></p>
                  <p>捕捉最新的技术新闻和事件</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">社区博客</span><p></p>
                  <p>PyTorch 生态系统故事</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">视频</span><p></p>
                  <p>了解最新的 PyTorch 教程、新内容等</p>
                </a><a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">社区故事</span><p></p>
                  <p>学习如何我们的社区使用 PyTorch 解决真实、日常的机器学习问题</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">活动</span><p></p>
                  <p>查找活动、网络研讨会和播客</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">通讯</span><p></p>
                  <p>跟踪最新更新</p>
                </a>
            </div>
          </div></li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">关于</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch 基金会</span><p></p>
                  <p>了解更多关于 PyTorch 基金会的信息</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">管理委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">云信用计划</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">技术顾问委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">员工</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">联系我们</span><p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">成为会员</a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>



   

    

    <div class="table-of-contents-link-wrapper">
      <span>目录</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="https://pytorch.org/docs/versions.html">主程序 (2.7.0+cpu ) ▼</a>
    </div>
    <div id="searchBox">
    <div class="searchbox" id="googleSearchBox">
      <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
      <div class="gcse-search"></div>
    </div>
    <div id="sphinxSearchBox" style="display: none;">
      <div role="search">
        <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
          <input type="text" name="q" placeholder="Search Docs">
          <input type="hidden" name="check_keywords" value="yes">
          <input type="hidden" name="area" value="default">
        </form>
      </div>
    </div>
  </div>
  <form id="searchForm">
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="google" checked="">谷歌搜索</label>
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="sphinx">经典搜索</label>
  </form>

  <script>
     document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
  </script>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/sparse.html">您正在查看不稳定开发者预览文档。请点击此处查看最新稳定版本的文档。</a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">社区</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch 治理 | 构建 + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch 贡献指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch 设计哲学</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch 治理 | 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch 治理 | 维护者</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者笔记</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">自动混合精度示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">广播语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU 多线程和 TorchScript 推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA 语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/custom_operators.html">PyTorch 自定义算子页面</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">分布式数据并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">扩展 PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">使用 autograd.Function 扩展 torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">常见问题解答</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/fsdp.html">FSDP 笔记</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/get_start_xpu.html">在 Intel GPU 上入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm)语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">大规模部署功能</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/libtorch_stable_abi.html">LibTorch 稳定 ABI</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">模块</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS 后端</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">多进程最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">数值精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">可重现性</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">序列化语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows 常见问题解答</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语言绑定</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">张量属性</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">张量视图</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="accelerator.html">torch.accelerator</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">理解 CUDA 内存使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">生成快照</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">使用可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">快照 API 参考</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.memory.html">torch.mtia.memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">元设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.html">torch.distributed.tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.fsdp.fully_shard.html">torch.distributed.fsdp.fully_shard</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">复数</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP 通信钩子</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">分布式 RPC 框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="size.html">torch.Size</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">类型信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">命名张量</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">命名张量操作覆盖率</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_environment_variables.html">火炬环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">库</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">火炬推荐</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch 在 XLA 设备上</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/ao">torchao</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        文档 &gt;</li>

        
      <li>torch.sparse</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/sparse.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg" width="16" height="16"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">快捷键</div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <span class="target" id="module-torch.sparse"></span><section id="torch-sparse">
<span id="sparse-docs"></span><h1>torch.sparse ¬</h1>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>PyTorch 的稀疏张量 API 处于测试阶段，未来可能会发生变化。我们非常欢迎通过 GitHub 问题报告功能提出功能请求、错误报告和一般建议。</p>
</div>
<section id="why-and-when-to-use-sparsity">
<h2>为什么以及何时使用稀疏性 ¬</h2>
<p>默认情况下，PyTorch 将 <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> 元素连续存储在物理内存中。这导致各种需要快速访问元素的数组处理算法实现高效。</p>
<p>现在，一些用户可能会选择用元素大部分为零的 Tensor 来表示图邻接矩阵、剪枝权重或点云等数据。我们认识到这些是重要的应用，并旨在通过稀疏存储格式对这些用例提供性能优化。</p>
<p>近年来，已经开发了各种稀疏存储格式，如 COO、CSR/CSC、半结构化、LIL 等。虽然它们的布局各不相同，但它们都通过高效地表示零值元素来压缩数据。我们称与未指定、压缩元素相对的未压缩值为“未压缩值”。</p>
<p>通过压缩重复的零值，稀疏存储格式旨在在各种 CPU 和 GPU 上节省内存和计算资源。特别是对于高稀疏度或高度结构化稀疏度，这可以产生显著的性能影响。因此，稀疏存储格式可以被视为一种性能优化。</p>
<p>许多其他性能优化稀疏存储格式并不总是有优势。在尝试为您的用例使用稀疏格式时，您可能会发现执行时间增加而不是减少。</p>
<p>如果您在分析上预期会看到性能显著提升，但测量结果却是下降，请鼓励您在 GitHub 上创建一个 issue。这有助于我们优先实施高效的内核和更广泛性能优化。</p>
<p>我们使您能够轻松尝试不同的稀疏布局，并在它们之间进行转换，而不会对您特定应用的最佳选择有所偏见。</p>
</section>
<section id="functionality-overview">
<h2>功能概述 ¶</h2>
<p>我们希望能够通过提供每个布局的转换例程，使从给定的密集 Tensor 构建稀疏 Tensor 变得简单直接。</p>
<p>在下一个示例中，我们将默认的密集（带偏移量）布局的 2D Tensor 转换为基于 COO 内存布局的 2D Tensor。在这种情况下，仅存储非零元素的值和索引。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span>
<span class="go">tensor(indices=tensor([[0, 1],</span>
<span class="go">                       [1, 0]]),</span>
<span class="go">       values=tensor([2., 3.]),</span>
<span class="go">       size=(2, 2), nnz=2, layout=torch.sparse_coo)</span>
</pre></div>
</div>
<p>PyTorch 目前支持 COO、CSR、CSC、BSR 和 BSC。</p>
<p>我们还有一个原型实现来支持半结构化稀疏性：ref: semi-structured sparsity。请参阅参考资料以获取更多详细信息。</p>
<p>注意，我们对这些格式进行了一些轻微的概括。</p>
<p>批处理：如 GPU 等设备需要批处理以实现最佳性能，因此我们支持批维度。</p>
<p>我们目前提供了一种非常简单的批处理版本，其中稀疏格式的每个组件本身都被批处理。这也要求每个批处理条目指定相同数量的元素。在这个例子中，我们从 3D 稠密张量构建了一个 3D（批处理）CSR 张量。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]],</span> <span class="p">[[</span><span class="mf">4.</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="go">tensor(crow_indices=tensor([[0, 1, 3],</span>
<span class="go">                            [0, 1, 3]]),</span>
<span class="go">       col_indices=tensor([[0, 0, 1],</span>
<span class="go">                           [0, 0, 1]]),</span>
<span class="go">       values=tensor([[1., 2., 3.],</span>
<span class="go">                      [4., 5., 6.]]), size=(2, 2, 2), nnz=3,</span>
<span class="go">       layout=torch.sparse_csr)</span>
</pre></div>
</div>
<p>稠密维度：另一方面，一些数据，如图嵌入，可能更适合被视为向量的稀疏集合，而不是标量。</p>
<p>在本例中，我们从一个 3D 步进 Tensor 创建一个具有 2 个稀疏维度和 1 个密集维度的 3D 混合 COO 张量。如果 3D 步进 Tensor 中的整行都是零，则不会存储。然而，如果行中的任何值非零，则整个行都会被存储。这减少了索引的数量，因为我们只需要为每行提供一个索引，而不是为每个元素提供一个索引。但这也会增加值的存储量。由于只有整行都是零的行才能被发出，任何非零值的存在都会导致整行被存储。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]],</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="n">sparse_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor(indices=tensor([[0, 1],</span>
<span class="go">                       [1, 1]]),</span>
<span class="go">       values=tensor([[1., 2.],</span>
<span class="go">                      [3., 4.]]),</span>
<span class="go">       size=(2, 2, 2), nnz=2, layout=torch.sparse_coo)</span>
</pre></div>
</div>
</section>
<section id="operator-overview">
<h2>运算符概述</h2>
<p>基本上，对稀疏存储格式的 Tensor 的操作与对步进（或其它）存储格式的 Tensor 的操作行为相同。存储的特定性，即数据的物理布局，会影响操作的性能，但不应该影响语义。</p>
<p>我们正在积极增加稀疏张量的运算符覆盖范围。用户不应期望获得与密集 Tensor 相同的支持水平。请参阅我们的运算符文档以获取列表。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b_s</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b_s</span><span class="o">.</span><span class="n">cos</span><span class="p">()</span>
<span class="gt">Traceback (most recent call last):</span>
  File <span class="nb">"&lt;stdin&gt;"</span>, line <span class="m">1</span>, in <span class="n">&lt;module&gt;</span>
<span class="gr">RuntimeError</span>: <span class="n">unsupported tensor layout: SparseCsr</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b_s</span><span class="o">.</span><span class="n">sin</span><span class="p">()</span>
<span class="go">tensor(crow_indices=tensor([0, 3, 6]),</span>
<span class="go">       col_indices=tensor([2, 3, 4, 0, 1, 3]),</span>
<span class="go">       values=tensor([ 0.8415,  0.9093,  0.1411, -0.7568, -0.9589, -0.2794]),</span>
<span class="go">       size=(2, 6), nnz=6, layout=torch.sparse_csr)</span>
</pre></div>
</div>
<p>如上例所示，我们不支持保留非零的算术运算符，如 cos。非零保留的算术运算结果无法像输入那样充分利用稀疏存储格式，可能会造成内存的灾难性增长。我们反而依赖用户显式地将 Tensor 转换为稠密格式，然后再执行操作。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">b_s</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span><span class="o">.</span><span class="n">cos</span><span class="p">()</span>
<span class="go">tensor([[ 1.0000, -0.4161],</span>
<span class="go">        [-0.9900,  1.0000]])</span>
</pre></div>
</div>
<p>我们知道，一些用户希望忽略压缩零，例如在 cos 运算中而不是保留操作的精确语义。为此，我们可以指向 torch.masked 及其 MaskedTensor，它反过来也由稀疏存储格式和内核支持。</p>
<p>还要注意，目前用户无法选择输出布局。例如，将稀疏 Tensor 与常规的 strided Tensor 相加，结果将是 strided Tensor。一些用户可能希望保持稀疏布局，因为他们知道结果仍然足够稀疏。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span>
<span class="go">tensor([[0., 3.],</span>
<span class="go">        [3., 0.]])</span>
</pre></div>
</div>
<p>我们承认，能够高效产生不同输出布局的内核可能非常有用。后续操作可能会从接收特定布局中受益匪浅。我们正在开发一个 API 来控制结果布局，并认为这是一个重要的特性，有助于为任何给定模型规划更优的执行路径。</p>
</section>
<section id="sparse-semi-structured-tensors">
<span id="sparse-semi-structured-docs"></span><h2>稀疏半结构化张量</h2>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>稀疏半结构化张量目前是一个原型功能，可能会发生变化。请随时提交问题报告错误或分享反馈。</p>
</div>
<p>半结构化稀疏性是一种稀疏数据布局，首次在 NVIDIA 的 Ampere 架构中引入。它也被称为细粒度结构化稀疏性或 2:4 结构化稀疏性。</p>
<p>这种稀疏布局存储了每 2n 个元素中的 n 个元素，其中 n 由张量数据类型（dtype）的宽度决定。最常用的 dtype 是 float16，其中 n=2，因此术语“2:4 结构化稀疏”。</p>
<p>在这篇 NVIDIA 博客文章中更详细地解释了半结构化稀疏性。</p>
<p>在 PyTorch 中，半结构化稀疏性通过 Tensor 子类实现。通过子类化，我们可以重写 <code class="docutils literal "><span class="pre">__torch_dispatch__</span></code> ，这样我们可以在执行矩阵乘法时使用更快的稀疏内核。我们还可以在子类中将张量以压缩形式存储，以减少内存开销。</p>
<p>在这种压缩形式中，稀疏张量通过仅保留指定的元素和一些元数据（编码掩码）来存储。</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>半结构化稀疏张量的指定元素和元数据掩码存储在一个单独的扁平压缩张量中。它们被依次连接，形成一个连续的内存块。</p>
<p>压缩张量 = [原始张量的指定元素 | 元数据掩码]</p>
<p>对于大小为 (r, c) 的原始张量，我们期望前 m * k // 2 个元素是保留元素，其余的张量是元数据。</p>
<p>为了使用户更容易查看指定的元素和掩码，可以使用 <code class="docutils literal "><span class="pre">.indices()</span></code> 和 <code class="docutils literal "><span class="pre">.values()</span></code> 分别访问掩码和指定元素。</p>
<ul class="simple">
<li><p>返回一个大小为 (r, c//2) 的张量，其数据类型与密集矩阵相同。</p></li>
<li><p>对于 2:4 稀疏张量，元数据开销很小 - 每个指定元素仅占用 2 位。</p></li>
</ul>
</div>
<p>对于 2:4 稀疏张量，元数据开销很小 - 每个指定元素仅占用 2 位。</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>需要注意的是， <code class="docutils literal "><span class="pre">torch.float32</span></code> 仅支持 1:2 稀疏度，因此它不遵循上述公式。</p>
</div>
<p>这里我们分解如何计算 2:4 稀疏张量的压缩比（密集大小/稀疏大小）。</p>
<p>设(r, c) = tensor.shape，e = bitwidth(tensor.dtype)，因此对于 <code class="docutils literal "><span class="pre">torch.float16</span></code> 和 <code class="docutils literal "><span class="pre">torch.bfloat16</span></code> ，e = 16，对于 <code class="docutils literal "><span class="pre">torch.int8</span></code> ，e = 8。</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>M</mi><mrow><mi>d</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>e</mi></mrow></msub><mo>=</mo><mi>r</mi><mo>×</mo><mi>c</mi><mo>×</mo><mi>e</mi><mspace linebreak="newline"></mspace><msub><mi>M</mi><mrow><mi>s</mi><mi>p</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>e</mi></mrow></msub><mo>=</mo><msub><mi>M</mi><mrow><mi>s</mi><mi>p</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>f</mi><mi>i</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>+</mo><msub><mi>M</mi><mrow><mi>m</mi><mi>e</mi><mi>t</mi><mi>a</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>=</mo><mi>r</mi><mo>×</mo><mfrac><mi>c</mi><mn>2</mn></mfrac><mo>×</mo><mi>e</mi><mo>+</mo><mi>r</mi><mo>×</mo><mfrac><mi>c</mi><mn>2</mn></mfrac><mo>×</mo><mn>2</mn><mo>=</mo><mfrac><mrow><mi>r</mi><mi>c</mi><mi>e</mi></mrow><mn>2</mn></mfrac><mo>+</mo><mi>r</mi><mi>c</mi><mo>=</mo><mi>r</mi><mi>c</mi><mi>e</mi><mo stretchy="false">(</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>+</mo><mfrac><mn>1</mn><mi>e</mi></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">M_{dense} = r \times c \times e \\
M_{sparse} = M_{specified} + M_{metadata} = r \times \frac{c}{2} \times e + r \times \frac{c}{2} \times 2 = \frac{rce}{2} + rc =rce(\frac{1}{2} +\frac{1}{e})

</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">se</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">c</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">e</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">rse</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">ec</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.7935600000000003em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.10756em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.7935600000000003em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.10756em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.7935600000000003em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.10756em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">rce</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">rc</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord mathnormal">rce</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">e</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></div><p>通过这些计算，我们可以确定原始密集表示和新稀疏表示的总内存占用。</p>
<p>这为我们提供了一个简单的压缩比公式，该公式仅依赖于张量数据类型的位宽。</p>
<div class="math">
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>C</mi><mo>=</mo><mfrac><msub><mi>M</mi><mrow><mi>s</mi><mi>p</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>e</mi></mrow></msub><msub><mi>M</mi><mrow><mi>d</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>e</mi></mrow></msub></mfrac><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>+</mo><mfrac><mn>1</mn><mi>e</mi></mfrac></mrow><annotation encoding="application/x-tex">C = \frac{M_{sparse}}{M_{dense}} =  \frac{1}{2} + \frac{1}{e}

</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.19633em;vertical-align:-0.8360000000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.36033em;"><span style="top:-2.3139999999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">se</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">rse</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">e</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><p>使用此公式，我们发现对于 <code class="docutils literal "><span class="pre">torch.float16</span></code> 或 <code class="docutils literal "><span class="pre">torch.bfloat16</span></code> ，压缩比是 56.25%，对于 <code class="docutils literal "><span class="pre">torch.int8</span></code> ，压缩比是 62.5%。</p>
<section id="constructing-sparse-semi-structured-tensors">
<h3>构建稀疏半结构张量</h3>
<p>您可以通过简单地使用 <code class="docutils literal "><span class="pre">torch.to_sparse_semi_structured</span></code> 函数将稠密张量转换为稀疏半结构张量。</p>
<p>请注意，我们仅支持 CUDA 张量，因为半结构稀疏性的硬件兼容性仅限于 NVIDIA GPU。</p>
<p>支持以下数据类型用于半结构化稀疏性。请注意，每种数据类型都有自己的形状约束和压缩因子。</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 19%">
<col style="width: 56%">
<col style="width: 13%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>PyTorch 数据类型</p></th>
<th class="head"><p>形状约束</p></th>
<th class="head"><p>压缩因子</p></th>
<th class="head"><p>稀疏模式</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal "><span class="pre">torch.float16</span></code></p></td>
<td><p>张量必须是二维的，且(r, c)都必须是 64 的正整数倍</p></td>
<td><p>9/16</p></td>
<td><p>2:4</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal "><span class="pre">torch.bfloat16</span></code></p></td>
<td><p>张量必须是二维的，且(r, c)都必须是 64 的正整数倍</p></td>
<td><p>9/16</p></td>
<td><p>2:4</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal "><span class="pre">torch.int8</span></code></p></td>
<td><p>张量必须是二维的，且(r, c)都必须是 128 的正整数倍</p></td>
<td><p>10/16</p></td>
<td><p>2:4</p></td>
</tr>
</tbody>
</table>
<p>构建半结构化稀疏张量时，首先创建一个符合 2:4（或半结构化）稀疏格式的常规密集张量。为此，我们将一个小的 1x4 条带进行平铺，创建一个 16x16 的密集 float16 张量。之后，我们可以调用 <code class="docutils literal "><span class="pre">to_sparse_semi_structured</span></code> 函数来压缩它以加速推理。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.sparse</span> <span class="kn">import</span> <span class="n">to_sparse_semi_structured</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="go">tensor([[0., 0., 1.,  ..., 0., 1., 1.],</span>
<span class="go">        [0., 0., 1.,  ..., 0., 1., 1.],</span>
<span class="go">        [0., 0., 1.,  ..., 0., 1., 1.],</span>
<span class="go">        ...,</span>
<span class="go">        [0., 0., 1.,  ..., 0., 1., 1.],</span>
<span class="go">        [0., 0., 1.,  ..., 0., 1., 1.],</span>
<span class="go">        [0., 0., 1.,  ..., 0., 1., 1.]], device='cuda:0', dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A_sparse</span> <span class="o">=</span> <span class="n">to_sparse_semi_structured</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="go">SparseSemiStructuredTensor(shape=torch.Size([128, 128]), transposed=False, values=tensor([[1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        ...,</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16), metadata=tensor([[-4370, -4370, -4370,  ..., -4370, -4370, -4370],</span>
<span class="go">        [-4370, -4370, -4370,  ..., -4370, -4370, -4370],</span>
<span class="go">        [-4370, -4370, -4370,  ..., -4370, -4370, -4370],</span>
<span class="go">        ...,</span>
<span class="go">        [-4370, -4370, -4370,  ..., -4370, -4370, -4370],</span>
<span class="go">        [-4370, -4370, -4370,  ..., -4370, -4370, -4370],</span>
<span class="go">        [-4370, -4370, -4370,  ..., -4370, -4370, -4370]], device='cuda:0',</span>
<span class="go">dtype=torch.int16))</span>
</pre></div>
</div>
</section>
<section id="sparse-semi-structured-tensor-operations">
<h3>稀疏半结构化张量操作</h3>
<p>目前，支持以下操作用于半结构化稀疏张量：</p>
<ul class="simple">
<li><p>torch.addmm(bias, dense, sparse.t())</p></li>
<li><p>torch.mm(稠密矩阵, 稀疏矩阵)</p></li>
<li><p>torch.mm(稀疏矩阵, 稠密矩阵)</p></li>
<li><p>aten.linear.default(稠密矩阵, 稀疏矩阵, 偏置)</p></li>
<li><p>aten.t.default(稀疏矩阵)</p></li>
<li><p>aten.t.detach(sparse)</p></li>
</ul>
<p>要使用这些操作，只需传递 <code class="docutils literal "><span class="pre">to_sparse_semi_structured(tensor)</span></code> 的输出，而不是使用 <code class="docutils literal "><span class="pre">tensor</span></code> ，一旦你的张量以半结构化稀疏格式包含 0，如下所示：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a_sparse</span> <span class="o">=</span> <span class="n">to_sparse_semi_structured</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a_sparse</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
<span class="go">True</span>
</pre></div>
</div>
</section>
<section id="accelerating-nn-linear-with-semi-structured-sparsity">
<h3>加速具有半结构化稀疏性的 nn.Linear ¶</h3>
<p>如果权重已经是半结构化稀疏的，你可以用几行代码加速模型中的线性层：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">tile</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">to_sparse_semi_structured</span><span class="p">(</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</section>
</section>
<section id="sparse-coo-tensors">
<span id="sparse-coo-docs"></span><h2>稀疏 COO 张量 ¶</h2>
<p>PyTorch 实现了所谓的坐标格式，或称 COO 格式，作为实现稀疏张量的存储格式之一。在 COO 格式中，指定的元素存储为元素索引和对应值的元组。特别是，</p>
<blockquote>
<div><ul class="simple">
<li><p>指定元素的索引收集在大小为 <code class="docutils literal "><span class="pre">(ndim,</span> <span class="pre">nse)</span></code> 的 <code class="docutils literal "><span class="pre">indices</span></code> 张量中，元素类型为 <code class="docutils literal "><span class="pre">torch.int64</span></code> ，</p></li>
<li><p>对应的值收集在大小为 <code class="docutils literal "><span class="pre">(nse,)</span></code> 的 <code class="docutils literal "><span class="pre">values</span></code> 张量中，元素类型为任意整数或浮点数，</p></li>
</ul>
</div></blockquote>
<p>其中 <code class="docutils literal "><span class="pre">ndim</span></code> 是张量的维度， <code class="docutils literal "><span class="pre">nse</span></code> 是指定的元素数量。</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">稀疏 COO 张量的内存消耗至少为 <code class="docutils literal "><span class="pre">(ndim</span> <span class="pre">*</span>
<span class="pre">8</span> <span class="pre">+</span> <span class="pre">&lt;size</span> <span class="pre">of</span> <span class="pre">element</span> <span class="pre">type</span> <span class="pre">in</span> <span class="pre">bytes&gt;)</span> <span class="pre">*</span> <span class="pre">nse</span></code> 字节（加上存储其他张量数据产生的常数开销）。</font></font></font></p>
<p>稀疏张量的内存消耗至少为 <code class="docutils literal "><span class="pre">product(&lt;tensor</span> <span class="pre">shape&gt;)</span> <span class="pre">*</span> <span class="pre">&lt;size</span> <span class="pre">of</span> <span class="pre">element</span> <span class="pre">type</span> <span class="pre">in</span> <span class="pre">bytes&gt;</span></code> 。</p>
<p>例如，使用 COO 张量布局时，一个包含 100,000 个非零 32 位浮点数的 10,000 x 10,000 张量的内存消耗至少为 <code class="docutils literal "><span class="pre">(2</span> <span class="pre">*</span> <span class="pre">8</span> <span class="pre">+</span> <span class="pre">4)</span> <span class="pre">*</span> <span class="pre">100</span> <span class="pre">000</span> <span class="pre">=</span> <span class="pre">2</span> <span class="pre">000</span> <span class="pre">000</span></code> 字节，而使用默认的步进张量布局时至少为 <code class="docutils literal "><span class="pre">10</span> <span class="pre">000</span> <span class="pre">*</span> <span class="pre">10</span> <span class="pre">000</span> <span class="pre">*</span> <span class="pre">4</span> <span class="pre">=</span> <span class="pre">400</span> <span class="pre">000</span> <span class="pre">000</span></code> 字节。请注意，使用 COO 存储格式可以节省 200 倍的内存。</p>
</div>
<section id="construction">
<h3>构建</h3>
<p>可以通过提供索引张量和值张量以及稀疏张量的大小（当无法从索引和值张量中推断出来时）到一个函数 <code class="xref py py-func docutils literal "><span class="pre">torch.sparse_coo_tensor()</span></code> 来构建一个稀疏的 COO 张量。</p>
<p>假设我们想定义一个稀疏张量，其中包含位置（0，2）处的条目 3，位置（1，0）处的条目 4，以及位置（1，2）处的条目 5。未指定的元素默认具有相同的值，填充值，默认为零。然后我们可以这样写：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="go">         [2, 0, 2]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span>  <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span>
<span class="go">tensor(indices=tensor([[0, 1, 1],</span>
<span class="go">                       [2, 0, 2]]),</span>
<span class="go">       values=tensor([3, 4, 5]),</span>
<span class="go">       size=(2, 3), nnz=3, layout=torch.sparse_coo)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go">tensor([[0, 0, 3],</span>
<span class="go">        [4, 0, 5]])</span>
</pre></div>
</div>
<p>注意，输入 <code class="docutils literal "><span class="pre">i</span></code> 不是一个索引元组的列表。如果你想以这种方式写入你的索引，你应该在传递给稀疏构造函数之前进行转置：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span>  <span class="p">[</span><span class="mi">3</span><span class="p">,</span>      <span class="mi">4</span><span class="p">,</span>      <span class="mi">5</span>    <span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">)),</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Or another equivalent formulation to get s</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]))</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go">tensor([[0, 0, 3],</span>
<span class="go">        [4, 0, 5]])</span>
</pre></div>
</div>
<p>可以仅指定其大小来构造一个空的稀疏 COO 张量：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor(indices=tensor([], size=(2, 0)),</span>
<span class="go">       values=tensor([], size=(0,)),</span>
<span class="go">       size=(2, 3), nnz=0, layout=torch.sparse_coo)</span>
</pre></div>
</div>
</section>
<section id="sparse-hybrid-coo-tensors">
<span id="sparse-hybrid-coo-docs"></span><h3>稀疏混合 COO 张量</h3>
<p>PyTorch 实现了稀疏张量向稀疏张量（连续）张量的扩展，具有标量值的稀疏张量称为混合张量。</p>
<p>PyTorch 混合 COO 张量通过允许 <code class="docutils literal "><span class="pre">values</span></code> 张量成为一个多维张量来扩展稀疏 COO 张量，因此我们有：</p>
<blockquote>
<div><ul class="simple">
<li><p>指定元素的索引被收集在大小为 <code class="docutils literal "><span class="pre">(sparse_dims,</span> <span class="pre">nse)</span></code> 、元素类型为 <code class="docutils literal "><span class="pre">torch.int64</span></code> 的 <code class="docutils literal "><span class="pre">indices</span></code> 张量中，</p></li>
<li><p>对应的（张量）值被收集在大小为 <code class="docutils literal "><span class="pre">(nse,</span> <span class="pre">dense_dims)</span></code> 、元素类型为任意整数或浮点数的 <code class="docutils literal "><span class="pre">values</span></code> 张量中。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>我们使用（M + K）维张量来表示一个 N 维稀疏混合张量，其中 M 和 K 分别是稀疏和密集维度的数量，且满足 M + K == N。</p>
</div>
<p>假设我们想要创建一个（2 + 1）维张量，其中[3, 4]位于位置（0, 2），[5, 6]位于位置（1, 0），[7, 8]位于位置（1, 2）。我们会写成</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="go">         [2, 0, 2]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span>  <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span>
<span class="go">tensor(indices=tensor([[0, 1, 1],</span>
<span class="go">                       [2, 0, 2]]),</span>
<span class="go">       values=tensor([[3, 4],</span>
<span class="go">                      [5, 6],</span>
<span class="go">                      [7, 8]]),</span>
<span class="go">       size=(2, 3, 2), nnz=3, layout=torch.sparse_coo)</span>
</pre></div>
</div>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go">tensor([[[0, 0],</span>
<span class="go">         [0, 0],</span>
<span class="go">         [3, 4]],</span>
<span class="go">        [[5, 6],</span>
<span class="go">         [0, 0],</span>
<span class="go">         [7, 8]]])</span>
</pre></div>
</div>
<p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">一般地，如果 <code class="docutils literal "><span class="pre">s</span></code> 是一个稀疏的 COO 张量， <code class="docutils literal "><span class="pre">M</span> <span class="pre">=</span>
<span class="pre">s.sparse_dim()</span></code> ， <code class="docutils literal "><span class="pre">K</span> <span class="pre">=</span> <span class="pre">s.dense_dim()</span></code> ，那么我们有以下不变量：</font></font></font></p>
<blockquote>
<div><ul class="simple">
<li><p> <code class="docutils literal "><span class="pre">M</span> <span class="pre">+</span> <span class="pre">K</span> <span class="pre">==</span> <span class="pre">len(s.shape)</span> <span class="pre">==</span> <span class="pre">s.ndim</span></code> 的张量维度是稀疏和密集维度数量的总和，</p></li>
<li><p>- 稀疏索引是显式存储的，</p></li>
<li><p>- 混合张量的值是 K 维张量，</p></li>
<li><p>- 值以步长张量的形式存储。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>稠密维度始终跟在稀疏维度之后，即不支持稠密和稀疏维度的混合。</p>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>要确保构建的稀疏张量具有一致的索引、值和大小，可以在创建张量时通过 <code class="docutils literal "><span class="pre">check_invariants=True</span></code> 关键字参数启用不变性检查，或者使用 <code class="xref py py-class docutils literal "><span class="pre">torch.sparse.check_sparse_tensor_invariants</span></code> 上下文管理器实例全局启用。默认情况下，稀疏张量的不变性检查是禁用的。</p>
</div>
</section>
<section id="uncoalesced-sparse-coo-tensors">
<span id="sparse-uncoalesced-coo-docs"></span><h3>未合并的稀疏 COO 张量</h3>
<p>PyTorch 稀疏 COO 张量格式允许存在未合并的稀疏张量，其中索引中可能存在重复的坐标；在这种情况下，解释是，该索引处的值是所有重复值条目的总和。例如，可以为相同的索引 <code class="docutils literal "><span class="pre">1</span></code> 指定多个值，从而得到一个 1-D 未合并的张量：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span>  <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span>
<span class="go">tensor(indices=tensor([[1, 1]]),</span>
<span class="go">       values=tensor(  [3, 4]),</span>
<span class="go">       size=(3,), nnz=2, layout=torch.sparse_coo)</span>
</pre></div>
</div>
<p>而合并过程将通过求和将多值元素累积为单个值：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
<span class="go">tensor(indices=tensor([[1]]),</span>
<span class="go">       values=tensor([7]),</span>
<span class="go">       size=(3,), nnz=1, layout=torch.sparse_coo)</span>
</pre></div>
</div>
<p>通常情况下， <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.coalesce()</span></code> 方法的输出是一个具有以下特性的稀疏张量：</p>
<ul class="simple">
<li><p>指定张量元素的索引是唯一的，</p></li>
<li><p>索引按字典顺序排序，</p></li>
<li><p> <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.is_coalesced()</span></code> 返回 <code class="docutils literal "><span class="pre">True</span></code> 。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>在大多数情况下，你不必关心稀疏张量是否合并，因为大多数操作在稀疏合并或未合并的张量上工作方式相同。</p>
<p>然而，某些操作在未合并的张量上可以更有效地实现，而某些操作在合并的张量上可以更有效地实现。</p>
<p>例如，稀疏 COO 张量的加法是通过简单地连接索引张量和值张量来实现的：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="go">tensor(indices=tensor([[0, 0, 1, 1]]),</span>
<span class="go">       values=tensor([7, 8, 5, 6]),</span>
<span class="go">       size=(2,), nnz=4, layout=torch.sparse_coo)</span>
</pre></div>
</div>
<p>如果你反复执行可能产生重复条目的操作（例如， <code class="xref py py-func docutils literal "><span class="pre">torch.Tensor.add()</span></code> ），你应该偶尔合并你的稀疏张量，以防止它们变得过大。</p>
<p>另一方面，索引的字典序排列对于实现涉及许多元素选择操作（如切片或矩阵乘法）的算法是有利的。</p>
</div>
</section>
<section id="working-with-sparse-coo-tensors">
<h3>处理稀疏 COO 张量</h3>
<p>让我们考虑以下示例：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="go">         [2, 0, 2]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span>  <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<p>如上所述，稀疏 COO 张量是一个 <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> 实例，为了将其与使用其他布局的 Tensor 实例区分开来，可以使用 <code class="xref py py-attr docutils literal "><span class="pre">torch.Tensor.is_sparse</span></code> 或 <code class="xref py py-attr docutils literal "><span class="pre">torch.Tensor.layout</span></code> 属性：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">is_sparse</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span>
<span class="go">True</span>
</pre></div>
</div>
<p>可以使用方法 <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.sparse_dim()</span></code> 和 <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.dense_dim()</span></code> 分别获取稀疏和密集维度数。例如：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">sparse_dim</span><span class="p">(),</span> <span class="n">s</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span>
<span class="go">(2, 1)</span>
</pre></div>
</div>
<p>如果 <code class="docutils literal "><span class="pre">s</span></code> 是一个稀疏的 COO 张量，则其 COO 格式数据可以通过方法 <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.indices()</span></code> 和 <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.values()</span></code> 获取。</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>目前，只有在张量实例合并时才能获取 COO 格式数据：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
<span class="go">RuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first</span>
</pre></div>
</div>
<p>要获取未合并张量的 COO 格式数据，请使用 <code class="xref py py-func docutils literal "><span class="pre">torch.Tensor._values()</span></code> 和 <code class="xref py py-func docutils literal "><span class="pre">torch.Tensor._indices()</span></code> ：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">_indices</span><span class="p">()</span>
<span class="go">tensor([[0, 1, 1],</span>
<span class="go">        [2, 0, 2]])</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>调用 <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor._values()</span></code> 将返回一个分离的张量。要跟踪梯度，必须使用 <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.coalesce().values()</span></code> 。</p>
</div>
</div>
<p>构建一个新的稀疏 COO 张量会产生一个未合并的张量：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">()</span>
<span class="go">False</span>
</pre></div>
</div>
<p>但可以使用 <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.coalesce()</span></code> 方法构建稀疏 COO 张量的合并副本：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s2</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s2</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span>
<span class="go">tensor([[0, 1, 1],</span>
<span class="go">       [2, 0, 2]])</span>
</pre></div>
</div>
<p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">当处理未合并的稀疏 COO 张量时，必须考虑到未合并数据的加性：相同索引的值是求和的项，求和的结果给出了相应张量元素的值。例如，对稀疏未合并张量进行标量乘法可以通过将所有未合并的值乘以标量来实现，因为 <code class="docutils literal "><span class="pre">c</span> <span class="pre">*</span>
<span class="pre">(a</span> <span class="pre">+</span> <span class="pre">b)</span> <span class="pre">==</span> <span class="pre">c</span> <span class="pre">*</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">c</span> <span class="pre">*</span> <span class="pre">b</span></code> 成立。然而，任何非线性操作，例如平方根，不能通过将操作应用于未合并数据来实现，因为在一般情况下 <code class="docutils literal "><span class="pre">sqrt(a</span> <span class="pre">+</span> <span class="pre">b)</span> <span class="pre">==</span> <span class="pre">sqrt(a)</span> <span class="pre">+</span> <span class="pre">sqrt(b)</span></code> 不成立。</font></font></font></p>
<p>稀疏 COO 张量的切片（正步长）仅支持密集维度。索引支持稀疏和密集维度：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">tensor(indices=tensor([[0, 2]]),</span>
<span class="go">       values=tensor([[5, 6],</span>
<span class="go">                      [7, 8]]),</span>
<span class="go">       size=(3, 2), nnz=2, layout=torch.sparse_coo)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="go">tensor(6)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
<span class="go">tensor([6])</span>
</pre></div>
</div>
<p>在 PyTorch 中，稀疏张量的填充值不能显式指定，通常默认为零。然而，存在一些操作可能会以不同的方式解释填充值。例如， <code class="xref py py-func docutils literal "><span class="pre">torch.sparse.softmax()</span></code> 计算 softmax 时假定填充值为负无穷。</p>
</section>
</section>
<section id="sparse-compressed-tensors">
<span id="sparse-compressed-docs"></span><h2>稀疏压缩张量</h2>
<p>稀疏压缩张量代表一类稀疏张量，它们具有一个共同特征，即使用一种编码来压缩特定维度的索引，这种编码使得对稀疏压缩张量的线性代数核进行某些优化成为可能。这种编码基于压缩稀疏行（CSR）格式，PyTorch 稀疏压缩张量扩展了这种格式，支持稀疏张量批处理，允许多维张量值，并将稀疏张量值存储在密集块中。</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>我们使用(B + M + K)维张量来表示一个 N 维稀疏压缩混合张量，其中 B、M 和 K 分别是批处理、稀疏和密集维度的数量，使得 <code class="docutils literal "><span class="pre">B</span> <span class="pre">+</span> <span class="pre">M</span> <span class="pre">+</span> <span class="pre">K</span> <span class="pre">==</span> <span class="pre">N</span></code> 成立。稀疏压缩张量的稀疏维度数量始终为两个， <code class="docutils literal "><span class="pre">M</span> <span class="pre">==</span> <span class="pre">2</span></code> 。</p>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>我们说，如果一个索引张量 <code class="docutils literal "><span class="pre">compressed_indices</span></code> 满足以下不变性，则它使用 CSR 编码：</p>
<ul class="simple">
<li><p> <code class="docutils literal "><span class="pre">compressed_indices</span></code> 是一个连续的步长为 32 或 64 位的整数张量</p></li>
<li><p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "> <code class="docutils literal "><span class="pre">compressed_indices</span></code> 的形状是 <code class="docutils literal "><span class="pre">(*batchsize,</span>
<span class="pre">compressed_dim_size</span> <span class="pre">+</span> <span class="pre">1)</span></code> ，其中 <code class="docutils literal "><span class="pre">compressed_dim_size</span></code> 表示压缩的维度数量（例如行或列）</font></font></font></p></li>
<li><p> <code class="docutils literal "><span class="pre">compressed_indices[...,</span> <span class="pre">0]</span> <span class="pre">==</span> <span class="pre">0</span></code> ，其中 <code class="docutils literal "><span class="pre">...</span></code> 表示批处理索引</p></li>
<li><p> <code class="docutils literal "><span class="pre">compressed_indices[...,</span> <span class="pre">compressed_dim_size]</span> <span class="pre">==</span> <span class="pre">nse</span></code> 表示指定元素的数量</p></li>
<li><p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "> <code class="docutils literal "><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">compressed_indices[...,</span> <span class="pre">i]</span> <span class="pre">-</span> <span class="pre">compressed_indices[...,</span> <span class="pre">i</span> <span class="pre">-</span>
<span class="pre">1]</span> <span class="pre">&lt;=</span> <span class="pre">plain_dim_size</span></code> 对于 <code class="docutils literal "><span class="pre">i=1,</span> <span class="pre">...,</span> <span class="pre">compressed_dim_size</span></code> ，其中 <code class="docutils literal "><span class="pre">plain_dim_size</span></code> 是平面维度（垂直于压缩维度，例如列或行）。</font></font></font></p></li>
</ul>
<p>为了确保构建的稀疏张量具有一致的索引、值和大小，可以在创建张量时通过 <code class="docutils literal "><span class="pre">check_invariants=True</span></code> 关键字参数启用不变性检查，或使用 <code class="xref py py-class docutils literal "><span class="pre">torch.sparse.check_sparse_tensor_invariants</span></code> 上下文管理器实例全局启用。默认情况下，稀疏张量的不变性检查是禁用的。</p>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>将稀疏压缩布局泛化到 N 维张量可能会导致关于指定元素数量的混淆。当一个稀疏压缩张量包含批量维度时，指定元素的数量将对应于每个批次的此类元素数量。当一个稀疏压缩张量具有密集维度时，所考虑的元素现在是一个 K 维数组。对于块稀疏压缩布局，2 维块被视为指定的元素。以一个三维块稀疏张量为例，它有一个长度为 <code class="docutils literal "><span class="pre">b</span></code> 的批量维度和一个形状为 <code class="docutils literal "><span class="pre">p,</span> <span class="pre">q</span></code> 的块形状。如果这个张量有 <code class="docutils literal "><span class="pre">n</span></code> 个指定元素，那么实际上我们有 <code class="docutils literal "><span class="pre">n</span></code> 个块在每个批次中被指定。这个张量将有 <code class="docutils literal "><span class="pre">values</span></code> 个形状为 <code class="docutils literal "><span class="pre">(b,</span> <span class="pre">n,</span> <span class="pre">p,</span> <span class="pre">q)</span></code> 的元素。这种对指定元素数量的解释来自于所有稀疏压缩布局都是基于二维矩阵的压缩。批量维度被视为稀疏矩阵的堆叠，密集维度改变了元素的含义，从简单的标量值变为具有自己维度的数组。</p>
</div>
<section id="sparse-csr-tensor">
<span id="sparse-csr-docs"></span><h3>稀疏 CSR 张量</h3>
<p>CSR 格式相较于 COO 格式的优势在于更有效地利用存储空间，以及使用 MKL 和 MAGMA 后端进行稀疏矩阵-向量乘法等计算操作的速度更快。</p>
<p>在最简单的情况下，一个(0 + 2 + 0)维度的稀疏 CSR 张量由三个 1-D 张量组成： <code class="docutils literal "><span class="pre">crow_indices</span></code> ， <code class="docutils literal "><span class="pre">col_indices</span></code> 和 <code class="docutils literal "><span class="pre">values</span></code> ：</p>
<blockquote>
<div><ul class="simple">
<li><p> <code class="docutils literal "><span class="pre">crow_indices</span></code> 张量包含压缩的行索引。这是一个大小为 <code class="docutils literal "><span class="pre">nrows</span> <span class="pre">+</span> <span class="pre">1</span></code> （行数加 1）的 1-D 张量。 <code class="docutils literal "><span class="pre">crow_indices</span></code> 的最后一个元素是指定元素的数量， <code class="docutils literal "><span class="pre">nse</span></code> 。这个张量根据给定行的起始位置编码 <code class="docutils literal "><span class="pre">values</span></code> 和 <code class="docutils literal "><span class="pre">col_indices</span></code> 中的索引。张量中连续的数字减去前面的数字表示给定行中的元素数量。</p></li>
<li><p> <code class="docutils literal "><span class="pre">col_indices</span></code> 张量包含每个元素的列索引。这是一个大小为 <code class="docutils literal "><span class="pre">nse</span></code> 的 1-D 张量。</p></li>
<li><p> <code class="docutils literal "><span class="pre">values</span></code> 索引张量包含 CSR 张量元素的值。这是一个大小为 <code class="docutils literal "><span class="pre">nse</span></code> 的 1-D 张量。</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>索引张量 <code class="docutils literal "><span class="pre">crow_indices</span></code> 和 <code class="docutils literal "><span class="pre">col_indices</span></code> 的元素类型应为 <code class="docutils literal "><span class="pre">torch.int64</span></code> （默认）或 <code class="docutils literal "><span class="pre">torch.int32</span></code> 。如果您想使用 MKL 启用的矩阵运算，请使用 <code class="docutils literal "><span class="pre">torch.int32</span></code> 。这是由于 PyTorch 默认链接为 MKL LP64，它使用 32 位整数索引。</p>
</div>
<p>在一般情况下，(B + 2 + K) 维的稀疏 CSR 张量由两个 (B + 1) 维的索引张量 <code class="docutils literal "><span class="pre">crow_indices</span></code> 和 <code class="docutils literal "><span class="pre">col_indices</span></code> 以及 (1 + K) 维的 <code class="docutils literal "><span class="pre">values</span></code> 张量组成，使得</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal "><span class="pre">crow_indices.shape</span> <span class="pre">==</span> <span class="pre">(*batchsize,</span> <span class="pre">nrows</span> <span class="pre">+</span> <span class="pre">1)</span></code></p></li>
<li><p><code class="docutils literal "><span class="pre">col_indices.shape</span> <span class="pre">==</span> <span class="pre">(*batchsize,</span> <span class="pre">nse)</span></code></p></li>
<li><p><code class="docutils literal "><span class="pre">values.shape</span> <span class="pre">==</span> <span class="pre">(nse,</span> <span class="pre">*densesize)</span></code></p></li>
</ul>
</div></blockquote>
<p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">而稀疏 CSR 张量的形状为 <code class="docutils literal "><span class="pre">(*batchsize,</span> <span class="pre">nrows,</span>
<span class="pre">ncols,</span> <span class="pre">*densesize)</span></code> ，其中 <code class="docutils literal "><span class="pre">len(batchsize)</span> <span class="pre">==</span> <span class="pre">B</span></code> 和 <code class="docutils literal "><span class="pre">len(densesize)</span> <span class="pre">==</span> <span class="pre">K</span></code> 。</font></font></font></p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>稀疏 CSR 张量批次之间是相互依赖的：所有批次中指定的元素数量必须相同。这种有些人为的限制使得不同 CSR 批次的索引存储变得高效。</p>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">可以使用 <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.sparse_dim()</span></code> 和 <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.dense_dim()</span></code> 方法获取稀疏和稠密维度的数量。批次维度可以从张量形状中计算得出： <code class="docutils literal "><span class="pre">batchsize</span> <span class="pre">=</span> <span class="pre">tensor.shape[:-tensor.sparse_dim()</span> <span class="pre">-</span>
<span class="pre">tensor.dense_dim()]</span></code> 。</font></font></font></p>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">稀疏 CSR 张量的内存消耗至少为 <code class="docutils literal "><span class="pre">(nrows</span> <span class="pre">*</span> <span class="pre">8</span> <span class="pre">+</span> <span class="pre">(8</span> <span class="pre">+</span> <span class="pre">&lt;size</span> <span class="pre">of</span> <span class="pre">element</span> <span class="pre">type</span> <span class="pre">in</span> <span class="pre">bytes&gt;</span> <span class="pre">*</span>
<span class="pre">prod(densesize))</span> <span class="pre">*</span> <span class="pre">nse)</span> <span class="pre">*</span> <span class="pre">prod(batchsize)</span></code> 字节（加上存储其他张量数据产生的常数开销）。</font></font></font></p>
<p>以稀疏 COO 格式介绍中提到的相同示例数据为例，使用 CSR 张量布局时，一个 10 000 x 10 000 的张量，包含 100 000 个非零 32 位浮点数，其内存消耗至少为 <code class="docutils literal "><span class="pre">(10000</span> <span class="pre">*</span> <span class="pre">8</span> <span class="pre">+</span> <span class="pre">(8</span> <span class="pre">+</span> <span class="pre">4</span> <span class="pre">*</span> <span class="pre">1)</span> <span class="pre">*</span> <span class="pre">100</span> <span class="pre">000)</span> <span class="pre">*</span> <span class="pre">1</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">280</span> <span class="pre">000</span></code> 字节。注意，与使用 COO 和带步长格式相比，使用 CSR 存储格式可以节省 1.6 和 310 倍的空间。</p>
</div>
<section id="construction-of-csr-tensors">
<h4>CSR 张量的构建</h4>
<p>可以通过使用 <code class="xref py py-func docutils literal "><span class="pre">torch.sparse_csr_tensor()</span></code> 函数直接构建稀疏 CSR 张量。用户必须分别提供行和列索引以及值张量，其中行索引必须使用 CSR 压缩编码指定。 <code class="docutils literal "><span class="pre">size</span></code> 参数是可选的，如果不存在，将根据 <code class="docutils literal "><span class="pre">crow_indices</span></code> 和 <code class="docutils literal "><span class="pre">col_indices</span></code> 推断。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crow_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">col_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr_tensor</span><span class="p">(</span><span class="n">crow_indices</span><span class="p">,</span> <span class="n">col_indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span>
<span class="go">tensor(crow_indices=tensor([0, 2, 4]),</span>
<span class="go">       col_indices=tensor([0, 1, 0, 1]),</span>
<span class="go">       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,</span>
<span class="go">       dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go">tensor([[1., 2.],</span>
<span class="go">        [3., 4.]], dtype=torch.float64)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>推断出的 <code class="docutils literal "><span class="pre">size</span></code> 稀疏维度值是从 <code class="docutils literal "><span class="pre">crow_indices</span></code> 的大小和 <code class="docutils literal "><span class="pre">col_indices</span></code> 中的最大索引值计算得出的。如果需要列数大于推断出的 <code class="docutils literal "><span class="pre">size</span></code> ，则必须显式指定 <code class="docutils literal "><span class="pre">size</span></code> 参数。</p>
</div>
<p>从具有步进或稀疏 COO 张量的 2-D 稀疏 CSR 张量构建的最简单方法是使用 <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.to_sparse_csr()</span></code> 方法。张量中的任何零都将被解释为稀疏张量中的缺失值：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to_sparse_csr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span>
<span class="go">tensor(crow_indices=tensor([0, 1, 3, 3]),</span>
<span class="go">      col_indices=tensor([2, 0, 1]),</span>
<span class="go">      values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64)</span>
</pre></div>
</div>
</section>
<section id="csr-tensor-operations">
<h4>CSR 张量运算 ¶</h4>
<p>稀疏矩阵-向量乘法可以使用 <code class="xref py py-meth docutils literal "><span class="pre">tensor.matmul()</span></code> 方法执行。目前这是 CSR 张量上唯一支持的数学运算。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
<span class="go">tensor([[0.9078],</span>
<span class="go">        [1.3180],</span>
<span class="go">        [0.0000]], dtype=torch.float64)</span>
</pre></div>
</div>
</section>
</section>
<section id="sparse-csc-tensor">
<span id="sparse-csc-docs"></span><h3>稀疏 CSC 张量 ¶</h3>
<p>稀疏 CSC（压缩稀疏列）张量格式实现了用于存储二维张量的 CSC 格式，并扩展支持稀疏 CSC 张量批量和多维张量值。</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>稀疏 CSC 张量在交换稀疏维度时实际上是稀疏 CSR 张量的转置。</p>
</div>
<p>类似于稀疏 CSR 张量，稀疏 CSC 张量由三个张量组成： <code class="docutils literal "><span class="pre">ccol_indices</span></code> ， <code class="docutils literal "><span class="pre">row_indices</span></code> 和 <code class="docutils literal "><span class="pre">values</span></code> ：</p>
<blockquote>
<div><ul class="simple">
<li><p> <code class="docutils literal "><span class="pre">ccol_indices</span></code> 张量包含压缩的列索引。这是一个形状为 <code class="docutils literal "><span class="pre">(*batchsize,</span> <span class="pre">ncols</span> <span class="pre">+</span> <span class="pre">1)</span></code> 的(B + 1)-D 张量。最后一个元素是指定元素的数量， <code class="docutils literal "><span class="pre">nse</span></code> 。这个张量根据给定的列起始位置编码索引 <code class="docutils literal "><span class="pre">values</span></code> 和 <code class="docutils literal "><span class="pre">row_indices</span></code> 。张量中连续的数字减去前面的数字表示给定列中的元素数量。</p></li>
<li><p> <code class="docutils literal "><span class="pre">row_indices</span></code> 张量包含每个元素的行索引。这是一个形状为 <code class="docutils literal "><span class="pre">(*batchsize,</span> <span class="pre">nse)</span></code> 的(B + 1)-D 张量。</p></li>
<li><p> <code class="docutils literal "><span class="pre">values</span></code> 索引张量包含 CSC 张量元素的值。这是一个形状为 <code class="docutils literal "><span class="pre">(nse,</span> <span class="pre">*densesize)</span></code> 的(1 + K)-D 张量。</p></li>
</ul>
</div></blockquote>
<section id="construction-of-csc-tensors">
<h4>CSC 张量的构建</h4>
<p>可以直接使用 <code class="xref py py-func docutils literal "><span class="pre">torch.sparse_csc_tensor()</span></code> 函数构建稀疏的 CSC 张量。用户必须分别提供行索引和列索引以及值张量，其中列索引必须使用 CSR 压缩编码指定。 <code class="docutils literal "><span class="pre">size</span></code> 参数是可选的，如果不存在，将根据 <code class="docutils literal "><span class="pre">row_indices</span></code> 和 <code class="docutils literal "><span class="pre">ccol_indices</span></code> 张量推断出来。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ccol_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_csc_tensor</span><span class="p">(</span><span class="n">ccol_indices</span><span class="p">,</span> <span class="n">row_indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csc</span>
<span class="go">tensor(ccol_indices=tensor([0, 2, 4]),</span>
<span class="go">       row_indices=tensor([0, 1, 0, 1]),</span>
<span class="go">       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,</span>
<span class="go">       dtype=torch.float64, layout=torch.sparse_csc)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csc</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go">tensor([[1., 3.],</span>
<span class="go">        [2., 4.]], dtype=torch.float64)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>稀疏 CSC 张量构造函数中，压缩列索引参数位于行索引参数之前。</p>
</div>
<p>(0 + 2 + 0)维稀疏 CSC 张量可以通过使用 <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.to_sparse_csc()</span></code> 方法从任何二维张量构建。在(strided)张量中的任何零将被解释为稀疏张量中的缺失值：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to_sparse_csc</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span>
<span class="go">tensor(ccol_indices=tensor([0, 1, 2, 3, 3]),</span>
<span class="go">       row_indices=tensor([1, 1, 0]),</span>
<span class="go">       values=tensor([1., 2., 1.]), size=(3, 4), nnz=3, dtype=torch.float64,</span>
<span class="go">       layout=torch.sparse_csc)</span>
</pre></div>
</div>
</section>
</section>
<section id="sparse-bsr-tensor">
<span id="sparse-bsr-docs"></span><h3>稀疏 BSR 张量</h3>
<p>稀疏 BSR（块压缩稀疏行）张量格式实现了二维张量的 BSR 存储格式，并扩展支持稀疏 BSR 张量批量和多维张量块值。</p>
<p>稀疏 BSR 张量由三个张量组成： <code class="docutils literal "><span class="pre">crow_indices</span></code> ， <code class="docutils literal "><span class="pre">col_indices</span></code> 和 <code class="docutils literal "><span class="pre">values</span></code> ：</p>
<blockquote>
<div><ul class="simple">
<li><p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "> <code class="docutils literal "><span class="pre">crow_indices</span></code> 索引张量由压缩的行索引组成。这是一个形状为 <code class="docutils literal "><span class="pre">(*batchsize,</span>
<span class="pre">nrowblocks</span> <span class="pre">+</span> <span class="pre">1)</span></code> 的 (B + 1)-D 张量。最后一个元素是指定的块数量， <code class="docutils literal "><span class="pre">nse</span></code> 。这个张量根据给定的列块开始位置编码索引 <code class="docutils literal "><span class="pre">values</span></code> 和 <code class="docutils literal "><span class="pre">col_indices</span></code> 。张量中连续的数字减去前面的数字表示给定行中的块数量。</font></font></font></p></li>
<li><p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "> <code class="docutils literal "><span class="pre">col_indices</span></code> 索引张量包含每个元素的列块索引。这是一个形状为 <code class="docutils literal "><span class="pre">(*batchsize,</span>
<span class="pre">nse)</span></code> 的 (B + 1)-D 张量。</font></font></font></p></li>
<li><p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "> <code class="docutils literal "><span class="pre">values</span></code> 索引张量包含稀疏 BSR 张量元素的值，这些值收集到二维块中。这是一个形状为 <code class="docutils literal "><span class="pre">(nse,</span> <span class="pre">nrowblocks,</span> <span class="pre">ncolblocks,</span>
<span class="pre">*densesize)</span></code> 的 (1 + 2 + K)-D 张量。</font></font></font></p></li>
</ul>
</div></blockquote>
<section id="construction-of-bsr-tensors">
<h4>BSR 张量的构建</h4>
<p>稀疏 BSR 张量可以直接通过使用 <code class="xref py py-func docutils literal "><span class="pre">torch.sparse_bsr_tensor()</span></code> 函数来构建。用户必须分别提供行和列块索引以及值张量，其中行块索引必须使用 CSR 压缩编码指定。 <code class="docutils literal "><span class="pre">size</span></code> 参数是可选的，如果不存在，将根据 <code class="docutils literal "><span class="pre">crow_indices</span></code> 和 <code class="docutils literal "><span class="pre">col_indices</span></code> 张量推断出来。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crow_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">col_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]],</span>
<span class="gp">... </span>                       <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">]],</span>
<span class="gp">... </span>                       <span class="p">[[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span> <span class="p">[</span><span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">20</span><span class="p">]],</span>
<span class="gp">... </span>                       <span class="p">[[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">],</span> <span class="p">[</span><span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bsr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsr_tensor</span><span class="p">(</span><span class="n">crow_indices</span><span class="p">,</span> <span class="n">col_indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bsr</span>
<span class="go">tensor(crow_indices=tensor([0, 2, 4]),</span>
<span class="go">       col_indices=tensor([0, 1, 0, 1]),</span>
<span class="go">       values=tensor([[[ 0.,  1.,  2.],</span>
<span class="go">                       [ 6.,  7.,  8.]],</span>
<span class="go">                      [[ 3.,  4.,  5.],</span>
<span class="go">                       [ 9., 10., 11.]],</span>
<span class="go">                      [[12., 13., 14.],</span>
<span class="go">                       [18., 19., 20.]],</span>
<span class="go">                      [[15., 16., 17.],</span>
<span class="go">                       [21., 22., 23.]]]),</span>
<span class="go">       size=(4, 6), nnz=4, dtype=torch.float64, layout=torch.sparse_bsr)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bsr</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go">tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],</span>
<span class="go">        [ 6.,  7.,  8.,  9., 10., 11.],</span>
<span class="go">        [12., 13., 14., 15., 16., 17.],</span>
<span class="go">        [18., 19., 20., 21., 22., 23.]], dtype=torch.float64)</span>
</pre></div>
</div>
<p>(0 + 2 + 0)维稀疏 BSR 张量可以通过使用 <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.to_sparse_bsr()</span></code> 方法从任何二维张量构建，该方法还要求指定值块大小：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="gp">... </span>                      <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span>
<span class="gp">... </span>                      <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">],</span>
<span class="gp">... </span>                      <span class="p">[</span><span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bsr</span> <span class="o">=</span> <span class="n">dense</span><span class="o">.</span><span class="n">to_sparse_bsr</span><span class="p">(</span><span class="n">blocksize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bsr</span>
<span class="go">tensor(crow_indices=tensor([0, 2, 4]),</span>
<span class="go">       col_indices=tensor([0, 1, 0, 1]),</span>
<span class="go">       values=tensor([[[ 0,  1,  2],</span>
<span class="go">                       [ 6,  7,  8]],</span>
<span class="go">                      [[ 3,  4,  5],</span>
<span class="go">                       [ 9, 10, 11]],</span>
<span class="go">                      [[12, 13, 14],</span>
<span class="go">                       [18, 19, 20]],</span>
<span class="go">                      [[15, 16, 17],</span>
<span class="go">                       [21, 22, 23]]]), size=(4, 6), nnz=4,</span>
<span class="go">       layout=torch.sparse_bsr)</span>
</pre></div>
</div>
</section>
</section>
<section id="sparse-bsc-tensor">
<span id="sparse-bsc-docs"></span><h3>稀疏 BSC 张量</h3>
<p>稀疏 BSC（块压缩稀疏列）张量格式实现了 BSC 格式，用于存储二维张量，并扩展支持稀疏 BSC 张量批次和多维张量值块。</p>
<p>稀疏 BSC 张量由三个张量组成： <code class="docutils literal "><span class="pre">ccol_indices</span></code> ， <code class="docutils literal "><span class="pre">row_indices</span></code> 和 <code class="docutils literal "><span class="pre">values</span></code> ：</p>
<blockquote>
<div><ul class="simple">
<li><p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "> <code class="docutils literal "><span class="pre">ccol_indices</span></code> 张量包含压缩的列索引。这是一个形状为 <code class="docutils literal "><span class="pre">(*batchsize,</span>
<span class="pre">ncolblocks</span> <span class="pre">+</span> <span class="pre">1)</span></code> 的 (B + 1)-D 张量。最后一个元素是指定块的数目， <code class="docutils literal "><span class="pre">nse</span></code> 。这个张量根据给定的行块开始位置编码索引 <code class="docutils literal "><span class="pre">values</span></code> 和 <code class="docutils literal "><span class="pre">row_indices</span></code> 。张量中连续的数字减去前面的数字表示给定列中的块数。</font></font></font></p></li>
<li><p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "> <code class="docutils literal "><span class="pre">row_indices</span></code> 张量包含每个元素的行块索引。这是一个形状为 <code class="docutils literal "><span class="pre">(*batchsize,</span>
<span class="pre">nse)</span></code> 的 (B + 1)-D 张量。</font></font></font></p></li>
<li><p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  "> <code class="docutils literal "><span class="pre">values</span></code> 张量包含稀疏 BSC 张量元素值，收集到二维块中。这是一个形状为 <code class="docutils literal "><span class="pre">(nse,</span> <span class="pre">nrowblocks,</span> <span class="pre">ncolblocks,</span>
<span class="pre">*densesize)</span></code> 的 (1 + 2 + K)-D 张量。</font></font></font></p></li>
</ul>
</div></blockquote>
<section id="construction-of-bsc-tensors">
<h4>BSC 张量的构建</h4>
<p>稀疏 BSC 张量可以通过使用 <code class="xref py py-func docutils literal "><span class="pre">torch.sparse_bsc_tensor()</span></code> 函数直接构建。用户必须分别提供行和列块索引张量以及值张量，其中列块索引必须使用 CSR 压缩编码指定。 <code class="docutils literal "><span class="pre">size</span></code> 参数是可选的，如果不存在，将根据 <code class="docutils literal "><span class="pre">ccol_indices</span></code> 和 <code class="docutils literal "><span class="pre">row_indices</span></code> 张量推断出来。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ccol_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]],</span>
<span class="gp">... </span>                       <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">]],</span>
<span class="gp">... </span>                       <span class="p">[[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span> <span class="p">[</span><span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">20</span><span class="p">]],</span>
<span class="gp">... </span>                       <span class="p">[[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">],</span> <span class="p">[</span><span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bsc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_bsc_tensor</span><span class="p">(</span><span class="n">ccol_indices</span><span class="p">,</span> <span class="n">row_indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bsc</span>
<span class="go">tensor(ccol_indices=tensor([0, 2, 4]),</span>
<span class="go">       row_indices=tensor([0, 1, 0, 1]),</span>
<span class="go">       values=tensor([[[ 0.,  1.,  2.],</span>
<span class="go">                       [ 6.,  7.,  8.]],</span>
<span class="go">                      [[ 3.,  4.,  5.],</span>
<span class="go">                       [ 9., 10., 11.]],</span>
<span class="go">                      [[12., 13., 14.],</span>
<span class="go">                       [18., 19., 20.]],</span>
<span class="go">                      [[15., 16., 17.],</span>
<span class="go">                       [21., 22., 23.]]]), size=(4, 6), nnz=4,</span>
<span class="go">       dtype=torch.float64, layout=torch.sparse_bsc)</span>
</pre></div>
</div>
</section>
</section>
<section id="tools-for-working-with-sparse-compressed-tensors">
<h3>处理稀疏压缩张量的工具</h3>
<p>所有稀疏压缩张量（CSR、CSC、BSR 和 BSC 张量）在概念上非常相似，因为它们的索引数据分为两部分：所谓的压缩索引，使用 CSR 编码，以及所谓的普通索引，与压缩索引正交。这使得这些张量上的各种工具可以共享相同的实现，这些实现由张量布局参数化。</p>
<section id="construction-of-sparse-compressed-tensors">
<h4>稀疏压缩张量的构建</h4>
<p>稀疏 CSR、CSC、BSR 和 CSC 张量可以通过使用与上述讨论的构造函数 <code class="xref py py-func docutils literal "><span class="pre">torch.sparse_csr_tensor()</span></code> 、 <code class="xref py py-func docutils literal "><span class="pre">torch.sparse_csc_tensor()</span></code> 、 <code class="xref py py-func docutils literal "><span class="pre">torch.sparse_bsr_tensor()</span></code> 和 <code class="xref py py-func docutils literal "><span class="pre">torch.sparse_bsc_tensor()</span></code> 具有相同接口的 <code class="xref py py-func docutils literal "><span class="pre">torch.sparse_compressed_tensor()</span></code> 函数构建，但需要额外的 <code class="docutils literal "><span class="pre">layout</span></code> 参数。以下示例说明了使用相同输入数据构建 CSR 和 CSC 张量的方法，通过指定相应的布局参数给 <code class="xref py py-func docutils literal "><span class="pre">torch.sparse_compressed_tensor()</span></code> 函数：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">compressed_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plain_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_compressed_tensor</span><span class="p">(</span><span class="n">compressed_indices</span><span class="p">,</span> <span class="n">plain_indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_csr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csr</span>
<span class="go">tensor(crow_indices=tensor([0, 2, 4]),</span>
<span class="go">       col_indices=tensor([0, 1, 0, 1]),</span>
<span class="go">       values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4,</span>
<span class="go">       layout=torch.sparse_csr)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_compressed_tensor</span><span class="p">(</span><span class="n">compressed_indices</span><span class="p">,</span> <span class="n">plain_indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_csc</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csc</span>
<span class="go">tensor(ccol_indices=tensor([0, 2, 4]),</span>
<span class="go">       row_indices=tensor([0, 1, 0, 1]),</span>
<span class="go">       values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4,</span>
<span class="go">       layout=torch.sparse_csc)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">csr</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span> <span class="o">==</span> <span class="n">csc</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="go">tensor(True)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="supported-operations">
<span id="sparse-ops-docs"></span><h2>支持的操作</h2>
<section id="linear-algebra-operations">
<h3>线性代数运算</h3>
<p>以下表格总结了支持稀疏矩阵的线性代数操作，其中操作数的布局可能不同。其中 <code class="docutils literal "><span class="pre">T[layout]</span></code> 表示具有给定布局的张量，同样地， <code class="docutils literal "><span class="pre">M[layout]</span></code> 表示矩阵（2-D PyTorch 张量）， <code class="docutils literal "><span class="pre">V[layout]</span></code> 表示向量（1-D PyTorch 张量）。此外， <code class="docutils literal "><span class="pre">f</span></code> 表示标量（浮点数或 0-D PyTorch 张量）， <code class="docutils literal "><span class="pre">*</span></code> 是逐元素乘法， <code class="docutils literal "><span class="pre">@</span></code> 是矩阵乘法。</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%">
<col style="width: 6%">
<col style="width: 71%">
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>PyTorch 操作</p></th>
<th class="head"><p>稀疏梯度？</p></th>
<th class="head"><p>布局签名</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.mv.html#torch.mv" title="torch.mv"><code class="xref py py-func docutils literal "><span class="pre">torch.mv()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">M[sparse_coo]</span> <span class="pre">@</span> <span class="pre">V[strided]</span> <span class="pre">-&gt;</span> <span class="pre">V[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.mv.html#torch.mv" title="torch.mv"><code class="xref py py-func docutils literal "><span class="pre">torch.mv()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">M[sparse_csr]</span> <span class="pre">@</span> <span class="pre">V[strided]</span> <span class="pre">-&gt;</span> <span class="pre">V[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.matmul.html#torch.matmul" title="torch.matmul"><code class="xref py py-func docutils literal "><span class="pre">torch.matmul()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">M[sparse_coo]</span> <span class="pre">@</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.matmul.html#torch.matmul" title="torch.matmul"><code class="xref py py-func docutils literal "><span class="pre">torch.matmul()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">M[sparse_csr]</span> <span class="pre">@</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.matmul.html#torch.matmul" title="torch.matmul"><code class="xref py py-func docutils literal "><span class="pre">torch.matmul()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">M[SparseSemiStructured]</span> <span class="pre">@</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.matmul.html#torch.matmul" title="torch.matmul"><code class="xref py py-func docutils literal "><span class="pre">torch.matmul()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">M[strided]</span> <span class="pre">@</span> <span class="pre">M[SparseSemiStructured]</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.mm.html#torch.mm" title="torch.mm"><code class="xref py py-func docutils literal "><span class="pre">torch.mm()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">M[sparse_coo]</span> <span class="pre">@</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.mm.html#torch.mm" title="torch.mm"><code class="xref py py-func docutils literal "><span class="pre">torch.mm()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">M[SparseSemiStructured]</span> <span class="pre">@</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.mm.html#torch.mm" title="torch.mm"><code class="xref py py-func docutils literal "><span class="pre">torch.mm()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">M[strided]</span> <span class="pre">@</span> <span class="pre">M[SparseSemiStructured]</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.sparse.mm.html#torch.sparse.mm" title="torch.sparse.mm"><code class="xref py py-func docutils literal "><span class="pre">torch.sparse.mm()</span></code></a></p></td>
<td><p>是的</p></td>
<td><p><code class="docutils literal "><span class="pre">M[sparse_coo]</span> <span class="pre">@</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.smm.html#torch.smm" title="torch.smm"><code class="xref py py-func docutils literal "><span class="pre">torch.smm()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">M[sparse_coo]</span> <span class="pre">@</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[sparse_coo]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.hspmm.html#torch.hspmm" title="torch.hspmm"><code class="xref py py-func docutils literal "><span class="pre">torch.hspmm()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">M[sparse_coo]</span> <span class="pre">@</span> <span class="pre">M[strided]</span> <span class="pre">-&gt;</span> <span class="pre">M[hybrid</span> <span class="pre">sparse_coo]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.bmm.html#torch.bmm" title="torch.bmm"><code class="xref py py-func docutils literal "><span class="pre">torch.bmm()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">T[sparse_coo]</span> <span class="pre">@</span> <span class="pre">T[strided]</span> <span class="pre">-&gt;</span> <span class="pre">T[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.addmm.html#torch.addmm" title="torch.addmm"><code class="xref py py-func docutils literal "><span class="pre">torch.addmm()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">f</span> <span class="pre">*</span> <span class="pre">M[strided]</span> <span class="pre">+</span> <span class="pre">f</span> <span class="pre">*</span> <span class="pre">(M[sparse_coo]</span> <span class="pre">@</span> <span class="pre">M[strided])</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.addmm.html#torch.addmm" title="torch.addmm"><code class="xref py py-func docutils literal "><span class="pre">torch.addmm()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">f</span> <span class="pre">*</span> <span class="pre">M[strided]</span> <span class="pre">+</span> <span class="pre">f</span> <span class="pre">*</span> <span class="pre">(M[SparseSemiStructured]</span> <span class="pre">@</span> <span class="pre">M[strided])</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.addmm.html#torch.addmm" title="torch.addmm"><code class="xref py py-func docutils literal "><span class="pre">torch.addmm()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">f</span> <span class="pre">*</span> <span class="pre">M[strided]</span> <span class="pre">+</span> <span class="pre">f</span> <span class="pre">*</span> <span class="pre">(M[strided]</span> <span class="pre">@</span> <span class="pre">M[SparseSemiStructured])</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.sparse.addmm.html#torch.sparse.addmm" title="torch.sparse.addmm"><code class="xref py py-func docutils literal "><span class="pre">torch.sparse.addmm()</span></code></a></p></td>
<td><p>是的</p></td>
<td><p><code class="docutils literal "><span class="pre">f</span> <span class="pre">*</span> <span class="pre">M[strided]</span> <span class="pre">+</span> <span class="pre">f</span> <span class="pre">*</span> <span class="pre">(M[sparse_coo]</span> <span class="pre">@</span> <span class="pre">M[strided])</span> <span class="pre">-&gt;</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.sparse.spsolve.html#torch.sparse.spsolve" title="torch.sparse.spsolve"><code class="xref py py-func docutils literal "><span class="pre">torch.sparse.spsolve()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">SOLVE(M[sparse_csr],</span> <span class="pre">V[strided])</span> <span class="pre">-&gt;</span> <span class="pre">V[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.sspaddmm.html#torch.sspaddmm" title="torch.sspaddmm"><code class="xref py py-func docutils literal "><span class="pre">torch.sspaddmm()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">f</span> <span class="pre">*</span> <span class="pre">M[sparse_coo]</span> <span class="pre">+</span> <span class="pre">f</span> <span class="pre">*</span> <span class="pre">(M[sparse_coo]</span> <span class="pre">@</span> <span class="pre">M[strided])</span> <span class="pre">-&gt;</span> <span class="pre">M[sparse_coo]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.lobpcg.html#torch.lobpcg" title="torch.lobpcg"><code class="xref py py-func docutils literal "><span class="pre">torch.lobpcg()</span></code></a></p></td>
<td><p>否</p></td>
<td><p><code class="docutils literal "><span class="pre">GENEIG(M[sparse_coo])</span> <span class="pre">-&gt;</span> <span class="pre">M[strided],</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.pca_lowrank.html#torch.pca_lowrank" title="torch.pca_lowrank"><code class="xref py py-func docutils literal "><span class="pre">torch.pca_lowrank()</span></code></a></p></td>
<td><p>是的</p></td>
<td><p><code class="docutils literal "><span class="pre">PCA(M[sparse_coo])</span> <span class="pre">-&gt;</span> <span class="pre">M[strided],</span> <span class="pre">M[strided],</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.svd_lowrank.html#torch.svd_lowrank" title="torch.svd_lowrank"><code class="xref py py-func docutils literal "><span class="pre">torch.svd_lowrank()</span></code></a></p></td>
<td><p>是的</p></td>
<td><p><code class="docutils literal "><span class="pre">SVD(M[sparse_coo])</span> <span class="pre">-&gt;</span> <span class="pre">M[strided],</span> <span class="pre">M[strided],</span> <span class="pre">M[strided]</span></code></p></td>
</tr>
</tbody>
</table>
<p>“稀疏梯度？”列表示 PyTorch 操作是否支持对稀疏矩阵参数的逆向传播。除了 <code class="xref py py-func docutils literal "><span class="pre">torch.smm()</span></code> 之外，所有 PyTorch 操作都支持对带偏移的矩阵参数的逆向传播。</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">目前，PyTorch 不支持使用布局签名 <code class="docutils literal "><span class="pre">M[strided]</span> <span class="pre">@</span> <span class="pre">M[sparse_coo]</span></code> 进行矩阵乘法。但是，应用程序仍然可以使用矩阵关系 <code class="docutils literal "><span class="pre">D</span> <span class="pre">@</span>
<span class="pre">S</span> <span class="pre">==</span> <span class="pre">(S.t()</span> <span class="pre">@</span> <span class="pre">D.t()).t()</span></code> 来计算。</font></font></font></p>
</div>
</section>
<section id="tensor-methods-and-sparse">
<h3>张量方法和稀疏</h3>
<p>以下张量方法与稀疏张量相关：</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse" title="torch.Tensor.is_sparse"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_sparse</span></code></a></p></td>
<td><p>如果张量使用稀疏 COO 存储布局，则为 <code class="docutils literal "><span class="pre">True</span></code> ，否则为 <code class="docutils literal "><span class="pre">False</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_sparse_csr.html#torch.Tensor.is_sparse_csr" title="torch.Tensor.is_sparse_csr"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_sparse_csr</span></code></a></p></td>
<td><p>如果张量使用稀疏 CSR 存储布局，则为 <code class="docutils literal "><span class="pre">True</span></code> ，否则为 <code class="docutils literal "><span class="pre">False</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim" title="torch.Tensor.dense_dim"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.dense_dim</span></code></a></p></td>
<td><p>返回稀疏张量中的密集维度数量 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim" title="torch.Tensor.sparse_dim"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sparse_dim</span></code></a></p></td>
<td><p>返回稀疏张量中的稀疏维度数量 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask" title="torch.Tensor.sparse_mask"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sparse_mask</span></code></a></p></td>
<td><p>返回一个新的稀疏张量，其值来自步长张量 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> ，并通过稀疏张量 <code class="xref py py-attr docutils literal "><span class="pre">mask</span></code> 的索引进行过滤。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse" title="torch.Tensor.to_sparse"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_sparse</span></code></a></p></td>
<td><p>返回张量的稀疏副本。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse_coo.html#torch.Tensor.to_sparse_coo" title="torch.Tensor.to_sparse_coo"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_sparse_coo</span></code></a></p></td>
<td><p>将张量转换为坐标格式。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr" title="torch.Tensor.to_sparse_csr"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_sparse_csr</span></code></a></p></td>
<td><p>将张量转换为压缩行存储格式（CSR）。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc" title="torch.Tensor.to_sparse_csc"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_sparse_csc</span></code></a></p></td>
<td><p>将张量转换为压缩列存储（CSC）格式。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr" title="torch.Tensor.to_sparse_bsr"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_sparse_bsr</span></code></a></p></td>
<td><p>将张量转换为给定块大小的块稀疏行（BSR）存储格式。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.to_sparse_bsc.html#torch.Tensor.to_sparse_bsc" title="torch.Tensor.to_sparse_bsc"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_sparse_bsc</span></code></a></p></td>
<td><p>将张量转换为给定块大小的块稀疏列（BSC）存储格式。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense" title="torch.Tensor.to_dense"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.to_dense</span></code></a></p></td>
<td><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 不是带偏移量的张量，则创建 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 的带偏移量副本，否则返回 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.values.html#torch.Tensor.values" title="torch.Tensor.values"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.values</span></code></a></p></td>
<td><p>返回稀疏 COO 张量的值张量。</p></td>
</tr>
</tbody>
</table>
<p>以下 Tensor 方法仅适用于稀疏 COO 张量：</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.coalesce.html#torch.Tensor.coalesce" title="torch.Tensor.coalesce"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.coalesce</span></code></a></p></td>
<td><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 是一个未合并的张量，则返回 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 的合并副本。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.sparse_resize_.html#torch.Tensor.sparse_resize_" title="torch.Tensor.sparse_resize_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sparse_resize_</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 稀疏张量调整到所需的大小和稀疏与密集维度的数量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.sparse_resize_and_clear_.html#torch.Tensor.sparse_resize_and_clear_" title="torch.Tensor.sparse_resize_and_clear_"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.sparse_resize_and_clear_</span></code></a></p></td>
<td><p>从稀疏张量 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 中移除所有指定的元素，并将 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 调整到所需的大小和稀疏与密集维度的数量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.is_coalesced.html#torch.Tensor.is_coalesced" title="torch.Tensor.is_coalesced"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.is_coalesced</span></code></a></p></td>
<td><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 是一个合并的稀疏 COO 张量，则返回 <code class="docutils literal "><span class="pre">True</span></code> ，否则返回 <code class="docutils literal "><span class="pre">False</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.indices.html#torch.Tensor.indices" title="torch.Tensor.indices"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.indices</span></code></a></p></td>
<td><p>返回稀疏 COO 张量的索引张量。</p></td>
</tr>
</tbody>
</table>
<p>以下方法仅适用于稀疏 CSR 张量和稀疏 BSR 张量。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.crow_indices.html#torch.Tensor.crow_indices" title="torch.Tensor.crow_indices"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.crow_indices</span></code></a></p></td>
<td><p>返回包含 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量压缩行索引的张量，当 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 是一个布局为 <code class="docutils literal "><span class="pre">sparse_csr</span></code> 的稀疏 CSR 张量时。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.col_indices.html#torch.Tensor.col_indices" title="torch.Tensor.col_indices"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.col_indices</span></code></a></p></td>
<td><p>返回包含 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量列索引的张量，当 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 是一个布局为 <code class="docutils literal "><span class="pre">sparse_csr</span></code> 的稀疏 CSR 张量时。</p></td>
</tr>
</tbody>
</table>
<p>以下方法仅适用于稀疏 CSC 张量和稀疏 BSC 张量。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.Tensor.row_indices.html#torch.Tensor.row_indices" title="torch.Tensor.row_indices"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.row_indices</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.Tensor.ccol_indices.html#torch.Tensor.ccol_indices" title="torch.Tensor.ccol_indices"><code class="xref py py-obj docutils literal "><span class="pre">Tensor.ccol_indices</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>以下 Tensor 方法支持稀疏 COO 张量。</p>
<p><a class="reference internal" href="generated/torch.Tensor.add.html#torch.Tensor.add" title="torch.Tensor.add"><code class="xref py py-meth docutils literal "><span class="pre">add()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.add_.html#torch.Tensor.add_" title="torch.Tensor.add_"><code class="xref py py-meth docutils literal "><span class="pre">add_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.addmm.html#torch.Tensor.addmm" title="torch.Tensor.addmm"><code class="xref py py-meth docutils literal "><span class="pre">addmm()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_" title="torch.Tensor.addmm_"><code class="xref py py-meth docutils literal "><span class="pre">addmm_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.any.html#torch.Tensor.any" title="torch.Tensor.any"><code class="xref py py-meth docutils literal "><span class="pre">any()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.asin.html#torch.Tensor.asin" title="torch.Tensor.asin"><code class="xref py py-meth docutils literal "><span class="pre">asin()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.asin_.html#torch.Tensor.asin_" title="torch.Tensor.asin_"><code class="xref py py-meth docutils literal "><span class="pre">asin_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin" title="torch.Tensor.arcsin"><code class="xref py py-meth docutils literal "><span class="pre">arcsin()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_" title="torch.Tensor.arcsin_"><code class="xref py py-meth docutils literal "><span class="pre">arcsin_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.bmm.html#torch.Tensor.bmm" title="torch.Tensor.bmm"><code class="xref py py-meth docutils literal "><span class="pre">bmm()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.clone.html#torch.Tensor.clone" title="torch.Tensor.clone"><code class="xref py py-meth docutils literal "><span class="pre">clone()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad" title="torch.Tensor.deg2rad"><code class="xref py py-meth docutils literal "><span class="pre">deg2rad()</span></code></a>
<code class="xref py py-meth docutils literal "><span class="pre">deg2rad_()</span></code>
<a class="reference internal" href="generated/torch.Tensor.detach.html#torch.Tensor.detach" title="torch.Tensor.detach"><code class="xref py py-meth docutils literal "><span class="pre">detach()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.detach_.html#torch.Tensor.detach_" title="torch.Tensor.detach_"><code class="xref py py-meth docutils literal "><span class="pre">detach_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.dim.html#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-meth docutils literal "><span class="pre">dim()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.div.html#torch.Tensor.div" title="torch.Tensor.div"><code class="xref py py-meth docutils literal "><span class="pre">div()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.div_.html#torch.Tensor.div_" title="torch.Tensor.div_"><code class="xref py py-meth docutils literal "><span class="pre">div_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide" title="torch.Tensor.floor_divide"><code class="xref py py-meth docutils literal "><span class="pre">floor_divide()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_" title="torch.Tensor.floor_divide_"><code class="xref py py-meth docutils literal "><span class="pre">floor_divide_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.get_device.html#torch.Tensor.get_device" title="torch.Tensor.get_device"><code class="xref py py-meth docutils literal "><span class="pre">get_device()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.index_select.html#torch.Tensor.index_select" title="torch.Tensor.index_select"><code class="xref py py-meth docutils literal "><span class="pre">index_select()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.isnan.html#torch.Tensor.isnan" title="torch.Tensor.isnan"><code class="xref py py-meth docutils literal "><span class="pre">isnan()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.log1p.html#torch.Tensor.log1p" title="torch.Tensor.log1p"><code class="xref py py-meth docutils literal "><span class="pre">log1p()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_" title="torch.Tensor.log1p_"><code class="xref py py-meth docutils literal "><span class="pre">log1p_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.mm.html#torch.Tensor.mm" title="torch.Tensor.mm"><code class="xref py py-meth docutils literal "><span class="pre">mm()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.mul.html#torch.Tensor.mul" title="torch.Tensor.mul"><code class="xref py py-meth docutils literal "><span class="pre">mul()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.mul_.html#torch.Tensor.mul_" title="torch.Tensor.mul_"><code class="xref py py-meth docutils literal "><span class="pre">mul_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.mv.html#torch.Tensor.mv" title="torch.Tensor.mv"><code class="xref py py-meth docutils literal "><span class="pre">mv()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy" title="torch.Tensor.narrow_copy"><code class="xref py py-meth docutils literal "><span class="pre">narrow_copy()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.neg.html#torch.Tensor.neg" title="torch.Tensor.neg"><code class="xref py py-meth docutils literal "><span class="pre">neg()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.neg_.html#torch.Tensor.neg_" title="torch.Tensor.neg_"><code class="xref py py-meth docutils literal "><span class="pre">neg_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.negative.html#torch.Tensor.negative" title="torch.Tensor.negative"><code class="xref py py-meth docutils literal "><span class="pre">negative()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.negative_.html#torch.Tensor.negative_" title="torch.Tensor.negative_"><code class="xref py py-meth docutils literal "><span class="pre">negative_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.numel.html#torch.Tensor.numel" title="torch.Tensor.numel"><code class="xref py py-meth docutils literal "><span class="pre">numel()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg" title="torch.Tensor.rad2deg"><code class="xref py py-meth docutils literal "><span class="pre">rad2deg()</span></code></a>
<code class="xref py py-meth docutils literal "><span class="pre">rad2deg_()</span></code>
<a class="reference internal" href="generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_" title="torch.Tensor.resize_as_"><code class="xref py py-meth docutils literal "><span class="pre">resize_as_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.size.html#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-meth docutils literal "><span class="pre">size()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.pow.html#torch.Tensor.pow" title="torch.Tensor.pow"><code class="xref py py-meth docutils literal "><span class="pre">pow()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt" title="torch.Tensor.sqrt"><code class="xref py py-meth docutils literal "><span class="pre">sqrt()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.square.html#torch.Tensor.square" title="torch.Tensor.square"><code class="xref py py-meth docutils literal "><span class="pre">square()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.smm.html#torch.Tensor.smm" title="torch.Tensor.smm"><code class="xref py py-meth docutils literal "><span class="pre">smm()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm" title="torch.Tensor.sspaddmm"><code class="xref py py-meth docutils literal "><span class="pre">sspaddmm()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.sub.html#torch.Tensor.sub" title="torch.Tensor.sub"><code class="xref py py-meth docutils literal "><span class="pre">sub()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.sub_.html#torch.Tensor.sub_" title="torch.Tensor.sub_"><code class="xref py py-meth docutils literal "><span class="pre">sub_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.t.html#torch.Tensor.t" title="torch.Tensor.t"><code class="xref py py-meth docutils literal "><span class="pre">t()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.t_.html#torch.Tensor.t_" title="torch.Tensor.t_"><code class="xref py py-meth docutils literal "><span class="pre">t_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.transpose.html#torch.Tensor.transpose" title="torch.Tensor.transpose"><code class="xref py py-meth docutils literal "><span class="pre">transpose()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_" title="torch.Tensor.transpose_"><code class="xref py py-meth docutils literal "><span class="pre">transpose_()</span></code></a>
<a class="reference internal" href="generated/torch.Tensor.zero_.html#torch.Tensor.zero_" title="torch.Tensor.zero_"><code class="xref py py-meth docutils literal "><span class="pre">zero_()</span></code></a></p>
</section>
<section id="torch-functions-specific-to-sparse-tensors">
<h3>火炬函数针对稀疏张量特定功能</h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.sparse_coo_tensor"><a class="reference internal" href="generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor" title="torch.sparse_coo_tensor"><code class="xref py py-obj docutils literal "><span class="pre">sparse_coo_tensor</span></code></a></p></td>
<td><p>在指定的 <code class="xref py py-attr docutils literal "><span class="pre">indices</span></code> 处构建具有指定值的 COO(坐标)格式的稀疏张量</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sparse_csr_tensor"><a class="reference internal" href="generated/torch.sparse_csr_tensor.html#torch.sparse_csr_tensor" title="torch.sparse_csr_tensor"><code class="xref py py-obj docutils literal "><span class="pre">sparse_csr_tensor</span></code></a></p></td>
<td><p>在指定的 <code class="xref py py-attr docutils literal "><span class="pre">crow_indices</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">col_indices</span></code> 处构建具有指定值的 CSR(压缩稀疏行)格式的稀疏张量</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.sparse_csc_tensor"><a class="reference internal" href="generated/torch.sparse_csc_tensor.html#torch.sparse_csc_tensor" title="torch.sparse_csc_tensor"><code class="xref py py-obj docutils literal "><span class="pre">sparse_csc_tensor</span></code></a></p></td>
<td><p>在指定的 <code class="xref py py-attr docutils literal "><span class="pre">ccol_indices</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">row_indices</span></code> 处构建具有指定值的 CSC(压缩稀疏列)格式的稀疏张量</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sparse_bsr_tensor"><a class="reference internal" href="generated/torch.sparse_bsr_tensor.html#torch.sparse_bsr_tensor" title="torch.sparse_bsr_tensor"><code class="xref py py-obj docutils literal "><span class="pre">sparse_bsr_tensor</span></code></a></p></td>
<td><p>构建一个在指定 <code class="xref py py-attr docutils literal "><span class="pre">crow_indices</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">col_indices</span></code> 的二维块上的 BSR（块压缩稀疏行）稀疏张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.sparse_bsc_tensor"><a class="reference internal" href="generated/torch.sparse_bsc_tensor.html#torch.sparse_bsc_tensor" title="torch.sparse_bsc_tensor"><code class="xref py py-obj docutils literal "><span class="pre">sparse_bsc_tensor</span></code></a></p></td>
<td><p>构建一个在指定 <code class="xref py py-attr docutils literal "><span class="pre">ccol_indices</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">row_indices</span></code> 的二维块上的 BSC（块压缩稀疏列）稀疏张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sparse_compressed_tensor"><a class="reference internal" href="generated/torch.sparse_compressed_tensor.html#torch.sparse_compressed_tensor" title="torch.sparse_compressed_tensor"><code class="xref py py-obj docutils literal "><span class="pre">sparse_compressed_tensor</span></code></a></p></td>
<td><p>在 CSR（压缩稀疏行）、CSC（压缩稀疏列）、BSR（块压缩稀疏行）或 BSC（块压缩稀疏列）格式中构建稀疏张量，并在指定 <code class="xref py py-attr docutils literal "><span class="pre">compressed_indices</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">plain_indices</span></code> 处指定值。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.sparse.sum.html#torch.sparse.sum" title="torch.sparse.sum"><code class="xref py py-obj docutils literal "><span class="pre">sparse.sum</span></code></a></p></td>
<td><p>返回给定稀疏张量每行的和。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.sparse.addmm.html#torch.sparse.addmm" title="torch.sparse.addmm"><code class="xref py py-obj docutils literal "><span class="pre">sparse.addmm</span></code></a></p></td>
<td><p>这个函数在正向与 <code class="xref py py-func docutils literal "><span class="pre">torch.addmm()</span></code> 执行完全相同的功能，但支持对稀疏 COO 矩阵 <code class="xref py py-attr docutils literal "><span class="pre">mat1</span></code> 进行反向操作。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.sparse.sampled_addmm.html#torch.sparse.sampled_addmm" title="torch.sparse.sampled_addmm"><code class="xref py py-obj docutils literal "><span class="pre">sparse.sampled_addmm</span></code></a></p></td>
<td><p>在由 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的稀疏模式指定的位置，对密集矩阵 <code class="xref py py-attr docutils literal "><span class="pre">mat1</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">mat2</span></code> 执行矩阵乘法。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.sparse.mm.html#torch.sparse.mm" title="torch.sparse.mm"><code class="xref py py-obj docutils literal "><span class="pre">sparse.mm</span></code></a></p></td>
<td><p>对稀疏矩阵 <code class="xref py py-attr docutils literal "><span class="pre">mat1</span></code> 执行矩阵乘法。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.sspaddmm"><a class="reference internal" href="generated/torch.sspaddmm.html#torch.sspaddmm" title="torch.sspaddmm"><code class="xref py py-obj docutils literal "><span class="pre">sspaddmm</span></code></a></p></td>
<td><p>将稀疏张量 <code class="xref py py-attr docutils literal "><span class="pre">mat1</span></code> 与密集张量 <code class="xref py py-attr docutils literal "><span class="pre">mat2</span></code> 相乘，然后将稀疏张量 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 加到结果上。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.hspmm"><a class="reference internal" href="generated/torch.hspmm.html#torch.hspmm" title="torch.hspmm"><code class="xref py py-obj docutils literal "><span class="pre">hspmm</span></code></a></p></td>
<td><p>执行稀疏 COO 矩阵 <code class="xref py py-attr docutils literal "><span class="pre">mat1</span></code> 和带步长的矩阵 <code class="xref py py-attr docutils literal "><span class="pre">mat2</span></code> 的矩阵乘法。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.smm"><a class="reference internal" href="generated/torch.smm.html#torch.smm" title="torch.smm"><code class="xref py py-obj docutils literal "><span class="pre">smm</span></code></a></p></td>
<td><p>执行稀疏矩阵 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 与稠密矩阵 <code class="xref py py-attr docutils literal "><span class="pre">mat</span></code> 的矩阵乘法。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.sparse.softmax.html#torch.sparse.softmax" title="torch.sparse.softmax"><code class="xref py py-obj docutils literal "><span class="pre">sparse.softmax</span></code></a></p></td>
<td><p>应用 softmax 函数。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.sparse.spsolve.html#torch.sparse.spsolve" title="torch.sparse.spsolve"><code class="xref py py-obj docutils literal "><span class="pre">sparse.spsolve</span></code></a></p></td>
<td><p>计算具有唯一解的平方线性方程组的解。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.sparse.log_softmax.html#torch.sparse.log_softmax" title="torch.sparse.log_softmax"><code class="xref py py-obj docutils literal "><span class="pre">sparse.log_softmax</span></code></a></p></td>
<td><p>应用 softmax 函数后跟对数。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.sparse.spdiags.html#torch.sparse.spdiags" title="torch.sparse.spdiags"><code class="xref py py-obj docutils literal "><span class="pre">sparse.spdiags</span></code></a></p></td>
<td><p>通过将 <code class="xref py py-attr docutils literal "><span class="pre">diagonals</span></code> 的行值放置在输出指定的对角线上来创建一个稀疏的 2D 张量</p></td>
</tr>
</tbody>
</table>
</section>
<section id="other-functions">
<h3>其他函数 ¶</h3>
<p>以下 <code class="xref py py-mod docutils literal "><span class="pre">torch</span></code> 函数支持稀疏张量：</p>
<p><a class="reference internal" href="generated/torch.cat.html#torch.cat" title="torch.cat"><code class="xref py py-func docutils literal "><span class="pre">cat()</span></code></a>
<a class="reference internal" href="generated/torch.dstack.html#torch.dstack" title="torch.dstack"><code class="xref py py-func docutils literal "><span class="pre">dstack()</span></code></a>
<a class="reference internal" href="generated/torch.empty.html#torch.empty" title="torch.empty"><code class="xref py py-func docutils literal "><span class="pre">empty()</span></code></a>
<a class="reference internal" href="generated/torch.empty_like.html#torch.empty_like" title="torch.empty_like"><code class="xref py py-func docutils literal "><span class="pre">empty_like()</span></code></a>
<a class="reference internal" href="generated/torch.hstack.html#torch.hstack" title="torch.hstack"><code class="xref py py-func docutils literal "><span class="pre">hstack()</span></code></a>
<a class="reference internal" href="generated/torch.index_select.html#torch.index_select" title="torch.index_select"><code class="xref py py-func docutils literal "><span class="pre">index_select()</span></code></a>
<a class="reference internal" href="generated/torch.is_complex.html#torch.is_complex" title="torch.is_complex"><code class="xref py py-func docutils literal "><span class="pre">is_complex()</span></code></a>
<a class="reference internal" href="generated/torch.is_floating_point.html#torch.is_floating_point" title="torch.is_floating_point"><code class="xref py py-func docutils literal "><span class="pre">is_floating_point()</span></code></a>
<a class="reference internal" href="generated/torch.is_nonzero.html#torch.is_nonzero" title="torch.is_nonzero"><code class="xref py py-func docutils literal "><span class="pre">is_nonzero()</span></code></a>
<code class="xref py py-func docutils literal "><span class="pre">is_same_size()</span></code>
<code class="xref py py-func docutils literal "><span class="pre">is_signed()</span></code>
<a class="reference internal" href="generated/torch.is_tensor.html#torch.is_tensor" title="torch.is_tensor"><code class="xref py py-func docutils literal "><span class="pre">is_tensor()</span></code></a>
<a class="reference internal" href="generated/torch.lobpcg.html#torch.lobpcg" title="torch.lobpcg"><code class="xref py py-func docutils literal "><span class="pre">lobpcg()</span></code></a>
<a class="reference internal" href="generated/torch.mm.html#torch.mm" title="torch.mm"><code class="xref py py-func docutils literal "><span class="pre">mm()</span></code></a>
<code class="xref py py-func docutils literal "><span class="pre">native_norm()</span></code>
<a class="reference internal" href="generated/torch.pca_lowrank.html#torch.pca_lowrank" title="torch.pca_lowrank"><code class="xref py py-func docutils literal "><span class="pre">pca_lowrank()</span></code></a>
<a class="reference internal" href="generated/torch.select.html#torch.select" title="torch.select"><code class="xref py py-func docutils literal "><span class="pre">select()</span></code></a>
<a class="reference internal" href="generated/torch.stack.html#torch.stack" title="torch.stack"><code class="xref py py-func docutils literal "><span class="pre">stack()</span></code></a>
<a class="reference internal" href="generated/torch.svd_lowrank.html#torch.svd_lowrank" title="torch.svd_lowrank"><code class="xref py py-func docutils literal "><span class="pre">svd_lowrank()</span></code></a>
<a class="reference internal" href="generated/torch.unsqueeze.html#torch.unsqueeze" title="torch.unsqueeze"><code class="xref py py-func docutils literal "><span class="pre">unsqueeze()</span></code></a>
<a class="reference internal" href="generated/torch.vstack.html#torch.vstack" title="torch.vstack"><code class="xref py py-func docutils literal "><span class="pre">vstack()</span></code></a>
<a class="reference internal" href="generated/torch.zeros.html#torch.zeros" title="torch.zeros"><code class="xref py py-func docutils literal "><span class="pre">zeros()</span></code></a>
<a class="reference internal" href="generated/torch.zeros_like.html#torch.zeros_like" title="torch.zeros_like"><code class="xref py py-func docutils literal "><span class="pre">zeros_like()</span></code></a></p>
<p>要管理检查稀疏张量不变量，请参阅：</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.sparse.check_sparse_tensor_invariants.html#torch.sparse.check_sparse_tensor_invariants" title="torch.sparse.check_sparse_tensor_invariants"><code class="xref py py-obj docutils literal "><span class="pre">sparse.check_sparse_tensor_invariants</span></code></a></p></td>
<td><p>控制检查稀疏张量不变性的工具。</p></td>
</tr>
</tbody>
</table>
<p>要使用稀疏张量与 <code class="xref py py-func docutils literal "><span class="pre">gradcheck()</span></code> 函数，请参阅：</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.sparse.as_sparse_gradcheck.html#torch.sparse.as_sparse_gradcheck" title="torch.sparse.as_sparse_gradcheck"><code class="xref py py-obj docutils literal "><span class="pre">sparse.as_sparse_gradcheck</span></code></a></p></td>
<td><p>装饰函数，以扩展稀疏张量的 gradcheck。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="zero-preserving-unary-functions">
<h3>零保持一元函数 ¶</h3>
<p>我们旨在支持所有“零保持一元函数”：将零映射到零的单变量函数。</p>
<p>如果您发现我们遗漏了您需要的某个零保持一元函数，请鼓励您提交一个功能请求的问题。一如既往，请在提交问题之前，请先尝试使用搜索功能。</p>
<p>以下运算符目前支持稀疏的 COO/CSR/CSC/BSR/CSR 张量输入。</p>
<p><a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal "><span class="pre">abs()</span></code></a>
<a class="reference internal" href="generated/torch.asin.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal "><span class="pre">asin()</span></code></a>
<a class="reference internal" href="generated/torch.asinh.html#torch.asinh" title="torch.asinh"><code class="xref py py-func docutils literal "><span class="pre">asinh()</span></code></a>
<a class="reference internal" href="generated/torch.atan.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal "><span class="pre">atan()</span></code></a>
<a class="reference internal" href="generated/torch.atanh.html#torch.atanh" title="torch.atanh"><code class="xref py py-func docutils literal "><span class="pre">atanh()</span></code></a>
<a class="reference internal" href="generated/torch.ceil.html#torch.ceil" title="torch.ceil"><code class="xref py py-func docutils literal "><span class="pre">ceil()</span></code></a>
<a class="reference internal" href="generated/torch.conj_physical.html#torch.conj_physical" title="torch.conj_physical"><code class="xref py py-func docutils literal "><span class="pre">conj_physical()</span></code></a>
<a class="reference internal" href="generated/torch.floor.html#torch.floor" title="torch.floor"><code class="xref py py-func docutils literal "><span class="pre">floor()</span></code></a>
<a class="reference internal" href="generated/torch.log1p.html#torch.log1p" title="torch.log1p"><code class="xref py py-func docutils literal "><span class="pre">log1p()</span></code></a>
<a class="reference internal" href="generated/torch.neg.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal "><span class="pre">neg()</span></code></a>
<a class="reference internal" href="generated/torch.round.html#torch.round" title="torch.round"><code class="xref py py-func docutils literal "><span class="pre">round()</span></code></a>
<a class="reference internal" href="generated/torch.sin.html#torch.sin" title="torch.sin"><code class="xref py py-func docutils literal "><span class="pre">sin()</span></code></a>
<a class="reference internal" href="generated/torch.sinh.html#torch.sinh" title="torch.sinh"><code class="xref py py-func docutils literal "><span class="pre">sinh()</span></code></a>
<a class="reference internal" href="generated/torch.sign.html#torch.sign" title="torch.sign"><code class="xref py py-func docutils literal "><span class="pre">sign()</span></code></a>
<a class="reference internal" href="generated/torch.sgn.html#torch.sgn" title="torch.sgn"><code class="xref py py-func docutils literal "><span class="pre">sgn()</span></code></a>
<a class="reference internal" href="generated/torch.signbit.html#torch.signbit" title="torch.signbit"><code class="xref py py-func docutils literal "><span class="pre">signbit()</span></code></a>
<a class="reference internal" href="generated/torch.tan.html#torch.tan" title="torch.tan"><code class="xref py py-func docutils literal "><span class="pre">tan()</span></code></a>
<a class="reference internal" href="generated/torch.tanh.html#torch.tanh" title="torch.tanh"><code class="xref py py-func docutils literal "><span class="pre">tanh()</span></code></a>
<a class="reference internal" href="generated/torch.trunc.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal "><span class="pre">trunc()</span></code></a>
<a class="reference internal" href="generated/torch.expm1.html#torch.expm1" title="torch.expm1"><code class="xref py py-func docutils literal "><span class="pre">expm1()</span></code></a>
<a class="reference internal" href="generated/torch.sqrt.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-func docutils literal "><span class="pre">sqrt()</span></code></a>
<a class="reference internal" href="generated/torch.angle.html#torch.angle" title="torch.angle"><code class="xref py py-func docutils literal "><span class="pre">angle()</span></code></a>
<a class="reference internal" href="generated/torch.isinf.html#torch.isinf" title="torch.isinf"><code class="xref py py-func docutils literal "><span class="pre">isinf()</span></code></a>
<a class="reference internal" href="generated/torch.isposinf.html#torch.isposinf" title="torch.isposinf"><code class="xref py py-func docutils literal "><span class="pre">isposinf()</span></code></a>
<a class="reference internal" href="generated/torch.isneginf.html#torch.isneginf" title="torch.isneginf"><code class="xref py py-func docutils literal "><span class="pre">isneginf()</span></code></a>
<a class="reference internal" href="generated/torch.isnan.html#torch.isnan" title="torch.isnan"><code class="xref py py-func docutils literal "><span class="pre">isnan()</span></code></a>
<a class="reference internal" href="generated/torch.erf.html#torch.erf" title="torch.erf"><code class="xref py py-func docutils literal "><span class="pre">erf()</span></code></a>
<a class="reference internal" href="generated/torch.erfinv.html#torch.erfinv" title="torch.erfinv"><code class="xref py py-func docutils literal "><span class="pre">erfinv()</span></code></a></p>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        下一页 <img height="16" width="16" class="next-page" src="_static/images/chevron-right-orange.svg"> <img height="16" width="16" class="previous-page" src="_static/images/chevron-right-orange.svg"> 上一页
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>© 版权所有 PyTorch 贡献者。</p>
  </div>
    
      <div>使用 Sphinx 构建，主题由 Read the Docs 提供。</div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.sparse</a><ul>
<li><a class="reference internal" href="#why-and-when-to-use-sparsity">为什么和何时使用稀疏性</a></li>
<li><a class="reference internal" href="#functionality-overview">功能概述</a></li>
<li><a class="reference internal" href="#operator-overview">操作员概述</a></li>
<li><a class="reference internal" href="#sparse-semi-structured-tensors">稀疏半结构化张量</a><ul>
<li><a class="reference internal" href="#constructing-sparse-semi-structured-tensors">构建稀疏半结构化张量</a></li>
<li><a class="reference internal" href="#sparse-semi-structured-tensor-operations">稀疏半结构化张量运算</a></li>
<li><a class="reference internal" href="#accelerating-nn-linear-with-semi-structured-sparsity">加速具有半结构化稀疏性的 nn.Linear</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sparse-coo-tensors">稀疏 COO 张量</a><ul>
<li><a class="reference internal" href="#construction">施工</a></li>
<li><a class="reference internal" href="#sparse-hybrid-coo-tensors">稀疏混合 COO 张量</a></li>
<li><a class="reference internal" href="#uncoalesced-sparse-coo-tensors">未合并的稀疏 COO 张量</a></li>
<li><a class="reference internal" href="#working-with-sparse-coo-tensors">处理稀疏 COO 张量</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sparse-compressed-tensors">稀疏压缩张量</a><ul>
<li><a class="reference internal" href="#sparse-csr-tensor">稀疏 CSR 张量</a><ul>
<li><a class="reference internal" href="#construction-of-csr-tensors">CSR 张量的构建</a></li>
<li><a class="reference internal" href="#csr-tensor-operations">CSR 张量操作</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sparse-csc-tensor">稀疏 CSC 张量</a><ul>
<li><a class="reference internal" href="#construction-of-csc-tensors">CSC 张量的构建</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sparse-bsr-tensor">稀疏 BSR 张量</a><ul>
<li><a class="reference internal" href="#construction-of-bsr-tensors">BSR 张量的构建</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sparse-bsc-tensor">稀疏 BSC 张量</a><ul>
<li><a class="reference internal" href="#construction-of-bsc-tensors">BSC 张量的构建</a></li>
</ul>
</li>
<li><a class="reference internal" href="#tools-for-working-with-sparse-compressed-tensors">处理稀疏压缩张量的工具</a><ul>
<li><a class="reference internal" href="#construction-of-sparse-compressed-tensors">稀疏压缩张量的构建</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#supported-operations">支持的操作</a><ul>
<li><a class="reference internal" href="#linear-algebra-operations">线性代数运算</a></li>
<li><a class="reference internal" href="#tensor-methods-and-sparse">张量方法与稀疏</a><ul>
</ul>
</li>
<li><a class="reference internal" href="#torch-functions-specific-to-sparse-tensors">专门针对稀疏张量的 PyTorch 函数</a></li>
<li><a class="reference internal" href="#other-functions">其他函数</a><ul>
</ul>
</li>
<li><a class="reference internal" href="#zero-preserving-unary-functions">零保持一元函数</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script="" type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0">


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>文档</h2>
          <p>查看 PyTorch 的全面开发者文档</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">查看文档</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>教程</h2>
          <p>深入了解初学者和高级开发者的教程</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">查看教程</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>资源</h2>
          <p>查找开发资源并获得您的疑问解答</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">查看资源</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">开始使用</a></li>
            <li><a href="https://pytorch.org/features">功能</a></li>
            <li><a href="https://pytorch.org/ecosystem">生态系统</a></li>
            <li><a href="https://pytorch.org/blog/">博客</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">贡献</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">资源</a></li>
            <li><a href="https://pytorch.org/tutorials">教程</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">文档</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">讨论</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub 问题和任务</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">品牌指南</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">保持更新</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">推特</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">领英</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch 播客</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">苹果</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">谷歌</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">亚马逊</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">条款</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">隐私</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© 版权所有 Linux 基金会。PyTorch 基金会是 Linux 基金会的一个项目。有关本网站的使用条款、商标政策以及其他适用于 PyTorch 基金会的政策，请参阅 www.linuxfoundation.org/policies/。PyTorch 基金会支持 PyTorch 开源项目，该项目已被确立为 LF Projects, LLC 的 PyTorch 项目系列。有关适用于 PyTorch 项目系列 LF Projects, LLC 的政策，请参阅 www.lfprojects.org/policies/。</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">为分析流量并优化您的体验，我们在本网站上提供 cookies。通过点击或导航，您同意允许我们使用 cookies。作为本站点的当前维护者，Facebook 的 Cookies 政策适用。了解更多信息，包括可用的控制选项：Cookies 政策。</p>
    <img class="close-button" src="_static/images/pytorch-x.svg" width="16" height="16">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>学习</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">开始学习</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">教程</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">学习基础知识</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch 菜谱</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">PyTorch 入门 - YouTube 系列</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>生态系统</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">工具</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">社区</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">论坛</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">开发者资源</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">贡献者奖项 - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">关于 PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch 文档</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>文档</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch 领域</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>博客 &amp; 新闻</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch 博客</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">社区博客</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">视频</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">社区故事</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">活动</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">通讯</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>关于</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch 基金会</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">治理委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">云信用计划</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">技术顾问委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">员工</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">联系我们</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

</body></html>