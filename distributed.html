<!DOCTYPE html>
<html lang="zh_CN">
<head>
  <meta charset="UTF-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Distributed communication package - torch.distributed — PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/distributed.html">
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css">
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css">
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css">
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css">
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css">
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css">
    <link rel="index" title="Index" href="genindex.html">
    <link rel="search" title="Search" href="search.html">
    <link rel="next" title="torch.distributed.tensor" href="distributed.tensor.html">
    <link rel="prev" title="Fake tensor" href="torch.compiler_fake_tensor.html">

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head><body class="pytorch-body"><div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">学习</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class="dropdown-title">开始使用</span>
                  <p>在本地运行 PyTorch 或快速开始使用支持的云平台之一</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">教程</span><p></p>
                  <p>PyTorch 教程中的新内容</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">学习基础知识</span><p></p>
                  <p>熟悉 PyTorch 的概念和模块</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch 食谱</span><p></p>
                  <p>精简版、可直接部署的 PyTorch 代码示例</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">PyTorch 入门 - YouTube 系列</span><p></p>
                  <p>通过我们引人入胜的 YouTube 教程系列掌握 PyTorch 基础知识</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">生态系统</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">工具</span><p></p>
                  <p>了解 PyTorch 生态系统中的工具和框架</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">社区</span>
                  <p>加入 PyTorch 开发者社区，贡献、学习并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">论坛</span>
                  <p>讨论 PyTorch 代码、问题、安装、研究的地方</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">开发者资源</span>
                  <p>查找资源并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">贡献者奖项 - 2024</span><p></p>
                  <p>本届 PyTorch 会议揭晓获奖者</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">关于 PyTorch Edge</span><p></p>
                  <p>为边缘设备构建创新和隐私感知的 AI 体验</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span><p></p>
                  <p>基于移动和边缘设备的端到端推理能力解决方案</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch 文档</span><p></p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">文档</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span><p></p>
                  <p>探索文档以获取全面指导，了解如何使用 PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch 领域</span><p></p>
                  <p>阅读 PyTorch 领域的文档，了解更多关于特定领域库的信息</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">博客与新闻</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch 博客</span><p></p>
                  <p>捕捉最新的技术新闻和事件</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">社区博客</span><p></p>
                  <p>PyTorch 生态系统故事</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">视频</span><p></p>
                  <p>了解最新的 PyTorch 教程、新内容等</p>
                </a><a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">社区故事</span><p></p>
                  <p>学习如何我们的社区使用 PyTorch 解决真实、日常的机器学习问题</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">活动</span><p></p>
                  <p>查找活动、网络研讨会和播客</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">通讯</span><p></p>
                  <p>跟踪最新更新</p>
                </a>
            </div>
          </div></li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">关于</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch 基金会</span><p></p>
                  <p>了解更多关于 PyTorch 基金会的信息</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">管理委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">云信用计划</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">技术顾问委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">员工</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">联系我们</span><p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">成为会员</a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>



   

    

    <div class="table-of-contents-link-wrapper">
      <span>目录</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="https://pytorch.org/docs/versions.html">主程序 (2.7.0+cpu ) ▼</a>
    </div>
    <div id="searchBox">
    <div class="searchbox" id="googleSearchBox">
      <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
      <div class="gcse-search"></div>
    </div>
    <div id="sphinxSearchBox" style="display: none;">
      <div role="search">
        <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
          <input type="text" name="q" placeholder="Search Docs">
          <input type="hidden" name="check_keywords" value="yes">
          <input type="hidden" name="area" value="default">
        </form>
      </div>
    </div>
  </div>
  <form id="searchForm">
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="google" checked="">谷歌搜索</label>
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="sphinx">经典搜索</label>
  </form>

  <script>
     document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
  </script>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/distributed.html">您正在查看不稳定开发者预览文档。请点击此处查看最新稳定版本的文档。</a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">社区</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch 治理 | 构建 + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch 贡献指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch 设计哲学</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch 治理 | 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch 治理 | 维护者</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者笔记</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">自动混合精度示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">广播语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU 多线程和 TorchScript 推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA 语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/custom_operators.html">PyTorch 自定义算子页面</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">分布式数据并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">扩展 PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">使用 autograd.Function 扩展 torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">常见问题解答</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/fsdp.html">FSDP 笔记</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/get_start_xpu.html">在 Intel GPU 上入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm)语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">大规模部署的功能</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/libtorch_stable_abi.html">LibTorch 稳定 ABI</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">模块</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS 后端</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">多进程最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">数值精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">可重现性</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">序列化语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows 常见问题解答</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语言绑定</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">张量属性</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">张量视图</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="accelerator.html">torch.accelerator</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">理解 CUDA 内存使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">生成快照</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">使用可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">快照 API 参考</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.memory.html">torch.mtia.memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">元设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch.export</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.html">torch.distributed.tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.fsdp.fully_shard.html">torch.distributed.fsdp.fully_shard</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">复数</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP 通信钩子</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">分布式 RPC 框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">类型信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">命名张量</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">命名张量操作覆盖率</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_environment_variables.html">火炬环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">库</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">火炬推荐</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch 在 XLA 设备上</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/ao">torchao</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        文档 &gt;</li>

        
      <li>分布式通信包 - torch.distributed</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/distributed.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg" width="16" height="16"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">快捷键</div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="distributed-communication-package-torch-distributed">
<h1>分布式通信包 - torch.distributed</h1>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>请参阅 PyTorch 分布式概述以了解有关分布式训练的所有相关功能的简要介绍。</p>
</div>
<span class="target" id="module-torch.distributed"></span><section id="backends">
<h2>后端</h2>
<p>支持三个内置后端，每个后端都有不同的功能。下表显示了与 CPU / CUDA 张量一起可用的函数。如果用于构建 PyTorch 的实现支持，则 MPI 仅支持 CUDA。</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>后端</p></th>
<th class="head" colspan="2"><p><code class="docutils literal "><span class="pre">gloo</span></code></p></th>
<th class="head" colspan="2"><p><code class="docutils literal "><span class="pre">mpi</span></code></p></th>
<th class="head" colspan="2"><p><code class="docutils literal "><span class="pre">nccl</span></code></p></th>
</tr>
<tr class="row-even"><th class="head"><p>设备</p></th>
<th class="head"><p>CPU</p></th>
<th class="head"><p>GPU</p></th>
<th class="head"><p>CPU</p></th>
<th class="head"><p>GPU</p></th>
<th class="head"><p>CPU</p></th>
<th class="head"><p>GPU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-odd"><td><p>发送</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-even"><td><p>接收</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-odd"><td><p>广播</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-even"><td><p>全量归约</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-odd"><td><p>归约</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-even"><td><p>全局聚合</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-odd"><td><p>收集</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-even"><td><p>分散</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-odd"><td><p>减少分散</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-even"><td><p>全部到全部</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-odd"><td><p>障碍</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
</tbody>
</table>
<section id="backends-that-come-with-pytorch">
<h3>随 PyTorch 一起提供的后端</h3>
<p>PyTorch 分布式包支持 Linux（稳定）、MacOS（稳定）和 Windows（原型）。默认情况下，对于 Linux，Gloo 和 NCCL 后端已构建并包含在 PyTorch 分布式中（仅当使用 CUDA 构建时为 NCCL）。MPI 是一个可选后端，只能在从源代码构建 PyTorch 时包含。（例如，在已安装 MPI 的主机上构建 PyTorch。）</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>截至 PyTorch v1.8 版本，Windows 支持所有集体通信后端，除了 NCCL，如果 <code class="xref py py-func docutils literal "><span class="pre">init_process_group()</span></code> 的 init_method 参数指向一个文件，它必须遵循以下模式：</p>
<ul class="simple">
<li><p>本地文件系统， <code class="docutils literal "><span class="pre">init_method="file:///d:/tmp/some_file"</span></code> </p></li>
<li><p>共享文件系统， <code class="docutils literal "><span class="pre">init_method="file://////{machine_name}/{share_folder_name}/some_file"</span></code> </p></li>
</ul>
<p>与 Linux 平台相同，您可以通过设置环境变量 MASTER_ADDR 和 MASTER_PORT 来启用 TcpStore。</p>
</div>
</section>
<section id="which-backend-to-use">
<h3>使用哪个后端？</h3>
<p>过去我们经常被问到：“我应该使用哪个后端？”</p>
<ul class="simple">
<li><p>经验法则</p>
<ul>
<li><p>使用 NCCL 后端进行分布式 GPU 训练</p></li>
<li><p>使用 Gloo 后端进行分布式 CPU 训练。</p></li>
</ul>
</li>
<li><p>使用 InfiniBand 互连的 GPU 主机</p>
<ul>
<li><p>使用 NCCL，因为它是目前唯一支持 InfiniBand 和 GPUDirect 的后端。</p></li>
</ul>
</li>
<li><p>使用以太网互连的 GPU 主机</p>
<ul>
<li><p>使用 NCCL，因为它目前提供了最佳的分布式 GPU 训练性能，尤其是对于多进程单节点或多节点分布式训练。如果您在使用 NCCL 时遇到任何问题，请使用 Gloo 作为后备选项。（请注意，Gloo 目前对于 GPU 的运行速度比 NCCL 慢。）</p></li>
</ul>
</li>
<li><p>带有 InfiniBand 互连的 CPU 主机</p>
<ul>
<li><p>如果您的 InfiniBand 已启用 IP over IB，请使用 Gloo，否则请使用 MPI。我们计划在即将发布的版本中为 Gloo 添加 InfiniBand 支持。</p></li>
</ul>
</li>
<li><p>带有以太网互连的 CPU 主机</p>
<ul>
<li><p>使用 Gloo，除非你有特定理由使用 MPI。</p></li>
</ul>
</li>
</ul>
</section>
<section id="common-environment-variables">
<h3>常见环境变量 ¶</h3>
<section id="choosing-the-network-interface-to-use">
<h4>选择要使用的网络接口 ¶</h4>
<p>默认情况下，NCCL 和 Gloo 后端都会尝试找到正确的网络接口来使用。如果自动检测到的接口不正确，您可以使用以下环境变量来覆盖它（适用于相应后端）：</p>
<ul class="simple">
<li><p>NCCL_SOCKET_IFNAME，例如 <code class="docutils literal "><span class="pre">export</span> <span class="pre">NCCL_SOCKET_IFNAME=eth0</span></code> </p></li>
<li><p>GLOO_SOCKET_IFNAME，例如 <code class="docutils literal "><span class="pre">export</span> <span class="pre">GLOO_SOCKET_IFNAME=eth0</span></code> </p></li>
</ul>
<p>如果您使用 Gloo 后端，可以通过逗号分隔指定多个接口，例如： <code class="docutils literal "><span class="pre">export</span> <span class="pre">GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3</span></code> 。后端将以轮询方式在这些接口上调度操作。所有进程必须在此变量中指定相同数量的接口，这是强制性的。</p>
</section>
<section id="other-nccl-environment-variables">
<h4>其他 NCCL 环境变量</h4>
<p>调试 - 如果出现 NCCL 失败，您可以将 <code class="docutils literal "><span class="pre">NCCL_DEBUG=INFO</span></code> 设置为打印明确的警告信息以及基本的 NCCL 初始化信息。</p>
<p>您还可以使用 <code class="docutils literal "><span class="pre">NCCL_DEBUG_SUBSYS</span></code> 来获取关于 NCCL 特定方面的更多详细信息。例如， <code class="docutils literal "><span class="pre">NCCL_DEBUG_SUBSYS=COLL</span></code> 会打印集体调用的日志，这在调试挂起时可能很有帮助，尤其是由于集体类型或消息大小不匹配引起的挂起。在拓扑检测失败的情况下，将 <code class="docutils literal "><span class="pre">NCCL_DEBUG_SUBSYS=GRAPH</span></code> 设置为检查详细的检测结果并将其保存为参考，如果需要 NCCL 团队进一步的帮助，这将很有帮助。</p>
<p>性能调优 - NCCL 根据其拓扑检测自动进行调优，以节省用户的调优工作量。在某些基于套接字的系统上，用户仍然可以尝试调整 <code class="docutils literal "><span class="pre">NCCL_SOCKET_NTHREADS</span></code> 和 <code class="docutils literal "><span class="pre">NCCL_NSOCKS_PERTHREAD</span></code> 以增加套接字网络带宽。这两个环境变量已经被 NCCL 为一些云提供商（如 AWS 或 GCP）预先调优。</p>
<p>有关 NCCL 环境变量的完整列表，请参阅 NVIDIA NCCL 的官方文档。</p>
</section>
</section>
</section>
<section id="basics">
<span id="distributed-basics"></span><h2>基础 §</h2>
<p>torch.distributed 包提供了 PyTorch 的支持和跨多个计算节点（运行在一台或多台机器上）的通信原语，以支持多进程并行。类 <code class="xref py py-func docutils literal "><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code> 基于此功能，为任何 PyTorch 模型提供同步分布式训练作为包装器。这与 Multiprocessing 包提供的并行方式不同，即 torch.multiprocessing 和 <code class="xref py py-func docutils literal "><span class="pre">torch.nn.DataParallel()</span></code> ，因为它支持多个网络连接的机器，并且用户必须为每个进程显式启动主训练脚本的单独副本。</p>
<p>在单机同步的情况下，torch.distributed 或 <code class="xref py py-func docutils literal "><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code> 包装器可能仍然比其他数据并行方法具有优势，包括 <code class="xref py py-func docutils literal "><span class="pre">torch.nn.DataParallel()</span></code> ：</p>
<ul class="simple">
<li><p>每个进程维护自己的优化器，并在每次迭代中执行完整的优化步骤。虽然这看起来可能是多余的，因为梯度已经被收集并平均到各个进程中，因此对于每个进程都是相同的，但这意味着不需要参数广播步骤，从而减少了在节点之间传输张量所花费的时间。</p></li>
<li><p>每个进程都包含一个独立的 Python 解释器，消除了从单个 Python 进程驱动多个执行线程、模型副本或 GPU 所带来的额外解释器开销和“GIL-thrashing”。这对于大量使用 Python 运行时的模型尤为重要，包括具有循环层的模型或许多小型组件的模型。</p></li>
</ul>
</section>
<section id="initialization">
<h2>初始化</h2>
<p>在调用任何其他方法之前，需要使用 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.init_process_group()</span></code> 或 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.device_mesh.init_device_mesh()</span></code> 函数初始化该包。这两个函数都会阻塞，直到所有进程都加入。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>初始化不是线程安全的。进程组创建应从单个线程执行，以防止跨 rank 的“UUID”分配不一致，并防止初始化过程中可能导致的挂起。</p>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.is_available">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">is_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed.html#is_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/__init__.py#L14"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.is_available" title="Permalink to this definition">¶</a></dt>
<dd><p>返回 <code class="docutils literal "><span class="pre">True</span></code> 如果分布式包可用。</p>
<p>否则， <code class="docutils literal "><span class="pre">torch.distributed</span></code> 不公开其他任何 API。目前， <code class="docutils literal "><span class="pre">torch.distributed</span></code> 在 Linux、MacOS 和 Windows 上可用。在从源代码构建 PyTorch 时设置 <code class="docutils literal "><span class="pre">USE_DISTRIBUTED=1</span></code> 以启用它。目前，默认值为 Linux 和 Windows 的 <code class="docutils literal "><span class="pre">USE_DISTRIBUTED=1</span></code> ，MacOS 的 <code class="docutils literal "><span class="pre">USE_DISTRIBUTED=0</span></code> 。</p>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">布尔型</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.init_process_group">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">init_process_group</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">world_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">store</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pg_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#init_process_group"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L1530"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.init_process_group" title="Permalink to this definition">¶</a></dt>
<dd><p>初始化默认的分布式进程组。</p>
<p>这也将初始化分布式包。</p>
<dl class="simple">
<dt>初始化进程组主要有两种方式：</dt><dd><ol class="arabic simple">
<li><p>明确指定 <code class="docutils literal "><span class="pre">store</span></code> 、 <code class="docutils literal "><span class="pre">rank</span></code> 和 <code class="docutils literal "><span class="pre">world_size</span></code> 。</p></li>
<li><p>指定 <code class="docutils literal "><span class="pre">init_method</span></code> （一个 URL 字符串），表示如何发现对等节点。可选指定 <code class="docutils literal "><span class="pre">rank</span></code> 和 <code class="docutils literal "><span class="pre">world_size</span></code> ，或者将所有必需参数编码到 URL 中并省略它们。</p></li>
</ol>
</dd>
</dl>
<p>如果两者都没有指定，则假定 <code class="docutils literal "><span class="pre">init_method</span></code> 为“env://”。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>后端（str 或 Backend，可选）- 要使用的后端。根据构建时配置，有效值包括 <code class="docutils literal "><span class="pre">mpi</span></code> 、 <code class="docutils literal "><span class="pre">gloo</span></code> 、 <code class="docutils literal "><span class="pre">nccl</span></code> 、 <code class="docutils literal "><span class="pre">ucc</span></code> 或由第三方插件注册的后端。自 2.6 版本起，如果未提供 <code class="docutils literal "><span class="pre">backend</span></code> ，c10d 将使用为设备类型指定的后端注册（如果提供了 device_id 关键字参数）。目前已知的默认注册包括： <code class="docutils literal "><span class="pre">nccl</span></code> 用于 <code class="docutils literal "><span class="pre">cuda</span></code> ， <code class="docutils literal "><span class="pre">gloo</span></code> 用于 <code class="docutils literal "><span class="pre">cpu</span></code> 。如果未提供 <code class="docutils literal "><span class="pre">backend</span></code> 或 <code class="docutils literal "><span class="pre">device_id</span></code> ，c10d 将在运行时机器上检测加速器并使用为该检测到的加速器注册的后端（或 <code class="docutils literal "><span class="pre">cpu</span></code> ）。此字段可以作为小写字符串提供（例如， <code class="docutils literal "><span class="pre">"gloo"</span></code> ），也可以通过 <code class="xref py py-class docutils literal "><span class="pre">Backend</span></code> 属性访问（例如， <code class="docutils literal "><span class="pre">Backend.GLOO</span></code> ）。如果使用 <code class="docutils literal "><span class="pre">nccl</span></code> 后端在每台机器上使用多个进程，则每个进程必须对每个使用的 GPU 具有独占访问权限，因为进程间共享 GPU 可能会导致死锁或 NCCL 无效使用。 <code class="docutils literal "><span class="pre">ucc</span></code> 后端是实验性的。</p></li>
<li><p>初始化方法（字符串，可选）- 指定如何初始化进程组的 URL。如果没有指定 <code class="docutils literal "><span class="pre">init_method</span></code> 或 <code class="docutils literal "><span class="pre">store</span></code> ，则默认为“env://”。与 <code class="docutils literal "><span class="pre">store</span></code> 互斥。</p></li>
<li><p>world_size（整数，可选）- 参与作业的进程数量。如果指定了 <code class="docutils literal "><span class="pre">store</span></code> ，则为必需。</p></li>
<li><p>rank（整数，可选）- 当前进程的排名（它应该是一个介于 0 和 <code class="docutils literal "><span class="pre">world_size</span></code> -1 之间的数字）。如果指定了 <code class="docutils literal "><span class="pre">store</span></code> ，则为必需。</p></li>
<li><p>store（存储，可选）- 可供所有工作者访问的键/值存储，用于交换连接/地址信息。与 <code class="docutils literal "><span class="pre">init_method</span></code> 互斥。</p></li>
<li><p>超时（timedelta，可选）- 对进程组执行的操作的超时时间。默认值为 NCCL 的 10 分钟和其他后端的 30 分钟。这是集体操作将被异步中止并进程崩溃的持续时间。这是由于 CUDA 执行是异步的，继续执行用户代码不再安全，因为失败的异步 NCCL 操作可能会导致后续在损坏数据上运行的 CUDA 操作。当设置 TORCH_NCCL_BLOCKING_WAIT 时，进程将阻塞并等待此超时。</p></li>
<li><p>group_name（str，可选，已弃用）- 组名。此参数被忽略</p></li>
<li><p>pg_options（ProcessGroupOptions，可选）- 指定在构建特定进程组时需要传递的附加选项的进程组选项。目前，我们支持的是 <code class="docutils literal "><span class="pre">ProcessGroupNCCL.Options</span></code> 用于 <code class="docutils literal "><span class="pre">nccl</span></code> 后端， <code class="docutils literal "><span class="pre">is_high_priority_stream</span></code> 可以指定，以便 nccl 后端可以在计算内核等待时选择高优先级的 CUDA 流。有关配置 nccl 的其他可用选项，请参阅 https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t</p></li>
<li><p>device_id (torch.device, optional) – 将此进程“绑定”到单个、特定的设备上，允许进行后端特定的优化。目前这有两个效果，仅在 NCCL 下：通信器立即形成（立即调用 <code class="docutils literal "><span class="pre">ncclCommInit*</span></code> 而不是正常的延迟调用）并且子组将尽可能使用 <code class="docutils literal "><span class="pre">ncclCommSplit</span></code> 以避免创建组的额外开销。如果您想尽早了解 NCCL 初始化错误，也可以使用此字段。</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>要启用 <code class="docutils literal "><span class="pre">backend</span> <span class="pre">==</span> <span class="pre">Backend.MPI</span></code> ，PyTorch 需要在支持 MPI 的系统上从源代码构建。</p>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>多个后端的支持是实验性的。目前如果没有指定后端，将创建 <code class="docutils literal "><span class="pre">gloo</span></code> 和 <code class="docutils literal "><span class="pre">nccl</span></code> 后端。 <code class="docutils literal "><span class="pre">gloo</span></code> 后端将用于与 CPU 张量相关的集体操作，而 <code class="docutils literal "><span class="pre">nccl</span></code> 后端将用于与 CUDA 张量相关的集体操作。可以通过传递格式为“:,:”的字符串来指定自定义后端，例如“cpu:gloo,cuda:custom_backend”。</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.device_mesh.init_device_mesh">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.device_mesh.</span></span><span class="sig-name descname"><span class="pre">init_device_mesh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mesh_shape</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mesh_dim_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/device_mesh.html#init_device_mesh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/device_mesh.py#L979"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.device_mesh.init_device_mesh" title="Permalink to this definition">¶</a></dt>
<dd><p>根据 device_type、mesh_shape 和 mesh_dim_names 参数初始化 DeviceMesh。</p>
<p>这将创建一个具有 n 维数组布局的 DeviceMesh，其中 n 是 mesh_shape 的长度。如果提供了 mesh_dim_names，则每个维度将标记为 mesh_dim_names[i]。</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>init_device_mesh 遵循 SPMD 编程模型，意味着相同的 PyTorch Python 程序在集群的所有进程/排名上运行。确保 mesh_shape（描述设备布局的 nD 数组的维度）在所有排名上相同。不一致的 mesh_shape 可能导致挂起。</p>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>如果找不到进程组，init_device_mesh 将在幕后初始化所需的分布式进程组/组以进行分布式通信。</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>device_type (str) – 网状结构的设备类型。目前支持：“cpu”，“cuda/cuda-like”。不允许传入带有 GPU 索引的设备类型，如“cuda:0”。</p></li>
<li><p>mesh_shape (Tuple[int]) – 定义描述设备布局的多维数组维度的元组。</p></li>
<li><p>mesh_dim_names (Tuple[str], optional) – 一个元组，包含要分配给描述设备布局的多维数组每个维度的网状维度名称。其长度必须与 mesh_shape 的长度匹配。mesh_dim_names 中的每个字符串必须是唯一的。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal "><span class="pre">DeviceMesh</span></code> 代表设备布局的对象。</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.distributed.device_mesh.DeviceMesh" title="torch.distributed.device_mesh.DeviceMesh">设备网格</a></p>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.device_mesh</span> <span class="kn">import</span> <span class="n">init_device_mesh</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mesh_1d</span> <span class="o">=</span> <span class="n">init_device_mesh</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">,</span> <span class="n">mesh_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mesh_2d</span> <span class="o">=</span> <span class="n">init_device_mesh</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">,</span> <span class="n">mesh_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">(</span><span class="s2">"dp"</span><span class="p">,</span> <span class="s2">"tp"</span><span class="p">))</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.is_initialized">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">is_initialized</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#is_initialized"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L1269"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.is_initialized" title="Permalink to this definition">¶</a></dt>
<dd><p>检查默认进程组是否已初始化。</p>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">布尔型</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.is_mpi_available">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">is_mpi_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#is_mpi_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L1224"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.is_mpi_available" title="Permalink to this definition">¶</a></dt>
<dd><p>检查 MPI 后端是否可用。</p>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">布尔型</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.is_nccl_available">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">is_nccl_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#is_nccl_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L1229"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.is_nccl_available" title="Permalink to this definition">¶</a></dt>
<dd><p>检查 NCCL 后端是否可用。</p>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">布尔型</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.is_gloo_available">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">is_gloo_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#is_gloo_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L1234"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.is_gloo_available" title="Permalink to this definition">¶</a></dt>
<dd><p>检查 Gloo 后端是否可用。</p>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">布尔型</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.distributed_c10d.is_xccl_available">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.distributed_c10d.</span></span><span class="sig-name descname"><span class="pre">is_xccl_available</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#is_xccl_available"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L1244"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.distributed_c10d.is_xccl_available" title="Permalink to this definition">¶</a></dt>
<dd><p>检查 XCCL 后端是否可用。</p>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">布尔型</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.is_torchelastic_launched">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">is_torchelastic_launched</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#is_torchelastic_launched"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L1274"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.is_torchelastic_launched" title="Permalink to this definition">¶</a></dt>
<dd><p>检查此进程是否以 <code class="docutils literal "><span class="pre">torch.distributed.elastic</span></code> （即 torchelastic）启动。</p>
<p> <code class="docutils literal "><span class="pre">TORCHELASTIC_RUN_ID</span></code> 环境变量的存在被用作判断当前进程是否以 torchelastic 启动的代理。这是一个合理的代理，因为 <code class="docutils literal "><span class="pre">TORCHELASTIC_RUN_ID</span></code> 映射到会合 ID，它始终是一个非空值，表示用于对等发现的作业 ID。</p>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">布尔型</a></p>
</dd>
</dl>
</dd></dl>

<hr class="docutils">
<p>目前支持三种初始化方法：</p>
<section id="tcp-initialization">
<h3>TCP 初始化 §</h3>
<p>使用 TCP 初始化有两种方式，都需要一个所有进程都能访问的网络地址以及一个期望的 <code class="docutils literal "><span class="pre">world_size</span></code> 。第一种方式需要指定属于 rank 0 进程的地址。这种初始化方法要求所有进程都手动指定 rank。</p>
<p>注意，在最新的分布式包中不再支持多播地址。 <code class="docutils literal "><span class="pre">group_name</span></code> 也已弃用。</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="c1"># Use address of one of the machines</span>
<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">'tcp://10.1.1.20:23456'</span><span class="p">,</span>
                        <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="shared-file-system-initialization">
<h3>共享文件系统初始化 §</h3>
<p>另一种初始化方法利用的是组内所有机器都能访问的共享文件系统，以及一个期望的 <code class="docutils literal "><span class="pre">world_size</span></code> 。URL 应该以 <code class="docutils literal "><span class="pre">file://</span></code> 开头，并包含共享文件系统上（在现有目录中）一个不存在的文件的路径。文件系统初始化将自动创建该文件（如果不存在），但不会删除该文件。因此，您需要确保在下次 <code class="xref py py-func docutils literal "><span class="pre">init_process_group()</span></code> 调用相同的文件路径/名称之前清理该文件。</p>
<p>注意，在最新发布的分布式包中不再支持自动排名分配，并且 <code class="docutils literal "><span class="pre">group_name</span></code> 也已弃用。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>此方法假设文件系统支持使用 <code class="docutils literal "><span class="pre">fcntl</span></code> 进行锁定 - 大多数本地系统和 NFS 都支持。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>此方法始终会创建文件，并尽力在程序结束时清理并删除文件。换句话说，每次使用文件初始化方法进行初始化时，都需要一个全新的空文件才能成功。如果再次使用之前初始化（未清理）的同一文件，这将是意外的行为，通常会导致死锁和失败。因此，尽管此方法会尽力清理文件，但如果自动删除失败，您有责任确保在训练结束时删除文件，以防止在下次使用时再次使用同一文件。如果您计划多次在同一个文件名上调用 <code class="xref py py-func docutils literal "><span class="pre">init_process_group()</span></code> ，这尤为重要。换句话说，如果文件未被删除/清理，并且您再次在文件上调用 <code class="xref py py-func docutils literal "><span class="pre">init_process_group()</span></code> ，则预期会出现失败。这里的经验法则是，确保每次调用 <code class="xref py py-func docutils literal "><span class="pre">init_process_group()</span></code> 时文件不存在或为空。</p>
</div>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="c1"># rank should always be specified</span>
<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">'file:///mnt/nfs/sharedfile'</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="environment-variable-initialization">
<h3>环境变量初始化</h3>
<p>此方法将从环境变量中读取配置，允许用户完全自定义信息获取方式。需要设置的变量包括：</p>
<ul class="simple">
<li><p> <code class="docutils literal "><span class="pre">MASTER_PORT</span></code> - 必需；必须在机器上具有排名 0 的空闲端口</p></li>
<li><p> <code class="docutils literal "><span class="pre">MASTER_ADDR</span></code> - 必需（排名 0 除外）；排名 0 节点的地址</p></li>
<li><p> <code class="docutils literal "><span class="pre">WORLD_SIZE</span></code> - 必需；可以在此处设置，也可以在 init 函数调用中设置</p></li>
<li><p> <code class="docutils literal "><span class="pre">RANK</span></code> - 必需项；可以在此处设置，也可以在 init 函数调用时设置</p></li>
</ul>
<p>将使用排名为 0 的机器来设置所有连接。</p>
<p>这是默认方法，意味着 <code class="docutils literal "><span class="pre">init_method</span></code> 不需要指定（或可以是 <code class="docutils literal "><span class="pre">env://</span></code> ）。</p>
</section>
</section>
<section id="post-initialization">
<h2>初始化后 ¶</h2>
<p>运行 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.init_process_group()</span></code> 后，可以使用以下功能。要检查进程组是否已经初始化，请使用 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.is_initialized()</span></code> 。</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.Backend">
class torch.distributed.Backend(name)[source][source]</dt>
<dd><p>一个类似于枚举的后端类。</p>
<p>可用后端：GLOO、NCCL、UCC、MPI、XCCL 以及其他已注册后端。</p>
<p>该类的值是全小写的字符串，例如， <code class="docutils literal "><span class="pre">"gloo"</span></code> 。它们可以作为属性访问，例如， <code class="docutils literal "><span class="pre">Backend.NCCL</span></code> 。</p>
<p>该类可以直接调用以解析字符串，例如， <code class="docutils literal "><span class="pre">Backend(backend_str)</span></code> 将检查 <code class="docutils literal "><span class="pre">backend_str</span></code> 是否有效，如果是，则返回解析后的全小写字符串。它也接受大写字符串，例如， <code class="docutils literal "><span class="pre">Backend("GLOO")</span></code> 返回 <code class="docutils literal "><span class="pre">"gloo"</span></code> 。</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>条目 <code class="docutils literal "><span class="pre">Backend.UNDEFINED</span></code> 存在，但仅用作某些字段的初始值。用户不应直接使用它或假设其存在。</p>
</div>
<dl class="field-list simple">
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Backend.register_backend">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">register_backend</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">func</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extended_api</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">devices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#Backend.register_backend"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L302"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.Backend.register_backend" title="Permalink to this definition">¶</a></dt>
<dd><p>使用给定的名称和实例化函数注册新的后端。</p>
<p>此类方法由第三方 <code class="docutils literal "><span class="pre">ProcessGroup</span></code> 扩展用于注册新的后端。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>name (str) – <code class="docutils literal "><span class="pre">ProcessGroup</span></code> 扩展的后端名称。它应与 <code class="docutils literal "><span class="pre">init_process_group()</span></code> 中的名称匹配。</p></li>
<li><p>func (函数) – 实例化后端的函数处理器。该函数应在后端扩展中实现，并接受四个参数，包括 <code class="docutils literal "><span class="pre">store</span></code> 、 <code class="docutils literal "><span class="pre">rank</span></code> 、 <code class="docutils literal "><span class="pre">world_size</span></code> 和 <code class="docutils literal "><span class="pre">timeout</span></code> 。</p></li>
<li><p>extended_api (bool, 可选) – 后端是否支持扩展参数结构。默认： <code class="docutils literal "><span class="pre">False</span></code> 。如果设置为 <code class="docutils literal "><span class="pre">True</span></code> ，后端将获取 <code class="docutils literal "><span class="pre">c10d::DistributedBackendOptions</span></code> 的实例，以及后端实现定义的进程组选项对象。</p></li>
<li><p>device (str 或 str 列表，可选) – 此后端支持的设备类型，例如“cpu”，“cuda”等。如果为 None，则假设同时支持“cpu”和“cuda”。</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>对第三方后端的支持是实验性的，可能随时更改。</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.get_backend">
torch.distributed.get_backend(group=None)[source][source]</dt>
<dd><p>返回给定进程组的后端。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><p>group (ProcessGroup, 可选) – 要工作的进程组。默认为通用主进程组。如果指定了另一个特定组，调用进程必须是 <code class="xref py py-attr docutils literal "><span class="pre">group</span></code> 的成员。</p>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>给定进程组的后端作为小写字符串。</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.distributed.Backend" title="torch.distributed.distributed_c10d.Backend"><em>后端</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.get_rank">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">get_rank</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#get_rank"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L2271"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.get_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>返回当前进程在提供的 <code class="docutils literal "><span class="pre">group</span></code> 中的排名，默认情况下返回。</p>
<p>排名是分配给分布式进程组中每个进程的唯一标识符。它们始终是连续的整数，范围从 0 到 <code class="docutils literal "><span class="pre">world_size</span></code> 。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><p>group (进程组，可选) – 要工作的进程组。如果为 None，则使用默认进程组。</p>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>进程组的等级为-1，如果不是该组的一部分</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.get_world_size">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">get_world_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#get_world_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L2298"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.get_world_size" title="Permalink to this definition">¶</a></dt>
<dd><p>返回当前进程组中的进程数量。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><p>group (进程组，可选) – 要工作的进程组。如果为 None，则使用默认进程组。</p>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>进程组的世界大小为-1，如果不属于该组</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="shutdown">
<h2>关闭</h2>
<p>在退出时通过调用 <code class="xref py py-func docutils literal "><span class="pre">destroy_process_group()</span></code> 来清理资源是很重要的。</p>
<p>最简单的模式是在训练脚本中不再需要通信的地方，通常在 main()函数的末尾，通过调用 <code class="xref py py-func docutils literal "><span class="pre">destroy_process_group()</span></code> 来销毁每个进程组和后端，对于组参数使用默认值 None。这个调用应该在每个训练进程中只执行一次，而不是在外部进程启动器级别执行。</p>
<p>如果在超时时间内不是所有进程在进程组中调用 <code class="xref py py-func docutils literal "><span class="pre">destroy_process_group()</span></code> ，尤其是在应用程序中有多个进程组时（例如，对于 N-D 并行），则可能在退出时挂起。这是因为 ProcessGroupNCCL 的析构函数调用 ncclCommAbort，它必须集体调用，但如果由 Python 的 GC 调用，则调用 ProcessGroupNCCL 的析构函数的顺序是不确定的。调用 <code class="xref py py-func docutils literal "><span class="pre">destroy_process_group()</span></code> 有助于确保 ncclCommAbort 在各个进程中的调用顺序一致，并避免在 ProcessGroupNCCL 的析构函数中调用 ncclCommAbort。</p>
<section id="reinitialization">
<h3>重初始化</h3>
<p>可以使用 destroy_process_group 来销毁单个进程组。一个用例可能是容错训练，其中进程组可能在运行时被销毁然后重新初始化。在这种情况下，在调用 destroy 之后和随后初始化之前，使用除 torch.distributed primitives 之外的其他方式同步训练进程至关重要。由于实现这种同步的难度，这种行为目前不受支持/未经验证，被视为已知问题。如果这是阻碍您的用例，请提交 github 问题或 RFC。</p>
</section>
</section>
<hr class="docutils">
<section id="groups">
<h2>组</h2>
<p>默认情况下，集体操作在默认组（也称为世界）上运行，并要求所有进程进入分布式函数调用。然而，某些工作负载可以从更细粒度的通信中受益。这时分布式组就派上用场了。可以使用 <code class="xref py py-func docutils literal "><span class="pre">new_group()</span></code> 函数创建新组，包含所有进程的任意子集。它返回一个不可见的组句柄，可以作为 <code class="docutils literal "><span class="pre">group</span></code> 参数传递给所有集体（集体是在某些已知编程模式中交换信息的分布式函数）。</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.new_group">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">new_group</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ranks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pg_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_local_synchronization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_desc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#new_group"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L4973"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.new_group" title="Permalink to this definition">¶</a></dt>
<dd><p>创建一个新的分布式组。</p>
<p>此函数要求主组中的所有进程（即所有属于分布式作业的进程）进入此函数，即使它们不是组的成员。此外，所有进程应按相同顺序创建组。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>安全并发使用：在使用带有 <code class="docutils literal "><span class="pre">NCCL</span></code> 后端的多个进程组时，用户必须确保跨节点集体操作的执行顺序全局一致。</p>
<p>如果进程内的多个线程发出集体操作，则需要显式同步以确保顺序一致。</p>
<p>当使用 torch.distributed 通信 API 的异步变体时，会返回一个工作对象，并将通信内核入队到单独的 CUDA 流中，允许通信和计算的重叠。一旦在一个进程组中发出一个或多个异步操作，它们必须通过调用 work.wait() 与其他 CUDA 流同步，然后再使用另一个进程组。</p>
<p>更多详情请参阅《同时使用多个 NCCL 通信器的使用方法》。</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>ranks (list[int]) – 群组成员的等级列表。如果为 <code class="docutils literal "><span class="pre">None</span></code> ，则将设置为所有等级。默认为 <code class="docutils literal "><span class="pre">None</span></code> 。</p></li>
<li><p>timeout (timedelta, optional) – 详细信息和默认值请参阅 init_process_group。</p></li>
<li><p>backend (str or Backend, optional) – 要使用的后端。根据构建时配置，有效值是 <code class="docutils literal "><span class="pre">gloo</span></code> 和 <code class="docutils literal "><span class="pre">nccl</span></code> 。默认使用与全局组相同的后端。此字段应作为小写字符串提供（例如， <code class="docutils literal "><span class="pre">"gloo"</span></code> ），也可以通过 <code class="xref py py-class docutils literal "><span class="pre">Backend</span></code> 属性访问（例如， <code class="docutils literal "><span class="pre">Backend.GLOO</span></code> ）。如果传入 <code class="docutils literal "><span class="pre">None</span></code> ，则将使用对应于默认进程组的后端。默认为 <code class="docutils literal "><span class="pre">None</span></code> 。</p></li>
<li><p>pg_options (ProcessGroupOptions, optional) – 指定在构建特定进程组时需要传递的附加选项的进程组选项。例如，对于 <code class="docutils literal "><span class="pre">nccl</span></code> 后端，可以指定 <code class="docutils literal "><span class="pre">is_high_priority_stream</span></code> 以使进程组能够获取高优先级的 CUDA 流。有关配置 nccl 的其他可用选项，请参阅 https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t</p></li>
<li><p>使用本地同步（bool，可选）- 在进程组创建结束时执行组内屏障。与之前不同，非成员 rank 无需调用 API 也不需要加入屏障。</p></li>
<li><p>group_desc（str，可选）- 描述进程组的字符串。</p></li>
<li><p>device_id（torch.device，可选）- 将此进程“绑定”到单个、特定的设备上，如果提供此字段，new_group 调用将尝试立即为该设备初始化通信后端。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>可用于集体调用或 GroupMember.NON_GROUP_MEMBER 的分布式组的句柄，如果 rank 不是 <code class="docutils literal "><span class="pre">ranks</span></code> 的一部分。</p>
</dd>
</dl>
<p>注意：use_local_synchronization 与 MPI 不兼容。</p>
<p>注意：当 use_local_synchronization=True 时，在较大的集群和较小的进程组中可能会显著提高速度，但必须小心，因为它会改变集群的行为，因为非成员排名不会加入 group barrier()。</p>
<p>注意：当每个排名创建多个重叠的进程组时，use_local_synchronization=True 可能会导致死锁。为了避免这种情况，请确保所有排名遵循相同的全局创建顺序。</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.get_group_rank">
torch.distributed.get_group_rank(group, global_rank)[source][source]</dt>
<dd><p>将全局排名转换为组排名。</p>
<p> <code class="docutils literal "><span class="pre">global_rank</span></code> 必须是 <code class="docutils literal "><span class="pre">group</span></code> 的部分，否则会引发 RuntimeError。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>组（ProcessGroup）- 用于查找相对排名的 ProcessGroup。</p></li>
<li><p>global_rank（int）- 要查询的全局排名。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p> <code class="docutils literal "><span class="pre">global_rank</span></code> 相对于 <code class="docutils literal "><span class="pre">group</span></code> 的组别排名</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a></p>
</dd>
</dl>
<p>注意。在默认进程组上调用此函数返回恒等值</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.get_global_rank">
torch.distributed.get_global_rank(group, group_rank)[source][source]</dt>
<dd><p>将组别排名转换为全局排名。</p>
<p> <code class="docutils literal "><span class="pre">group_rank</span></code> 必须是组的一部分，否则会引发 RuntimeError。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>group (ProcessGroup) – 要查找全局排名的 ProcessGroup。</p></li>
<li><p>group_rank (int) – 要查询的组排名。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p> <code class="docutils literal "><span class="pre">group_rank</span></code> 相对于 <code class="docutils literal "><span class="pre">group</span></code> 的全局排名。</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a></p>
</dd>
</dl>
<p>注意：在默认进程组上调用此函数返回恒等映射</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.get_process_group_ranks">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">get_process_group_ranks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">group</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#get_process_group_ranks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L1076"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.get_process_group_ranks" title="Permalink to this definition">¶</a></dt>
<dd><p>获取与 <code class="docutils literal "><span class="pre">group</span></code> 关联的所有 rank。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><p>group (进程组) – 获取所有 rank 的进程组。</p>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>全球排名列表，按组排名排序。</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[int]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="devicemesh">
<h2>设备网格</h2>
<p>设备网格是一种高级抽象，用于管理进程组（或 NCCL 通信器）。它允许用户轻松创建跨节点和节点内进程组，无需担心如何为不同的子进程组正确设置排名，并有助于轻松管理这些分布式进程组。可以使用 <code class="xref py py-func docutils literal "><span class="pre">init_device_mesh()</span></code> 函数创建新的设备网格，其中网格形状描述了设备拓扑。</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.device_mesh.DeviceMesh">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.device_mesh.</span></span><span class="sig-name descname"><span class="pre">DeviceMesh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mesh</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mesh_dim_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_init_backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/device_mesh.html#DeviceMesh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/device_mesh.py#L383"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.device_mesh.DeviceMesh" title="Permalink to this definition">¶</a></dt>
<dd><p>DeviceMesh 表示设备网格，其中设备的布局可以用 n 维维度数组表示，n 维数组的每个值都是默认进程组 rank 的全局 ID。</p>
<p>DeviceMesh 可用于描述集群中设备的布局，并在集群内部设备列表之间作为通信的代理。</p>
<p>DeviceMesh 可以用作上下文管理器。</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>DeviceMesh 遵循 SPMD 编程模型，这意味着相同的 PyTorch Python 程序在集群中的所有进程/排名上运行。因此，用户需要确保网格数组（描述设备布局）在所有排名上应保持一致。不一致的网格将导致静默挂起。</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>device_type（字符串）- 网格的设备类型。目前支持：“cpu”，“cuda/cuda-like”。</p></li>
<li><p>mesh（ndarray）- 一个多维数组或整数张量，描述设备的布局，其中 ID 是默认进程组的全局 ID。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>代表设备布局的 <code class="xref py py-class docutils literal "><span class="pre">DeviceMesh</span></code> 对象。</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.distributed.device_mesh.DeviceMesh" title="torch.distributed.device_mesh.DeviceMesh">设备网格</a></p>
</dd>
</dl>
<p>以下程序以 SPMD 方式在每个进程/排名上运行。在这个例子中，我们有 2 个主机，每个主机有 4 个 GPU。对网格的第一个维度的归约将跨越列（0, 4）..和（3, 7），对网格的第二个维度的归约将跨越行（0, 1, 2, 3）和（4, 5, 6, 7）。</p>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.device_mesh</span> <span class="kn">import</span> <span class="n">DeviceMesh</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Initialize device mesh as (2, 4) to represent the topology</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># of cross-host(dim 0), and within-host (dim 1).</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mesh</span> <span class="o">=</span> <span class="n">DeviceMesh</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]])</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.device_mesh.DeviceMesh.from_group">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_group</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">group</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mesh_dim_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/device_mesh.html#DeviceMesh.from_group"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/device_mesh.py#L793"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.device_mesh.DeviceMesh.from_group" title="Permalink to this definition">¶</a></dt>
<dd><p>从现有的 <code class="xref py py-class docutils literal "><span class="pre">ProcessGroup</span></code> 或现有 <code class="xref py py-class docutils literal "><span class="pre">ProcessGroup</span></code> 的列表中构建 <code class="xref py py-class docutils literal "><span class="pre">DeviceMesh</span></code> 和 <code class="docutils literal "><span class="pre">device_type</span></code> 。</p>
<p>构建的设备网格的维度数等于传入的组数。例如，如果传入一个进程组，则生成的 DeviceMesh 是一个一维网格。如果传入 2 个进程组列表，则生成的 DeviceMesh 是一个二维网格。</p>
<p>如果传入多个组，则必须提供 <code class="docutils literal "><span class="pre">mesh</span></code> 和 <code class="docutils literal "><span class="pre">mesh_dim_names</span></code> 参数。传入的进程组的顺序决定了网格的拓扑结构。例如，第一个进程组将是 DeviceMesh 的 0 维。传入的网格张量必须具有与传入的进程组数相同的维度数，并且网格张量中维度的顺序必须与传入的进程组顺序匹配。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>group（进程组或进程组列表）- 已存在的 ProcessGroup 或已存在的 ProcessGroup 列表。</p></li>
<li><p>device_type（字符串）- 网格的设备类型。目前支持：“cpu”，“cuda/cuda-like”。不允许传入带有 GPU 索引的设备类型，例如“cuda:0”。</p></li>
<li><p>mesh（torch.Tensor 或 ArrayLike，可选）- 描述设备布局的多维数组或整数张量，其中 ID 是默认进程组的全局 ID。默认值为 None。</p></li>
<li><p>mesh_dim_names（tuple[str]，可选）- 将分配给描述设备布局的多维数组每个维度的网格维度名称的元组。其长度必须与 mesh_shape 的长度匹配。mesh_dim_names 中的每个字符串必须是唯一的。默认值为 None。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>代表设备布局的 <code class="xref py py-class docutils literal "><span class="pre">DeviceMesh</span></code> 对象。</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.distributed.device_mesh.DeviceMesh" title="torch.distributed.device_mesh.DeviceMesh">DeviceMesh</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.device_mesh.DeviceMesh.get_all_groups">
<span class="sig-name descname"><span class="pre">get_all_groups</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/device_mesh.html#DeviceMesh.get_all_groups"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/device_mesh.py#L784"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.device_mesh.DeviceMesh.get_all_groups" title="Permalink to this definition">¶</a></dt>
<dd><p>返回所有网格维度的 ProcessGroups 列表。</p>
<dl class="field-list simple">
<dt class="field-odd">返回<span class="colon">:</span></dt>
<dd class="field-odd"><p>一个包含 <code class="xref py py-class docutils literal "><span class="pre">ProcessGroup</span></code> 对象的列表。</p>
</dd>
<dt class="field-even">返回类型<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)">list</a>[torch.distributed.distributed_c10d.ProcessGroup]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.device_mesh.DeviceMesh.get_coordinate">
<span class="sig-name descname"><span class="pre">get_coordinate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/device_mesh.html#DeviceMesh.get_coordinate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/device_mesh.py#L952"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.device_mesh.DeviceMesh.get_coordinate" title="Permalink to this definition">¶</a></dt>
<dd><p>返回此秩相对于网格所有维度的相对索引。如果此秩不属于网格，则返回 None。</p>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.13)"><em>Optional</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)">list</a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.device_mesh.DeviceMesh.get_group">
get_group(mesh_dim=None)[来源][来源] ¶</dt>
<dd><p>返回由 mesh_dim 指定的单个 ProcessGroup，如果未指定 mesh_dim 且 DeviceMesh 为 1 维，则返回网格中的唯一 ProcessGroup。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>mesh_dim (str/python:int, 可选) – 它可以是网格维度的名称或索引</p></li>
<li><p>None.（网格维度的。默认为） –</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>一个 <code class="xref py py-class docutils literal "><span class="pre">ProcessGroup</span></code> 对象。</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>流程组</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.device_mesh.DeviceMesh.get_local_rank">
get_local_rank(mesh_dim=None)[来源][来源] ¶</dt>
<dd><p>返回给定 DeviceMesh 的 mesh_dim 的本地排名。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>mesh_dim (str/python:int, 可选) – 它可以是网格维度的名称或索引</p></li>
<li><p>None. (网格维度的。默认是) –</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>整数表示局部排名。</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a></p>
</dd>
</dl>
<p>以下程序以 SPMD 方式在每个进程/排名上运行。在这个例子中，我们有 2 个主机，每个主机有 4 个 GPU。在排名 0、1、2、3 上调用 mesh_2d.get_local_rank(mesh_dim=0)将返回 0。在排名 4、5、6、7 上调用 mesh_2d.get_local_rank(mesh_dim=0)将返回 1。在排名 0、4 上调用 mesh_2d.get_local_rank(mesh_dim=1)将返回 0。在排名 1、5 上调用 mesh_2d.get_local_rank(mesh_dim=1)将返回 1。在排名 2、6 上调用 mesh_2d.get_local_rank(mesh_dim=1)将返回 2。在排名 3、7 上调用 mesh_2d.get_local_rank(mesh_dim=1)将返回 3。</p>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.distributed.device_mesh</span> <span class="kn">import</span> <span class="n">DeviceMesh</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Initialize device mesh as (2, 4) to represent the topology</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># of cross-host(dim 0), and within-host (dim 1).</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mesh</span> <span class="o">=</span> <span class="n">DeviceMesh</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.device_mesh.DeviceMesh.get_rank">
<span class="sig-name descname"><span class="pre">get_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/device_mesh.html#DeviceMesh.get_rank"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/device_mesh.py#L904"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.device_mesh.DeviceMesh.get_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>返回当前的全局排名。</p>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="point-to-point-communication">
<h2>点对点通信</h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.send">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">send</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_dst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#send"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L2407"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.send" title="Permalink to this definition">¶</a></dt>
<dd><p>同步发送张量。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>NCCL 后端不支持 <code class="docutils literal "><span class="pre">tag</span></code> 。</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>张量（Tensor）- 要发送的张量。</p></li>
<li><p>dst（整数）- 全局进程组上的目标 rank（无论 <code class="docutils literal "><span class="pre">group</span></code> 参数）。目标 rank 不应与当前进程的 rank 相同。</p></li>
<li><p>group（进程组，可选）- 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>tag（整数，可选）- 用于匹配发送与远程接收的标签</p></li>
<li><p>group_dst（整数，可选）- 目标 rank 在 <code class="docutils literal "><span class="pre">group</span></code> 上。不能同时指定 <code class="docutils literal "><span class="pre">dst</span></code> 和 <code class="docutils literal "><span class="pre">group_dst</span></code> 。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.recv">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">recv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_src</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#recv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L2439"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.recv" title="Permalink to this definition">¶</a></dt>
<dd><p>同步接收张量。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>NCCL 后端不支持 <code class="docutils literal "><span class="pre">tag</span></code> 。</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>张量（Tensor）- 用接收到的数据填充的张量。</p></li>
<li><p>src（int，可选）- 全局进程组上的源 rank（无论 <code class="docutils literal "><span class="pre">group</span></code> 参数）。如果未指定，将从任何进程接收。</p></li>
<li><p>group（进程组，可选）- 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>tag（整数，可选）- 用于匹配接收与远程发送的标签</p></li>
<li><p>group_src（整数，可选）- 目标 rank 在 <code class="docutils literal "><span class="pre">group</span></code> 上。同时指定 <code class="docutils literal "><span class="pre">src</span></code> 和 <code class="docutils literal "><span class="pre">group_src</span></code> 是无效的。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>发送者 rank 为-1，如果不在组内</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a></p>
</dd>
</dl>
</dd></dl>

<p> <code class="xref py py-func docutils literal "><span class="pre">isend()</span></code> 和 <code class="xref py py-func docutils literal "><span class="pre">irecv()</span></code> 使用时返回分布式请求对象。通常，此对象类型未指定，因为它们不应手动创建，但它们保证支持两种方法：</p>
<ul class="simple">
<li><p> <code class="docutils literal "><span class="pre">is_completed()</span></code> - 如果操作已完成则返回 True</p></li>
<li><p> <code class="docutils literal "><span class="pre">wait()</span></code> - 将阻塞进程直到操作完成。 <code class="docutils literal "><span class="pre">is_completed()</span></code> 一旦返回将保证返回 True。</p></li>
</ul>
<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.isend">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">isend</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_dst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#isend"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L2317"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.isend" title="Permalink to this definition">¶</a></dt>
<dd><p>异步发送张量。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>在请求完成之前修改 <code class="docutils literal "><span class="pre">tensor</span></code> 会导致未定义的行为。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p> <code class="docutils literal "><span class="pre">tag</span></code> 不支持 NCCL 后端。</p>
</div>
<p>与阻塞的 send 不同，isend 允许 src == dst rank，即发送到自身。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>张量（Tensor）- 要发送的张量。</p></li>
<li><p>dst（整数）- 在全局进程组上的目标 rank（无论 <code class="docutils literal "><span class="pre">group</span></code> 参数）</p></li>
<li><p>group（进程组，可选）- 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>tag（整数，可选）- 标签以匹配发送与远程接收。</p></li>
<li><p>group_dst (int, 可选) – 目标秩在 <code class="docutils literal "><span class="pre">group</span></code> 上。同时指定 <code class="docutils literal "><span class="pre">dst</span></code> 和 <code class="docutils literal "><span class="pre">group_dst</span></code> 是无效的</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>分布式请求对象。如果不是组的一部分则为 None</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p>可选[工作]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.irecv">
torch.distributed.irecv(tensor, src=None, group=None, tag=0, group_src=None)[源代码][源代码]</dt>
<dd><p>异步接收张量。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>NCCL 后端不支持 <code class="docutils literal "><span class="pre">tag</span></code> 。</p>
</div>
<p>与 recv 不同，recv 是阻塞的，irecv 允许 src == dst rank，即从自身接收。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>张量（Tensor）- 用接收到的数据填充的张量。</p></li>
<li><p>源（int，可选）- 全局进程组上的源排名（无论是否指定 <code class="docutils literal "><span class="pre">group</span></code> 参数）。如果未指定，将接收来自任何进程的数据。</p></li>
<li><p>组（ProcessGroup，可选）- 要工作的进程组。如果为 None，将使用默认进程组。</p></li>
<li><p>标签（int，可选）- 用于匹配接收与远程发送的标签</p></li>
<li><p>目标_rank（int，可选）- 在 <code class="docutils literal "><span class="pre">group</span></code> 上的目标排名。不能同时指定 <code class="docutils literal "><span class="pre">src</span></code> 和 <code class="docutils literal "><span class="pre">group_src</span></code> 。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>分布式请求对象。如果不是组的一部分则为空</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p>可选[工作]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.send_object_list">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">send_object_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">object_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_dst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#send_object_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L3184"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.send_object_list" title="Permalink to this definition">¶</a></dt>
<dd><p>以同步方式发送可序列化的对象列表。</p>
<p>与 <code class="xref py py-func docutils literal "><span class="pre">send()</span></code> 类似，但可以传递 Python 对象。注意， <code class="docutils literal "><span class="pre">object_list</span></code> 中的所有对象都必须是可序列化的，才能发送。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>object_list (List[Any]) – 要发送的输入对象列表。每个对象都必须是可序列化的。接收方必须提供大小相等的列表。</p></li>
<li><p>dst (int) – 发送 <code class="docutils literal "><span class="pre">object_list</span></code> 的目标排名。目标排名基于全局进程组（不考虑 <code class="docutils literal "><span class="pre">group</span></code> 参数）</p></li>
<li><p>group (Optional[ProcessGroup]) – (ProcessGroup, 可选): 要工作的进程组。如果为 None，则使用默认进程组。默认为 <code class="docutils literal "><span class="pre">None</span></code> 。</p></li>
<li><p>device ( <code class="docutils literal "><span class="pre">torch.device</span></code> , 可选) – 如果不为 None，则对象将被序列化并转换为张量，在发送前移动到 <code class="docutils literal "><span class="pre">device</span></code> 。默认为 <code class="docutils literal "><span class="pre">None</span></code> 。</p></li>
<li><p>group_dst (int, 可选) – 在 <code class="docutils literal "><span class="pre">group</span></code> 上的目标排名。必须指定 <code class="docutils literal "><span class="pre">dst</span></code> 和 <code class="docutils literal "><span class="pre">group_dst</span></code> 中的一个，但不能同时指定两个</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="docutils literal "><span class="pre">None</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>对于基于 NCCL 的过程组，对象内部张量表示必须在通信之前移动到 GPU 设备上。在这种情况下，使用的设备由 <code class="docutils literal "><span class="pre">torch.cuda.current_device()</span></code> 指定，并且用户有责任确保这一点，以便每个 rank 都有一个单独的 GPU，通过 <code class="docutils literal "><span class="pre">torch.cuda.set_device()</span></code> 。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p> <code class="xref py py-func docutils literal "><span class="pre">send_object_list()</span></code> 隐式使用 <code class="docutils literal "><span class="pre">pickle</span></code> 模块，已知此模块不安全。可以构造恶意的 pickle 数据，在反序列化过程中执行任意代码。仅在使用您信任的数据时调用此函数。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>使用 GPU 张量调用 <code class="xref py py-func docutils literal "><span class="pre">send_object_list()</span></code> 不受良好支持且效率低下，因为它会涉及 GPU 到 CPU 的传输，因为张量将被序列化。请考虑使用 <code class="xref py py-func docutils literal "><span class="pre">send()</span></code> 代替。</p>
</div>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: Process group initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assumes backend is not NCCL</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Assumes world_size of 2.</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"foo"</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">}]</span> <span class="c1"># any picklable object</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">dist</span><span class="o">.</span><span class="n">send_object_list</span><span class="p">(</span><span class="n">objects</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">dist</span><span class="o">.</span><span class="n">recv_object_list</span><span class="p">(</span><span class="n">objects</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">objects</span>
<span class="go">['foo', 12, {1: 2}]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.recv_object_list">
torch.distributed.recv_object_list(object_list, src=None, group=None, device=None, group_src=None)[source][source]</dt>
<dd><p>同步接收可序列化的对象。</p>
<p>与 <code class="xref py py-func docutils literal "><span class="pre">recv()</span></code> 类似，但可以接收 Python 对象。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>object_list (List[Any]) – 要接收的对象列表。必须提供与发送列表大小相等的尺寸列表。</p></li>
<li><p>src (int, 可选) – 接收 <code class="docutils literal "><span class="pre">object_list</span></code> 的源 rank。源 rank 基于全局进程组（无论 <code class="docutils literal "><span class="pre">group</span></code> 参数如何）。如果设置为 None，则从任何 rank 接收。默认为 <code class="docutils literal "><span class="pre">None</span></code> 。</p></li>
<li><p>group (Optional[ProcessGroup]) – (ProcessGroup, optional): 要工作的进程组。如果为 None，则使用默认进程组。默认为 <code class="docutils literal "><span class="pre">None</span></code> 。</p></li>
<li><p>device ( <code class="docutils literal "><span class="pre">torch.device</span></code> , optional) – 如果不为 None，则在设备上接收。默认为 <code class="docutils literal "><span class="pre">None</span></code> 。</p></li>
<li><p>group_src (int, optional) – 在 <code class="docutils literal "><span class="pre">group</span></code> 上的目标排名。如果指定了 <code class="docutils literal "><span class="pre">src</span></code> 和 <code class="docutils literal "><span class="pre">group_src</span></code> ，则无效。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>发送者排名。如果排名不是组的一部分，则为 -1。如果排名是组的一部分， <code class="docutils literal "><span class="pre">object_list</span></code> 将包含来自 <code class="docutils literal "><span class="pre">src</span></code> 排名的发送对象。</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>对于基于 NCCL 的过程组，对象内部张量表示必须在通信之前移动到 GPU 设备上。在这种情况下，使用的设备由 <code class="docutils literal "><span class="pre">torch.cuda.current_device()</span></code> 指定，并且用户有责任确保这一点，以便每个 rank 都有一个单独的 GPU，通过 <code class="docutils literal "><span class="pre">torch.cuda.set_device()</span></code> 。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p> <code class="xref py py-func docutils literal "><span class="pre">recv_object_list()</span></code> 隐式使用 <code class="docutils literal "><span class="pre">pickle</span></code> 模块，已知此模块不安全。可以构造恶意的 pickle 数据，在反序列化过程中执行任意代码。仅在使用您信任的数据时调用此函数。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>使用 GPU 张量调用 <code class="xref py py-func docutils literal "><span class="pre">recv_object_list()</span></code> 不受良好支持且效率低下，因为它会涉及 GPU 到 CPU 的传输，因为张量会被序列化。请考虑使用 <code class="xref py py-func docutils literal "><span class="pre">recv()</span></code> 。</p>
</div>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: Process group initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assumes backend is not NCCL</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Assumes world_size of 2.</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"foo"</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">}]</span> <span class="c1"># any picklable object</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">dist</span><span class="o">.</span><span class="n">send_object_list</span><span class="p">(</span><span class="n">objects</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">dist</span><span class="o">.</span><span class="n">recv_object_list</span><span class="p">(</span><span class="n">objects</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">objects</span>
<span class="go">['foo', 12, {1: 2}]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.batch_isend_irecv">
torch.distributed.batch_isend_irecv(p2p_op_list)[source][source]</dt>
<dd><p>异步发送或接收一批张量并返回请求列表。</p>
<p>处理 <code class="docutils literal "><span class="pre">p2p_op_list</span></code> 中的每个操作并返回相应的请求。目前支持 NCCL、Gloo 和 UCC 后端。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><p>p2p_op_list (list[torch.distributed.distributed_c10d.P2POp]) – 一个点对点操作列表（每个操作器的类型为 <code class="docutils literal "><span class="pre">torch.distributed.P2POp</span></code> ）。列表中 isend/irecv 的顺序很重要，需要与远程端的相应 isend/irecv 匹配。</p>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>由调用 op_list 中相应操作返回的分布式请求对象列表。</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[torch.distributed.distributed_c10d.Work]</p>
</dd>
</dl>
<p class="rubric">示例</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">send_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recv_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">send_op</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">,</span> <span class="n">send_tensor</span><span class="p">,</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">world_size</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recv_op</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">P2POp</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">dist</span><span class="o">.</span><span class="n">irecv</span><span class="p">,</span> <span class="n">recv_tensor</span><span class="p">,</span> <span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">world_size</span><span class="p">)</span> <span class="o">%</span> <span class="n">world_size</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reqs</span> <span class="o">=</span> <span class="n">batch_isend_irecv</span><span class="p">([</span><span class="n">send_op</span><span class="p">,</span> <span class="n">recv_op</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">req</span> <span class="ow">in</span> <span class="n">reqs</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">req</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recv_tensor</span>
<span class="go">tensor([2, 3])     # Rank 0</span>
<span class="go">tensor([0, 1])     # Rank 1</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>注意，当使用 NCCL PG 后端调用此 API 时，用户必须使用 torch.cuda.set_device 设置当前 GPU 设备，否则可能会导致意外的挂起问题。</p>
<p>此外，如果此 API 是传递给 <code class="docutils literal "><span class="pre">dist.P2POp</span></code> 的 <code class="docutils literal "><span class="pre">group</span></code> 中的第一个集体调用，则 <code class="docutils literal "><span class="pre">group</span></code> 的所有等级必须参与此 API 调用；否则，行为未定义。如果此 API 调用不是 <code class="docutils literal "><span class="pre">group</span></code> 中的第一个集体调用，则允许仅涉及 <code class="docutils literal "><span class="pre">group</span></code> 子集等级的批处理 P2P 操作。</p>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.P2POp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">P2POp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">op</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">peer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_peer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#P2POp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L472"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.P2POp" title="Permalink to this definition">¶</a></dt>
<dd><p>一个用于构建 <code class="docutils literal "><span class="pre">batch_isend_irecv</span></code> 点对点操作的类。</p>
<p>此类构建 P2P 操作类型、通信缓冲区、对等方等级、进程组和标签。此类实例将被传递给 <code class="docutils literal "><span class="pre">batch_isend_irecv</span></code> 以进行点对点通信。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>op (Callable) – 一个用于向或从对等进程发送或接收数据的函数。 <code class="docutils literal "><span class="pre">op</span></code> 的类型为 <code class="docutils literal "><span class="pre">torch.distributed.isend</span></code> 或 <code class="docutils literal "><span class="pre">torch.distributed.irecv</span></code> 之一。</p></li>
<li><p>tensor (Tensor) – 要发送或接收的张量。</p></li>
<li><p>peer (int, optional) – 目标或源排名。</p></li>
<li><p>group (ProcessGroup, optional) – 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>标签（int，可选）- 用于匹配发送与接收的标签。</p></li>
<li><p>group_peer（int，可选）- 目标或源排名。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="synchronous-and-asynchronous-collective-operations">
<h2>同步和异步集体操作 ¶</h2>
<p>每个集体操作函数都支持以下两种操作，这取决于传递给集体操作的 <code class="docutils literal "><span class="pre">async_op</span></code> 标志的设置：</p>
<p>同步操作 - 默认模式，当 <code class="docutils literal "><span class="pre">async_op</span></code> 设置为 <code class="docutils literal "><span class="pre">False</span></code> 时。当函数返回时，保证执行集体操作。在 CUDA 操作的情况下，不能保证 CUDA 操作完成，因为 CUDA 操作是异步的。对于 CPU 集体操作，任何进一步调用集体操作输出的函数都将按预期行为。对于 CUDA 集体操作，在同一 CUDA 流上使用输出的函数调用将按预期行为。用户必须注意在不同流下运行时的同步问题。有关 CUDA 语义（如流同步）的详细信息，请参阅 CUDA 语义。请参阅下面的脚本，以查看 CPU 和 CUDA 操作在这些语义上的差异示例。</p>
<p>异步操作 - 当 <code class="docutils literal "><span class="pre">async_op</span></code> 设置为 True 时。集体操作函数返回一个分布式请求对象。通常，您不需要手动创建它，并且它保证支持两种方法：</p>
<ul class="simple">
<li><p> <code class="docutils literal "><span class="pre">is_completed()</span></code> - 在 CPU 集体操作的情况下，如果完成则返回 <code class="docutils literal "><span class="pre">True</span></code> 。在 CUDA 操作的情况下，如果操作已成功入队到 CUDA 流中，并且输出可以在默认流上使用而无需进一步同步，则返回 <code class="docutils literal "><span class="pre">True</span></code> 。</p></li>
<li><p> <code class="docutils literal "><span class="pre">wait()</span></code> - 在 CPU 集体操作的情况下，将阻塞进程直到操作完成。在 CUDA 集体操作的情况下，将阻塞当前活动的 CUDA 流直到操作完成（但不会阻塞 CPU）。</p></li>
<li><p> <code class="docutils literal "><span class="pre">get_future()</span></code> - 返回 <code class="docutils literal "><span class="pre">torch._C.Future</span></code> 对象。支持 NCCL，也支持 GLOO 和 MPI 上的大多数操作，但除了对等操作外。注意：随着我们继续采用 Futures 和合并 API， <code class="docutils literal "><span class="pre">get_future()</span></code> 调用可能变得不再必要。</p></li>
</ul>
<p><strong>示例</strong></p>
<p>以下代码可以作为使用分布式集体操作时 CUDA 操作的语义参考。它显示了在使用不同 CUDA 流上的集体输出时显式同步的必要性：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="c1"># Code runs on each rank.</span>
<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">"nccl"</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">rank</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>
<span class="n">handle</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># Wait ensures the operation is enqueued, but not necessarily complete.</span>
<span class="n">handle</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
<span class="c1"># Using result on non-default stream.</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">default_stream</span><span class="p">())</span>
    <span class="n">output</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="c1"># if the explicit call to wait_stream was omitted, the output below will be</span>
    <span class="c1"># non-deterministically 1 or 101, depending on whether the allreduce overwrote</span>
    <span class="c1"># the value after the add completed.</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="collective-functions">
<h2>集合函数 ¶</h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.broadcast">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">broadcast</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_src</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#broadcast"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L2674"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.broadcast" title="Permalink to this definition">¶</a></dt>
<dd><p>将张量广播到整个组。</p>
<p> <code class="docutils literal "><span class="pre">tensor</span></code> 所有参与集体操作的所有进程中必须具有相同数量的元素。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>张量（Tensor）- 如果 <code class="docutils literal "><span class="pre">src</span></code> 是当前进程的秩，则发送的数据，否则用于保存接收数据的张量。</p></li>
<li><p>src（整数）- 在全局进程组上的源秩（无论 <code class="docutils literal "><span class="pre">group</span></code> 参数如何）。</p></li>
<li><p>group（ProcessGroup，可选）- 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>async_op（布尔值，可选）- 此操作是否应为异步操作。</p></li>
<li><p>group_src（int）- 在 <code class="docutils literal "><span class="pre">group</span></code> 上的源排名。必须指定 <code class="docutils literal "><span class="pre">group_src</span></code> 和 <code class="docutils literal "><span class="pre">src</span></code> 中的一个，但不能同时指定两个。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>异步工作处理，如果 async_op 设置为 True。如果没有 async_op 或者不是组的一部分，则为 None。</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.broadcast_object_list">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">broadcast_object_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">object_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_src</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#broadcast_object_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L3385"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.broadcast_object_list" title="Permalink to this definition">¶</a></dt>
<dd><p>将 <code class="docutils literal "><span class="pre">object_list</span></code> 中的可序列化对象广播到整个组。</p>
<p>与 <code class="xref py py-func docutils literal "><span class="pre">broadcast()</span></code> 相似，但可以传入 Python 对象。注意， <code class="docutils literal "><span class="pre">object_list</span></code> 中的所有对象都必须是可序列化的，以便进行广播。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>object_list (List[Any]) – 要广播的输入对象列表。每个对象都必须是可序列化的。只有 <code class="docutils literal "><span class="pre">src</span></code> 排名上的对象将被广播，但每个排名必须提供大小相等的列表。</p></li>
<li><p>src (int) – 要从哪个源排名广播 <code class="docutils literal "><span class="pre">object_list</span></code> 。源排名基于全局进程组（不考虑 <code class="docutils literal "><span class="pre">group</span></code> 参数）。</p></li>
<li><p>group (Optional[ProcessGroup]) – (ProcessGroup, 可选)：要工作的进程组。如果为 None，则使用默认进程组。默认为 <code class="docutils literal "><span class="pre">None</span></code> 。</p></li>
<li><p>设备（ <code class="docutils literal "><span class="pre">torch.device</span></code> ，可选）- 如果不为空，则将对象序列化并转换为张量，然后移动到 <code class="docutils literal "><span class="pre">device</span></code> 进行广播。默认为 <code class="docutils literal "><span class="pre">None</span></code> 。</p></li>
<li><p>group_src（int）- <code class="docutils literal "><span class="pre">group</span></code> 上的源秩。不得同时指定 <code class="docutils literal "><span class="pre">group_src</span></code> 和 <code class="docutils literal "><span class="pre">src</span></code> 中的一个，但不能同时都不指定。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p> <code class="docutils literal "><span class="pre">None</span></code> 。如果秩是组的一部分， <code class="docutils literal "><span class="pre">object_list</span></code> 将包含来自 <code class="docutils literal "><span class="pre">src</span></code> 秩的广播对象。</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>对于基于 NCCL 的进程组，在通信之前必须将对象的内部张量表示移动到 GPU 设备。在这种情况下，使用的设备由 <code class="docutils literal "><span class="pre">torch.cuda.current_device()</span></code> 给出，并且用户有责任确保这样设置，以便每个秩都有一个单独的 GPU，通过 <code class="docutils literal "><span class="pre">torch.cuda.set_device()</span></code> 。</p>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>注意，此 API 与 <code class="xref py py-func docutils literal "><span class="pre">broadcast()</span></code> 集体略有不同，因为它不提供 <code class="docutils literal "><span class="pre">async_op</span></code> 句柄，因此将是一个阻塞调用。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p> <code class="xref py py-func docutils literal "><span class="pre">broadcast_object_list()</span></code> 隐式使用 <code class="docutils literal "><span class="pre">pickle</span></code> 模块，已知此模块不安全。可以构造恶意 pickle 数据，在反序列化时执行任意代码。请仅使用您信任的数据调用此函数。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>使用 GPU 张量调用 <code class="xref py py-func docutils literal "><span class="pre">broadcast_object_list()</span></code> 不支持且效率低下，因为它会涉及 GPU 到 CPU 的传输，因为张量将被序列化。请考虑使用 <code class="xref py py-func docutils literal "><span class="pre">broadcast()</span></code> 代替。</p>
</div>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: Process group initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Assumes world_size of 3.</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"foo"</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">}]</span> <span class="c1"># any picklable object</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assumes backend is not NCCL</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">broadcast_object_list</span><span class="p">(</span><span class="n">objects</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">objects</span>
<span class="go">['foo', 12, {1: 2}]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.all_reduce">
torch.distributed.all_reduce(tensor, op=, group=None, async_op=False)[source][source]</dt>
<dd><p>在所有机器上以相同的方式减少张量数据，使所有机器都获得最终结果。</p>
<p>调用 <code class="docutils literal "><span class="pre">tensor</span></code> 后，在所有进程中将进行位运算相同。</p>
<p>支持复杂张量。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>张量（Tensor）- 集体的输入和输出。该函数在原地操作。</p></li>
<li><p>op（可选）- <code class="docutils literal "><span class="pre">torch.distributed.ReduceOp</span></code> 枚举中的值之一。指定用于逐元素减少的操作。</p></li>
<li><p>group（ProcessGroup，可选）- 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>async_op（bool，可选）- 此操作是否应为异步操作</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>如果 async_op 设置为 True，则为异步工作句柄。如果不是 async_op 或不是组的一部分，则为 None</p>
</dd>
</dl>
<p class="rubric">示例</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># All tensors below are of torch.int64 type.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We have 2 process groups, 2 ranks.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">"cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span>
<span class="go">tensor([1, 2], device='cuda:0') # Rank 0</span>
<span class="go">tensor([3, 4], device='cuda:1') # Rank 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span>
<span class="go">tensor([4, 6], device='cuda:0') # Rank 0</span>
<span class="go">tensor([4, 6], device='cuda:1') # Rank 1</span>
</pre></div>
</div>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># All tensors below are of torch.cfloat type.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We have 2 process groups, 2 ranks.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="p">,</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="n">j</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
<span class="gp">... </span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span>
<span class="go">tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0</span>
<span class="go">tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span>
<span class="go">tensor([4.+4.j, 6.+6.j], device='cuda:0') # Rank 0</span>
<span class="go">tensor([4.+4.j, 6.+6.j], device='cuda:1') # Rank 1</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.reduce">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">reduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dst=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op=&lt;RedOpType.SUM:</span> <span class="pre">0&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_dst=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#reduce"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L2883"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>在所有机器上对张量数据进行聚合。</p>
<p>只有进程编号为 <code class="docutils literal "><span class="pre">dst</span></code> 的进程将接收最终结果。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>tensor (Tensor) – 集合的输入和输出。该函数在原地操作。</p></li>
<li><p>dst (int) – 全局进程组上的目标排名（无论是否有 <code class="docutils literal "><span class="pre">group</span></code> 参数）</p></li>
<li><p>op (可选) – <code class="docutils literal "><span class="pre">torch.distributed.ReduceOp</span></code> 枚举值之一。指定用于逐元素减少的操作。</p></li>
<li><p>group (ProcessGroup，可选) – 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>async_op (bool，可选) – 此操作是否应为异步操作</p></li>
<li><p>group_dst (int) – 目标排名在 <code class="docutils literal "><span class="pre">group</span></code> 上。必须指定 <code class="docutils literal "><span class="pre">group_dst</span></code> 和 <code class="docutils literal "><span class="pre">dst</span></code> 中的一个，但不能同时指定两个。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>异步工作处理，如果 async_op 设置为 True。如果没有 async_op 或不属于该组，则为 None。</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.all_gather">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#all_gather"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L3641"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.all_gather" title="Permalink to this definition">¶</a></dt>
<dd><p>将整个组中的张量收集到一个列表中。</p>
<p>支持复杂且尺寸不均匀的张量。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>tensor_list（列表[Tensor]）- 输出列表。它应包含正确尺寸的张量，用于集体输出。支持尺寸不均匀的张量。</p></li>
<li><p>tensor（Tensor）- 要从当前进程广播的张量。</p></li>
<li><p>group（进程组，可选）- 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>async_op (bool, 可选) – 是否将此操作设置为异步操作</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>如果 async_op 设置为 True，则处理异步工作。如果没有 async_op 或不属于该组，则为 None</p>
</dd>
</dl>
<p class="rubric">示例</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># All tensors below are of torch.int64 dtype.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We have 2 process groups, 2 ranks.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">"cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">... </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_list</span>
<span class="go">[tensor([0, 0], device='cuda:0'), tensor([0, 0], device='cuda:0')] # Rank 0</span>
<span class="go">[tensor([0, 0], device='cuda:1'), tensor([0, 0], device='cuda:1')] # Rank 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span>
<span class="go">tensor([1, 2], device='cuda:0') # Rank 0</span>
<span class="go">tensor([3, 4], device='cuda:1') # Rank 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_list</span>
<span class="go">[tensor([1, 2], device='cuda:0'), tensor([3, 4], device='cuda:0')] # Rank 0</span>
<span class="go">[tensor([1, 2], device='cuda:1'), tensor([3, 4], device='cuda:1')] # Rank 1</span>
</pre></div>
</div>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># All tensors below are of torch.cfloat dtype.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We have 2 process groups, 2 ranks.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">... </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_list</span>
<span class="go">[tensor([0.+0.j, 0.+0.j], device='cuda:0'), tensor([0.+0.j, 0.+0.j], device='cuda:0')] # Rank 0</span>
<span class="go">[tensor([0.+0.j, 0.+0.j], device='cuda:1'), tensor([0.+0.j, 0.+0.j], device='cuda:1')] # Rank 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="p">,</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="n">j</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
<span class="gp">... </span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span>
<span class="go">tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0</span>
<span class="go">tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_list</span>
<span class="go">[tensor([1.+1.j, 2.+2.j], device='cuda:0'), tensor([3.+3.j, 4.+4.j], device='cuda:0')] # Rank 0</span>
<span class="go">[tensor([1.+1.j, 2.+2.j], device='cuda:1'), tensor([3.+3.j, 4.+4.j], device='cuda:1')] # Rank 1</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.all_gather_into_tensor">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">all_gather_into_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#all_gather_into_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L3736"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.all_gather_into_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>从所有排名收集张量并将它们放入单个输出张量中。</p>
<p>此函数要求每个进程上的张量大小必须相同。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>output_tensor（张量）- 用于容纳来自所有进程的张量元素的输出张量。它必须具有以下形式之一：（i）所有输入张量沿主维度的连接；有关“连接”的定义，请参阅 <code class="docutils literal "><span class="pre">torch.cat()</span></code> ；（ii）所有输入张量沿主维度的堆叠；有关“堆叠”的定义，请参阅 <code class="docutils literal "><span class="pre">torch.stack()</span></code> 。以下示例可能更好地解释所支持的输出形式。</p></li>
<li><p>input_tensor（张量）- 从当前进程收集的张量。与 <code class="docutils literal "><span class="pre">all_gather</span></code> API 不同，此 API 中的输入张量必须在所有进程中具有相同的大小。</p></li>
<li><p>group（进程组，可选）- 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>async_op (布尔值，可选) – 是否将此操作设置为异步操作</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>如果 async_op 设置为 True，则处理异步工作。如果没有 async_op 或不属于该组，则为 None</p>
</dd>
</dl>
<p class="rubric">示例</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># All tensors below are of torch.int64 dtype and on CUDA devices.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We have two ranks.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">"cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_in</span>
<span class="go">tensor([1, 2], device='cuda:0') # Rank 0</span>
<span class="go">tensor([3, 4], device='cuda:1') # Rank 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Output in concatenation form</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">world_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span><span class="n">tensor_out</span><span class="p">,</span> <span class="n">tensor_in</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_out</span>
<span class="go">tensor([1, 2, 3, 4], device='cuda:0') # Rank 0</span>
<span class="go">tensor([1, 2, 3, 4], device='cuda:1') # Rank 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Output in stack form</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_out2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span><span class="n">tensor_out2</span><span class="p">,</span> <span class="n">tensor_in</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_out2</span>
<span class="go">tensor([[1, 2],</span>
<span class="go">        [3, 4]], device='cuda:0') # Rank 0</span>
<span class="go">tensor([[1, 2],</span>
<span class="go">        [3, 4]], device='cuda:1') # Rank 1</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>Gloo 后端不支持此 API。</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.all_gather_object">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">all_gather_object</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">object_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#all_gather_object"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L2968"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.all_gather_object" title="Permalink to this definition">¶</a></dt>
<dd><p>从整个组中收集可序列化的对象到一个列表中。</p>
<p>与 <code class="xref py py-func docutils literal "><span class="pre">all_gather()</span></code> 类似，但可以传入 Python 对象。注意，对象必须是可序列化的才能被收集。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>object_list (列表[任意]) – 输出列表。它应该正确地调整大小以适应该集体的大小，并将包含输出。</p></li>
<li><p>obj (任意) – 从当前进程广播的可序列化 Python 对象。</p></li>
<li><p>group（进程组，可选）- 要工作的进程组。如果为 None，则使用默认进程组。默认为 <code class="docutils literal "><span class="pre">None</span></code> 。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>如果调用方进程属于此组，则集体操作的输出将被填充到输入 <code class="docutils literal "><span class="pre">object_list</span></code> 。如果调用方进程不属于该组，则传入的 <code class="docutils literal "><span class="pre">object_list</span></code> 将保持不变。</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>注意，此 API 与 <code class="xref py py-func docutils literal "><span class="pre">all_gather()</span></code> 集体操作略有不同，因为它不提供 <code class="docutils literal "><span class="pre">async_op</span></code> 句柄，因此将是一个阻塞调用。</p>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>对于基于 NCCL 的进程组，在通信之前必须将对象的内部张量表示移动到 GPU 设备。在这种情况下，使用的设备由 <code class="docutils literal "><span class="pre">torch.cuda.current_device()</span></code> 给出，并且用户有责任确保这样设置，以便每个进程都有一个单独的 GPU，通过 <code class="docutils literal "><span class="pre">torch.cuda.set_device()</span></code> 。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p> <code class="xref py py-func docutils literal "><span class="pre">all_gather_object()</span></code> 隐式使用 <code class="docutils literal "><span class="pre">pickle</span></code> 模块，已知存在安全风险。可能构造恶意 pickle 数据，在反序列化时执行任意代码。请仅使用您信任的数据调用此函数。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>使用 <code class="xref py py-func docutils literal "><span class="pre">all_gather_object()</span></code> 与 GPU 张量不兼容，效率低下，因为它会引发 GPU -&gt; CPU 的数据传输，因为张量将被序列化。请考虑使用 <code class="xref py py-func docutils literal "><span class="pre">all_gather()</span></code> 代替。</p>
</div>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: Process group initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assumes world_size of 3.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gather_objects</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"foo"</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">}]</span> <span class="c1"># any picklable object</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">gather_objects</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">all_gather_object</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">gather_objects</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">['foo', 12, {1: 2}]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.gather">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gather_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_dst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#gather"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L3971"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>在单个进程中收集张量列表。</p>
<p>此函数要求每个进程上的张量大小必须相同。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>张量（Tensor）- 输入张量。</p></li>
<li><p>gather_list（可选列表[Tensor]）- 用于收集数据的适当大小、相同大小的张量列表（默认为 None，必须在目标排名上指定）</p></li>
<li><p>dst（可选 int）- 全局进程组上的目标排名（无论 <code class="docutils literal "><span class="pre">group</span></code> 参数）。（如果 <code class="docutils literal "><span class="pre">dst</span></code> 和 <code class="docutils literal "><span class="pre">group_dst</span></code> 都为 None，则默认为全局排名 0）</p></li>
<li><p>group（进程组，可选）- 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>async_op（布尔值，可选）- 此操作是否应为异步操作</p></li>
<li><p>group_dst（整数，可选）- 目标 rank 在 <code class="docutils literal "><span class="pre">group</span></code> 上。同时指定 <code class="docutils literal "><span class="pre">dst</span></code> 和 <code class="docutils literal "><span class="pre">group_dst</span></code> 无效</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>如果 async_op 设置为 True，则为异步工作句柄。如果不是 async_op 或不是组的一部分，则为 None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>注意，gather_list 中的所有张量必须具有相同的大小。</p>
</div>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># We have 2 process groups, 2 ranks.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s1">'cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tensor_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">+</span> <span class="n">rank</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">gather_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">gather_list</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">gather_list</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Rank 0 gets gathered data.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gather_list</span>
<span class="go">[tensor([1., 1.], device='cuda:0'), tensor([2., 2.], device='cuda:0')] # Rank 0</span>
<span class="go">None                                                                   # Rank 1</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.gather_object">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">gather_object</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obj</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">object_gather_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_dst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#gather_object"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L3059"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.gather_object" title="Permalink to this definition">¶</a></dt>
<dd><p>从整个组中收集可序列化的对象到单个进程中。</p>
<p>与 <code class="xref py py-func docutils literal "><span class="pre">gather()</span></code> 类似，但可以传递 Python 对象。请注意，对象必须可序列化才能被收集。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>obj (Any) – 输入对象。必须是可序列化的。</p></li>
<li><p>object_gather_list (list[Any]) – 输出列表。在 <code class="docutils literal "><span class="pre">dst</span></code> 排序上，它应该与该集体的大小正确匹配，并将包含输出。在非目标节点上必须是 <code class="docutils literal "><span class="pre">None</span></code> 。（默认为 <code class="docutils literal "><span class="pre">None</span></code> ）</p></li>
<li><p>dst (int, optional) – 全局进程组上的目标节点（无论 <code class="docutils literal "><span class="pre">group</span></code> 参数）。（如果 <code class="docutils literal "><span class="pre">dst</span></code> 和 <code class="docutils literal "><span class="pre">group_dst</span></code> 都为 None，则默认为全局节点 0）</p></li>
<li><p>group (Optional[ProcessGroup]) – （进程组，可选）：要工作的进程组。如果为 None，则使用默认进程组。默认为 <code class="docutils literal "><span class="pre">None</span></code> 。</p></li>
<li><p>group_dst (int, 可选) – 目标秩在 <code class="docutils literal "><span class="pre">group</span></code> 上。同时指定 <code class="docutils literal "><span class="pre">dst</span></code> 和 <code class="docutils literal "><span class="pre">group_dst</span></code> 是无效的。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>无。在 <code class="docutils literal "><span class="pre">dst</span></code> 秩上， <code class="docutils literal "><span class="pre">object_gather_list</span></code> 将包含集体的输出。</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>注意，此 API 与 gather 集体略有不同，因为它不提供 async_op 处理句柄，因此将是一个阻塞调用。</p>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>对于基于 NCCL 的处理组，在通信之前，对象的内部张量表示必须移动到 GPU 设备上。在这种情况下，使用的设备由 <code class="docutils literal "><span class="pre">torch.cuda.current_device()</span></code> 给出，并且确保每个秩有一个单独的 GPU 是用户的责任，通过 <code class="docutils literal "><span class="pre">torch.cuda.set_device()</span></code> 实现。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p> <code class="xref py py-func docutils literal "><span class="pre">gather_object()</span></code> 隐式使用 <code class="docutils literal "><span class="pre">pickle</span></code> 模块，已知存在安全风险。可能构造恶意 pickle 数据，在反序列化时执行任意代码。请仅使用您信任的数据调用此函数。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>使用 <code class="xref py py-func docutils literal "><span class="pre">gather_object()</span></code> 与 GPU 张量不兼容，效率低下，因为它会引发 GPU -&gt; CPU 的数据传输，因为张量将被序列化。请考虑使用 <code class="xref py py-func docutils literal "><span class="pre">gather()</span></code> 代替。</p>
</div>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: Process group initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assumes world_size of 3.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gather_objects</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"foo"</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">}]</span> <span class="c1"># any picklable object</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">gather_objects</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">gather_object</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">gather_objects</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()],</span>
<span class="gp">... </span>    <span class="n">output</span> <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">dst</span><span class="o">=</span><span class="mi">0</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># On rank 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">['foo', 12, {1: 2}]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.scatter">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">scatter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scatter_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_src</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#scatter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L4051"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>将张量列表分散到组中的所有进程中。</p>
<p>每个进程将恰好接收一个张量，并将数据存储在 <code class="docutils literal "><span class="pre">tensor</span></code> 参数中。</p>
<p>支持复杂张量。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>张量（Tensor）- 输出张量。</p></li>
<li><p>scatter_list（列表[Tensor]）- 要分散的张量列表（默认为 None，必须在源 rank 上指定）。</p></li>
<li><p>src（int）- 全局进程组上的源排名（无论是否有 <code class="docutils literal "><span class="pre">group</span></code> 参数）。（如果 <code class="docutils literal "><span class="pre">src</span></code> 和 <code class="docutils literal "><span class="pre">group_src</span></code> 都为 None，则默认为全局排名 0）</p></li>
<li><p>group（ProcessGroup，可选）- 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>async_op（bool，可选）- 此操作是否应为异步操作</p></li>
<li><p>group_src（int，可选）- 在 <code class="docutils literal "><span class="pre">group</span></code> 上的源排名。不能同时指定 <code class="docutils literal "><span class="pre">src</span></code> 和 <code class="docutils literal "><span class="pre">group_src</span></code> </p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>异步工作处理，如果 async_op 设置为 True。如果没有 async_op 或者不属于该组，则为 None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>注意 scatter_list 中的所有张量必须具有相同的大小。</p>
</div>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: Process group initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s1">'cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tensor_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Assumes world_size of 2.</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Only tensors, all of which must be the same size.</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">t_ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tensor_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">t_fives</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tensor_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scatter_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">t_ones</span><span class="p">,</span> <span class="n">t_fives</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scatter_list</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">scatter_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Rank i gets scatter_list[i].</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tensor</span>
<span class="go">tensor([1., 1.], device='cuda:0') # Rank 0</span>
<span class="go">tensor([5., 5.], device='cuda:1') # Rank 1</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.scatter_object_list">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">scatter_object_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scatter_object_output_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scatter_object_input_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_src</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#scatter_object_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L3511"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.scatter_object_list" title="Permalink to this definition">¶</a></dt>
<dd><p>将可序列化的对象在 <code class="docutils literal "><span class="pre">scatter_object_input_list</span></code> 中分散到整个组。</p>
<p>与 <code class="xref py py-func docutils literal "><span class="pre">scatter()</span></code> 类似，但可以传入 Python 对象。在每个 rank 上，分散的对象将存储在 <code class="docutils literal "><span class="pre">scatter_object_output_list</span></code> 的第一个元素中。请注意， <code class="docutils literal "><span class="pre">scatter_object_input_list</span></code> 中的所有对象都必须是可序列化的，以便进行分散。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>scatter_object_output_list（Any 类型的列表）- 非空列表，其第一个元素将存储分散到该 rank 的对象。</p></li>
<li><p>scatter_object_input_list（Any 类型的列表，可选）- 要分散的输入对象列表。每个对象都必须是可序列化的。只有 <code class="docutils literal "><span class="pre">src</span></code> rank 上的对象将被分散，对于非 src rank，参数可以是 <code class="docutils literal "><span class="pre">None</span></code> 。</p></li>
<li><p>src（整数）- 分散 <code class="docutils literal "><span class="pre">scatter_object_input_list</span></code> 的源 rank。源 rank 基于全局进程组（不考虑 <code class="docutils literal "><span class="pre">group</span></code> 参数）。（如果 <code class="docutils literal "><span class="pre">src</span></code> 和 <code class="docutils literal "><span class="pre">group_src</span></code> 都为 None，则默认为全局 rank 0）</p></li>
<li><p>group (Optional[ProcessGroup]) – (ProcessGroup, optional): 要工作的进程组。如果为 None，则使用默认进程组。默认为 <code class="docutils literal "><span class="pre">None</span></code> 。</p></li>
<li><p>group_src (int, optional) – 在 <code class="docutils literal "><span class="pre">group</span></code> 上的源排名。无效同时指定 <code class="docutils literal "><span class="pre">src</span></code> 和 <code class="docutils literal "><span class="pre">group_src</span></code> 。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p> <code class="docutils literal "><span class="pre">None</span></code> 。如果排名属于该组， <code class="docutils literal "><span class="pre">scatter_object_output_list</span></code> 将将其第一个元素设置为该排名的散列对象。</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>注意，此 API 与 scatter 集合体略有不同，因为它不提供 <code class="docutils literal "><span class="pre">async_op</span></code> 处理器，因此将是一个阻塞调用。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p> <code class="xref py py-func docutils literal "><span class="pre">scatter_object_list()</span></code> 隐式使用 <code class="docutils literal "><span class="pre">pickle</span></code> 模块，已知存在安全风险。可能构造恶意 pickle 数据，在反序列化时执行任意代码。请仅使用您信任的数据调用此函数。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>使用 <code class="xref py py-func docutils literal "><span class="pre">scatter_object_list()</span></code> 与 GPU 张量不兼容，效率低下，因为它会引发 GPU -&gt; CPU 的数据传输，因为张量将被序列化。请考虑使用 <code class="xref py py-func docutils literal "><span class="pre">scatter()</span></code> 代替。</p>
</div>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: Process group initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Assumes world_size of 3.</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"foo"</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">2</span><span class="p">}]</span> <span class="c1"># any picklable object</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Can be any list on non-src ranks, elements are not used.</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">objects</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">scatter_object_list</span><span class="p">(</span><span class="n">output_list</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Rank i gets objects[i]. For example, on rank 2:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_list</span>
<span class="go">[{1: 2}]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.reduce_scatter">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">reduce_scatter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op=&lt;RedOpType.SUM:</span> <span class="pre">0&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op=False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#reduce_scatter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L4154"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.reduce_scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>将张量列表聚合后分散到组中的所有进程。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>输出（张量）- 输出张量。</p></li>
<li><p>输入列表（list[Tensor]）- 要进行归约和散列的张量列表。</p></li>
<li><p>op（可选）- 来自 <code class="docutils literal "><span class="pre">torch.distributed.ReduceOp</span></code> 枚举值之一。指定用于逐元素归约的操作。</p></li>
<li><p>group（ProcessGroup，可选）- 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>async_op (布尔值，可选) – 是否将此操作设置为异步操作。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>如果 async_op 设置为 True，则处理异步工作。如果不是 async_op 或不属于该组，则为 None。</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.reduce_scatter_tensor">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">reduce_scatter_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op=&lt;RedOpType.SUM:</span> <span class="pre">0&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op=False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#reduce_scatter_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L4193"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.reduce_scatter_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>将张量减少后分散到组内所有进程。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>输出（张量）- 输出张量。它应该在所有进程中具有相同的大小。</p></li>
<li><p>输入（张量）- 需要减少和分散的输入张量。其大小应为输出张量大小乘以全局大小。输入张量可以具有以下形状之一：（i）输出张量在主维度上的连接，或（ii）输出张量在主维度上的堆叠。有关“连接”的定义，请参阅 <code class="docutils literal "><span class="pre">torch.cat()</span></code> 。有关“堆叠”的定义，请参阅 <code class="docutils literal "><span class="pre">torch.stack()</span></code> 。</p></li>
<li><p>组（ProcessGroup，可选）- 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>async_op (布尔值，可选) – 是否将此操作设置为异步操作。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>如果 async_op 设置为 True，则处理异步工作。如果不是 async_op 或不属于该组，则为 None。</p>
</dd>
</dl>
<p class="rubric">示例</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># All tensors below are of torch.int64 dtype and on CUDA devices.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We have two ranks.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">"cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Input in concatenation form</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">world_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_in</span>
<span class="go">tensor([0, 1, 2, 3], device='cuda:0') # Rank 0</span>
<span class="go">tensor([0, 1, 2, 3], device='cuda:1') # Rank 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">reduce_scatter_tensor</span><span class="p">(</span><span class="n">tensor_out</span><span class="p">,</span> <span class="n">tensor_in</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_out</span>
<span class="go">tensor([0, 2], device='cuda:0') # Rank 0</span>
<span class="go">tensor([4, 6], device='cuda:1') # Rank 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Input in stack form</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor_in</span><span class="p">,</span> <span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_in</span>
<span class="go">tensor([[0, 1],</span>
<span class="go">        [2, 3]], device='cuda:0') # Rank 0</span>
<span class="go">tensor([[0, 1],</span>
<span class="go">        [2, 3]], device='cuda:1') # Rank 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">reduce_scatter_tensor</span><span class="p">(</span><span class="n">tensor_out</span><span class="p">,</span> <span class="n">tensor_in</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_out</span>
<span class="go">tensor([0, 2], device='cuda:0') # Rank 0</span>
<span class="go">tensor([4, 6], device='cuda:1') # Rank 1</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>Gloo 后端不支持此 API。</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.all_to_all_single">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">all_to_all_single</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_split_sizes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_split_sizes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#all_to_all_single"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L4322"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.all_to_all_single" title="Permalink to this definition">¶</a></dt>
<dd><p>将输入张量分割，然后将分割后的列表分散到组中的所有进程中。</p>
<p>之后，从组中的所有进程中拼接接收到的张量，并返回单个输出张量。</p>
<p>支持复杂张量。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>输出（张量）- 收集拼接的输出张量。</p></li>
<li><p>输入（张量）- 要散列的输入张量。</p></li>
<li><p>output_split_sizes - （可选的 Int 列表）：如果指定，则为 dim 0 的输出分割大小，如果没有指定或为空，则 <code class="docutils literal "><span class="pre">output</span></code> 张量的 dim 0 必须能被 <code class="docutils literal "><span class="pre">world_size</span></code> 整除。</p></li>
<li><p>input_split_sizes - （可选的 Int 列表）：如果指定，则为 dim 0 的输入分割大小，如果没有指定或为空，则 <code class="docutils literal "><span class="pre">input</span></code> 张量的 dim 0 必须能被 <code class="docutils literal "><span class="pre">world_size</span></code> 整除。</p></li>
<li><p>group（进程组，可选）- 要工作的进程组。如果没有指定，将使用默认进程组。</p></li>
<li><p>async_op (布尔值，可选) – 是否将此操作设置为异步操作。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>如果 async_op 设置为 True，则处理异步工作。如果不是 async_op 或不属于该组，则为 None。</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>all_to_all_single 是实验性的，可能会更改。</p>
</div>
<p class="rubric">示例</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">+</span> <span class="n">rank</span> <span class="o">*</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([0, 1, 2, 3])     # Rank 0</span>
<span class="go">tensor([4, 5, 6, 7])     # Rank 1</span>
<span class="go">tensor([8, 9, 10, 11])   # Rank 2</span>
<span class="go">tensor([12, 13, 14, 15]) # Rank 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">all_to_all_single</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([0, 4, 8, 12])    # Rank 0</span>
<span class="go">tensor([1, 5, 9, 13])    # Rank 1</span>
<span class="go">tensor([2, 6, 10, 14])   # Rank 2</span>
<span class="go">tensor([3, 7, 11, 15])   # Rank 3</span>
</pre></div>
</div>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Essentially, it is similar to following operation:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scatter_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">world_size</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gather_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">world_size</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">dist</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">gather_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">scatter_list</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">rank</span> <span class="k">else</span> <span class="p">[],</span> <span class="n">src</span> <span class="o">=</span> <span class="n">i</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Another example with uneven split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0</span>
<span class="go">tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1</span>
<span class="go">tensor([20, 21, 22, 23, 24])                                     # Rank 2</span>
<span class="go">tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_splits</span>
<span class="go">[2, 2, 1, 1]                                                     # Rank 0</span>
<span class="go">[3, 2, 2, 2]                                                     # Rank 1</span>
<span class="go">[2, 1, 1, 1]                                                     # Rank 2</span>
<span class="go">[2, 2, 2, 1]                                                     # Rank 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_splits</span>
<span class="go">[2, 3, 2, 2]                                                     # Rank 0</span>
<span class="go">[2, 2, 1, 2]                                                     # Rank 1</span>
<span class="go">[1, 2, 1, 2]                                                     # Rank 2</span>
<span class="go">[1, 2, 1, 1]                                                     # Rank 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">all_to_all_single</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_splits</span><span class="p">,</span> <span class="n">input_splits</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0</span>
<span class="go">tensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1</span>
<span class="go">tensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2</span>
<span class="go">tensor([ 5, 17, 18, 24, 36])                                     # Rank 3</span>
</pre></div>
</div>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Another example with tensors of torch.cfloat type.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="p">,</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="n">j</span><span class="p">,</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">3</span><span class="n">j</span><span class="p">,</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">4</span><span class="n">j</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span>
<span class="gp">... </span><span class="p">)</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">rank</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0</span>
<span class="go">tensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1</span>
<span class="go">tensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2</span>
<span class="go">tensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">all_to_all_single</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0</span>
<span class="go">tensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1</span>
<span class="go">tensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2</span>
<span class="go">tensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.all_to_all">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">all_to_all</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_tensor_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_tensor_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#all_to_all"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L4467"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.all_to_all" title="Permalink to this definition">¶</a></dt>
<dd><p>将输入张量列表分散到组中的所有进程中，并返回输出列表中收集的张量列表。</p>
<p>支持复杂张量。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>output_tensor_list（列表[Tensor]）- 每个 rank 收集一个张量的张量列表。</p></li>
<li><p>input_tensor_list（列表[Tensor]）- 每个 rank 分散一个张量的张量列表。</p></li>
<li><p>group（进程组，可选）- 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>async_op（布尔值，可选）- 此操作是否应为异步操作。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>如果 async_op 设置为 True，则为异步工作处理。如果不是 async_op 或不是组的一部分，则为 None。</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>all_to_all 是实验性的，可能会更改。</p>
</div>
<p class="rubric">示例</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">+</span> <span class="n">rank</span> <span class="o">*</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0</span>
<span class="go">[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1</span>
<span class="go">[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2</span>
<span class="go">[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">all_to_all</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0</span>
<span class="go">[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1</span>
<span class="go">[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2</span>
<span class="go">[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3</span>
</pre></div>
</div>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Essentially, it is similar to following operation:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scatter_list</span> <span class="o">=</span> <span class="nb">input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gather_list</span> <span class="o">=</span> <span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">dist</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">gather_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">scatter_list</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">rank</span> <span class="k">else</span> <span class="p">[],</span> <span class="n">src</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0</span>
<span class="go">tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1</span>
<span class="go">tensor([20, 21, 22, 23, 24])                                     # Rank 2</span>
<span class="go">tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_splits</span>
<span class="go">[2, 2, 1, 1]                                                     # Rank 0</span>
<span class="go">[3, 2, 2, 2]                                                     # Rank 1</span>
<span class="go">[2, 1, 1, 1]                                                     # Rank 2</span>
<span class="go">[2, 2, 2, 1]                                                     # Rank 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_splits</span>
<span class="go">[2, 3, 2, 2]                                                     # Rank 0</span>
<span class="go">[2, 2, 1, 2]                                                     # Rank 1</span>
<span class="go">[1, 2, 1, 2]                                                     # Rank 2</span>
<span class="go">[1, 2, 1, 1]                                                     # Rank 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">input_splits</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0</span>
<span class="go">[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1</span>
<span class="go">[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2</span>
<span class="go">[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">all_to_all</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0</span>
<span class="go">[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1</span>
<span class="go">[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2</span>
<span class="go">[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3</span>
</pre></div>
</div>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Another example with tensors of torch.cfloat type.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="p">,</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="n">j</span><span class="p">,</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">3</span><span class="n">j</span><span class="p">,</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">4</span><span class="n">j</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span>
<span class="gp">... </span><span class="p">)</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">rank</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">[tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0</span>
<span class="go">[tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1</span>
<span class="go">[tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2</span>
<span class="go">[tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">all_to_all</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">[tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0</span>
<span class="go">[tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1</span>
<span class="go">[tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2</span>
<span class="go">[tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.barrier">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">barrier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">async_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#barrier"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L4585"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.barrier" title="Permalink to this definition">¶</a></dt>
<dd><p>同步所有进程。</p>
<p>如果 async_op 为 False，或者如果在 wait()上调用异步工作句柄，则此集体操作将阻塞进程，直到整个组进入此函数。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>group (进程组，可选) – 要工作的进程组。如果为 None，则使用默认进程组。</p></li>
<li><p>async_op (bool, 可选) – 是否将此操作设置为异步操作</p></li>
<li><p>device_ids ([int], 可选) – 设备/GPU ID 列表。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>如果 async_op 设置为 True，则为异步工作句柄。如果没有 async_op 或不属于该组，则为 None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>ProcessGroupNCCL 现在会阻塞 CPU 线程，直到屏障集体操作完成。</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.monitored_barrier">
<span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">monitored_barrier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wait_all_ranks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/distributed/distributed_c10d.html#monitored_barrier"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/distributed/distributed_c10d.py#L4630"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.distributed.monitored_barrier" title="Permalink to this definition">¶</a></dt>
<dd><p>同步进程类似于 <code class="docutils literal "><span class="pre">torch.distributed.barrier</span></code> ，但考虑可配置的超时时间。</p>
<p>能够报告在提供超时时间内未通过此屏障的进程编号。具体来说，对于非零编号，将阻塞直到从编号 0 的进程发送/接收操作完成。编号 0 将阻塞直到其他所有编号的发送/接收操作完成，并将报告未及时响应的编号。请注意，如果某个编号没有达到 monitored_barrier（例如由于挂起），则所有其他编号都会在 monitored_barrier 中失败。</p>
<p>此集体操作将阻塞组中的所有进程/编号，直到整个组成功退出函数，这使得它在调试和同步方面非常有用。然而，它可能会影响性能，并且仅应用于调试或需要主机端完全同步点的场景。为了调试目的，可以在应用程序的集体调用之前插入此屏障，以检查是否有编号不同步。</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>注意，此集合仅支持使用 GLOO 后端。</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>group（进程组，可选）- 要工作的进程组。如果为 <code class="docutils literal "><span class="pre">None</span></code> ，则使用默认进程组。</p></li>
<li><p>timeout（datetime.timedelta，可选）- 监控屏障的超时时间。如果为 <code class="docutils literal "><span class="pre">None</span></code> ，则使用默认进程组超时时间。</p></li>
<li><p>wait_all_ranks（bool，可选）- 是否收集所有失败的进程。默认情况下，这是 <code class="docutils literal "><span class="pre">False</span></code> 和 <code class="docutils literal "><span class="pre">monitored_barrier</span></code> 在 rank 0 上将抛出第一个遇到的失败进程以快速失败。通过设置 <code class="docutils literal "><span class="pre">wait_all_ranks=True</span></code> <code class="docutils literal "><span class="pre">monitored_barrier</span></code> 将收集所有失败的进程并抛出一个包含所有失败进程信息的错误。</p></li>
</ul>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="docutils literal "><span class="pre">None</span></code>.</p>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Note: Process group initialization omitted on each rank.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">dist</span><span class="o">.</span><span class="n">monitored_barrier</span><span class="p">()</span> <span class="c1"># Raises exception indicating that</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># rank 1 did not call into monitored_barrier.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Example with wait_all_ranks=True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">dist</span><span class="o">.</span><span class="n">monitored_barrier</span><span class="p">(</span><span class="n">wait_all_ranks</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># Raises exception</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># indicating that ranks 1, 2, ... world_size - 1 did not call into</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># monitored_barrier.</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.Work">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">Work</span></span><a class="headerlink" href="#torch.distributed.Work" title="Permalink to this definition">¶</a></dt>
<dd><p>一个 Work 对象代表 PyTorch 分布式包中挂起的异步操作的句柄。它由非阻塞的集体操作返回，例如 dist.all_reduce(tensor, async_op=True)。</p>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Work.boxed">
<span class="sig-name descname"><span class="pre">boxed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Work</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><span class="pre">object</span></a></span></span><a class="headerlink" href="#torch.distributed.Work.boxed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Work.exception">
<span class="sig-name descname"><span class="pre">exception</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Work</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">std::__exception_ptr::exception_ptr</span></span></span><a class="headerlink" href="#torch.distributed.Work.exception" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Work.get_future">
get_future(self:torch._C._distributed_c10d.Work) → torch.Future</dt>
<dd><dl class="field-list simple">
<dt class="field-odd">返回<span class="colon">:</span></dt>
<dd class="field-odd"><p>一个与 <code class="docutils literal "><span class="pre">Work</span></code> 完成相关的 <code class="docutils literal "><span class="pre">torch.futures.Future</span></code> 对象。例如，可以通过 <code class="docutils literal "><span class="pre">fut</span> <span class="pre">=</span> <span class="pre">process_group.allreduce(tensors).get_future()</span></code> 获取 future 对象。</p>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">下面是一个使用 <code class="docutils literal "><span class="pre">get_future`</span> <span class="pre">API</span> <span class="pre">to</span> <span class="pre">retrieve</span> <span class="pre">a</span> <span class="pre">Future</span> <span class="pre">associated</span> <span class="pre">with</span> <span class="pre">the</span> <span class="pre">completion</span> <span class="pre">of</span>
<span class="pre">``allreduce</span></code> 的简单 allreduce DDP 通信钩子的示例。</font></font></font></p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">allreduce</span><span class="p">(</span><span class="n">process_group</span><span class="p">:</span> <span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">,</span> <span class="n">bucket</span><span class="p">:</span> <span class="n">dist</span><span class="o">.</span><span class="n">GradBucket</span><span class="p">):</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">Future</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">group_to_use</span> <span class="o">=</span> <span class="n">process_group</span> <span class="k">if</span> <span class="n">process_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">tensor</span> <span class="o">=</span> <span class="n">bucket</span><span class="o">.</span><span class="n">buffer</span><span class="p">()</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">group_to_use</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group_to_use</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">get_future</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ddp_model</span><span class="o">.</span><span class="n">register_comm_hook</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hook</span><span class="o">=</span><span class="n">allreduce</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p> <code class="docutils literal "><span class="pre">get_future</span></code> API 支持 NCCL，部分支持 GLOO 和 MPI 后端（不支持像 send/recv 这样的对等操作）并将返回一个 <code class="docutils literal "><span class="pre">torch.futures.Future</span></code> 。</p>
<p>在上面的示例中， <code class="docutils literal "><span class="pre">allreduce</span></code> 将使用 NCCL 后端在 GPU 上执行工作， <code class="docutils literal "><span class="pre">fut.wait()</span></code> 将在将适当的 NCCL 流与 PyTorch 当前设备流同步后返回，以确保我们可以进行异步 CUDA 执行，并且它不会等待整个操作在 GPU 上完成。请注意， <code class="docutils literal "><span class="pre">CUDAFuture</span></code> 不支持 <code class="docutils literal "><span class="pre">TORCH_NCCL_BLOCKING_WAIT</span></code> 标志或 NCCL 的 <code class="docutils literal "><span class="pre">barrier()</span></code> 。此外，如果通过 <code class="docutils literal "><span class="pre">fut.then()</span></code> 添加了回调函数，它将等待 <code class="docutils literal "><span class="pre">WorkNCCL</span></code> 的 NCCL 流与 <code class="docutils literal "><span class="pre">ProcessGroupNCCL</span></code> 的专用回调流同步，并在回调流上运行回调后立即调用回调。 <code class="docutils literal "><span class="pre">fut.then()</span></code> 将返回另一个 <code class="docutils literal "><span class="pre">CUDAFuture</span></code> ，该 <code class="docutils literal "><span class="pre">CUDAFuture</span></code>  包含回调的返回值和一个记录回调流的 <code class="docutils literal "><span class="pre">CUDAEvent</span></code> 。</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>对于 CPU 工作， <code class="docutils literal "><span class="pre">fut.done()</span></code> 当工作完成且 value() 张量准备就绪时返回 true。</p></li>
<li><p>对于 GPU 工作， <code class="docutils literal "><span class="pre">fut.done()</span></code> 只有在操作已入队时才返回 true。</p></li>
<li><p>对于混合 CPU-GPU 工作（例如，使用 GLOO 发送 GPU 张量）， <code class="docutils literal "><span class="pre">fut.done()</span></code> 当张量到达各自的节点时返回 true，但还不一定在各自的 GPU 上同步（类似于 GPU 工作）。</p></li>
</ol>
</div></blockquote>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Work.get_future_result">
get_future_result(self:torch._C._distributed_c10d.Work) → torch.Future</dt>
<dd><dl class="field-list simple">
<dt class="field-odd">返回<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <code class="docutils literal "><span class="pre">torch.futures.Future</span></code> object of int type which maps to the enum type of WorkResult. As an example, a future object can be retrieved by <code class="docutils literal "><span class="pre">fut</span> <span class="pre">=</span> <span class="pre">process_group.allreduce(tensor).get_future_result()</span></code> .</p>
</dd>
</dl>
<dl class="simple">
<dt>示例::</font></font></font></dt><dd><p><font class=" " lang="zh-CN"><br hidden=""><font class="   "><font class="  ">用户可以使用 <code class="docutils literal "><span class="pre">fut.wait()</span></code> 阻塞等待工作完成，并通过 <code class="docutils literal "><span class="pre">fut.value()</span></code> 获取 WorkResult。此外，用户还可以使用 <code class="docutils literal "><span class="pre">fut.then(call_back_func)</span></code> 注册一个回调函数，在工作完成时调用，而不会阻塞当前线程。</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p> <code class="docutils literal "><span class="pre">get_future_result</span></code> API 支持 NCCL</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Work.is_completed">
<span class="sig-name descname"><span class="pre">is_completed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Work</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#torch.distributed.Work.is_completed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Work.is_success">
<span class="sig-name descname"><span class="pre">is_success</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Work</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#torch.distributed.Work.is_success" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Work.result">
<span class="sig-name descname"><span class="pre">result</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Work</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><span class="pre">torch.Tensor</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torch.distributed.Work.result" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Work.source_rank">
<span class="sig-name descname"><span class="pre">source_rank</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Work</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#torch.distributed.Work.source_rank" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Work.synchronize">
synchronize(self:torch._C._distributed_c10d.Work) → None</dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Work.unbox">
静态解包(arg0object) → torch._C._distributed_c10d.Work</dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Work.wait">
wait(self:torch._C._distributed_c10d.Work, timeoutdatetime.timedelta=datetime.timedelta(0)) → bool</dt>
<dd><dl class="field-list simple">
<dt class="field-odd">返回<span class="colon">:</span></dt>
<dd class="field-odd"><p>true/false.</p>
</dd>
</dl>
<dl class="simple">
<dt>示例::</dt><dd><dl class="simple">
<dt>尝试:</dt><dd><p>work.wait(timeout)</p>
</dd>
<dt>except:</dt><dd><p># 处理中</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>在正常情况下，用户无需设置超时。调用 wait() 与调用 synchronize() 相同：让当前流在 NCCL 工作完成时阻塞。然而，如果设置了超时，它将阻塞 CPU 线程，直到 NCCL 工作完成或超时。如果超时，将抛出异常。</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.ReduceOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">ReduceOp</span></span><a class="headerlink" href="#torch.distributed.ReduceOp" title="Permalink to this definition">¶</a></dt>
<dd><p>用于可用归约操作的枚举类： <code class="docutils literal "><span class="pre">SUM</span></code> ， <code class="docutils literal "><span class="pre">PRODUCT</span></code> ， <code class="docutils literal "><span class="pre">MIN</span></code> ， <code class="docutils literal "><span class="pre">MAX</span></code> ， <code class="docutils literal "><span class="pre">BAND</span></code> ， <code class="docutils literal "><span class="pre">BOR</span></code> ， <code class="docutils literal "><span class="pre">BXOR</span></code> ，以及 <code class="docutils literal "><span class="pre">PREMUL_SUM</span></code> 。</p>
<p>使用 <code class="docutils literal "><span class="pre">NCCL</span></code> 后端时， <code class="docutils literal "><span class="pre">BAND</span></code> 、 <code class="docutils literal "><span class="pre">BOR</span></code> 和 <code class="docutils literal "><span class="pre">BXOR</span></code> 的缩减不可用。</p>
<p> <code class="docutils literal "><span class="pre">AVG</span></code> 在求和前将值除以世界大小。 <code class="docutils literal "><span class="pre">AVG</span></code> 仅在 <code class="docutils literal "><span class="pre">NCCL</span></code> 后端可用，并且仅适用于 NCCL 2.10 或更高版本。</p>
<p> <code class="docutils literal "><span class="pre">PREMUL_SUM</span></code> 在本地将输入乘以给定的标量后再进行缩减。 <code class="docutils literal "><span class="pre">PREMUL_SUM</span></code> 仅在 <code class="docutils literal "><span class="pre">NCCL</span></code> 后端可用，并且仅适用于 NCCL 2.11 或更高版本。用户应使用 <code class="docutils literal "><span class="pre">torch.distributed._make_nccl_premul_sum</span></code> 。</p>
<p>此外， <code class="docutils literal "><span class="pre">MAX</span></code> 、 <code class="docutils literal "><span class="pre">MIN</span></code> 和 <code class="docutils literal "><span class="pre">PRODUCT</span></code> 不支持复数张量。</p>
<p>这个类的值可以作为属性访问，例如， <code class="docutils literal "><span class="pre">ReduceOp.SUM</span></code> 。它们用于指定减少集体策略，例如， <code class="xref py py-func docutils literal "><span class="pre">reduce()</span></code> 。</p>
<p>此类不支持 <code class="docutils literal "><span class="pre">__members__</span></code> 属性。</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.reduce_op">
torch.distributed.reduce_op 类</dt>
<dd><p>已废弃的枚举类，用于减少操作： <code class="docutils literal "><span class="pre">SUM</span></code> ， <code class="docutils literal "><span class="pre">PRODUCT</span></code> ， <code class="docutils literal "><span class="pre">MIN</span></code> ，和 <code class="docutils literal "><span class="pre">MAX</span></code> 。</p>
<p>建议使用 <code class="xref py py-class docutils literal "><span class="pre">ReduceOp</span></code> 代替。</p>
</dd></dl>

</section>
<section id="distributed-key-value-store">
<h2>分布式键值存储 ¶</h2>
<p>分布式包自带分布式键值存储，可用于在组内进程间共享信息，以及初始化分布式包在 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.init_process_group()</span></code> （通过显式创建存储作为指定 <code class="docutils literal "><span class="pre">init_method</span></code> 的替代。）键值存储有 3 种选择： <code class="xref py py-class docutils literal "><span class="pre">TCPStore</span></code> ， <code class="xref py py-class docutils literal "><span class="pre">FileStore</span></code> ，和 <code class="xref py py-class docutils literal "><span class="pre">HashStore</span></code> 。</p>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.Store">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.distributed.</span></span><span class="sig-name descname"><span class="pre">Store</span></span><a class="headerlink" href="#torch.distributed.Store" title="Permalink to this definition">¶</a></dt>
<dd><p>所有存储实现的基类，例如 PyTorch 分布式提供的 3 个：( <code class="xref py py-class docutils literal "><span class="pre">TCPStore</span></code> ， <code class="xref py py-class docutils literal "><span class="pre">FileStore</span></code> 和 <code class="xref py py-class docutils literal "><span class="pre">HashStore</span></code> )。</p>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.__init__">
__init__(self:torch._C._distributed_c10d.Store) → None</dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.add">
add(self:torch._C._distributed_c10d.Store, arg0str, arg1int) → int</dt>
<dd><p>对于给定的 <code class="docutils literal "><span class="pre">key</span></code> ，第一次调用 add 将在存储中创建与 <code class="docutils literal "><span class="pre">key</span></code> 关联的计数器，初始化为 <code class="docutils literal "><span class="pre">amount</span></code> 。随后的调用 add 与相同的 <code class="docutils literal "><span class="pre">key</span></code> 将计数器增加指定的 <code class="docutils literal "><span class="pre">amount</span></code> 。如果使用已通过 <code class="xref py py-meth docutils literal "><span class="pre">set()</span></code> 在存储中设置的键调用 <code class="xref py py-meth docutils literal "><span class="pre">add()</span></code> ，将导致异常。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>key (str) – 存储中的键，其计数将被增加。</p></li>
<li><p>amount (int) – 增加计数器的数量。</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Using TCPStore as an example, other store types can also be used</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Should return 7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.append">
<span class="sig-name descname"><span class="pre">append</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Store</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#torch.distributed.Store.append" title="Permalink to this definition">¶</a></dt>
<dd><p>将键值对根据提供的 <code class="docutils literal "><span class="pre">key</span></code> 和 <code class="docutils literal "><span class="pre">value</span></code> 追加到存储中。如果 <code class="docutils literal "><span class="pre">key</span></code> 不存在于存储中，则将其创建。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>key (str) – 要附加到存储中的键。</p></li>
<li><p>value (str) – 与 <code class="docutils literal "><span class="pre">key</span></code> 关联的值，要添加到存储中。</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="s2">"po"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="s2">"tato"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Should return "potato"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.check">
<span class="sig-name descname"><span class="pre">check</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Store</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#torch.distributed.Store.check" title="Permalink to this definition">¶</a></dt>
<dd><p>检查调用，以确定给定的 <code class="docutils literal "><span class="pre">keys</span></code> 列表中是否有值存储在存储中。在正常情况下，此调用将立即返回，但仍可能遇到一些边缘死锁情况，例如在 TCPStore 被销毁后调用 check。使用 <code class="xref py py-meth docutils literal "><span class="pre">check()</span></code> 调用要检查是否存储在存储中的键列表。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><p>keys (lisr[str]) – 查询是否存储在存储中的键。</p>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Using TCPStore as an example, other store types can also be used</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Should return 7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">check</span><span class="p">([</span><span class="s2">"first_key"</span><span class="p">])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.compare_set">
<span class="sig-name descname"><span class="pre">compare_set</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Store</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a></span></span><a class="headerlink" href="#torch.distributed.Store.compare_set" title="Permalink to this definition">¶</a></dt>
<dd><p>根据提供的 <code class="docutils literal "><span class="pre">key</span></code> 将键值对插入存储中，并在插入前对 <code class="docutils literal "><span class="pre">expected_value</span></code> 和 <code class="docutils literal "><span class="pre">desired_value</span></code> 进行比较。如果 <code class="docutils literal "><span class="pre">key</span></code> 的 <code class="docutils literal "><span class="pre">expected_value</span></code> 已在存储中存在或如果 <code class="docutils literal "><span class="pre">expected_value</span></code> 为空字符串，则 <code class="docutils literal "><span class="pre">desired_value</span></code> 将被设置。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>key (str) – 要在存储中检查的键。</p></li>
<li><p>expected_value (str) – 要在插入前检查的与 <code class="docutils literal "><span class="pre">key</span></code> 关联的值。</p></li>
<li><p>desired_value (str) – 要添加到存储中的与 <code class="docutils literal "><span class="pre">key</span></code> 关联的值。</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">"key"</span><span class="p">,</span> <span class="s2">"first_value"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">compare_set</span><span class="p">(</span><span class="s2">"key"</span><span class="p">,</span> <span class="s2">"first_value"</span><span class="p">,</span> <span class="s2">"second_value"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Should return "second_value"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"key"</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.delete_key">
<span class="sig-name descname"><span class="pre">delete_key</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Store</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#torch.distributed.Store.delete_key" title="Permalink to this definition">¶</a></dt>
<dd><p>从存储中删除与 <code class="docutils literal "><span class="pre">key</span></code> 关联的键值对。如果键成功删除，则返回 true，否则返回 false。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>该 <code class="docutils literal "><span class="pre">delete_key</span></code> API 仅支持 <code class="xref py py-class docutils literal "><span class="pre">TCPStore</span></code> 和 <code class="xref py py-class docutils literal "><span class="pre">HashStore</span></code> 。使用此 API 与 <code class="xref py py-class docutils literal "><span class="pre">FileStore</span></code> 将导致异常。</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><p>key（字符串）- 要从存储中删除的键</p>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>如果 <code class="docutils literal "><span class="pre">key</span></code> 已被删除，则为 True，否则为 False。</p>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Using TCPStore as an example, HashStore can also be used</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># This should return true</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">delete_key</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># This should return false</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">delete_key</span><span class="p">(</span><span class="s2">"bad_key"</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.get">
<span class="sig-name descname"><span class="pre">get</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Store</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a></span></span><a class="headerlink" href="#torch.distributed.Store.get" title="Permalink to this definition">¶</a></dt>
<dd><p>获取与给定 <code class="docutils literal "><span class="pre">key</span></code> 在存储中关联的值。如果 <code class="docutils literal "><span class="pre">key</span></code> 不在存储中，函数将在抛出异常之前等待 <code class="docutils literal "><span class="pre">timeout</span></code> ，该值在初始化存储时定义。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><p>key (str) – 函数将返回与此键关联的值。</p>
</dd>
<dt class="field-even">返回<span class="colon">:</span></dt>
<dd class="field-even"><p>如果 <code class="docutils literal "><span class="pre">key</span></code> 在存储中，则与 <code class="docutils literal "><span class="pre">key</span></code> 关联的值。</p>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="s2">"first_value"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Should return "first_value"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.has_extended_api">
<span class="sig-name descname"><span class="pre">has_extended_api</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Store</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#torch.distributed.Store.has_extended_api" title="Permalink to this definition">¶</a></dt>
<dd><p>返回 true 如果存储支持扩展操作。</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.multi_get">
<span class="sig-name descname"><span class="pre">multi_get</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Store</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#torch.distributed.Store.multi_get" title="Permalink to this definition">¶</a></dt>
<dd><p>获取 <code class="docutils literal "><span class="pre">keys</span></code> 中的所有值。如果 <code class="docutils literal "><span class="pre">keys</span></code> 中的任何键不在存储中，函数将等待 <code class="docutils literal "><span class="pre">timeout</span></code> 。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><p>keys (List[str]) – 要从存储中检索的键。</p>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="s2">"po"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">"second_key"</span><span class="p">,</span> <span class="s2">"tato"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Should return [b"po", b"tato"]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">multi_get</span><span class="p">([</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="s2">"second_key"</span><span class="p">])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.multi_set">
<span class="sig-name descname"><span class="pre">multi_set</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Store</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#torch.distributed.Store.multi_set" title="Permalink to this definition">¶</a></dt>
<dd><p>根据提供的 <code class="docutils literal "><span class="pre">keys</span></code> 和 <code class="docutils literal "><span class="pre">values</span></code> 将键值对列表插入存储中。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>keys (List[str]) – 要插入的键。</p></li>
<li><p>values (List[str]) – 要插入的值列表。</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">multi_set</span><span class="p">([</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="s2">"second_key"</span><span class="p">],</span> <span class="p">[</span><span class="s2">"po"</span><span class="p">,</span> <span class="s2">"tato"</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Should return b"po"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.num_keys">
<span class="sig-name descname"><span class="pre">num_keys</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Store</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#torch.distributed.Store.num_keys" title="Permalink to this definition">¶</a></dt>
<dd><p>返回存储中设置的键的数量。请注意，这个数字通常比通过 <code class="xref py py-meth docutils literal "><span class="pre">set()</span></code> 和 <code class="xref py py-meth docutils literal "><span class="pre">add()</span></code> 添加的键的数量多一个，因为有一个键用于协调使用存储的所有工作者。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>当与 <code class="xref py py-class docutils literal "><span class="pre">TCPStore</span></code> 结合使用时， <code class="docutils literal "><span class="pre">num_keys</span></code> 返回写入底层文件的键的数量。如果销毁存储并使用同一文件创建另一个存储，则原始键将被保留。</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">返回<span class="colon">:</span></dt>
<dd class="field-odd"><p>当前存储中键的数量。</p>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Using TCPStore as an example, other store types can also be used</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="s2">"first_value"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># This should return 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">num_keys</span><span class="p">()</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.set">
设置(self:torch._C._distributed_c10d.Store, arg0str, arg1str) → None ¶</dt>
<dd><p>将键值对插入到根据提供的 <code class="docutils literal "><span class="pre">key</span></code> 和 <code class="docutils literal "><span class="pre">value</span></code> 指定的存储中。如果 <code class="docutils literal "><span class="pre">key</span></code> 已在存储中存在，则将使用新提供的 <code class="docutils literal "><span class="pre">value</span></code> 覆盖旧值。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>键（字符串）- 要添加到存储的键。</p></li>
<li><p>value (str) – 与 <code class="docutils literal "><span class="pre">key</span></code> 关联要添加到存储中的值。</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="s2">"first_value"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Should return "first_value"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.set_timeout">
<span class="sig-name descname"><span class="pre">set_timeout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Store</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg0</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/datetime.html#datetime.timedelta" title="(in Python v3.13)"><span class="pre">datetime.timedelta</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#torch.distributed.Store.set_timeout" title="Permalink to this definition">¶</a></dt>
<dd><p>设置存储的默认超时。此超时在初始化以及 <code class="xref py py-meth docutils literal "><span class="pre">wait()</span></code> 和 <code class="xref py py-meth docutils literal "><span class="pre">get()</span></code> 中使用。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><p>timeout (timedelta) – 要在存储中设置的超时。</p>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Using TCPStore as an example, other store types can also be used</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">set_timeout</span><span class="p">(</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># This will throw an exception after 10 seconds</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">wait</span><span class="p">([</span><span class="s2">"bad_key"</span><span class="p">])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributed.Store.timeout">
属性超时 ¶</dt>
<dd><p>获取存储的超时时间。</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.Store.wait">
<span class="sig-name descname"><span class="pre">wait</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.Store.wait" title="Permalink to this definition">¶</a></dt>
<dd><p>重载函数。</p>
<ol class="arabic simple">
<li><p>wait(self: torch._C._distributed_c10d.Store, arg0: list[str]) -&gt; None</p></li>
</ol>
<p>等待 <code class="docutils literal "><span class="pre">keys</span></code> 中的每个键被添加到存储中。如果 <code class="docutils literal "><span class="pre">timeout</span></code> （在存储初始化期间设置）之前没有设置所有键，则 <code class="docutils literal "><span class="pre">wait</span></code> 将抛出异常。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><p>keys (list) – 等待直到这些键在存储中设置好的键列表。</p>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Using TCPStore as an example, other store types can also be used</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># This will throw an exception after 30 seconds</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">wait</span><span class="p">([</span><span class="s2">"bad_key"</span><span class="p">])</span>
</pre></div>
</div>
</dd>
</dl>
<ol class="arabic simple" start="2">
<li><p>wait(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: datetime.timedelta) -&gt; None</p></li>
</ol>
<p>等待每个键在 <code class="docutils literal "><span class="pre">keys</span></code> 中被添加到存储中，如果键未被通过 <code class="docutils literal "><span class="pre">timeout</span></code> 设置，则抛出异常。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>keys（列表）- 等待直到这些键在存储中设置。</p></li>
<li><p>timeout（timedelta）- 在抛出异常之前等待键被添加的时间。</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Using TCPStore as an example, other store types can also be used</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># This will throw an exception after 10 seconds</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">wait</span><span class="p">([</span><span class="s2">"bad_key"</span><span class="p">],</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.TCPStore">
类 torch.distributed.TCPStore</dt>
<dd><p>基于 TCP 的分布式键值存储实现。服务器存储持有数据，而客户端存储可以通过 TCP 连接到服务器存储，执行插入键值对、检索键值对等操作。应始终初始化一个服务器存储，因为客户端存储将等待服务器建立连接。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>host_name (str) – 服务器存储应运行的计算机名或 IP 地址。</p></li>
<li><p>port (int) – 服务器存储应监听请求的端口号。</p></li>
<li><p>world_size (int, optional) – 存储用户总数（客户端数量加 1 为服务器）。默认为 None（None 表示非固定数量的存储用户）。</p></li>
<li><p>is_master (bool, optional) – 当初始化服务器存储时为 True，客户端存储时为 False。默认为 False。</p></li>
<li><p>timeout (timedelta, optional) – 存储在初始化期间以及用于 <code class="xref py py-meth docutils literal "><span class="pre">get()</span></code> 和 <code class="xref py py-meth docutils literal "><span class="pre">wait()</span></code> 等方法时的超时时间。默认为 timedelta(seconds=300)。</p></li>
<li><p>wait_for_workers (bool, optional) – 是否等待所有工作进程连接到服务器存储。仅在 world_size 为固定值时适用。默认为 True。</p></li>
<li><p>multi_tenant (bool, optional) – 如果为 True，当前进程中的所有具有相同主机/端口的 <code class="docutils literal "><span class="pre">TCPStore</span></code> 实例将使用相同的底层 <code class="docutils literal "><span class="pre">TCPServer</span></code> 。默认为 False。</p></li>
<li><p>master_listen_fd (int, 可选) – 如果指定，底层 <code class="docutils literal "><span class="pre">TCPServer</span></code> 将监听此文件描述符，它必须是一个已绑定到 <code class="docutils literal "><span class="pre">port</span></code> 的套接字。在某些场景中，这有助于避免端口分配竞争。默认值为 None（表示服务器创建一个新的套接字并尝试将其绑定到 <code class="docutils literal "><span class="pre">port</span></code> ）。</p></li>
<li><p>use_libuv (bool, 可选) – 如果为 True，则使用 libuv 作为 <code class="docutils literal "><span class="pre">TCPServer</span></code> 后端。默认值为 True。</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Run on process 1 (server)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">server_store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">1234</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Run on process 2 (client)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">client_store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">TCPStore</span><span class="p">(</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="mi">1234</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use any of the store methods from either the client or server after initialization</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">server_store</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="s2">"first_value"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">client_store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.TCPStore.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torch.distributed.TCPStore" title="torch._C._distributed_c10d.TCPStore"><span class="pre">torch._C._distributed_c10d.TCPStore</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">host_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">port</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">world_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_master</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/datetime.html#datetime.timedelta" title="(in Python v3.13)"><span class="pre">datetime.timedelta</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">datetime.timedelta(seconds=300)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wait_for_workers</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_tenant</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">master_listen_fd</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_libuv</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#torch.distributed.TCPStore.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>创建一个新的 TCPStore。</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributed.TCPStore.host">
属性主机</dt>
<dd><p>获取商店监听请求的主机名。</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributed.TCPStore.libuvBackend">
属性 libuvBackend</dt>
<dd><p>如果使用 libuv 后端，则返回 True。</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributed.TCPStore.port">
属性端口 ¶</dt>
<dd><p>获取商店监听请求的端口号。</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.HashStore">
类 torch.distributed.HashStore ¶</dt>
<dd><p>基于底层哈希表的线程安全存储实现。此存储可以在同一进程内使用（例如，由其他线程使用），但不能跨进程使用。</p>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">HashStore</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># store can be used from other threads</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use any of the store methods after initialization</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="s2">"first_value"</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.HashStore.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#torch.distributed.HashStore" title="torch._C._distributed_c10d.HashStore"><span class="pre">torch._C._distributed_c10d.HashStore</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#torch.distributed.HashStore.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>创建一个新的 HashStore。</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.FileStore">
class torch.distributed.FileStore</dt>
<dd><p>使用文件存储底层键值对的存储实现。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>文件名（str）- 存储键值对的文件路径</p></li>
<li><p>world_size（int，可选）- 使用存储的总进程数。默认为-1（负值表示非固定数量的存储用户）</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>示例::</dt><dd><div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store1</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">FileStore</span><span class="p">(</span><span class="s2">"/tmp/filestore"</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store2</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">FileStore</span><span class="p">(</span><span class="s2">"/tmp/filestore"</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use any of the store methods from either the client or server after initialization</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store1</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">,</span> <span class="s2">"first_value"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">store2</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"first_key"</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.FileStore.__init__">
__init__(self:torch._C._distributed_c10d.FileStore, file_name: str, world_size: int = -1) → None ¶</dt>
<dd><p>创建一个新的 FileStore。</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributed.FileStore.path">
属性路径</dt>
<dd><p>获取 FileStore 存储键值对所使用的文件路径。</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.PrefixStore">
类 torch.distributed.PrefixStore</dt>
<dd><p>对 3 个键值存储（ <code class="xref py py-class docutils literal "><span class="pre">TCPStore</span></code> 、 <code class="xref py py-class docutils literal "><span class="pre">FileStore</span></code> 和 <code class="xref py py-class docutils literal "><span class="pre">HashStore</span></code> ）中的任何一个进行包装，为存储中插入的每个键添加前缀。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>prefix (str) – 在每个键插入存储之前附加的前缀字符串。</p></li>
<li><p>store (torch.distributed.store) – 构成底层键值存储的存储对象。</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch.distributed.PrefixStore.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.PrefixStore</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">store</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch._C._distributed_c10d.Store</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#torch.distributed.PrefixStore.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>创建一个新的 PrefixStore。</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torch.distributed.PrefixStore.underlying_store">
属性 underlying_store ¶</dt>
<dd><p>获取 PrefixStore 封装的底层存储对象。</p>
</dd></dl>

</dd></dl>

</section>
<section id="profiling-collective-communication">
<h2>集体通信分析 ¶</h2>
<p>注意，您可以使用 <code class="docutils literal "><span class="pre">torch.profiler</span></code> （推荐，仅在 1.8.1 之后可用）或 <code class="docutils literal "><span class="pre">torch.autograd.profiler</span></code> 来分析此处提到的集体通信和点对点通信 API。所有开箱即用的后端（ <code class="docutils literal "><span class="pre">gloo</span></code> ， <code class="docutils literal "><span class="pre">nccl</span></code> ， <code class="docutils literal "><span class="pre">mpi</span></code> ）都受支持，并且集体通信的使用将在分析输出/跟踪中按预期呈现。分析您的代码与任何常规 torch 操作相同：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="p">():</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</pre></div>
</div>
<p>请参阅分析器文档以全面了解分析器功能。</p>
</section>
<section id="multi-gpu-collective-functions">
<h2>多 GPU 集体函数 ¶</h2>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>多 GPU 函数（代表每个 CPU 线程有多个 GPU）已被弃用。截至今天，PyTorch 分布式首选的编程模型是每个线程一个设备，如本文档中的 API 所示。如果您是后端开发者并希望支持每个线程多个设备，请联系 PyTorch 分布式维护者。</p>
</div>
</section>
<section id="third-party-backends">
<span id="distributed-launch"></span><h2>第三方后端 ¶</h2>
<p>除了内置的 GLOO/MPI/NCCL 后端，PyTorch 分布式还通过运行时注册机制支持第三方后端。有关如何通过 C++ 扩展开发第三方后端的参考，请参阅教程 - 自定义 C++ 和 CUDA 扩展和 <code class="docutils literal "><span class="pre">test/cpp_extensions/cpp_c10d_extension.cpp</span></code> 。第三方后端的能力由它们自己的实现决定。</p>
<p>新后端继承自 <code class="docutils literal "><span class="pre">c10d::ProcessGroup</span></code> ，并在导入时通过 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.Backend.register_backend()</span></code> 注册后端名称和实例化接口。</p>
<p>当手动导入此后端并使用相应的后端名称调用 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.init_process_group()</span></code> 时， <code class="docutils literal "><span class="pre">torch.distributed</span></code> 包将在此新后端上运行。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>第三方后端的支持是实验性的，可能随时更改。</p>
</div>
</section>
<section id="launch-utility">
<h2>启动实用程序</h2>
<p>torch.distributed 包还提供了 torch.distributed.launch 的启动实用程序。这个辅助实用程序可以用于在每个节点上启动多个进程以进行分布式训练。</p>
<span class="target" id="module-torch.distributed.launch"></span><p>模块 <code class="docutils literal "><span class="pre">torch.distributed.launch</span></code> 。</p>
<p> <code class="docutils literal "><span class="pre">torch.distributed.launch</span></code> 是一个模块，可以在每个训练节点上启动多个分布式训练进程。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>此模块将因 torchrun 而弃用。</p>
</div>
<p>该实用程序可用于单节点分布式训练，其中每个节点将启动一个或多个进程。该实用程序可用于 CPU 训练或 GPU 训练。如果用于 GPU 训练，每个分布式进程将在单个 GPU 上运行。这可以实现单节点训练性能的显著提升。它还可以用于多节点分布式训练，通过在每个节点上启动多个进程，以实现多节点分布式训练性能的显著提升。对于具有多个 Infiniband 接口且支持直接 GPU 的系统，这将特别有益，因为所有这些接口都可以用于聚合通信带宽。</p>
<p>在单节点分布式训练或多节点分布式训练的两种情况下，此实用程序将启动每个节点上的给定数量的进程（ <code class="docutils literal "><span class="pre">--nproc-per-node</span></code> ）。如果用于 GPU 训练，此数量需要小于或等于当前系统上的 GPU 数量（ <code class="docutils literal "><span class="pre">nproc_per_node</span></code> ），并且每个进程将运行在从 GPU 0 到 GPU（nproc_per_node - 1）的单个 GPU 上。</p>
<p><strong>如何使用此模块：</strong></p>
<ol class="arabic simple">
<li><p>单节点多进程分布式训练</p></li>
</ol>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc</span><span class="o">-</span><span class="n">per</span><span class="o">-</span><span class="n">node</span><span class="o">=</span><span class="n">NUM_GPUS_YOU_HAVE</span>
           <span class="n">YOUR_TRAINING_SCRIPT</span><span class="o">.</span><span class="n">py</span> <span class="p">(</span><span class="o">--</span><span class="n">arg1</span> <span class="o">--</span><span class="n">arg2</span> <span class="o">--</span><span class="n">arg3</span> <span class="ow">and</span> <span class="nb">all</span> <span class="n">other</span>
           <span class="n">arguments</span> <span class="n">of</span> <span class="n">your</span> <span class="n">training</span> <span class="n">script</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>多节点多进程分布式训练：（例如两个节点）</p></li>
</ol>
<p>节点 1：（IP：192.168.1.1，并有一个空闲端口：1234）</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc</span><span class="o">-</span><span class="n">per</span><span class="o">-</span><span class="n">node</span><span class="o">=</span><span class="n">NUM_GPUS_YOU_HAVE</span>
           <span class="o">--</span><span class="n">nnodes</span><span class="o">=</span><span class="mi">2</span> <span class="o">--</span><span class="n">node</span><span class="o">-</span><span class="n">rank</span><span class="o">=</span><span class="mi">0</span> <span class="o">--</span><span class="n">master</span><span class="o">-</span><span class="n">addr</span><span class="o">=</span><span class="s2">"192.168.1.1"</span>
           <span class="o">--</span><span class="n">master</span><span class="o">-</span><span class="n">port</span><span class="o">=</span><span class="mi">1234</span> <span class="n">YOUR_TRAINING_SCRIPT</span><span class="o">.</span><span class="n">py</span> <span class="p">(</span><span class="o">--</span><span class="n">arg1</span> <span class="o">--</span><span class="n">arg2</span> <span class="o">--</span><span class="n">arg3</span>
           <span class="ow">and</span> <span class="nb">all</span> <span class="n">other</span> <span class="n">arguments</span> <span class="n">of</span> <span class="n">your</span> <span class="n">training</span> <span class="n">script</span><span class="p">)</span>
</pre></div>
</div>
<p>节点 2：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc</span><span class="o">-</span><span class="n">per</span><span class="o">-</span><span class="n">node</span><span class="o">=</span><span class="n">NUM_GPUS_YOU_HAVE</span>
           <span class="o">--</span><span class="n">nnodes</span><span class="o">=</span><span class="mi">2</span> <span class="o">--</span><span class="n">node</span><span class="o">-</span><span class="n">rank</span><span class="o">=</span><span class="mi">1</span> <span class="o">--</span><span class="n">master</span><span class="o">-</span><span class="n">addr</span><span class="o">=</span><span class="s2">"192.168.1.1"</span>
           <span class="o">--</span><span class="n">master</span><span class="o">-</span><span class="n">port</span><span class="o">=</span><span class="mi">1234</span> <span class="n">YOUR_TRAINING_SCRIPT</span><span class="o">.</span><span class="n">py</span> <span class="p">(</span><span class="o">--</span><span class="n">arg1</span> <span class="o">--</span><span class="n">arg2</span> <span class="o">--</span><span class="n">arg3</span>
           <span class="ow">and</span> <span class="nb">all</span> <span class="n">other</span> <span class="n">arguments</span> <span class="n">of</span> <span class="n">your</span> <span class="n">training</span> <span class="n">script</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>查看此模块提供的可选参数：</p></li>
</ol>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">help</span>
</pre></div>
</div>
<p><strong>重要通知：</strong></p>
<p>1. 此实用程序和多进程分布式（单节点或多节点）GPU 训练目前仅通过 NCCL 分布式后端实现最佳性能。因此，NCCL 后端是推荐用于 GPU 训练的后端。</p>
<p>2. 在您的训练程序中，您必须解析命令行参数： <code class="docutils literal "><span class="pre">--local-rank=LOCAL_PROCESS_RANK</span></code> ，该参数将由本模块提供。如果您的训练程序使用 GPU，您应确保您的代码仅在 LOCAL_PROCESS_RANK 的 GPU 设备上运行。这可以通过以下方式实现：</p>
<p>解析本地 rank 参数</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">"--local-rank"</span><span class="p">,</span> <span class="s2">"--local_rank"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</pre></div>
</div>
<p>使用以下任一方式设置您的设备为本地 rank</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>  <span class="c1"># before your code runs</span>
</pre></div>
</div>
<p>或者</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="c1"># your code to run</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="o">...</span>
</pre></div>
</div>
<div class="versionchanged">
<p>版本 2.0.0 变更：启动器将传递 <code class="docutils literal "><span class="pre">--local-rank=&lt;rank&gt;</span></code> 参数给您的脚本。从 PyTorch 2.0.0 版本开始，推荐使用破折号 <code class="docutils literal "><span class="pre">--local-rank</span></code> ，而不是之前使用的下划线 <code class="docutils literal "><span class="pre">--local_rank</span></code> 。</p>
<p>为了向后兼容，用户可能需要在他们的参数解析代码中处理这两种情况。这意味着在参数解析器中包括 <code class="docutils literal "><span class="pre">"--local-rank"</span></code> 和 <code class="docutils literal "><span class="pre">"--local_rank"</span></code> 。如果只提供 <code class="docutils literal "><span class="pre">"--local_rank"</span></code> ，启动器将触发错误：“错误：未识别的参数：–local-rank=”。对于仅支持 PyTorch 2.0.0+的训练代码，包括 <code class="docutils literal "><span class="pre">"--local-rank"</span></code> 应该足够。</p>
</div>
<p>3. 在您的训练程序中，您应该在开始时调用以下函数以启动分布式后端。强烈建议使用 <code class="docutils literal "><span class="pre">init_method=env://</span></code> 。其他初始化方法（例如 <code class="docutils literal "><span class="pre">tcp://</span></code> ）可能可行，但 <code class="docutils literal "><span class="pre">env://</span></code> 是本模块官方支持的方法。</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">'YOUR BACKEND'</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                     <span class="n">init_method</span><span class="o">=</span><span class="s1">'env://'</span><span class="p">)</span>
</pre></div>
</div>
<p>4. 在您的训练程序中，您可以使用常规的分布式函数或使用 <code class="xref py py-func docutils literal "><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code> 模块。如果您的训练程序使用 GPU 进行训练并希望使用 <code class="xref py py-func docutils literal "><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code> 模块，以下是配置方法。</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                                  <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                                                  <span class="n">output_device</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>
</pre></div>
</div>
<p>请确保将 <code class="docutils literal "><span class="pre">device_ids</span></code> 参数设置为您的代码将操作的唯一 GPU 设备 ID。这通常是进程的本地 rank。换句话说， <code class="docutils literal "><span class="pre">device_ids</span></code> 需要是 <code class="docutils literal "><span class="pre">[args.local_rank]</span></code> ， <code class="docutils literal "><span class="pre">output_device</span></code> 需要是 <code class="docutils literal "><span class="pre">args.local_rank</span></code> ，以便使用此实用程序。</p>
<p>5. 另一种通过环境变量 <code class="docutils literal "><span class="pre">LOCAL_RANK</span></code> 将 <code class="docutils literal "><span class="pre">local_rank</span></code> 传递给子进程的方法。当您使用 <code class="docutils literal "><span class="pre">--use-env=True</span></code> 启动脚本时，此行为会被启用。您必须调整上述子进程示例，将 <code class="docutils literal "><span class="pre">args.local_rank</span></code> 替换为 <code class="docutils literal "><span class="pre">os.environ['LOCAL_RANK']</span></code> ；当您指定此标志时，启动器不会传递 <code class="docutils literal "><span class="pre">--local-rank</span></code> 。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p> <code class="docutils literal "><span class="pre">local_rank</span></code> 并非全局唯一：它仅在机器上的每个进程中是唯一的。因此，不要用它来决定是否，例如，写入网络文件系统。有关如何正确执行此操作的示例，请参阅 https://github.com/pytorch/pytorch/issues/12042。</p>
</div>
</section>
<section id="spawn-utility">
<h2>生成工具 ¶</h2>
<p>多进程包 - torch.multiprocessing 包也提供了一个 <code class="docutils literal "><span class="pre">spawn</span></code> 函数在 <code class="xref py py-func docutils literal "><span class="pre">torch.multiprocessing.spawn()</span></code> 中。此辅助函数可以用来生成多个进程。它通过传入要运行的函数并生成 N 个进程来运行它。这也可以用于多进程分布式训练。</p>
<p>有关如何使用它的参考，请参阅 PyTorch 示例 - ImageNet 实现</p>
<p>注意，此功能需要 Python 3.4 或更高版本。</p>
</section>
<section id="debugging-torch-distributed-applications">
<h2>调试 <code class="docutils literal "><span class="pre">torch.distributed</span></code> 应用程序</h2>
<p>调试分布式应用程序可能具有挑战性，因为难以理解的挂起、崩溃或跨排名不一致的行为。 <code class="docutils literal "><span class="pre">torch.distributed</span></code> 提供了一套工具，以自助方式帮助调试训练应用程序：</p>
<section id="python-breakpoint">
<h3>Python 断点</h3>
<p>在分布式环境中使用 Python 的调试器非常方便，但因为它默认不工作，所以很多人根本不使用它。PyTorch 提供了一个围绕 pdb 的定制包装器，简化了这一过程。</p>
<p>torch.distributed.breakpoint 使得这个过程变得简单。内部，它以两种方式定制了 pdb 的断点行为，但其他方面则与正常的 pdb 相同。1. 仅在用户指定的 rank 上附加调试器。2. 通过使用 torch.distributed.barrier() 确保所有其他 rank 停止，一旦调试 rank 发出 continue 命令，该 barrier 将释放。3. 将子进程的 stdin 重定向，使其连接到您的终端。</p>
<p>要使用它，只需在所有 rank 上发出 torch.distributed.breakpoint(rank)，每个情况下的 rank 值都相同。</p>
</section>
<section id="monitored-barrier">
<h3>监控屏障</h3>
<p>截至 v1.10 版本， <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.monitored_barrier()</span></code> 作为 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.barrier()</span></code> 的替代方案存在，当崩溃时，<code class="xref py py-func docutils literal "><span class="pre">torch.distributed.barrier()</span></code> 会提供有关可能出错的排名的有用信息，即不是所有排名在提供的超时时间内调用 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.monitored_barrier()</span></code> 。 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.monitored_barrier()</span></code> 通过使用 <code class="docutils literal "><span class="pre">send</span></code> / <code class="docutils literal "><span class="pre">recv</span></code> 通信原语实现主机端屏障，其过程类似于确认，允许排名 0 报告哪些排名未能及时确认屏障。例如，考虑以下函数，其中排名 1 未调用 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.monitored_barrier()</span></code> （在实际中，这可能是由于应用程序错误或前一个集体操作中的挂起）：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>


<span class="k">def</span> <span class="nf">worker</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">"nccl"</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># monitored barrier requires gloo process group to perform host-side sync.</span>
    <span class="n">group_gloo</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">"gloo"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">monitored_barrier</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="n">group_gloo</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"MASTER_ADDR"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"localhost"</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"MASTER_PORT"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"29501"</span>
    <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">worker</span><span class="p">,</span> <span class="n">nprocs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">())</span>
</pre></div>
</div>
<p>在排名 0 上产生以下错误信息，使用户能够确定哪些排名可能存在问题，并进一步调查：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">Rank</span> <span class="mi">1</span> <span class="n">failed</span> <span class="n">to</span> <span class="k">pass</span> <span class="n">monitoredBarrier</span> <span class="ow">in</span> <span class="mi">2000</span> <span class="n">ms</span>
 <span class="n">Original</span> <span class="n">exception</span><span class="p">:</span>
<span class="p">[</span><span class="n">gloo</span><span class="o">/</span><span class="n">transport</span><span class="o">/</span><span class="n">tcp</span><span class="o">/</span><span class="n">pair</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">598</span><span class="p">]</span> <span class="n">Connection</span> <span class="n">closed</span> <span class="n">by</span> <span class="n">peer</span> <span class="p">[</span><span class="mi">2401</span><span class="p">:</span><span class="n">db00</span><span class="p">:</span><span class="n">eef0</span><span class="p">:</span><span class="mi">1100</span><span class="p">:</span><span class="mi">3560</span><span class="p">:</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="n">c05</span><span class="p">:</span><span class="mi">25</span><span class="n">d</span><span class="p">]:</span><span class="mi">8594</span>
</pre></div>
</div>
</section>
<section id="torch-distributed-debug">
<h3><code class="docutils literal "><span class="pre">TORCH_DISTRIBUTED_DEBUG</span></code><a class="headerlink" href="#torch-distributed-debug" title="Permalink to this heading">¶</a></h3>
<p>使用 <code class="docutils literal "><span class="pre">TORCH_CPP_LOG_LEVEL=INFO</span></code> ，可以通过环境变量 <code class="docutils literal "><span class="pre">TORCH_DISTRIBUTED_DEBUG</span></code> 触发额外的有用日志记录和集体同步检查，以确保所有排名都适当同步。 <code class="docutils literal "><span class="pre">TORCH_DISTRIBUTED_DEBUG</span></code> 可以设置为 <code class="docutils literal "><span class="pre">OFF</span></code> （默认值）、 <code class="docutils literal "><span class="pre">INFO</span></code> 或 <code class="docutils literal "><span class="pre">DETAIL</span></code> ，具体取决于所需的调试级别。请注意，最详细的选项 <code class="docutils literal "><span class="pre">DETAIL</span></code> 可能会影响应用程序性能，因此仅在调试问题时使用。</p>
<p>设置 <code class="docutils literal "><span class="pre">TORCH_DISTRIBUTED_DEBUG=INFO</span></code> 将在使用 <code class="xref py py-func docutils literal "><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code> 训练的模型初始化时产生额外的调试日志，并且 <code class="docutils literal "><span class="pre">TORCH_DISTRIBUTED_DEBUG=DETAIL</span></code> 将额外记录选定迭代次数的运行时性能统计信息。这些运行时统计信息包括前向时间、反向时间、梯度通信时间等数据。例如，给定以下应用程序：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>


<span class="k">class</span> <span class="nc">TwoLinLayerNet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">worker</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">"nccl"</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"init model"</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">TwoLinLayerNet</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"init ddp"</span><span class="p">)</span>
    <span class="n">ddp_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>

    <span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"train"</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">ddp_model</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"MASTER_ADDR"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"localhost"</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"MASTER_PORT"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"29501"</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"TORCH_CPP_LOG_LEVEL"</span><span class="p">]</span><span class="o">=</span><span class="s2">"INFO"</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span>
        <span class="s2">"TORCH_DISTRIBUTED_DEBUG"</span>
    <span class="p">]</span> <span class="o">=</span> <span class="s2">"DETAIL"</span>  <span class="c1"># set to DETAIL for runtime logging.</span>
    <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">worker</span><span class="p">,</span> <span class="n">nprocs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">())</span>
</pre></div>
</div>
<p>初始化时渲染以下日志：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="n">I0607</span> <span class="mi">16</span><span class="p">:</span><span class="mi">10</span><span class="p">:</span><span class="mf">35.739390</span> <span class="mi">515217</span> <span class="n">logger</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">173</span><span class="p">]</span> <span class="p">[</span><span class="n">Rank</span> <span class="mi">0</span><span class="p">]:</span> <span class="n">DDP</span> <span class="n">Initialized</span> <span class="k">with</span><span class="p">:</span>
<span class="n">broadcast_buffers</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">bucket_cap_bytes</span><span class="p">:</span> <span class="mi">26214400</span>
<span class="n">find_unused_parameters</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">gradient_as_bucket_view</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">is_multi_device_module</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">iteration</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">num_parameter_tensors</span><span class="p">:</span> <span class="mi">2</span>
<span class="n">output_device</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">rank</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">total_parameter_size_bytes</span><span class="p">:</span> <span class="mi">440</span>
<span class="n">world_size</span><span class="p">:</span> <span class="mi">2</span>
<span class="n">backend_name</span><span class="p">:</span> <span class="n">nccl</span>
<span class="n">bucket_sizes</span><span class="p">:</span> <span class="mi">440</span>
<span class="n">cuda_visible_devices</span><span class="p">:</span> <span class="n">N</span><span class="o">/</span><span class="n">A</span>
<span class="n">device_ids</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">dtypes</span><span class="p">:</span> <span class="nb">float</span>
<span class="n">master_addr</span><span class="p">:</span> <span class="n">localhost</span>
<span class="n">master_port</span><span class="p">:</span> <span class="mi">29501</span>
<span class="n">module_name</span><span class="p">:</span> <span class="n">TwoLinLayerNet</span>
<span class="n">nccl_async_error_handling</span><span class="p">:</span> <span class="n">N</span><span class="o">/</span><span class="n">A</span>
<span class="n">nccl_blocking_wait</span><span class="p">:</span> <span class="n">N</span><span class="o">/</span><span class="n">A</span>
<span class="n">nccl_debug</span><span class="p">:</span> <span class="n">WARN</span>
<span class="n">nccl_ib_timeout</span><span class="p">:</span> <span class="n">N</span><span class="o">/</span><span class="n">A</span>
<span class="n">nccl_nthreads</span><span class="p">:</span> <span class="n">N</span><span class="o">/</span><span class="n">A</span>
<span class="n">nccl_socket_ifname</span><span class="p">:</span> <span class="n">N</span><span class="o">/</span><span class="n">A</span>
<span class="n">torch_distributed_debug</span><span class="p">:</span> <span class="n">INFO</span>
</pre></div>
</div>
<p>在运行时（当设置 <code class="docutils literal "><span class="pre">TORCH_DISTRIBUTED_DEBUG=DETAIL</span></code> 时）渲染以下日志：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="n">I0607</span> <span class="mi">16</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mf">58.085681</span> <span class="mi">544067</span> <span class="n">logger</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">344</span><span class="p">]</span> <span class="p">[</span><span class="n">Rank</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">]</span> <span class="n">Training</span> <span class="n">TwoLinLayerNet</span> <span class="n">unused_parameter_size</span><span class="o">=</span><span class="mi">0</span>
 <span class="n">Avg</span> <span class="n">forward</span> <span class="n">compute</span> <span class="n">time</span><span class="p">:</span> <span class="mi">40838608</span>
 <span class="n">Avg</span> <span class="n">backward</span> <span class="n">compute</span> <span class="n">time</span><span class="p">:</span> <span class="mi">5983335</span>
<span class="n">Avg</span> <span class="n">backward</span> <span class="n">comm</span><span class="o">.</span> <span class="n">time</span><span class="p">:</span> <span class="mi">4326421</span>
 <span class="n">Avg</span> <span class="n">backward</span> <span class="n">comm</span><span class="o">/</span><span class="n">comp</span> <span class="n">overlap</span> <span class="n">time</span><span class="p">:</span> <span class="mi">4207652</span>
<span class="n">I0607</span> <span class="mi">16</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mf">58.085693</span> <span class="mi">544066</span> <span class="n">logger</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">344</span><span class="p">]</span> <span class="p">[</span><span class="n">Rank</span> <span class="mi">0</span> <span class="o">/</span> <span class="mi">2</span><span class="p">]</span> <span class="n">Training</span> <span class="n">TwoLinLayerNet</span> <span class="n">unused_parameter_size</span><span class="o">=</span><span class="mi">0</span>
 <span class="n">Avg</span> <span class="n">forward</span> <span class="n">compute</span> <span class="n">time</span><span class="p">:</span> <span class="mi">42850427</span>
 <span class="n">Avg</span> <span class="n">backward</span> <span class="n">compute</span> <span class="n">time</span><span class="p">:</span> <span class="mi">3885553</span>
<span class="n">Avg</span> <span class="n">backward</span> <span class="n">comm</span><span class="o">.</span> <span class="n">time</span><span class="p">:</span> <span class="mi">2357981</span>
 <span class="n">Avg</span> <span class="n">backward</span> <span class="n">comm</span><span class="o">/</span><span class="n">comp</span> <span class="n">overlap</span> <span class="n">time</span><span class="p">:</span> <span class="mi">2234674</span>
</pre></div>
</div>
<p>此外， <code class="docutils literal "><span class="pre">TORCH_DISTRIBUTED_DEBUG=INFO</span></code> 由于模型中存在未使用的参数，增强了 <code class="xref py py-func docutils literal "><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code> 的崩溃日志功能。目前，如果正向传递中可能存在未使用的参数，则必须将 <code class="docutils literal "><span class="pre">find_unused_parameters=True</span></code> 传递给 <code class="xref py py-func docutils literal "><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code> 的初始化，并且从 v1.10 版本开始，所有模型输出都必须用于损失计算，因为 <code class="xref py py-func docutils literal "><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code> 不支持反向传递中的未使用参数。这些限制对于大型模型尤其具有挑战性，因此当发生错误崩溃时， <code class="xref py py-func docutils literal "><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code> 将记录所有未使用的参数的完全限定名称。例如，在上面的应用程序中，如果我们修改 <code class="docutils literal "><span class="pre">loss</span></code> 以计算为 <code class="docutils literal "><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">output[1]</span></code> ，那么 <code class="docutils literal "><span class="pre">TwoLinLayerNet.a</span></code> 在反向传递中不会接收到梯度，从而导致 <code class="docutils literal "><span class="pre">DDP</span></code> 失败。在崩溃时，用户会收到有关未使用参数的信息，对于大型模型来说，手动查找这些信息可能具有挑战性：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span>RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing
 the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return va
lue of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: a.weight
Parameter indices which did not receive grad for rank 0: 0
</pre></div>
</div>
<p>设置 <code class="docutils literal "><span class="pre">TORCH_DISTRIBUTED_DEBUG=DETAIL</span></code> 将触发用户发出的每个集体调用（无论是直接还是间接，如 DDP <code class="docutils literal "><span class="pre">allreduce</span></code> ）的额外一致性和同步检查。这是通过创建一个包装进程组来完成的，该进程组包装了 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.init_process_group()</span></code> 和 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.new_group()</span></code> API 返回的所有进程组。因此，这些 API 将返回一个包装进程组，它可以像常规进程组一样使用，但在将集体调度到底层进程组之前执行一致性检查。目前，这些检查包括一个 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.monitored_barrier()</span></code> ，确保所有进程完成其未完成的集体调用，并报告卡住的进程。然后，通过确保所有集体函数匹配并且使用一致的张量形状调用，对集体本身进行一致性检查。如果不一致，则在应用程序崩溃时包含详细的错误报告，而不是挂起或无信息的错误消息。例如，考虑以下具有与 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.all_reduce()</span></code> 不匹配输入形状的函数：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>


<span class="k">def</span> <span class="nf">worker</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">"nccl"</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span> <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"MASTER_ADDR"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"localhost"</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"MASTER_PORT"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"29501"</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"TORCH_CPP_LOG_LEVEL"</span><span class="p">]</span><span class="o">=</span><span class="s2">"INFO"</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"TORCH_DISTRIBUTED_DEBUG"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"DETAIL"</span>
    <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">worker</span><span class="p">,</span> <span class="n">nprocs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">())</span>
</pre></div>
</div>
<p>使用 <code class="docutils literal "><span class="pre">NCCL</span></code> 后端，这样的应用程序可能会挂起，这在非平凡场景中很难找到根本原因。如果用户启用 <code class="docutils literal "><span class="pre">TORCH_DISTRIBUTED_DEBUG=DETAIL</span></code> 并重新运行应用程序，以下错误消息将揭示根本原因：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="n">work</span> <span class="o">=</span> <span class="n">default_pg</span><span class="o">.</span><span class="n">allreduce</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">opts</span><span class="p">)</span>
<span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">Error</span> <span class="n">when</span> <span class="n">verifying</span> <span class="n">shape</span> <span class="n">tensors</span> <span class="k">for</span> <span class="n">collective</span> <span class="n">ALLREDUCE</span> <span class="n">on</span> <span class="n">rank</span> <span class="mf">0.</span> <span class="n">This</span> <span class="n">likely</span> <span class="n">indicates</span> <span class="n">that</span> <span class="nb">input</span> <span class="n">shapes</span> <span class="n">into</span> <span class="n">the</span> <span class="n">collective</span> <span class="n">are</span> <span class="n">mismatched</span> <span class="n">across</span> <span class="n">ranks</span><span class="o">.</span> <span class="n">Got</span> <span class="n">shapes</span><span class="p">:</span>  <span class="mi">10</span>
<span class="p">[</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span> <span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>在运行时对调试级别进行细粒度控制时，还可以使用 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.set_debug_level()</span></code> 、 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.set_debug_level_from_env()</span></code> 和 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.get_debug_level()</span></code> 函数。</p>
</div>
<p>此外，可以使用 TORCH_DISTRIBUTED_DEBUG=DETAIL 与 TORCH_SHOW_CPP_STACKTRACES=1 结合使用，以记录检测到集体不同步时的整个调用栈。这些集体不同步检查将适用于所有使用 <code class="docutils literal "><span class="pre">c10d</span></code> 集体调用并由 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.init_process_group()</span></code> 和 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.new_group()</span></code> API 创建的进程组支持的应用程序。</p>
</section>
</section>
<section id="logging">
<h2>记录日志</h2>
<p>除了通过 <code class="xref py py-func docutils literal "><span class="pre">torch.distributed.monitored_barrier()</span></code> 和 <code class="docutils literal "><span class="pre">TORCH_DISTRIBUTED_DEBUG</span></code> 的显式调试支持外， <code class="docutils literal "><span class="pre">torch.distributed</span></code> 的底层 C++ 库还会以各种级别输出日志消息。这些消息有助于理解分布式训练作业的执行状态，以及排查网络连接故障等问题。以下矩阵显示了如何通过组合 <code class="docutils literal "><span class="pre">TORCH_CPP_LOG_LEVEL</span></code> 和 <code class="docutils literal "><span class="pre">TORCH_DISTRIBUTED_DEBUG</span></code> 环境变量来调整日志级别。</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><code class="docutils literal "><span class="pre">TORCH_CPP_LOG_LEVEL</span></code></p></th>
<th class="head"><p><code class="docutils literal "><span class="pre">TORCH_DISTRIBUTED_DEBUG</span></code></p></th>
<th class="head"><p>有效日志级别</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal "><span class="pre">ERROR</span></code></p></td>
<td><p>忽略</p></td>
<td><p>错误</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal "><span class="pre">WARNING</span></code></p></td>
<td><p>忽略</p></td>
<td><p>警告</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal "><span class="pre">INFO</span></code></p></td>
<td><p>忽略</p></td>
<td><p>信息</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal "><span class="pre">INFO</span></code></p></td>
<td><p><code class="docutils literal "><span class="pre">INFO</span></code></p></td>
<td><p>调试</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal "><span class="pre">INFO</span></code></p></td>
<td><p><code class="docutils literal "><span class="pre">DETAIL</span></code></p></td>
<td><p>跟踪（又称全部）</p></td>
</tr>
</tbody>
</table>
<p>分布式组件引发自定义的从 RuntimeError 派生的异常类型：</p>
<ul class="simple">
<li><p>torch.distributed.DistError：这是所有分布式异常的基类型。</p></li>
<li><p>torch.distributed.DistBackendError：当发生特定后端错误时抛出此异常。例如，如果使用 NCCL 后端，并且用户尝试使用 NCCL 库不可用的 GPU。</p></li>
<li><p>torch.distributed.DistNetworkError：当网络库遇到错误时抛出此异常（例如：连接由对方重置）</p></li>
<li><p>torch.distributed.DistStoreError：当存储遇到错误时抛出此异常（例如：TCPStore 超时）</p></li>
</ul>
<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.DistError">
类 torch.distributed.DistError</dt>
<dd><p>分布式库发生错误时引发的异常</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.DistBackendError">
torch.distributed.DistBackendError 类</dt>
<dd><p>在分布式计算中发生后端错误时引发的异常</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.DistNetworkError">
class torch.distributed.DistNetworkError</dt>
<dd><p>在分布式计算中发生网络错误时引发的异常</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.distributed.DistStoreError">
class torch.distributed.DistStoreError</dt>
<dd><p>在分布式存储中发生错误时引发的异常</p>
</dd></dl>

<p>如果您正在运行单节点训练，那么可能方便地交互式地中断您的脚本。我们提供了一种方便地中断单个 rank 的方法：</p>
<dl class="py function">
<dt class="sig sig-object py" id="torch.distributed.breakpoint">
torch.distributed.breakpoint(rank=0, skip=0)[source][source]</dt>
<dd><p>设置断点，但仅限于单个 rank。其他 rank 将等待您完成断点后再继续。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>rank（整数）- 要在哪个 rank 上断点。默认： <code class="docutils literal "><span class="pre">0</span></code> </p></li>
<li><p>skip（整数）- 跳过第一个 <code class="docutils literal "><span class="pre">skip</span></code> 次对该断点的调用。默认： <code class="docutils literal "><span class="pre">0</span></code> 。</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-torch.distributed.algorithms"></span><span class="target" id="module-torch.distributed.algorithms.ddp_comm_hooks"></span><span class="target" id="module-torch.distributed.algorithms.model_averaging"></span><span class="target" id="module-torch.distributed.elastic"></span><span class="target" id="module-torch.distributed.elastic.utils"></span><span class="target" id="module-torch.distributed.elastic.utils.data"></span><span class="target" id="module-torch.distributed.launcher"></span><span class="target" id="module-torch.distributed.nn"></span><span class="target" id="module-torch.distributed.nn.api"></span><span class="target" id="module-torch.distributed.nn.jit"></span><span class="target" id="module-torch.distributed.nn.jit.templates"></span><span class="target" id="module-torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook"></span><span class="target" id="module-torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks"></span><span class="target" id="module-torch.distributed.algorithms.ddp_comm_hooks.default_hooks"></span><span class="target" id="module-torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks"></span><span class="target" id="module-torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks"></span><span class="target" id="module-torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook"></span><span class="target" id="module-torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook"></span><span class="target" id="module-torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks"></span><span class="target" id="module-torch.distributed.algorithms.join"></span><span class="target" id="module-torch.distributed.algorithms.model_averaging.averagers"></span><span class="target" id="module-torch.distributed.algorithms.model_averaging.hierarchical_model_averager"></span><span class="target" id="module-torch.distributed.algorithms.model_averaging.utils"></span><span class="target" id="module-torch.distributed.argparse_util"></span><span class="target" id="module-torch.distributed.c10d_logger"></span><span class="target" id="module-torch.distributed.checkpoint.api"></span><span class="target" id="module-torch.distributed.checkpoint.default_planner"></span><span class="target" id="module-torch.distributed.checkpoint.filesystem"></span><span class="target" id="module-torch.distributed.checkpoint.metadata"></span><span class="target" id="module-torch.distributed.checkpoint.optimizer"></span><span class="target" id="module-torch.distributed.checkpoint.planner"></span><span class="target" id="module-torch.distributed.checkpoint.planner_helpers"></span><span class="target" id="module-torch.distributed.checkpoint.resharding"></span><span class="target" id="module-torch.distributed.checkpoint.state_dict_loader"></span><span class="target" id="module-torch.distributed.checkpoint.state_dict_saver"></span><span class="target" id="module-torch.distributed.checkpoint.stateful"></span><span class="target" id="module-torch.distributed.checkpoint.storage"></span><span class="target" id="module-torch.distributed.checkpoint.utils"></span><span class="target" id="module-torch.distributed.collective_utils"></span><span class="target" id="module-torch.distributed.constants"></span><span class="target" id="module-torch.distributed.device_mesh"></span><span class="target" id="module-torch.distributed.distributed_c10d"></span><span class="target" id="module-torch.distributed.elastic.agent.server.api"></span><span class="target" id="module-torch.distributed.elastic.agent.server.local_elastic_agent"></span><span class="target" id="module-torch.distributed.elastic.events.api"></span><span class="target" id="module-torch.distributed.elastic.events.handlers"></span><span class="target" id="module-torch.distributed.elastic.metrics.api"></span><span class="target" id="module-torch.distributed.elastic.multiprocessing.api"></span><span class="target" id="module-torch.distributed.elastic.multiprocessing.errors.error_handler"></span><span class="target" id="module-torch.distributed.elastic.multiprocessing.errors.handlers"></span><span class="target" id="module-torch.distributed.elastic.multiprocessing.redirects"></span><span class="target" id="module-torch.distributed.elastic.multiprocessing.tail_log"></span><span class="target" id="module-torch.distributed.elastic.rendezvous.api"></span><span class="target" id="module-torch.distributed.elastic.rendezvous.c10d_rendezvous_backend"></span><span class="target" id="module-torch.distributed.elastic.rendezvous.dynamic_rendezvous"></span><span class="target" id="module-torch.distributed.elastic.rendezvous.etcd_rendezvous"></span><span class="target" id="module-torch.distributed.elastic.rendezvous.etcd_rendezvous_backend"></span><span class="target" id="module-torch.distributed.elastic.rendezvous.etcd_server"></span><span class="target" id="module-torch.distributed.elastic.rendezvous.etcd_store"></span><span class="target" id="module-torch.distributed.elastic.rendezvous.static_tcp_rendezvous"></span><span class="target" id="module-torch.distributed.elastic.rendezvous.utils"></span><span class="target" id="module-torch.distributed.elastic.timer.api"></span><span class="target" id="module-torch.distributed.elastic.timer.file_based_local_timer"></span><span class="target" id="module-torch.distributed.elastic.timer.local_timer"></span><span class="target" id="module-torch.distributed.elastic.utils.api"></span><span class="target" id="module-torch.distributed.elastic.utils.data.cycling_iterator"></span><span class="target" id="module-torch.distributed.elastic.utils.data.elastic_distributed_sampler"></span><span class="target" id="module-torch.distributed.elastic.utils.distributed"></span><span class="target" id="module-torch.distributed.elastic.utils.log_level"></span><span class="target" id="module-torch.distributed.elastic.utils.logging"></span><span class="target" id="module-torch.distributed.elastic.utils.store"></span><span class="target" id="module-torch.distributed.fsdp.api"></span><span class="target" id="module-torch.distributed.fsdp.fully_sharded_data_parallel"></span><span class="target" id="module-torch.distributed.fsdp.sharded_grad_scaler"></span><span class="target" id="module-torch.distributed.fsdp.wrap"></span><span class="target" id="module-torch.distributed.launcher.api"></span><span class="target" id="module-torch.distributed.logging_handlers"></span><span class="target" id="module-torch.distributed.nn.api.remote_module"></span><span class="target" id="module-torch.distributed.nn.functional"></span><span class="target" id="module-torch.distributed.nn.jit.instantiator"></span><span class="target" id="module-torch.distributed.nn.jit.templates.remote_module_template"></span><span class="target" id="module-torch.distributed.optim.apply_optimizer_in_backward"></span><span class="target" id="module-torch.distributed.optim.functional_adadelta"></span><span class="target" id="module-torch.distributed.optim.functional_adagrad"></span><span class="target" id="module-torch.distributed.optim.functional_adam"></span><span class="target" id="module-torch.distributed.optim.functional_adamax"></span><span class="target" id="module-torch.distributed.optim.functional_adamw"></span><span class="target" id="module-torch.distributed.optim.functional_rmsprop"></span><span class="target" id="module-torch.distributed.optim.functional_rprop"></span><span class="target" id="module-torch.distributed.optim.functional_sgd"></span><span class="target" id="module-torch.distributed.optim.named_optimizer"></span><span class="target" id="module-torch.distributed.optim.optimizer"></span><span class="target" id="module-torch.distributed.optim.post_localSGD_optimizer"></span><span class="target" id="module-torch.distributed.optim.utils"></span><span class="target" id="module-torch.distributed.optim.zero_redundancy_optimizer"></span><span class="target" id="module-torch.distributed.remote_device"></span><span class="target" id="module-torch.distributed.rendezvous"></span><span class="target" id="module-torch.distributed.rpc.api"></span><span class="target" id="module-torch.distributed.rpc.backend_registry"></span><span class="target" id="module-torch.distributed.rpc.constants"></span><span class="target" id="module-torch.distributed.rpc.functions"></span><span class="target" id="module-torch.distributed.rpc.internal"></span><span class="target" id="module-torch.distributed.rpc.options"></span><span class="target" id="module-torch.distributed.rpc.rref_proxy"></span><span class="target" id="module-torch.distributed.rpc.server_process_global_profiler"></span><span class="target" id="module-torch.distributed.tensor.parallel.api"></span><span class="target" id="module-torch.distributed.tensor.parallel.ddp"></span><span class="target" id="module-torch.distributed.tensor.parallel.fsdp"></span><span class="target" id="module-torch.distributed.tensor.parallel.input_reshard"></span><span class="target" id="module-torch.distributed.tensor.parallel.loss"></span><span class="target" id="module-torch.distributed.tensor.parallel.style"></span><span class="target" id="module-torch.distributed.utils"></span><span class="target" id="module-torch.distributed.checkpoint.state_dict"></span></section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        下一个 <img height="16" width="16" class="next-page" src="_static/images/chevron-right-orange.svg"> <img height="16" width="16" class="previous-page" src="_static/images/chevron-right-orange.svg"> 上一个
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>© 版权所有 PyTorch 贡献者。</p>
  </div>
    
      <div>使用 Sphinx 构建，主题由 Read the Docs 提供。</div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">分布式通信包 - torch.distributed</a><ul>
<li><a class="reference internal" href="#backends">后端</a><ul>
<li><a class="reference internal" href="#backends-that-come-with-pytorch">带有 PyTorch 的后端</a></li>
<li><a class="reference internal" href="#which-backend-to-use">应该使用哪个后端？</a></li>
<li><a class="reference internal" href="#common-environment-variables">常见环境变量</a><ul>
<li><a class="reference internal" href="#choosing-the-network-interface-to-use">选择要使用的网络接口</a></li>
<li><a class="reference internal" href="#other-nccl-environment-variables">其他 NCCL 环境变量</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#basics">基础知识</a></li>
<li><a class="reference internal" href="#initialization">初始化</a><ul>
<li><a class="reference internal" href="#tcp-initialization">TCP 初始化</a></li>
<li><a class="reference internal" href="#shared-file-system-initialization">共享文件系统初始化</a></li>
<li><a class="reference internal" href="#environment-variable-initialization">环境变量初始化</a></li>
</ul>
</li>
<li><a class="reference internal" href="#post-initialization">初始化后</a></li>
<li><a class="reference internal" href="#shutdown">关闭</a><ul>
<li><a class="reference internal" href="#reinitialization">重置</a></li>
</ul>
</li>
<li><a class="reference internal" href="#groups">组</a></li>
<li><a class="reference internal" href="#devicemesh">设备网格</a></li>
<li><a class="reference internal" href="#point-to-point-communication">点对点通信</a></li>
<li><a class="reference internal" href="#synchronous-and-asynchronous-collective-operations">同步和异步的集体操作</a></li>
<li><a class="reference internal" href="#collective-functions">集合函数</a></li>
<li><a class="reference internal" href="#distributed-key-value-store">分布式键值存储</a></li>
<li><a class="reference internal" href="#profiling-collective-communication">集体通信性能分析</a></li>
<li><a class="reference internal" href="#multi-gpu-collective-functions">多 GPU 集体函数</a></li>
<li><a class="reference internal" href="#third-party-backends">第三方后端</a></li>
<li><a class="reference internal" href="#launch-utility">启动工具</a></li>
<li><a class="reference internal" href="#spawn-utility">生成工具</a></li>
<li><a class="reference internal" href="#debugging-torch-distributed-applications">调试 <code class="docutils literal "><span class="pre">torch.distributed</span></code> 应用程序</a><ul>
<li><a class="reference internal" href="#python-breakpoint">Python 断点</a></li>
<li><a class="reference internal" href="#monitored-barrier">监视屏障</a></li>
<li><a class="reference internal" href="#torch-distributed-debug"><code class="docutils literal "><span class="pre">TORCH_DISTRIBUTED_DEBUG</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#logging">记录</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script="" type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0">


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>文档</h2>
          <p>PyTorch 开发者文档全面访问</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">查看文档</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>教程</h2>
          <p>获取初学者和高级开发者的深入教程</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">查看教程</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>资源</h2>
          <p>查找开发资源并获得您的疑问解答</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">查看资源</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">开始使用</a></li>
            <li><a href="https://pytorch.org/features">功能</a></li>
            <li><a href="https://pytorch.org/ecosystem">生态系统</a></li>
            <li><a href="https://pytorch.org/blog/">博客</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">贡献</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">资源</a></li>
            <li><a href="https://pytorch.org/tutorials">教程</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">文档</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">讨论</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub 问题和任务</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">品牌指南</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">保持更新</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">推特</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">领英</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch 播客</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">苹果</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">谷歌</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">亚马逊</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">条款</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">隐私</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© 版权所有 Linux 基金会。PyTorch 基金会是 Linux 基金会的一个项目。有关本网站的使用条款、商标政策以及其他适用于 PyTorch 基金会的政策，请参阅 www.linuxfoundation.org/policies/。PyTorch 基金会支持 PyTorch 开源项目，该项目已被确立为 LF Projects, LLC 的 PyTorch 项目系列。有关适用于 PyTorch 项目系列 LF Projects, LLC 的政策，请参阅 www.lfprojects.org/policies/。</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">为分析流量并优化您的体验，我们在本网站上提供 cookies。通过点击或导航，您同意允许我们使用 cookies。作为本站点的当前维护者，Facebook 的 Cookies 政策适用。了解更多信息，包括可用的控制选项：Cookies 政策。</p>
    <img class="close-button" src="_static/images/pytorch-x.svg" width="16" height="16">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>学习</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">开始学习</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">教程</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">学习基础知识</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch 菜谱</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">PyTorch 入门 - YouTube 系列</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>生态系统</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">工具</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">社区</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">论坛</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">开发者资源</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">贡献者奖项 - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">关于 PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch 文档</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>文档</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch 领域</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>博客 &amp; 新闻</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch 博客</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">社区博客</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">视频</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">社区故事</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">活动</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">通讯</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>关于</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch 基金会</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">治理委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">云信用计划</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">技术顾问委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">员工</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">联系我们</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

</body></html>