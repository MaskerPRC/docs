<!DOCTYPE html>
<html lang="zh_CN">
<head>
  <meta charset="UTF-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch — PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/torch.html">
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css">
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css">
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css">
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css">
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css">
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css">
    <link rel="index" title="Index" href="genindex.html">
    <link rel="search" title="Search" href="search.html">
    <link rel="next" title="torch.is_tensor" href="generated/torch.is_tensor.html">
    <link rel="prev" title="torch::deploy has been moved to pytorch/multipy" href="deploy.html">

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head><body class="pytorch-body"><div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">学习</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class="dropdown-title">开始使用</span>
                  <p>在本地运行 PyTorch 或快速开始使用支持的云平台之一</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">教程</span><p></p>
                  <p>PyTorch 教程中的新内容</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">学习基础知识</span><p></p>
                  <p>熟悉 PyTorch 的概念和模块</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch 食谱</span><p></p>
                  <p>精简版、可直接部署的 PyTorch 代码示例</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">PyTorch 入门 - YouTube 系列</span><p></p>
                  <p>通过我们引人入胜的 YouTube 教程系列掌握 PyTorch 基础知识</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">生态系统</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">工具</span><p></p>
                  <p>了解 PyTorch 生态系统中的工具和框架</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">社区</span>
                  <p>加入 PyTorch 开发者社区，贡献、学习并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">论坛</span>
                  <p>讨论 PyTorch 代码、问题、安装、研究的地方</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">开发者资源</span>
                  <p>查找资源并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">贡献者奖项 - 2024</span><p></p>
                  <p>本届 PyTorch 会议揭晓获奖者</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">关于 PyTorch Edge</span><p></p>
                  <p>为边缘设备构建创新和隐私感知的 AI 体验</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span><p></p>
                  <p>基于移动和边缘设备的端到端推理能力解决方案</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch 文档</span><p></p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">文档</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span><p></p>
                  <p>探索文档以获取全面指导，了解如何使用 PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch 领域</span><p></p>
                  <p>阅读 PyTorch 领域的文档，了解更多关于特定领域库的信息</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">博客与新闻</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch 博客</span><p></p>
                  <p>捕捉最新的技术新闻和事件</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">社区博客</span><p></p>
                  <p>PyTorch 生态系统故事</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">视频</span><p></p>
                  <p>了解最新的 PyTorch 教程、新内容等</p>
                </a><a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">社区故事</span><p></p>
                  <p>学习如何我们的社区使用 PyTorch 解决真实、日常的机器学习问题</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">活动</span><p></p>
                  <p>查找活动、网络研讨会和播客</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">通讯</span><p></p>
                  <p>跟踪最新更新</p>
                </a>
            </div>
          </div></li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">关于</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch 基金会</span><p></p>
                  <p>了解更多关于 PyTorch 基金会的信息</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">管理委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">云信用计划</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">技术顾问委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">员工</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">联系我们</span><p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">成为会员</a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>



   

    

    <div class="table-of-contents-link-wrapper">
      <span>目录</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="https://pytorch.org/docs/versions.html">主程序 (2.7.0+cpu ) ▼</a>
    </div>
    <div id="searchBox">
    <div class="searchbox" id="googleSearchBox">
      <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
      <div class="gcse-search"></div>
    </div>
    <div id="sphinxSearchBox" style="display: none;">
      <div role="search">
        <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
          <input type="text" name="q" placeholder="Search Docs">
          <input type="hidden" name="check_keywords" value="yes">
          <input type="hidden" name="area" value="default">
        </form>
      </div>
    </div>
  </div>
  <form id="searchForm">
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="google" checked="">谷歌搜索</label>
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="sphinx">经典搜索</label>
  </form>

  <script>
     document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
  </script>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/torch.html">您正在查看不稳定开发者预览文档。请点击此处查看最新稳定版本的文档。</a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">社区</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch 治理 | 构建 + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch 贡献指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch 设计哲学</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch 治理 | 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch 治理 | 维护者</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者笔记</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">自动混合精度示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">广播语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU 多线程和 TorchScript 推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA 语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/custom_operators.html">PyTorch 自定义算子页面</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">分布式数据并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">扩展 PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">使用 autograd.Function 扩展 torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">常见问题解答</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/fsdp.html">FSDP 笔记</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/get_start_xpu.html">在 Intel GPU 上入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm)语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">大规模部署功能</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/libtorch_stable_abi.html">LibTorch 稳定 ABI</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">模块</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS 后端</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">多进程最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">数值精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">可重现性</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">序列化语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows 常见问题解答</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语言绑定</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">张量属性</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">索引视图</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.自动混合精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="accelerator.html">torch.accelerator</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">理解 CUDA 内存使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">生成快照</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">使用可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">摄像头 API 参考</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.memory.html">torch.mtia.memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">元设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.html">torch.distributed.tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.fsdp.fully_shard.html">torch.distributed.fsdp.fully_shard</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">复数</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP 通信钩子</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">分布式 RPC 框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">类型信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">命名张量</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">命名张量操作覆盖率</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_environment_variables.html">火炬环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">库</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">火炬推荐</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch 在 XLA 设备上</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/ao">torchao</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        文档 &gt;</li>

        
      <li>火炬</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/torch.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg" width="16" height="16"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">快捷键</div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="module-torch">
<span id="torch"></span><h1>火炬 ¶</h1>
<p>火炬包包含多维张量的数据结构，并定义了在这些张量上的数学运算。此外，它还提供了许多用于高效序列化张量和任意类型的实用工具，以及其他有用的实用工具。</p>
<p>它有一个 CUDA 对应版本，允许您在具有计算能力&gt;= 3.0 的 NVIDIA GPU 上运行您的张量计算。</p>
<section id="tensors">
<h2>张量 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.is_tensor"><a class="reference internal" href="generated/torch.is_tensor.html#torch.is_tensor" title="torch.is_tensor"><code class="xref py py-obj docutils literal "><span class="pre">is_tensor</span></code></a></p></td>
<td><p>返回 True，如果 obj 是 PyTorch 张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.is_storage"><a class="reference internal" href="generated/torch.is_storage.html#torch.is_storage" title="torch.is_storage"><code class="xref py py-obj docutils literal "><span class="pre">is_storage</span></code></a></p></td>
<td><p>返回 True，如果 obj 是 PyTorch 存储对象。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.is_complex"><a class="reference internal" href="generated/torch.is_complex.html#torch.is_complex" title="torch.is_complex"><code class="xref py py-obj docutils literal "><span class="pre">is_complex</span></code></a></p></td>
<td><p>返回 True，如果 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的数据类型是复杂数据类型，即 <code class="docutils literal "><span class="pre">torch.complex64</span></code> 、 <code class="docutils literal "><span class="pre">torch.complex128</span></code> 之一。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.is_conj"><a class="reference internal" href="generated/torch.is_conj.html#torch.is_conj" title="torch.is_conj"><code class="xref py py-obj docutils literal "><span class="pre">is_conj</span></code></a></p></td>
<td><p>返回 True，如果 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 是共轭张量，即其共轭位设置为 True。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.is_floating_point"><a class="reference internal" href="generated/torch.is_floating_point.html#torch.is_floating_point" title="torch.is_floating_point"><code class="xref py py-obj docutils literal "><span class="pre">is_floating_point</span></code></a></p></td>
<td><p>返回 True，如果 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的数据类型是浮点数据类型，即 <code class="docutils literal "><span class="pre">torch.float64</span></code> 、 <code class="docutils literal "><span class="pre">torch.float32</span></code> 、 <code class="docutils literal "><span class="pre">torch.float16</span></code> 和 <code class="docutils literal "><span class="pre">torch.bfloat16</span></code> 之一。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.is_nonzero"><a class="reference internal" href="generated/torch.is_nonzero.html#torch.is_nonzero" title="torch.is_nonzero"><code class="xref py py-obj docutils literal "><span class="pre">is_nonzero</span></code></a></p></td>
<td><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 是一个非零的单元素张量，并且经过类型转换后不等于零，则返回 True。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.set_default_dtype"><a class="reference internal" href="generated/torch.set_default_dtype.html#torch.set_default_dtype" title="torch.set_default_dtype"><code class="xref py py-obj docutils literal "><span class="pre">set_default_dtype</span></code></a></p></td>
<td><p>将默认的浮点数据类型设置为 <code class="xref py py-attr docutils literal "><span class="pre">d</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.get_default_dtype"><a class="reference internal" href="generated/torch.get_default_dtype.html#torch.get_default_dtype" title="torch.get_default_dtype"><code class="xref py py-obj docutils literal "><span class="pre">get_default_dtype</span></code></a></p></td>
<td><p>获取当前默认的浮点数据类型 <code class="xref py py-class docutils literal "><span class="pre">torch.dtype</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.set_default_device"><a class="reference internal" href="generated/torch.set_default_device.html#torch.set_default_device" title="torch.set_default_device"><code class="xref py py-obj docutils literal "><span class="pre">set_default_device</span></code></a></p></td>
<td><p>设置默认的 <code class="docutils literal "><span class="pre">torch.Tensor</span></code> 在 <code class="docutils literal "><span class="pre">device</span></code> 上分配。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.get_default_device"><a class="reference internal" href="generated/torch.get_default_device.html#torch.get_default_device" title="torch.get_default_device"><code class="xref py py-obj docutils literal "><span class="pre">get_default_device</span></code></a></p></td>
<td><p>获取默认的 <code class="docutils literal "><span class="pre">torch.Tensor</span></code> 在 <code class="docutils literal "><span class="pre">device</span></code> 上分配。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.set_default_tensor_type"><a class="reference internal" href="generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-obj docutils literal "><span class="pre">set_default_tensor_type</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.numel"><a class="reference internal" href="generated/torch.numel.html#torch.numel" title="torch.numel"><code class="xref py py-obj docutils literal "><span class="pre">numel</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量中的元素总数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.set_printoptions"><a class="reference internal" href="generated/torch.set_printoptions.html#torch.set_printoptions" title="torch.set_printoptions"><code class="xref py py-obj docutils literal "><span class="pre">set_printoptions</span></code></a></p></td>
<td><p>设置打印选项。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.set_flush_denormal"><a class="reference internal" href="generated/torch.set_flush_denormal.html#torch.set_flush_denormal" title="torch.set_flush_denormal"><code class="xref py py-obj docutils literal "><span class="pre">set_flush_denormal</span></code></a></p></td>
<td><p>禁用 CPU 上的非规范化浮点数。</p></td>
</tr>
</tbody>
</table>
<section id="creation-ops">
<span id="tensor-creation-ops"></span><h3>创建操作 ¶</h3>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>随机采样创建操作列在“随机采样”下，包括： <code class="xref py py-func docutils literal "><span class="pre">torch.rand()</span></code> <code class="xref py py-func docutils literal "><span class="pre">torch.rand_like()</span></code> <code class="xref py py-func docutils literal "><span class="pre">torch.randn()</span></code> <code class="xref py py-func docutils literal "><span class="pre">torch.randn_like()</span></code> <code class="xref py py-func docutils literal "><span class="pre">torch.randint()</span></code> <code class="xref py py-func docutils literal "><span class="pre">torch.randint_like()</span></code> <code class="xref py py-func docutils literal "><span class="pre">torch.randperm()</span></code> 。您还可以使用 <code class="xref py py-func docutils literal "><span class="pre">torch.empty()</span></code> 与就地随机采样方法一起创建 <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> ，其值来自更广泛的分布范围。</p>
</div>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.tensor"><a class="reference internal" href="generated/torch.tensor.html#torch.tensor" title="torch.tensor"><code class="xref py py-obj docutils literal "><span class="pre">tensor</span></code></a></p></td>
<td><p>通过复制 <code class="xref py py-attr docutils literal "><span class="pre">data</span></code> 构建一个没有 autograd 历史记录的 tensor（也称为“叶子 tensor”，见 Autograd 机制）。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sparse_coo_tensor"><a class="reference internal" href="generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor" title="torch.sparse_coo_tensor"><code class="xref py py-obj docutils literal "><span class="pre">sparse_coo_tensor</span></code></a></p></td>
<td><p>在指定的 <code class="xref py py-attr docutils literal "><span class="pre">indices</span></code> 处构建具有指定值的 COO(坐标)格式的稀疏张量</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.sparse_csr_tensor"><a class="reference internal" href="generated/torch.sparse_csr_tensor.html#torch.sparse_csr_tensor" title="torch.sparse_csr_tensor"><code class="xref py py-obj docutils literal "><span class="pre">sparse_csr_tensor</span></code></a></p></td>
<td><p>在指定的 <code class="xref py py-attr docutils literal "><span class="pre">crow_indices</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">col_indices</span></code> 处构建具有指定值的 CSR(压缩稀疏行)格式的稀疏张量</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sparse_csc_tensor"><a class="reference internal" href="generated/torch.sparse_csc_tensor.html#torch.sparse_csc_tensor" title="torch.sparse_csc_tensor"><code class="xref py py-obj docutils literal "><span class="pre">sparse_csc_tensor</span></code></a></p></td>
<td><p>在指定的 <code class="xref py py-attr docutils literal "><span class="pre">ccol_indices</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">row_indices</span></code> 处构建具有指定值的 CSC(压缩稀疏列)格式的稀疏张量</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.sparse_bsr_tensor"><a class="reference internal" href="generated/torch.sparse_bsr_tensor.html#torch.sparse_bsr_tensor" title="torch.sparse_bsr_tensor"><code class="xref py py-obj docutils literal "><span class="pre">sparse_bsr_tensor</span></code></a></p></td>
<td><p>构建一个在指定 <code class="xref py py-attr docutils literal "><span class="pre">crow_indices</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">col_indices</span></code> 的二维块上的 BSR（块压缩稀疏行）稀疏张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sparse_bsc_tensor"><a class="reference internal" href="generated/torch.sparse_bsc_tensor.html#torch.sparse_bsc_tensor" title="torch.sparse_bsc_tensor"><code class="xref py py-obj docutils literal "><span class="pre">sparse_bsc_tensor</span></code></a></p></td>
<td><p>构建一个在指定 <code class="xref py py-attr docutils literal "><span class="pre">ccol_indices</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">row_indices</span></code> 的二维块上的 BSC（块压缩稀疏列）稀疏张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.asarray"><a class="reference internal" href="generated/torch.asarray.html#torch.asarray" title="torch.asarray"><code class="xref py py-obj docutils literal "><span class="pre">asarray</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">obj</span></code> 转换为张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.as_tensor"><a class="reference internal" href="generated/torch.as_tensor.html#torch.as_tensor" title="torch.as_tensor"><code class="xref py py-obj docutils literal "><span class="pre">as_tensor</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">data</span></code> 转换为张量，尽可能共享数据和保留自动微分历史。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.as_strided"><a class="reference internal" href="generated/torch.as_strided.html#torch.as_strided" title="torch.as_strided"><code class="xref py py-obj docutils literal "><span class="pre">as_strided</span></code></a></p></td>
<td><p>创建一个视图，该视图是现有 torch.Tensor <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> ，并指定 <code class="xref py py-attr docutils literal "><span class="pre">size</span></code> 、 <code class="xref py py-attr docutils literal "><span class="pre">stride</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">storage_offset</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.from_file"><a class="reference internal" href="generated/torch.from_file.html#torch.from_file" title="torch.from_file"><code class="xref py py-obj docutils literal "><span class="pre">from_file</span></code></a></p></td>
<td><p>创建一个由内存映射文件支持的存储器后端的 CPU 张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.from_numpy"><a class="reference internal" href="generated/torch.from_numpy.html#torch.from_numpy" title="torch.from_numpy"><code class="xref py py-obj docutils literal "><span class="pre">from_numpy</span></code></a></p></td>
<td><p>从一个 <code class="xref py py-class docutils literal "><span class="pre">numpy.ndarray</span></code> 创建一个 <code class="xref py py-class docutils literal "><span class="pre">Tensor</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.from_dlpack"><a class="reference internal" href="generated/torch.from_dlpack.html#torch.from_dlpack" title="torch.from_dlpack"><code class="xref py py-obj docutils literal "><span class="pre">from_dlpack</span></code></a></p></td>
<td><p>将外部库中的张量转换为 <code class="docutils literal "><span class="pre">torch.Tensor</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.frombuffer"><a class="reference internal" href="generated/torch.frombuffer.html#torch.frombuffer" title="torch.frombuffer"><code class="xref py py-obj docutils literal "><span class="pre">frombuffer</span></code></a></p></td>
<td><p>从实现 Python 缓冲区协议的对象创建一个 1 维 <code class="xref py py-class docutils literal "><span class="pre">Tensor</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.zeros"><a class="reference internal" href="generated/torch.zeros.html#torch.zeros" title="torch.zeros"><code class="xref py py-obj docutils literal "><span class="pre">zeros</span></code></a></p></td>
<td><p>返回一个填充了标量值 0 的张量，形状由变量参数 <code class="xref py py-attr docutils literal "><span class="pre">size</span></code> 定义。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.zeros_like"><a class="reference internal" href="generated/torch.zeros_like.html#torch.zeros_like" title="torch.zeros_like"><code class="xref py py-obj docutils literal "><span class="pre">zeros_like</span></code></a></p></td>
<td><p>返回一个填充了标量值 0 的张量，大小与 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 相同。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.ones"><a class="reference internal" href="generated/torch.ones.html#torch.ones" title="torch.ones"><code class="xref py py-obj docutils literal "><span class="pre">ones</span></code></a></p></td>
<td><p>返回一个填充了标量值 1 的张量，其形状由变量参数 <code class="xref py py-attr docutils literal "><span class="pre">size</span></code> 定义。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.ones_like"><a class="reference internal" href="generated/torch.ones_like.html#torch.ones_like" title="torch.ones_like"><code class="xref py py-obj docutils literal "><span class="pre">ones_like</span></code></a></p></td>
<td><p>返回一个填充了标量值 1 的张量，其大小与 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 相同。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.arange"><a class="reference internal" href="generated/torch.arange.html#torch.arange" title="torch.arange"><code class="xref py py-obj docutils literal "><span class="pre">arange</span></code></a></p></td>
<td><p>返回一个大小为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">⌈</mo><mfrac><mrow><mtext>end</mtext><mo>−</mo><mtext>start</mtext></mrow><mtext>step</mtext></mfrac><mo fence="true">⌉</mo></mrow><annotation encoding="application/x-tex">\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1.80002em;vertical-align:-0.65002em;" class="strut"></span><span class="minner"><span style="top:0em;" class="mopen delimcenter"><span class="delimsizing size2">⌈</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.8801079999999999em;" class="vlist"><span style="top:-2.6550000000000002em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">step</span></span></span></span></span><span style="top:-3.23em;"><span style="height:3em;" class="pstrut"></span><span style="border-bottom-width:0.04em;" class="frac-line"></span></span><span style="top:-3.394em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">end</span></span><span class="mbin mtight">−</span><span class="mord text mtight"><span class="mord mtight">start</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.481108em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span style="top:0em;" class="mclose delimcenter"><span class="delimsizing size2">⌉</span></span></span></span></span></span> 的一维张量，其值从区间 <code class="docutils literal "><span class="pre">[start,</span> <span class="pre">end)</span></code> 开始，以公差 <code class="xref py py-attr docutils literal "><span class="pre">step</span></code> 取值。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.range"><a class="reference internal" href="generated/torch.range.html#torch.range" title="torch.range"><code class="xref py py-obj docutils literal "><span class="pre">range</span></code></a></p></td>
<td><p>返回一个大小为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo fence="true">⌊</mo><mfrac><mrow><mtext>end</mtext><mo>−</mo><mtext>start</mtext></mrow><mtext>step</mtext></mfrac><mo fence="true">⌋</mo></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1.80002em;vertical-align:-0.65002em;" class="strut"></span><span class="minner"><span style="top:0em;" class="mopen delimcenter"><span class="delimsizing size2">⌊</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.8801079999999999em;" class="vlist"><span style="top:-2.6550000000000002em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">step</span></span></span></span></span><span style="top:-3.23em;"><span style="height:3em;" class="pstrut"></span><span style="border-bottom-width:0.04em;" class="frac-line"></span></span><span style="top:-3.394em;"><span style="height:3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">end</span></span><span class="mbin mtight">−</span><span class="mord text mtight"><span class="mord mtight">start</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.481108em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span style="top:0em;" class="mclose delimcenter"><span class="delimsizing size2">⌋</span></span></span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">+</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.64444em;vertical-align:0em;" class="strut"></span><span class="mord">1</span></span></span></span> 的一维张量，其值从 <code class="xref py py-attr docutils literal "><span class="pre">start</span></code> 到 <code class="xref py py-attr docutils literal "><span class="pre">end</span></code> ，步长为 <code class="xref py py-attr docutils literal "><span class="pre">step</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.linspace"><a class="reference internal" href="generated/torch.linspace.html#torch.linspace" title="torch.linspace"><code class="xref py py-obj docutils literal "><span class="pre">linspace</span></code></a></p></td>
<td><p>创建一个大小为 <code class="xref py py-attr docutils literal "><span class="pre">steps</span></code> 的一维张量，其值从 <code class="xref py py-attr docutils literal "><span class="pre">start</span></code> 到 <code class="xref py py-attr docutils literal "><span class="pre">end</span></code> 均匀分布，包含两端。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.logspace"><a class="reference internal" href="generated/torch.logspace.html#torch.logspace" title="torch.logspace"><code class="xref py py-obj docutils literal "><span class="pre">logspace</span></code></a></p></td>
<td><p>创建一个大小为 <code class="xref py py-attr docutils literal "><span class="pre">steps</span></code> 的一维张量，其值从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>base</mtext><mtext>start</mtext></msup></mrow><annotation encoding="application/x-tex">{{\text{{base}}}}^{{\text{{start}}}}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.8778959999999999em;vertical-align:0em;" class="strut"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord text"><span class="mord"><span class="mord">base</span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height:0.8778959999999999em;" class="vlist"><span style="top:-3.1473400000000002em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight"><span class="mord mtight">start</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>base</mtext><mtext>end</mtext></msup></mrow><annotation encoding="application/x-tex">{{\text{{base}}}}^{{\text{{end}}}}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.9334479999999998em;vertical-align:0em;" class="strut"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord text"><span class="mord"><span class="mord">base</span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height:0.9334479999999998em;" class="vlist"><span style="top:-3.1473400000000002em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight"><span class="mord mtight">end</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 均匀分布，包含两端，以 <code class="xref py py-attr docutils literal "><span class="pre">base</span></code> 为底的对数尺度。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.eye"><a class="reference internal" href="generated/torch.eye.html#torch.eye" title="torch.eye"><code class="xref py py-obj docutils literal "><span class="pre">eye</span></code></a></p></td>
<td><p>返回一个对角线为 1，其余位置为 0 的 2-D 张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.empty"><a class="reference internal" href="generated/torch.empty.html#torch.empty" title="torch.empty"><code class="xref py py-obj docutils literal "><span class="pre">empty</span></code></a></p></td>
<td><p>返回一个填充未初始化数据的张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.empty_like"><a class="reference internal" href="generated/torch.empty_like.html#torch.empty_like" title="torch.empty_like"><code class="xref py py-obj docutils literal "><span class="pre">empty_like</span></code></a></p></td>
<td><p>返回与 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 相同大小的未初始化张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.empty_strided"><a class="reference internal" href="generated/torch.empty_strided.html#torch.empty_strided" title="torch.empty_strided"><code class="xref py py-obj docutils literal "><span class="pre">empty_strided</span></code></a></p></td>
<td><p>创建一个具有指定 <code class="xref py py-attr docutils literal "><span class="pre">size</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">stride</span></code> 的张量，并用未定义的数据填充。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.full"><a class="reference internal" href="generated/torch.full.html#torch.full" title="torch.full"><code class="xref py py-obj docutils literal "><span class="pre">full</span></code></a></p></td>
<td><p>创建一个大小为 <code class="xref py py-attr docutils literal "><span class="pre">size</span></code> ，用 <code class="xref py py-attr docutils literal "><span class="pre">fill_value</span></code> 填充的张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.full_like"><a class="reference internal" href="generated/torch.full_like.html#torch.full_like" title="torch.full_like"><code class="xref py py-obj docutils literal "><span class="pre">full_like</span></code></a></p></td>
<td><p>返回一个与 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 相同大小，用 <code class="xref py py-attr docutils literal "><span class="pre">fill_value</span></code> 填充的张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.quantize_per_tensor"><a class="reference internal" href="generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor" title="torch.quantize_per_tensor"><code class="xref py py-obj docutils literal "><span class="pre">quantize_per_tensor</span></code></a></p></td>
<td><p>将浮点张量转换为具有给定标度和零点的量化张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.quantize_per_channel"><a class="reference internal" href="generated/torch.quantize_per_channel.html#torch.quantize_per_channel" title="torch.quantize_per_channel"><code class="xref py py-obj docutils literal "><span class="pre">quantize_per_channel</span></code></a></p></td>
<td><p>将浮点张量转换为具有给定标度和零点的按通道量化的张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.dequantize"><a class="reference internal" href="generated/torch.dequantize.html#torch.dequantize" title="torch.dequantize"><code class="xref py py-obj docutils literal "><span class="pre">dequantize</span></code></a></p></td>
<td><p>通过反量化量化张量返回一个 fp32 张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.complex"><a class="reference internal" href="generated/torch.complex.html#torch.complex" title="torch.complex"><code class="xref py py-obj docutils literal "><span class="pre">complex</span></code></a></p></td>
<td><p>构建一个复数张量，其实部等于 <code class="xref py py-attr docutils literal "><span class="pre">real</span></code> ，虚部等于 <code class="xref py py-attr docutils literal "><span class="pre">imag</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.polar"><a class="reference internal" href="generated/torch.polar.html#torch.polar" title="torch.polar"><code class="xref py py-obj docutils literal "><span class="pre">polar</span></code></a></p></td>
<td><p>构建一个复数张量，其元素对应于绝对值为 <code class="xref py py-attr docutils literal "><span class="pre">abs</span></code> 和角度为 <code class="xref py py-attr docutils literal "><span class="pre">angle</span></code> 的极坐标。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.heaviside"><a class="reference internal" href="generated/torch.heaviside.html#torch.heaviside" title="torch.heaviside"><code class="xref py py-obj docutils literal "><span class="pre">heaviside</span></code></a></p></td>
<td><p>对 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的每个元素计算 Heaviside 阶跃函数。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="indexing-slicing-joining-mutating-ops">
<span id="indexing-slicing-joining"></span><h3>索引、切片、连接、修改操作</h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.adjoint"><a class="reference internal" href="generated/torch.adjoint.html#torch.adjoint" title="torch.adjoint"><code class="xref py py-obj docutils literal "><span class="pre">adjoint</span></code></a></p></td>
<td><p>返回一个张量，其共轭且最后两个维度已转置的视图。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.argwhere"><a class="reference internal" href="generated/torch.argwhere.html#torch.argwhere" title="torch.argwhere"><code class="xref py py-obj docutils literal "><span class="pre">argwhere</span></code></a></p></td>
<td><p>返回一个包含所有非零元素索引的张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.cat"><a class="reference internal" href="generated/torch.cat.html#torch.cat" title="torch.cat"><code class="xref py py-obj docutils literal "><span class="pre">cat</span></code></a></p></td>
<td><p>在指定维度上连接给定的张量序列 <code class="xref py py-attr docutils literal "><span class="pre">tensors</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.concat"><a class="reference internal" href="generated/torch.concat.html#torch.concat" title="torch.concat"><code class="xref py py-obj docutils literal "><span class="pre">concat</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.cat()</span></code> 的别名。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.concatenate"><a class="reference internal" href="generated/torch.concatenate.html#torch.concatenate" title="torch.concatenate"><code class="xref py py-obj docutils literal "><span class="pre">concatenate</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.cat()</span></code> 的别名。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.conj"><a class="reference internal" href="generated/torch.conj.html#torch.conj" title="torch.conj"><code class="xref py py-obj docutils literal "><span class="pre">conj</span></code></a></p></td>
<td><p>返回一个翻转共轭位的 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 视图。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.chunk"><a class="reference internal" href="generated/torch.chunk.html#torch.chunk" title="torch.chunk"><code class="xref py py-obj docutils literal "><span class="pre">chunk</span></code></a></p></td>
<td><p>尝试将张量分割成指定的块数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.dsplit"><a class="reference internal" href="generated/torch.dsplit.html#torch.dsplit" title="torch.dsplit"><code class="xref py py-obj docutils literal "><span class="pre">dsplit</span></code></a></p></td>
<td><p>将具有三个或更多维度的张量 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 深度分割成多个张量，根据 <code class="xref py py-attr docutils literal "><span class="pre">indices_or_sections</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.column_stack"><a class="reference internal" href="generated/torch.column_stack.html#torch.column_stack" title="torch.column_stack"><code class="xref py py-obj docutils literal "><span class="pre">column_stack</span></code></a></p></td>
<td><p>通过水平堆叠张量 <code class="xref py py-attr docutils literal "><span class="pre">tensors</span></code> 创建一个新的张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.dstack"><a class="reference internal" href="generated/torch.dstack.html#torch.dstack" title="torch.dstack"><code class="xref py py-obj docutils literal "><span class="pre">dstack</span></code></a></p></td>
<td><p>按序列深度堆叠张量（沿第三个轴）。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.gather"><a class="reference internal" href="generated/torch.gather.html#torch.gather" title="torch.gather"><code class="xref py py-obj docutils literal "><span class="pre">gather</span></code></a></p></td>
<td><p>沿由 dim 指定的轴收集值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.hsplit"><a class="reference internal" href="generated/torch.hsplit.html#torch.hsplit" title="torch.hsplit"><code class="xref py py-obj docutils literal "><span class="pre">hsplit</span></code></a></p></td>
<td><p>将具有一个或多个维度的张量 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 根据 <code class="xref py py-attr docutils literal "><span class="pre">indices_or_sections</span></code> 水平分割成多个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.hstack"><a class="reference internal" href="generated/torch.hstack.html#torch.hstack" title="torch.hstack"><code class="xref py py-obj docutils literal "><span class="pre">hstack</span></code></a></p></td>
<td><p>水平（按列）顺序堆叠张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.index_add"><a class="reference internal" href="generated/torch.index_add.html#torch.index_add" title="torch.index_add"><code class="xref py py-obj docutils literal "><span class="pre">index_add</span></code></a></p></td>
<td><p>请参阅 <code class="xref py py-meth docutils literal "><span class="pre">index_add_()</span></code> 以获取函数描述。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.index_copy"><a class="reference internal" href="generated/torch.index_copy.html#torch.index_copy" title="torch.index_copy"><code class="xref py py-obj docutils literal "><span class="pre">index_copy</span></code></a></p></td>
<td><p>请参阅 <code class="xref py py-meth docutils literal "><span class="pre">index_add_()</span></code> 以获取函数描述。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.index_reduce"><a class="reference internal" href="generated/torch.index_reduce.html#torch.index_reduce" title="torch.index_reduce"><code class="xref py py-obj docutils literal "><span class="pre">index_reduce</span></code></a></p></td>
<td><p>请参阅 <code class="xref py py-meth docutils literal "><span class="pre">index_reduce_()</span></code> 以获取函数描述。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.index_select"><a class="reference internal" href="generated/torch.index_select.html#torch.index_select" title="torch.index_select"><code class="xref py py-obj docutils literal "><span class="pre">index_select</span></code></a></p></td>
<td><p>返回一个新的张量，该张量沿维度 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 使用 <code class="xref py py-attr docutils literal "><span class="pre">index</span></code> 中的条目索引 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量，其中 <code class="xref py py-attr docutils literal "><span class="pre">index</span></code> 是一个 LongTensor。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.masked_select"><a class="reference internal" href="generated/torch.masked_select.html#torch.masked_select" title="torch.masked_select"><code class="xref py py-obj docutils literal "><span class="pre">masked_select</span></code></a></p></td>
<td><p>返回一个新的 1-D 张量，该张量根据布尔掩码 <code class="xref py py-attr docutils literal "><span class="pre">mask</span></code> 索引 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量，其中 <code class="xref py py-attr docutils literal "><span class="pre">mask</span></code> 是一个 BoolTensor。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.movedim"><a class="reference internal" href="generated/torch.movedim.html#torch.movedim" title="torch.movedim"><code class="xref py py-obj docutils literal "><span class="pre">movedim</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的维度移动到 <code class="xref py py-attr docutils literal "><span class="pre">source</span></code> 的位置上。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.moveaxis"><a class="reference internal" href="generated/torch.moveaxis.html#torch.moveaxis" title="torch.moveaxis"><code class="xref py py-obj docutils literal "><span class="pre">moveaxis</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.movedim()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.narrow"><a class="reference internal" href="generated/torch.narrow.html#torch.narrow" title="torch.narrow"><code class="xref py py-obj docutils literal "><span class="pre">narrow</span></code></a></p></td>
<td><p>返回一个新的张量，它是 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量的缩小版本。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.narrow_copy"><a class="reference internal" href="generated/torch.narrow_copy.html#torch.narrow_copy" title="torch.narrow_copy"><code class="xref py py-obj docutils literal "><span class="pre">narrow_copy</span></code></a></p></td>
<td><p>与 <code class="xref py py-meth docutils literal "><span class="pre">Tensor.narrow()</span></code> 相同，但返回的是副本而不是共享存储。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nonzero"><a class="reference internal" href="generated/torch.nonzero.html#torch.nonzero" title="torch.nonzero"><code class="xref py py-obj docutils literal "><span class="pre">nonzero</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.permute"><a class="reference internal" href="generated/torch.permute.html#torch.permute" title="torch.permute"><code class="xref py py-obj docutils literal "><span class="pre">permute</span></code></a></p></td>
<td><p>返回原始张量 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的一个视图，其维度已重新排列。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.reshape"><a class="reference internal" href="generated/torch.reshape.html#torch.reshape" title="torch.reshape"><code class="xref py py-obj docutils literal "><span class="pre">reshape</span></code></a></p></td>
<td><p>返回一个与 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 具有相同数据和元素数量的张量，但具有指定的形状。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.row_stack"><a class="reference internal" href="generated/torch.row_stack.html#torch.row_stack" title="torch.row_stack"><code class="xref py py-obj docutils literal "><span class="pre">row_stack</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.vstack()</span></code> 的别称。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.select"><a class="reference internal" href="generated/torch.select.html#torch.select" title="torch.select"><code class="xref py py-obj docutils literal "><span class="pre">select</span></code></a></p></td>
<td><p>沿选定维度在给定索引处切割 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.scatter"><a class="reference internal" href="generated/torch.scatter.html#torch.scatter" title="torch.scatter"><code class="xref py py-obj docutils literal "><span class="pre">scatter</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.scatter_()</span></code> 的非原地版本</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.diagonal_scatter"><a class="reference internal" href="generated/torch.diagonal_scatter.html#torch.diagonal_scatter" title="torch.diagonal_scatter"><code class="xref py py-obj docutils literal "><span class="pre">diagonal_scatter</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">src</span></code> 张量的值嵌入到 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的对角元素中，相对于 <code class="xref py py-attr docutils literal "><span class="pre">dim1</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">dim2</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.select_scatter"><a class="reference internal" href="generated/torch.select_scatter.html#torch.select_scatter" title="torch.select_scatter"><code class="xref py py-obj docutils literal "><span class="pre">select_scatter</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">src</span></code> 张量的值嵌入到 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 在给定索引处。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.slice_scatter"><a class="reference internal" href="generated/torch.slice_scatter.html#torch.slice_scatter" title="torch.slice_scatter"><code class="xref py py-obj docutils literal "><span class="pre">slice_scatter</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">src</span></code> 张量的值嵌入到 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 在给定维度处。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.scatter_add"><a class="reference internal" href="generated/torch.scatter_add.html#torch.scatter_add" title="torch.scatter_add"><code class="xref py py-obj docutils literal "><span class="pre">scatter_add</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.scatter_add_()</span></code> 的非原地版本</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.scatter_reduce"><a class="reference internal" href="generated/torch.scatter_reduce.html#torch.scatter_reduce" title="torch.scatter_reduce"><code class="xref py py-obj docutils literal "><span class="pre">scatter_reduce</span></code></a></p></td>
<td><p> <code class="xref py py-meth docutils literal "><span class="pre">torch.Tensor.scatter_reduce_()</span></code> 的非原地版本</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.split"><a class="reference internal" href="generated/torch.split.html#torch.split" title="torch.split"><code class="xref py py-obj docutils literal "><span class="pre">split</span></code></a></p></td>
<td><p>将张量分割成块。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.squeeze"><a class="reference internal" href="generated/torch.squeeze.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-obj docutils literal "><span class="pre">squeeze</span></code></a></p></td>
<td><p>返回一个移除了 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中所有指定维度的尺寸为 1 的张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.stack"><a class="reference internal" href="generated/torch.stack.html#torch.stack" title="torch.stack"><code class="xref py py-obj docutils literal "><span class="pre">stack</span></code></a></p></td>
<td><p>沿新维度连接一系列张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.swapaxes"><a class="reference internal" href="generated/torch.swapaxes.html#torch.swapaxes" title="torch.swapaxes"><code class="xref py py-obj docutils literal "><span class="pre">swapaxes</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.transpose()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.swapdims"><a class="reference internal" href="generated/torch.swapdims.html#torch.swapdims" title="torch.swapdims"><code class="xref py py-obj docutils literal "><span class="pre">swapdims</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.transpose()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.t"><a class="reference internal" href="generated/torch.t.html#torch.t" title="torch.t"><code class="xref py py-obj docutils literal "><span class="pre">t</span></code></a></p></td>
<td><p>预期 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 为&lt;=2 维张量，并交换维度 0 和 1。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.take"><a class="reference internal" href="generated/torch.take.html#torch.take" title="torch.take"><code class="xref py py-obj docutils literal "><span class="pre">take</span></code></a></p></td>
<td><p>返回一个新张量，包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 在给定索引处的元素。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.take_along_dim"><a class="reference internal" href="generated/torch.take_along_dim.html#torch.take_along_dim" title="torch.take_along_dim"><code class="xref py py-obj docutils literal "><span class="pre">take_along_dim</span></code></a></p></td>
<td><p>从 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中选取 1 维索引 <code class="xref py py-attr docutils literal "><span class="pre">indices</span></code> 沿给定 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 的值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.tensor_split"><a class="reference internal" href="generated/torch.tensor_split.html#torch.tensor_split" title="torch.tensor_split"><code class="xref py py-obj docutils literal "><span class="pre">tensor_split</span></code></a></p></td>
<td><p>将张量分割成多个子张量，所有子张量都是 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的视图，根据 <code class="xref py py-attr docutils literal "><span class="pre">indices_or_sections</span></code> 指定的索引或分区数量沿维度 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 进行分割。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.tile"><a class="reference internal" href="generated/torch.tile.html#torch.tile" title="torch.tile"><code class="xref py py-obj docutils literal "><span class="pre">tile</span></code></a></p></td>
<td><p>通过重复 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的元素构建一个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.transpose"><a class="reference internal" href="generated/torch.transpose.html#torch.transpose" title="torch.transpose"><code class="xref py py-obj docutils literal "><span class="pre">transpose</span></code></a></p></td>
<td><p>返回一个张量，它是 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的转置版本。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.unbind"><a class="reference internal" href="generated/torch.unbind.html#torch.unbind" title="torch.unbind"><code class="xref py py-obj docutils literal "><span class="pre">unbind</span></code></a></p></td>
<td><p>移除张量维度。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.unravel_index"><a class="reference internal" href="generated/torch.unravel_index.html#torch.unravel_index" title="torch.unravel_index"><code class="xref py py-obj docutils literal "><span class="pre">unravel_index</span></code></a></p></td>
<td><p>将平面索引的张量转换为坐标张量的元组，这些坐标张量可以索引到指定形状的任意张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.unsqueeze"><a class="reference internal" href="generated/torch.unsqueeze.html#torch.unsqueeze" title="torch.unsqueeze"><code class="xref py py-obj docutils literal "><span class="pre">unsqueeze</span></code></a></p></td>
<td><p>在指定位置插入一个大小为 1 的维度的新张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.vsplit"><a class="reference internal" href="generated/torch.vsplit.html#torch.vsplit" title="torch.vsplit"><code class="xref py py-obj docutils literal "><span class="pre">vsplit</span></code></a></p></td>
<td><p>将具有两个或更多维度的张量 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 垂直分割成多个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.vstack"><a class="reference internal" href="generated/torch.vstack.html#torch.vstack" title="torch.vstack"><code class="xref py py-obj docutils literal "><span class="pre">vstack</span></code></a></p></td>
<td><p>将张量按顺序垂直堆叠（按行）。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.where"><a class="reference internal" href="generated/torch.where.html#torch.where" title="torch.where"><code class="xref py py-obj docutils literal "><span class="pre">where</span></code></a></p></td>
<td><p>根据需要从 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 或 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 中选择元素，返回一个张量，具体取决于 <code class="xref py py-attr docutils literal "><span class="pre">condition</span></code> 。</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="accelerators">
<span id="id1"></span><h2>加速器 ¶</h2>
<p>在 PyTorch 仓库中，我们将“加速器”定义为与 CPU 一起使用以加速计算的 <code class="xref py py-class docutils literal "><span class="pre">torch.device</span></code> 。这些设备使用异步执行方案，以 <code class="xref py py-class docutils literal "><span class="pre">torch.Stream</span></code> 和 <code class="xref py py-class docutils literal "><span class="pre">torch.Event</span></code> 作为它们执行同步的主要方式。我们还假设在给定的主机上一次只能有一个这样的加速器可用。这使得我们可以将当前加速器用作相关概念（如固定内存、Stream 设备类型、FSDP 等）的默认设备。</p>
<p>截至 today，加速器设备包括（不分先后）“CUDA”、“MTIA”、“XPU”、“MPS”、“HPU”和 PrivateUse1（许多设备不在 PyTorch 仓库本身中）。</p>
<p>PyTorch 生态系统中的许多工具使用 fork 来创建子进程（例如数据加载或操作内并行），因此尽可能延迟任何可能阻止进一步 fork 的操作是很重要的。在这里尤其重要，因为大多数加速器的初始化都有这种效果。在实践中，你应该记住，默认情况下检查 <code class="xref py py-func docutils literal "><span class="pre">torch.accelerator.current_accelerator()</span></code> 是一个编译时检查，因此总是 fork 安全的。相反，向此函数传递 <code class="docutils literal "><span class="pre">check_available=True</span></code> 标志或调用 <code class="xref py py-func docutils literal "><span class="pre">torch.accelerator.is_available()</span></code> 通常会导致后续 fork 失败。</p>
<p>一些后端提供了实验性的 opt-in 选项，以使运行时可用性检查 fork 安全。例如，当使用 CUDA 设备时，可以使用 <code class="docutils literal "><span class="pre">PYTORCH_NVML_BASED_CUDA_CHECK=1</span></code> 。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.Stream"><a class="reference internal" href="generated/torch.Stream.html#torch.Stream" title="torch.Stream"><code class="xref py py-obj docutils literal "><span class="pre">Stream</span></code></a></p></td>
<td><p>按照先进先出（FIFO）顺序异步执行相应任务的有序队列。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.Event"><a class="reference internal" href="generated/torch.Event.html#torch.Event" title="torch.Event"><code class="xref py py-obj docutils literal "><span class="pre">Event</span></code></a></p></td>
<td><p>查询并记录流状态，以识别或控制流之间的依赖关系并测量时间。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="generators">
<span id="id2"></span><h2>生成器 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.Generator"><a class="reference internal" href="generated/torch.Generator.html#torch.Generator" title="torch.Generator"><code class="xref py py-obj docutils literal "><span class="pre">Generator</span></code></a></p></td>
<td><p>创建并返回一个生成器对象，该对象管理产生伪随机数的算法状态。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="random-sampling">
<span id="id3"></span><h2>随机抽样</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.seed"><a class="reference internal" href="generated/torch.seed.html#torch.seed" title="torch.seed"><code class="xref py py-obj docutils literal "><span class="pre">seed</span></code></a></p></td>
<td><p>将生成随机数的种子设置为所有设备上的非确定性随机数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.manual_seed"><a class="reference internal" href="generated/torch.manual_seed.html#torch.manual_seed" title="torch.manual_seed"><code class="xref py py-obj docutils literal "><span class="pre">manual_seed</span></code></a></p></td>
<td><p>将所有设备上生成随机数的种子设置为。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.initial_seed"><a class="reference internal" href="generated/torch.initial_seed.html#torch.initial_seed" title="torch.initial_seed"><code class="xref py py-obj docutils literal "><span class="pre">initial_seed</span></code></a></p></td>
<td><p>返回生成随机数的初始种子，作为 Python 长整型。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.get_rng_state"><a class="reference internal" href="generated/torch.get_rng_state.html#torch.get_rng_state" title="torch.get_rng_state"><code class="xref py py-obj docutils literal "><span class="pre">get_rng_state</span></code></a></p></td>
<td><p>返回随机数生成器的状态，以 torch.ByteTensor 的形式。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.set_rng_state"><a class="reference internal" href="generated/torch.set_rng_state.html#torch.set_rng_state" title="torch.set_rng_state"><code class="xref py py-obj docutils literal "><span class="pre">set_rng_state</span></code></a></p></td>
<td><p>设置随机数生成器的状态。</p></td>
</tr>
</tbody>
</table>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.torch.default_generator">
返回默认的 CPU torch.Generator</dt>
<dd></dd></dl>

<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.bernoulli"><a class="reference internal" href="generated/torch.bernoulli.html#torch.bernoulli" title="torch.bernoulli"><code class="xref py py-obj docutils literal "><span class="pre">bernoulli</span></code></a></p></td>
<td><p>从伯努利分布中抽取二进制随机数（0 或 1）。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.multinomial"><a class="reference internal" href="generated/torch.multinomial.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-obj docutils literal "><span class="pre">multinomial</span></code></a></p></td>
<td><p>返回一个张量，其中每行包含从多项式（更严格的定义是多元的，请参阅 <code class="xref py py-class docutils literal "><span class="pre">torch.distributions.multinomial.Multinomial</span></code> 获取更多详情）概率分布中采样的 <code class="xref py py-attr docutils literal "><span class="pre">num_samples</span></code> 索引。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.normal"><a class="reference internal" href="generated/torch.normal.html#torch.normal" title="torch.normal"><code class="xref py py-obj docutils literal "><span class="pre">normal</span></code></a></p></td>
<td><p>返回一个张量，其中的随机数是从给定的均值和标准差的不同正态分布中抽取的。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.poisson"><a class="reference internal" href="generated/torch.poisson.html#torch.poisson" title="torch.poisson"><code class="xref py py-obj docutils literal "><span class="pre">poisson</span></code></a></p></td>
<td><p>返回一个与 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 相同大小的张量，其中每个元素从由 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中对应元素给出的泊松分布中采样，即，</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.rand"><a class="reference internal" href="generated/torch.rand.html#torch.rand" title="torch.rand"><code class="xref py py-obj docutils literal "><span class="pre">rand</span></code></a></p></td>
<td><p>返回一个填充有均匀分布在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[0, 1)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span> 区间上的随机数的张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.rand_like"><a class="reference internal" href="generated/torch.rand_like.html#torch.rand_like" title="torch.rand_like"><code class="xref py py-obj docutils literal "><span class="pre">rand_like</span></code></a></p></td>
<td><p>返回一个与 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 相同大小的张量，其中填充有均匀分布在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[0, 1)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span> 区间上的随机数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.randint"><a class="reference internal" href="generated/torch.randint.html#torch.randint" title="torch.randint"><code class="xref py py-obj docutils literal "><span class="pre">randint</span></code></a></p></td>
<td><p>返回一个填充有在 <code class="xref py py-attr docutils literal "><span class="pre">low</span></code> （包含）和 <code class="xref py py-attr docutils literal "><span class="pre">high</span></code> （不包含）之间均匀生成的随机整数的张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.randint_like"><a class="reference internal" href="generated/torch.randint_like.html#torch.randint_like" title="torch.randint_like"><code class="xref py py-obj docutils literal "><span class="pre">randint_like</span></code></a></p></td>
<td><p>返回一个与 Tensor <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 形状相同的张量，其中填充了在 <code class="xref py py-attr docutils literal "><span class="pre">low</span></code> （包含）和 <code class="xref py py-attr docutils literal "><span class="pre">high</span></code> （不包含）之间均匀生成的随机整数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.randn"><a class="reference internal" href="generated/torch.randn.html#torch.randn" title="torch.randn"><code class="xref py py-obj docutils literal "><span class="pre">randn</span></code></a></p></td>
<td><p>返回一个填充有均值为 0、方差为 1 的随机数的正态分布张量（也称为标准正态分布）。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.randn_like"><a class="reference internal" href="generated/torch.randn_like.html#torch.randn_like" title="torch.randn_like"><code class="xref py py-obj docutils literal "><span class="pre">randn_like</span></code></a></p></td>
<td><p>返回一个与 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 大小相同的张量，其中填充了均值为 0、方差为 1 的正态分布随机数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.randperm"><a class="reference internal" href="generated/torch.randperm.html#torch.randperm" title="torch.randperm"><code class="xref py py-obj docutils literal "><span class="pre">randperm</span></code></a></p></td>
<td><p>返回从 <code class="docutils literal "><span class="pre">0</span></code> 到 <code class="docutils literal "><span class="pre">n</span> <span class="pre">-</span> <span class="pre">1</span></code> 的随机排列的整数。</p></td>
</tr>
</tbody>
</table>
<section id="in-place-random-sampling">
<span id="inplace-random-sampling"></span><h3>原地随机采样</h3>
<p>在张量上还定义了一些原地随机采样的函数。点击查看它们的文档：</p>
<ul class="simple">
<li><p> <code class="xref py py-func docutils literal "><span class="pre">torch.Tensor.bernoulli_()</span></code> - <code class="xref py py-func docutils literal "><span class="pre">torch.bernoulli()</span></code> 的原地版本</p></li>
<li><p> <code class="xref py py-func docutils literal "><span class="pre">torch.Tensor.cauchy_()</span></code> - 从柯西分布中抽取的数字</p></li>
<li><p> <code class="xref py py-func docutils literal "><span class="pre">torch.Tensor.exponential_()</span></code> - 从指数分布中抽取的数字</p></li>
<li><p> <code class="xref py py-func docutils literal "><span class="pre">torch.Tensor.geometric_()</span></code> - 从几何分布中抽取的元素</p></li>
<li><p> <code class="xref py py-func docutils literal "><span class="pre">torch.Tensor.log_normal_()</span></code> - 对数正态分布的样本</p></li>
<li><p> <code class="xref py py-func docutils literal "><span class="pre">torch.Tensor.normal_()</span></code> - <code class="xref py py-func docutils literal "><span class="pre">torch.normal()</span></code> 的就地版本</p></li>
<li><p>来自离散均匀分布的样本数</p></li>
<li><p>来自连续均匀分布的样本数</p></li>
</ul>
</section>
<section id="quasi-random-sampling">
<h3>准随机抽样</h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine" title="torch.quasirandom.SobolEngine"><code class="xref py py-obj docutils literal "><span class="pre">quasirandom.SobolEngine</span></code></a></p></td>
<td><p> <code class="xref py py-class docutils literal "><span class="pre">torch.quasirandom.SobolEngine</span></code> 是一个生成（打乱）Sobol 序列的引擎。</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="serialization">
<h2>序列化 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.save"><a class="reference internal" href="generated/torch.save.html#torch.save" title="torch.save"><code class="xref py py-obj docutils literal "><span class="pre">save</span></code></a></p></td>
<td><p>将对象保存到磁盘文件中。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.load"><a class="reference internal" href="generated/torch.load.html#torch.load" title="torch.load"><code class="xref py py-obj docutils literal "><span class="pre">load</span></code></a></p></td>
<td><p>从文件中加载使用 <code class="xref py py-func docutils literal "><span class="pre">torch.save()</span></code> 保存的对象。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="parallelism">
<h2>并行性 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.get_num_threads"><a class="reference internal" href="generated/torch.get_num_threads.html#torch.get_num_threads" title="torch.get_num_threads"><code class="xref py py-obj docutils literal "><span class="pre">get_num_threads</span></code></a></p></td>
<td><p>返回用于并行化 CPU 操作的线程数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.set_num_threads"><a class="reference internal" href="generated/torch.set_num_threads.html#torch.set_num_threads" title="torch.set_num_threads"><code class="xref py py-obj docutils literal "><span class="pre">set_num_threads</span></code></a></p></td>
<td><p>设置用于 CPU 内部并行处理的线程数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.get_num_interop_threads"><a class="reference internal" href="generated/torch.get_num_interop_threads.html#torch.get_num_interop_threads" title="torch.get_num_interop_threads"><code class="xref py py-obj docutils literal "><span class="pre">get_num_interop_threads</span></code></a></p></td>
<td><p>返回用于 CPU 间操作并行处理的线程数（例如。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.set_num_interop_threads"><a class="reference internal" href="generated/torch.set_num_interop_threads.html#torch.set_num_interop_threads" title="torch.set_num_interop_threads"><code class="xref py py-obj docutils literal "><span class="pre">set_num_interop_threads</span></code></a></p></td>
<td><p>设置用于间操作并行处理的线程数（例如。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="locally-disabling-gradient-computation">
<span id="torch-rst-local-disable-grad"></span><h2>局部禁用梯度计算</h2>
<p>上下文管理器 <code class="xref py py-func docutils literal "><span class="pre">torch.no_grad()</span></code> 、 <code class="xref py py-func docutils literal "><span class="pre">torch.enable_grad()</span></code> 和 <code class="xref py py-func docutils literal "><span class="pre">torch.set_grad_enabled()</span></code> 有助于在本地禁用和启用梯度计算。有关它们的使用方法，请参阅“本地禁用梯度计算”。这些上下文管理器是线程局部，因此如果您使用 <code class="docutils literal "><span class="pre">threading</span></code> 模块等将工作发送到另一个线程，则它们将不起作用。</p>
<p>示例：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">is_train</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">is_train</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># this can also be used as a function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
</pre></div>
</div>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.no_grad"><a class="reference internal" href="generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad"><code class="xref py py-obj docutils literal "><span class="pre">no_grad</span></code></a></p></td>
<td><p>禁用梯度计算的上下文管理器。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.enable_grad"><a class="reference internal" href="generated/torch.enable_grad.html#torch.enable_grad" title="torch.enable_grad"><code class="xref py py-obj docutils literal "><span class="pre">enable_grad</span></code></a></p></td>
<td><p>启用梯度计算的上下文管理器。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.autograd.grad_mode.set_grad_enabled.html#torch.autograd.grad_mode.set_grad_enabled" title="torch.autograd.grad_mode.set_grad_enabled"><code class="xref py py-obj docutils literal "><span class="pre">autograd.grad_mode.set_grad_enabled</span></code></a></p></td>
<td><p>设置梯度计算开启或关闭的上下文管理器。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.is_grad_enabled"><a class="reference internal" href="generated/torch.is_grad_enabled.html#torch.is_grad_enabled" title="torch.is_grad_enabled"><code class="xref py py-obj docutils literal "><span class="pre">is_grad_enabled</span></code></a></p></td>
<td><p>如果当前启用梯度模式，则返回 True。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.autograd.grad_mode.inference_mode.html#torch.autograd.grad_mode.inference_mode" title="torch.autograd.grad_mode.inference_mode"><code class="xref py py-obj docutils literal "><span class="pre">autograd.grad_mode.inference_mode</span></code></a></p></td>
<td><p>上下文管理器，用于启用或禁用推理模式。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.is_inference_mode_enabled"><a class="reference internal" href="generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled" title="torch.is_inference_mode_enabled"><code class="xref py py-obj docutils literal "><span class="pre">is_inference_mode_enabled</span></code></a></p></td>
<td><p>如果推理模式当前已启用，则返回 True。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="math-operations">
<h2>数学运算 ¶</h2>
<section id="constants">
<h3>常量 ¶</h3>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal "><span class="pre">inf</span></code></p></td>
<td><p>浮点正无穷大。别名 <code class="xref py py-attr docutils literal "><span class="pre">math.inf</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal "><span class="pre">nan</span></code></p></td>
<td><p>浮点数“非数字”值。此值不是一个合法的数字。 <code class="xref py py-attr docutils literal "><span class="pre">math.nan</span></code> 的别名。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="pointwise-ops">
<h3>点操作符</h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.abs"><a class="reference internal" href="generated/torch.abs.html#torch.abs" title="torch.abs"><code class="xref py py-obj docutils literal "><span class="pre">abs</span></code></a></p></td>
<td><p>计算每个元素在 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的绝对值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.absolute"><a class="reference internal" href="generated/torch.absolute.html#torch.absolute" title="torch.absolute"><code class="xref py py-obj docutils literal "><span class="pre">absolute</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.abs()</span></code> 的别名。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.acos"><a class="reference internal" href="generated/torch.acos.html#torch.acos" title="torch.acos"><code class="xref py py-obj docutils literal "><span class="pre">acos</span></code></a></p></td>
<td><p>计算每个元素在 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的反余弦值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.arccos"><a class="reference internal" href="generated/torch.arccos.html#torch.arccos" title="torch.arccos"><code class="xref py py-obj docutils literal "><span class="pre">arccos</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.acos()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.acosh"><a class="reference internal" href="generated/torch.acosh.html#torch.acosh" title="torch.acosh"><code class="xref py py-obj docutils literal "><span class="pre">acosh</span></code></a></p></td>
<td><p>返回一个新张量，包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 元素的反双曲余弦值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.arccosh"><a class="reference internal" href="generated/torch.arccosh.html#torch.arccosh" title="torch.arccosh"><code class="xref py py-obj docutils literal "><span class="pre">arccosh</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.acosh()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.add"><a class="reference internal" href="generated/torch.add.html#torch.add" title="torch.add"><code class="xref py py-obj docutils literal "><span class="pre">add</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 加上 <code class="xref py py-attr docutils literal "><span class="pre">alpha</span></code> 缩放的结果加到 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 上。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.addcdiv"><a class="reference internal" href="generated/torch.addcdiv.html#torch.addcdiv" title="torch.addcdiv"><code class="xref py py-obj docutils literal "><span class="pre">addcdiv</span></code></a></p></td>
<td><p>对 <code class="xref py py-attr docutils literal "><span class="pre">tensor1</span></code> 进行逐元素除法，将结果乘以标量 <code class="xref py py-attr docutils literal "><span class="pre">value</span></code> 并加到 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 上。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.addcmul"><a class="reference internal" href="generated/torch.addcmul.html#torch.addcmul" title="torch.addcmul"><code class="xref py py-obj docutils literal "><span class="pre">addcmul</span></code></a></p></td>
<td><p>对 <code class="xref py py-attr docutils literal "><span class="pre">tensor1</span></code> 进行逐元素乘法，将结果乘以标量 <code class="xref py py-attr docutils literal "><span class="pre">value</span></code> 并加到 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 上。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.angle"><a class="reference internal" href="generated/torch.angle.html#torch.angle" title="torch.angle"><code class="xref py py-obj docutils literal "><span class="pre">angle</span></code></a></p></td>
<td><p>计算给定 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量的元素角度（以弧度为单位）。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.asin"><a class="reference internal" href="generated/torch.asin.html#torch.asin" title="torch.asin"><code class="xref py py-obj docutils literal "><span class="pre">asin</span></code></a></p></td>
<td><p>返回一个新张量，包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的元素的反正弦值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.arcsin"><a class="reference internal" href="generated/torch.arcsin.html#torch.arcsin" title="torch.arcsin"><code class="xref py py-obj docutils literal "><span class="pre">arcsin</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.asin()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.asinh"><a class="reference internal" href="generated/torch.asinh.html#torch.asinh" title="torch.asinh"><code class="xref py py-obj docutils literal "><span class="pre">asinh</span></code></a></p></td>
<td><p>返回一个新张量，包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的元素的逆双曲正弦值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.arcsinh"><a class="reference internal" href="generated/torch.arcsinh.html#torch.arcsinh" title="torch.arcsinh"><code class="xref py py-obj docutils literal "><span class="pre">arcsinh</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.asinh()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.atan"><a class="reference internal" href="generated/torch.atan.html#torch.atan" title="torch.atan"><code class="xref py py-obj docutils literal "><span class="pre">atan</span></code></a></p></td>
<td><p>返回一个新张量，包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的元素的反正切。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.arctan"><a class="reference internal" href="generated/torch.arctan.html#torch.arctan" title="torch.arctan"><code class="xref py py-obj docutils literal "><span class="pre">arctan</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.atan()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.atanh"><a class="reference internal" href="generated/torch.atanh.html#torch.atanh" title="torch.atanh"><code class="xref py py-obj docutils literal "><span class="pre">atanh</span></code></a></p></td>
<td><p>返回一个新张量，包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的元素的双曲反正切。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.arctanh"><a class="reference internal" href="generated/torch.arctanh.html#torch.arctanh" title="torch.arctanh"><code class="xref py py-obj docutils literal "><span class="pre">arctanh</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.atanh()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.atan2"><a class="reference internal" href="generated/torch.atan2.html#torch.atan2" title="torch.atan2"><code class="xref py py-obj docutils literal "><span class="pre">atan2</span></code></a></p></td>
<td><p>考虑象限的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>input</mtext><mi>i</mi></msub><mi mathvariant="normal">/</mi><msub><mtext>other</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\text{input}_{i} / \text{other}_{i}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord"><span class="mord text"><span class="mord">input</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.21752399999999997em;" class="vlist"><span style="top:-2.4558600000000004em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.24414em;" class="vlist"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord"><span class="mord text"><span class="mord">other</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.31166399999999994em;" class="vlist"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> 的逐元素反正切。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.arctan2"><a class="reference internal" href="generated/torch.arctan2.html#torch.arctan2" title="torch.arctan2"><code class="xref py py-obj docutils literal "><span class="pre">arctan2</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.atan2()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.bitwise_not"><a class="reference internal" href="generated/torch.bitwise_not.html#torch.bitwise_not" title="torch.bitwise_not"><code class="xref py py-obj docutils literal "><span class="pre">bitwise_not</span></code></a></p></td>
<td><p>计算给定输入张量的按位非。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.bitwise_and"><a class="reference internal" href="generated/torch.bitwise_and.html#torch.bitwise_and" title="torch.bitwise_and"><code class="xref py py-obj docutils literal "><span class="pre">bitwise_and</span></code></a></p></td>
<td><p>计算 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 的按位与。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.bitwise_or"><a class="reference internal" href="generated/torch.bitwise_or.html#torch.bitwise_or" title="torch.bitwise_or"><code class="xref py py-obj docutils literal "><span class="pre">bitwise_or</span></code></a></p></td>
<td><p>计算 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 的按位或。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.bitwise_xor"><a class="reference internal" href="generated/torch.bitwise_xor.html#torch.bitwise_xor" title="torch.bitwise_xor"><code class="xref py py-obj docutils literal "><span class="pre">bitwise_xor</span></code></a></p></td>
<td><p>计算 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 的按位异或。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.bitwise_left_shift"><a class="reference internal" href="generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift" title="torch.bitwise_left_shift"><code class="xref py py-obj docutils literal "><span class="pre">bitwise_left_shift</span></code></a></p></td>
<td><p>计算对 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 进行 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 位左算术移位。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.bitwise_right_shift"><a class="reference internal" href="generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift" title="torch.bitwise_right_shift"><code class="xref py py-obj docutils literal "><span class="pre">bitwise_right_shift</span></code></a></p></td>
<td><p>计算对 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 进行 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 位右算术移位。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.ceil"><a class="reference internal" href="generated/torch.ceil.html#torch.ceil" title="torch.ceil"><code class="xref py py-obj docutils literal "><span class="pre">ceil</span></code></a></p></td>
<td><p>返回一个新张量，其中包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 元素的 ceil 值，即大于或等于每个元素的最小整数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.clamp"><a class="reference internal" href="generated/torch.clamp.html#torch.clamp" title="torch.clamp"><code class="xref py py-obj docutils literal "><span class="pre">clamp</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的所有元素夹在 [ <code class="xref py py-attr docutils literal "><span class="pre">min</span></code> , <code class="xref py py-attr docutils literal "><span class="pre">max</span></code> ] 范围内。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.clip"><a class="reference internal" href="generated/torch.clip.html#torch.clip" title="torch.clip"><code class="xref py py-obj docutils literal "><span class="pre">clip</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.clamp()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.conj_physical"><a class="reference internal" href="generated/torch.conj_physical.html#torch.conj_physical" title="torch.conj_physical"><code class="xref py py-obj docutils literal "><span class="pre">conj_physical</span></code></a></p></td>
<td><p>计算给定 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量的逐元素共轭。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.copysign"><a class="reference internal" href="generated/torch.copysign.html#torch.copysign" title="torch.copysign"><code class="xref py py-obj docutils literal "><span class="pre">copysign</span></code></a></p></td>
<td><p>创建一个具有 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的幅度和 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 的符号的新的浮点张量，逐元素。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.cos"><a class="reference internal" href="generated/torch.cos.html#torch.cos" title="torch.cos"><code class="xref py py-obj docutils literal "><span class="pre">cos</span></code></a></p></td>
<td><p>返回一个新张量，其中包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 元素的余弦值。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.cosh"><a class="reference internal" href="generated/torch.cosh.html#torch.cosh" title="torch.cosh"><code class="xref py py-obj docutils literal "><span class="pre">cosh</span></code></a></p></td>
<td><p>返回一个新张量，其中包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 元素的双曲余弦值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.deg2rad"><a class="reference internal" href="generated/torch.deg2rad.html#torch.deg2rad" title="torch.deg2rad"><code class="xref py py-obj docutils literal "><span class="pre">deg2rad</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的每个元素从角度（度）转换为弧度的新张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.div"><a class="reference internal" href="generated/torch.div.html#torch.div" title="torch.div"><code class="xref py py-obj docutils literal "><span class="pre">div</span></code></a></p></td>
<td><p>将输入 <code class="docutils literal "><span class="pre">input</span></code> 的每个元素除以对应的 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 元素。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.divide"><a class="reference internal" href="generated/torch.divide.html#torch.divide" title="torch.divide"><code class="xref py py-obj docutils literal "><span class="pre">divide</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.div()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.digamma"><a class="reference internal" href="generated/torch.digamma.html#torch.digamma" title="torch.digamma"><code class="xref py py-obj docutils literal "><span class="pre">digamma</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.digamma()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.erf"><a class="reference internal" href="generated/torch.erf.html#torch.erf" title="torch.erf"><code class="xref py py-obj docutils literal "><span class="pre">erf</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.erf()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.erfc"><a class="reference internal" href="generated/torch.erfc.html#torch.erfc" title="torch.erfc"><code class="xref py py-obj docutils literal "><span class="pre">erfc</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.erfc()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.erfinv"><a class="reference internal" href="generated/torch.erfinv.html#torch.erfinv" title="torch.erfinv"><code class="xref py py-obj docutils literal "><span class="pre">erfinv</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.erfinv()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.exp"><a class="reference internal" href="generated/torch.exp.html#torch.exp" title="torch.exp"><code class="xref py py-obj docutils literal "><span class="pre">exp</span></code></a></p></td>
<td><p>返回一个新张量，其元素为输入张量 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的指数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.exp2"><a class="reference internal" href="generated/torch.exp2.html#torch.exp2" title="torch.exp2"><code class="xref py py-obj docutils literal "><span class="pre">exp2</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.exp2()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.expm1"><a class="reference internal" href="generated/torch.expm1.html#torch.expm1" title="torch.expm1"><code class="xref py py-obj docutils literal "><span class="pre">expm1</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.expm1()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.fake_quantize_per_channel_affine"><a class="reference internal" href="generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine" title="torch.fake_quantize_per_channel_affine"><code class="xref py py-obj docutils literal "><span class="pre">fake_quantize_per_channel_affine</span></code></a></p></td>
<td><p>返回一个新张量，其中 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的数据在每个通道上使用 <code class="xref py py-attr docutils literal "><span class="pre">scale</span></code> 、 <code class="xref py py-attr docutils literal "><span class="pre">zero_point</span></code> 、 <code class="xref py py-attr docutils literal "><span class="pre">quant_min</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">quant_max</span></code> 进行伪量化，跨由 <code class="xref py py-attr docutils literal "><span class="pre">axis</span></code> 指定的通道。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.fake_quantize_per_tensor_affine"><a class="reference internal" href="generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine" title="torch.fake_quantize_per_tensor_affine"><code class="xref py py-obj docutils literal "><span class="pre">fake_quantize_per_tensor_affine</span></code></a></p></td>
<td><p>返回一个新张量，其中 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的数据使用 <code class="xref py py-attr docutils literal "><span class="pre">scale</span></code> 、 <code class="xref py py-attr docutils literal "><span class="pre">zero_point</span></code> 、 <code class="xref py py-attr docutils literal "><span class="pre">quant_min</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">quant_max</span></code> 进行伪量化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.fix"><a class="reference internal" href="generated/torch.fix.html#torch.fix" title="torch.fix"><code class="xref py py-obj docutils literal "><span class="pre">fix</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.trunc()</span></code> 的别名。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.float_power"><a class="reference internal" href="generated/torch.float_power.html#torch.float_power" title="torch.float_power"><code class="xref py py-obj docutils literal "><span class="pre">float_power</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 提升到 <code class="xref py py-attr docutils literal "><span class="pre">exponent</span></code> 的幂，逐元素，以双精度执行。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.floor"><a class="reference internal" href="generated/torch.floor.html#torch.floor" title="torch.floor"><code class="xref py py-obj docutils literal "><span class="pre">floor</span></code></a></p></td>
<td><p>返回一个新张量，包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中元素的向下取整，即每个元素的最大整数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.floor_divide"><a class="reference internal" href="generated/torch.floor_divide.html#torch.floor_divide" title="torch.floor_divide"><code class="xref py py-obj docutils literal "><span class="pre">floor_divide</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.fmod"><a class="reference internal" href="generated/torch.fmod.html#torch.fmod" title="torch.fmod"><code class="xref py py-obj docutils literal "><span class="pre">fmod</span></code></a></p></td>
<td><p>应用 C++的 std::fmod 逐元素操作。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.frac"><a class="reference internal" href="generated/torch.frac.html#torch.frac" title="torch.frac"><code class="xref py py-obj docutils literal "><span class="pre">frac</span></code></a></p></td>
<td><p>计算每个元素的分数部分。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.frexp"><a class="reference internal" href="generated/torch.frexp.html#torch.frexp" title="torch.frexp"><code class="xref py py-obj docutils literal "><span class="pre">frexp</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 分解为尾数和指数张量，使得 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>=</mo><mtext>mantissa</mtext><mo>×</mo><msup><mn>2</mn><mtext>exponent</mtext></msup></mrow><annotation encoding="application/x-tex">\text{input} = \text{mantissa} \times 2^{\text{exponent}}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.8623000000000001em;vertical-align:-0.19444em;" class="strut"></span><span class="mord text"><span class="mord">input</span></span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.75119em;vertical-align:-0.08333em;" class="strut"></span><span class="mord text"><span class="mord">mantissa</span></span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">×</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.7935559999999999em;vertical-align:0em;" class="strut"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height:0.7935559999999999em;" class="vlist"><span style="top:-3.063em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">exponent</span></span></span></span></span></span></span></span></span></span></span></span></span> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.gradient"><a class="reference internal" href="generated/torch.gradient.html#torch.gradient" title="torch.gradient"><code class="xref py py-obj docutils literal "><span class="pre">gradient</span></code></a></p></td>
<td><p>使用二阶精确中心差分法以及边界处的一阶或二阶估计，在一维或多维上估计函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>:</mo><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">g : \mathbb{R}^n \rightarrow \mathbb{R}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">g</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">:</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.68889em;vertical-align:0em;" class="strut"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height:0.664392em;" class="vlist"><span style="top:-3.063em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">→</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.68889em;vertical-align:0em;" class="strut"></span><span class="mord mathbb">R</span></span></span></span> 的梯度。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.imag"><a class="reference internal" href="generated/torch.imag.html#torch.imag" title="torch.imag"><code class="xref py py-obj docutils literal "><span class="pre">imag</span></code></a></p></td>
<td><p>返回一个包含 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 张量虚部的新张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.ldexp"><a class="reference internal" href="generated/torch.ldexp.html#torch.ldexp" title="torch.ldexp"><code class="xref py py-obj docutils literal "><span class="pre">ldexp</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 乘以 2 的 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 次方。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.lerp"><a class="reference internal" href="generated/torch.lerp.html#torch.lerp" title="torch.lerp"><code class="xref py py-obj docutils literal "><span class="pre">lerp</span></code></a></p></td>
<td><p>根据标量或张量 <code class="xref py py-attr docutils literal "><span class="pre">weight</span></code> 对两个张量 <code class="xref py py-attr docutils literal "><span class="pre">start</span></code> （由 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 给出）和 <code class="xref py py-attr docutils literal "><span class="pre">end</span></code> 进行线性插值，并返回结果张量 <code class="xref py py-attr docutils literal "><span class="pre">out</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.lgamma"><a class="reference internal" href="generated/torch.lgamma.html#torch.lgamma" title="torch.lgamma"><code class="xref py py-obj docutils literal "><span class="pre">lgamma</span></code></a></p></td>
<td><p>计算伽玛函数在 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 上的自然对数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.log"><a class="reference internal" href="generated/torch.log.html#torch.log" title="torch.log"><code class="xref py py-obj docutils literal "><span class="pre">log</span></code></a></p></td>
<td><p>返回一个新张量，包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中元素的自然对数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.log10"><a class="reference internal" href="generated/torch.log10.html#torch.log10" title="torch.log10"><code class="xref py py-obj docutils literal "><span class="pre">log10</span></code></a></p></td>
<td><p>返回一个新张量，其元素为 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的以 10 为底的对数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.log1p"><a class="reference internal" href="generated/torch.log1p.html#torch.log1p" title="torch.log1p"><code class="xref py py-obj docutils literal "><span class="pre">log1p</span></code></a></p></td>
<td><p>返回一个新张量，其元素为(1 + <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> )的自然对数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.log2"><a class="reference internal" href="generated/torch.log2.html#torch.log2" title="torch.log2"><code class="xref py py-obj docutils literal "><span class="pre">log2</span></code></a></p></td>
<td><p>返回一个新张量，其元素为 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的以 2 为底的对数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.logaddexp"><a class="reference internal" href="generated/torch.logaddexp.html#torch.logaddexp" title="torch.logaddexp"><code class="xref py py-obj docutils literal "><span class="pre">logaddexp</span></code></a></p></td>
<td><p>输入指数幂之和的对数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.logaddexp2"><a class="reference internal" href="generated/torch.logaddexp2.html#torch.logaddexp2" title="torch.logaddexp2"><code class="xref py py-obj docutils literal "><span class="pre">logaddexp2</span></code></a></p></td>
<td><p>输入在 2 为底下的指数幂之和的对数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.logical_and"><a class="reference internal" href="generated/torch.logical_and.html#torch.logical_and" title="torch.logical_and"><code class="xref py py-obj docutils literal "><span class="pre">logical_and</span></code></a></p></td>
<td><p>计算给定输入张量的逐元素逻辑与。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.logical_not"><a class="reference internal" href="generated/torch.logical_not.html#torch.logical_not" title="torch.logical_not"><code class="xref py py-obj docutils literal "><span class="pre">logical_not</span></code></a></p></td>
<td><p>计算给定输入张量的逐元素逻辑非。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.logical_or"><a class="reference internal" href="generated/torch.logical_or.html#torch.logical_or" title="torch.logical_or"><code class="xref py py-obj docutils literal "><span class="pre">logical_or</span></code></a></p></td>
<td><p>计算给定输入张量的逐元素逻辑或。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.logical_xor"><a class="reference internal" href="generated/torch.logical_xor.html#torch.logical_xor" title="torch.logical_xor"><code class="xref py py-obj docutils literal "><span class="pre">logical_xor</span></code></a></p></td>
<td><p>计算给定输入张量的逐元素逻辑异或。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.logit"><a class="reference internal" href="generated/torch.logit.html#torch.logit" title="torch.logit"><code class="xref py py-obj docutils literal "><span class="pre">logit</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.logit()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.hypot"><a class="reference internal" href="generated/torch.hypot.html#torch.hypot" title="torch.hypot"><code class="xref py py-obj docutils literal "><span class="pre">hypot</span></code></a></p></td>
<td><p>给定直角三角形的腿，返回其斜边。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.i0"><a class="reference internal" href="generated/torch.i0.html#torch.i0" title="torch.i0"><code class="xref py py-obj docutils literal "><span class="pre">i0</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.i0()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.igamma"><a class="reference internal" href="generated/torch.igamma.html#torch.igamma" title="torch.igamma"><code class="xref py py-obj docutils literal "><span class="pre">igamma</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.gammainc()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.igammac"><a class="reference internal" href="generated/torch.igammac.html#torch.igammac" title="torch.igammac"><code class="xref py py-obj docutils literal "><span class="pre">igammac</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.gammaincc()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.mul"><a class="reference internal" href="generated/torch.mul.html#torch.mul" title="torch.mul"><code class="xref py py-obj docutils literal "><span class="pre">mul</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 乘以 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.multiply"><a class="reference internal" href="generated/torch.multiply.html#torch.multiply" title="torch.multiply"><code class="xref py py-obj docutils literal "><span class="pre">multiply</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.mul()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.mvlgamma"><a class="reference internal" href="generated/torch.mvlgamma.html#torch.mvlgamma" title="torch.mvlgamma"><code class="xref py py-obj docutils literal "><span class="pre">mvlgamma</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.multigammaln()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nan_to_num"><a class="reference internal" href="generated/torch.nan_to_num.html#torch.nan_to_num" title="torch.nan_to_num"><code class="xref py py-obj docutils literal "><span class="pre">nan_to_num</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的 <code class="docutils literal "><span class="pre">NaN</span></code> 、正无穷和负无穷值分别替换为 <code class="xref py py-attr docutils literal "><span class="pre">nan</span></code> 、 <code class="xref py py-attr docutils literal "><span class="pre">posinf</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">neginf</span></code> 中指定的值。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.neg"><a class="reference internal" href="generated/torch.neg.html#torch.neg" title="torch.neg"><code class="xref py py-obj docutils literal "><span class="pre">neg</span></code></a></p></td>
<td><p>返回一个新张量，其元素为 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 元素的相反数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.negative"><a class="reference internal" href="generated/torch.negative.html#torch.negative" title="torch.negative"><code class="xref py py-obj docutils literal "><span class="pre">negative</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.neg()</span></code> 的别名。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nextafter"><a class="reference internal" href="generated/torch.nextafter.html#torch.nextafter" title="torch.nextafter"><code class="xref py py-obj docutils literal "><span class="pre">nextafter</span></code></a></p></td>
<td><p>按元素方式返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 之后相对于 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 的下一个浮点数值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.polygamma"><a class="reference internal" href="generated/torch.polygamma.html#torch.polygamma" title="torch.polygamma"><code class="xref py py-obj docutils literal "><span class="pre">polygamma</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.polygamma()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.positive"><a class="reference internal" href="generated/torch.positive.html#torch.positive" title="torch.positive"><code class="xref py py-obj docutils literal "><span class="pre">positive</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.pow"><a class="reference internal" href="generated/torch.pow.html#torch.pow" title="torch.pow"><code class="xref py py-obj docutils literal "><span class="pre">pow</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中每个元素的幂与 <code class="xref py py-attr docutils literal "><span class="pre">exponent</span></code> 相乘，并返回结果张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.quantized_batch_norm"><a class="reference internal" href="generated/torch.quantized_batch_norm.html#torch.quantized_batch_norm" title="torch.quantized_batch_norm"><code class="xref py py-obj docutils literal "><span class="pre">quantized_batch_norm</span></code></a></p></td>
<td><p>对一个 4D（NCHW）量化张量应用批量归一化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.quantized_max_pool1d"><a class="reference internal" href="generated/torch.quantized_max_pool1d.html#torch.quantized_max_pool1d" title="torch.quantized_max_pool1d"><code class="xref py py-obj docutils literal "><span class="pre">quantized_max_pool1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入量化张量应用 1D 最大池化。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.quantized_max_pool2d"><a class="reference internal" href="generated/torch.quantized_max_pool2d.html#torch.quantized_max_pool2d" title="torch.quantized_max_pool2d"><code class="xref py py-obj docutils literal "><span class="pre">quantized_max_pool2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入量化张量应用 2D 最大池化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.rad2deg"><a class="reference internal" href="generated/torch.rad2deg.html#torch.rad2deg" title="torch.rad2deg"><code class="xref py py-obj docutils literal "><span class="pre">rad2deg</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的每个元素从弧度转换为度，并返回新的张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.real"><a class="reference internal" href="generated/torch.real.html#torch.real" title="torch.real"><code class="xref py py-obj docutils literal "><span class="pre">real</span></code></a></p></td>
<td><p>返回包含 <code class="xref py py-attr docutils literal "><span class="pre">self</span></code> 矩阵实数值的新张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.reciprocal"><a class="reference internal" href="generated/torch.reciprocal.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-obj docutils literal "><span class="pre">reciprocal</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中每个元素倒数的新张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.remainder"><a class="reference internal" href="generated/torch.remainder.html#torch.remainder" title="torch.remainder"><code class="xref py py-obj docutils literal "><span class="pre">remainder</span></code></a></p></td>
<td><p>计算 Python 的逐元素取模运算。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.round"><a class="reference internal" href="generated/torch.round.html#torch.round" title="torch.round"><code class="xref py py-obj docutils literal "><span class="pre">round</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的元素四舍五入到最接近的整数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.rsqrt"><a class="reference internal" href="generated/torch.rsqrt.html#torch.rsqrt" title="torch.rsqrt"><code class="xref py py-obj docutils literal "><span class="pre">rsqrt</span></code></a></p></td>
<td><p>返回一个新张量，其中包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中每个元素的平方根的倒数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sigmoid"><a class="reference internal" href="generated/torch.sigmoid.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-obj docutils literal "><span class="pre">sigmoid</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.expit()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.sign"><a class="reference internal" href="generated/torch.sign.html#torch.sign" title="torch.sign"><code class="xref py py-obj docutils literal "><span class="pre">sign</span></code></a></p></td>
<td><p>返回一个新张量，其中包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中每个元素的符号。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sgn"><a class="reference internal" href="generated/torch.sgn.html#torch.sgn" title="torch.sgn"><code class="xref py py-obj docutils literal "><span class="pre">sgn</span></code></a></p></td>
<td><p>这个函数是 torch.sign()对复数张量的扩展。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.signbit"><a class="reference internal" href="generated/torch.signbit.html#torch.signbit" title="torch.signbit"><code class="xref py py-obj docutils literal "><span class="pre">signbit</span></code></a></p></td>
<td><p>测试 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中每个元素是否设置了符号位。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sin"><a class="reference internal" href="generated/torch.sin.html#torch.sin" title="torch.sin"><code class="xref py py-obj docutils literal "><span class="pre">sin</span></code></a></p></td>
<td><p>返回一个新张量，其中包含 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 元素的正弦值。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.sinc"><a class="reference internal" href="generated/torch.sinc.html#torch.sinc" title="torch.sinc"><code class="xref py py-obj docutils literal "><span class="pre">sinc</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.sinc()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sinh"><a class="reference internal" href="generated/torch.sinh.html#torch.sinh" title="torch.sinh"><code class="xref py py-obj docutils literal "><span class="pre">sinh</span></code></a></p></td>
<td><p>返回一个新张量，其元素为 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的双曲正弦。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.softmax"><a class="reference internal" href="generated/torch.softmax.html#torch.softmax" title="torch.softmax"><code class="xref py py-obj docutils literal "><span class="pre">softmax</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.nn.functional.softmax()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sqrt"><a class="reference internal" href="generated/torch.sqrt.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-obj docutils literal "><span class="pre">sqrt</span></code></a></p></td>
<td><p>返回一个新张量，其元素为 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的平方根。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.square"><a class="reference internal" href="generated/torch.square.html#torch.square" title="torch.square"><code class="xref py py-obj docutils literal "><span class="pre">square</span></code></a></p></td>
<td><p>返回一个新张量，其元素为 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的平方。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sub"><a class="reference internal" href="generated/torch.sub.html#torch.sub" title="torch.sub"><code class="xref py py-obj docutils literal "><span class="pre">sub</span></code></a></p></td>
<td><p>从 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中减去 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> ，并按 <code class="xref py py-attr docutils literal "><span class="pre">alpha</span></code> 缩放。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.subtract"><a class="reference internal" href="generated/torch.subtract.html#torch.subtract" title="torch.subtract"><code class="xref py py-obj docutils literal "><span class="pre">subtract</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.sub()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.tan"><a class="reference internal" href="generated/torch.tan.html#torch.tan" title="torch.tan"><code class="xref py py-obj docutils literal "><span class="pre">tan</span></code></a></p></td>
<td><p>返回一个新张量，其元素为 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的正切。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.tanh"><a class="reference internal" href="generated/torch.tanh.html#torch.tanh" title="torch.tanh"><code class="xref py py-obj docutils literal "><span class="pre">tanh</span></code></a></p></td>
<td><p>返回一个新张量，其元素为 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的双曲正切值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.true_divide"><a class="reference internal" href="generated/torch.true_divide.html#torch.true_divide" title="torch.true_divide"><code class="xref py py-obj docutils literal "><span class="pre">true_divide</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.div()</span></code> 和 <code class="docutils literal "><span class="pre">rounding_mode=None</span></code> 的别名。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.trunc"><a class="reference internal" href="generated/torch.trunc.html#torch.trunc" title="torch.trunc"><code class="xref py py-obj docutils literal "><span class="pre">trunc</span></code></a></p></td>
<td><p>返回一个新张量，其元素为 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的截断整数值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.xlogy"><a class="reference internal" href="generated/torch.xlogy.html#torch.xlogy" title="torch.xlogy"><code class="xref py py-obj docutils literal "><span class="pre">xlogy</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.special.xlogy()</span></code> 的别名</p></td>
</tr>
</tbody>
</table>
</section>
<section id="reduction-ops">
<h3>累加操作符 §</h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.argmax"><a class="reference internal" href="generated/torch.argmax.html#torch.argmax" title="torch.argmax"><code class="xref py py-obj docutils literal "><span class="pre">argmax</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量中所有元素的最大值的索引。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.argmin"><a class="reference internal" href="generated/torch.argmin.html#torch.argmin" title="torch.argmin"><code class="xref py py-obj docutils literal "><span class="pre">argmin</span></code></a></p></td>
<td><p>返回展平张量或沿维度的最小值索引。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.amax"><a class="reference internal" href="generated/torch.amax.html#torch.amax" title="torch.amax"><code class="xref py py-obj docutils literal "><span class="pre">amax</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量在给定维度 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 上的每个切片的最大值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.amin"><a class="reference internal" href="generated/torch.amin.html#torch.amin" title="torch.amin"><code class="xref py py-obj docutils literal "><span class="pre">amin</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量在给定维度 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 上的每个切片的最小值。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.aminmax"><a class="reference internal" href="generated/torch.aminmax.html#torch.aminmax" title="torch.aminmax"><code class="xref py py-obj docutils literal "><span class="pre">aminmax</span></code></a></p></td>
<td><p>计算张量 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的最小值和最大值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.all"><a class="reference internal" href="generated/torch.all.html#torch.all" title="torch.all"><code class="xref py py-obj docutils literal "><span class="pre">all</span></code></a></p></td>
<td><p>测试 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量中所有元素是否都评估为 True。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.any"><a class="reference internal" href="generated/torch.any.html#torch.any" title="torch.any"><code class="xref py py-obj docutils literal "><span class="pre">any</span></code></a></p></td>
<td><p>测试 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的任何元素是否评估为 True。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.max"><a class="reference internal" href="generated/torch.max.html#torch.max" title="torch.max"><code class="xref py py-obj docutils literal "><span class="pre">max</span></code></a></p></td>
<td><p>返回张量 <code class="docutils literal "><span class="pre">input</span></code> 中所有元素的最大值。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.min"><a class="reference internal" href="generated/torch.min.html#torch.min" title="torch.min"><code class="xref py py-obj docutils literal "><span class="pre">min</span></code></a></p></td>
<td><p>返回张量 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中所有元素的最小值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.dist"><a class="reference internal" href="generated/torch.dist.html#torch.dist" title="torch.dist"><code class="xref py py-obj docutils literal "><span class="pre">dist</span></code></a></p></td>
<td><p>返回（ <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> - <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> ）的 p 范数</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.logsumexp"><a class="reference internal" href="generated/torch.logsumexp.html#torch.logsumexp" title="torch.logsumexp"><code class="xref py py-obj docutils literal "><span class="pre">logsumexp</span></code></a></p></td>
<td><p>返回给定维度 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 下 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量每行指数和的对数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.mean"><a class="reference internal" href="generated/torch.mean.html#torch.mean" title="torch.mean"><code class="xref py py-obj docutils literal "><span class="pre">mean</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nanmean"><a class="reference internal" href="generated/torch.nanmean.html#torch.nanmean" title="torch.nanmean"><code class="xref py py-obj docutils literal "><span class="pre">nanmean</span></code></a></p></td>
<td><p>计算指定维度上所有非 NaN 元素的均值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.median"><a class="reference internal" href="generated/torch.median.html#torch.median" title="torch.median"><code class="xref py py-obj docutils literal "><span class="pre">median</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的值的中位数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nanmedian"><a class="reference internal" href="generated/torch.nanmedian.html#torch.nanmedian" title="torch.nanmedian"><code class="xref py py-obj docutils literal "><span class="pre">nanmedian</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中值的中间值，忽略 <code class="docutils literal "><span class="pre">NaN</span></code> 值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.mode"><a class="reference internal" href="generated/torch.mode.html#torch.mode" title="torch.mode"><code class="xref py py-obj docutils literal "><span class="pre">mode</span></code></a></p></td>
<td><p>返回一个 namedtuple <code class="docutils literal "><span class="pre">(values,</span> <span class="pre">indices)</span></code> ，其中 <code class="docutils literal "><span class="pre">values</span></code> 是 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量中每行的 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 维度的众数值，即在该行中出现次数最多的值， <code class="docutils literal "><span class="pre">indices</span></code> 是找到的每个众数值的索引位置。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.norm"><a class="reference internal" href="generated/torch.norm.html#torch.norm" title="torch.norm"><code class="xref py py-obj docutils literal "><span class="pre">norm</span></code></a></p></td>
<td><p>返回给定张量的矩阵范数或向量范数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nansum"><a class="reference internal" href="generated/torch.nansum.html#torch.nansum" title="torch.nansum"><code class="xref py py-obj docutils literal "><span class="pre">nansum</span></code></a></p></td>
<td><p>返回所有元素的总和，将非数字（NaNs）视为零。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.prod"><a class="reference internal" href="generated/torch.prod.html#torch.prod" title="torch.prod"><code class="xref py py-obj docutils literal "><span class="pre">prod</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量中所有元素的乘积。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.quantile"><a class="reference internal" href="generated/torch.quantile.html#torch.quantile" title="torch.quantile"><code class="xref py py-obj docutils literal "><span class="pre">quantile</span></code></a></p></td>
<td><p>计算沿 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 维度的 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量每行的 q-分位数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nanquantile"><a class="reference internal" href="generated/torch.nanquantile.html#torch.nanquantile" title="torch.nanquantile"><code class="xref py py-obj docutils literal "><span class="pre">nanquantile</span></code></a></p></td>
<td><p>这是一种变体，它“忽略” <code class="docutils literal "><span class="pre">NaN</span></code> 的值，计算 <code class="xref py py-attr docutils literal "><span class="pre">q</span></code> 的量数，仿佛 <code class="docutils literal "><span class="pre">NaN</span></code> 的值在 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中不存在。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.std"><a class="reference internal" href="generated/torch.std.html#torch.std" title="torch.std"><code class="xref py py-obj docutils literal "><span class="pre">std</span></code></a></p></td>
<td><p>计算由 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 指定的维度上的标准差。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.std_mean"><a class="reference internal" href="generated/torch.std_mean.html#torch.std_mean" title="torch.std_mean"><code class="xref py py-obj docutils literal "><span class="pre">std_mean</span></code></a></p></td>
<td><p>计算指定维度 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 的标准差和平均值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sum"><a class="reference internal" href="generated/torch.sum.html#torch.sum" title="torch.sum"><code class="xref py py-obj docutils literal "><span class="pre">sum</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量中所有元素的总和。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.unique"><a class="reference internal" href="generated/torch.unique.html#torch.unique" title="torch.unique"><code class="xref py py-obj docutils literal "><span class="pre">unique</span></code></a></p></td>
<td><p>返回输入张量中的唯一元素。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.unique_consecutive"><a class="reference internal" href="generated/torch.unique_consecutive.html#torch.unique_consecutive" title="torch.unique_consecutive"><code class="xref py py-obj docutils literal "><span class="pre">unique_consecutive</span></code></a></p></td>
<td><p>删除每个连续等效元素组中除了第一个元素之外的所有元素。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.var"><a class="reference internal" href="generated/torch.var.html#torch.var" title="torch.var"><code class="xref py py-obj docutils literal "><span class="pre">var</span></code></a></p></td>
<td><p>计算由 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 指定的维度上的方差。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.var_mean"><a class="reference internal" href="generated/torch.var_mean.html#torch.var_mean" title="torch.var_mean"><code class="xref py py-obj docutils literal "><span class="pre">var_mean</span></code></a></p></td>
<td><p>计算指定维度 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 的方差和平均值。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.count_nonzero"><a class="reference internal" href="generated/torch.count_nonzero.html#torch.count_nonzero" title="torch.count_nonzero"><code class="xref py py-obj docutils literal "><span class="pre">count_nonzero</span></code></a></p></td>
<td><p>统计张量 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 在给定 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 方向上的非零值数量。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="comparison-ops">
<h3>比较操作符</h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.allclose"><a class="reference internal" href="generated/torch.allclose.html#torch.allclose" title="torch.allclose"><code class="xref py py-obj docutils literal "><span class="pre">allclose</span></code></a></p></td>
<td><p>此函数检查 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 是否满足以下条件：</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.argsort"><a class="reference internal" href="generated/torch.argsort.html#torch.argsort" title="torch.argsort"><code class="xref py py-obj docutils literal "><span class="pre">argsort</span></code></a></p></td>
<td><p>返回按给定维度按值升序排序张量的索引。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.eq"><a class="reference internal" href="generated/torch.eq.html#torch.eq" title="torch.eq"><code class="xref py py-obj docutils literal "><span class="pre">eq</span></code></a></p></td>
<td><p>计算逐元素相等性。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.equal"><a class="reference internal" href="generated/torch.equal.html#torch.equal" title="torch.equal"><code class="xref py py-obj docutils literal "><span class="pre">equal</span></code></a></p></td>
<td><p> <code class="docutils literal "><span class="pre">True</span></code> 如果两个张量具有相同的大小和元素， <code class="docutils literal "><span class="pre">False</span></code> 否则。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.ge"><a class="reference internal" href="generated/torch.ge.html#torch.ge" title="torch.ge"><code class="xref py py-obj docutils literal "><span class="pre">ge</span></code></a></p></td>
<td><p>计算元素级 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>≥</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \geq \text{other}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.8623000000000001em;vertical-align:-0.19444em;" class="strut"></span><span class="mord text"><span class="mord">input</span></span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">≥</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mord text"><span class="mord">other</span></span></span></span></span> </p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.greater_equal"><a class="reference internal" href="generated/torch.greater_equal.html#torch.greater_equal" title="torch.greater_equal"><code class="xref py py-obj docutils literal "><span class="pre">greater_equal</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.ge()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.gt"><a class="reference internal" href="generated/torch.gt.html#torch.gt" title="torch.gt"><code class="xref py py-obj docutils literal "><span class="pre">gt</span></code></a></p></td>
<td><p>计算元素级 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>&gt;</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} &gt; \text{other}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.8623000000000001em;vertical-align:-0.19444em;" class="strut"></span><span class="mord text"><span class="mord">input</span></span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">&gt;</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mord text"><span class="mord">other</span></span></span></span></span> </p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.greater"><a class="reference internal" href="generated/torch.greater.html#torch.greater" title="torch.greater"><code class="xref py py-obj docutils literal "><span class="pre">greater</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.gt()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.isclose"><a class="reference internal" href="generated/torch.isclose.html#torch.isclose" title="torch.isclose"><code class="xref py py-obj docutils literal "><span class="pre">isclose</span></code></a></p></td>
<td><p>返回一个新张量，其中包含布尔元素，表示 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的每个元素是否与 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 中的相应元素“接近”。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.isfinite"><a class="reference internal" href="generated/torch.isfinite.html#torch.isfinite" title="torch.isfinite"><code class="xref py py-obj docutils literal "><span class="pre">isfinite</span></code></a></p></td>
<td><p>返回一个新张量，其中包含布尔元素，表示每个元素是否有限。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.isin"><a class="reference internal" href="generated/torch.isin.html#torch.isin" title="torch.isin"><code class="xref py py-obj docutils literal "><span class="pre">isin</span></code></a></p></td>
<td><p>测试 <code class="xref py py-attr docutils literal "><span class="pre">elements</span></code> 中的每个元素是否在 <code class="xref py py-attr docutils literal "><span class="pre">test_elements</span></code> 中。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.isinf"><a class="reference internal" href="generated/torch.isinf.html#torch.isinf" title="torch.isinf"><code class="xref py py-obj docutils literal "><span class="pre">isinf</span></code></a></p></td>
<td><p>测试 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的每个元素是否为正无穷或负无穷。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.isposinf"><a class="reference internal" href="generated/torch.isposinf.html#torch.isposinf" title="torch.isposinf"><code class="xref py py-obj docutils literal "><span class="pre">isposinf</span></code></a></p></td>
<td><p>测试 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的每个元素是否为正无穷。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.isneginf"><a class="reference internal" href="generated/torch.isneginf.html#torch.isneginf" title="torch.isneginf"><code class="xref py py-obj docutils literal "><span class="pre">isneginf</span></code></a></p></td>
<td><p>测试 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的每个元素是否为负无穷。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.isnan"><a class="reference internal" href="generated/torch.isnan.html#torch.isnan" title="torch.isnan"><code class="xref py py-obj docutils literal "><span class="pre">isnan</span></code></a></p></td>
<td><p>返回一个新张量，其中的布尔元素表示 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中每个元素是否为 NaN。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.isreal"><a class="reference internal" href="generated/torch.isreal.html#torch.isreal" title="torch.isreal"><code class="xref py py-obj docutils literal "><span class="pre">isreal</span></code></a></p></td>
<td><p>返回一个新张量，其中的布尔元素表示 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 中的每个元素是否为实数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.kthvalue"><a class="reference internal" href="generated/torch.kthvalue.html#torch.kthvalue" title="torch.kthvalue"><code class="xref py py-obj docutils literal "><span class="pre">kthvalue</span></code></a></p></td>
<td><p>返回一个命名元组 <code class="docutils literal "><span class="pre">(values,</span> <span class="pre">indices)</span></code> ，其中 <code class="docutils literal "><span class="pre">values</span></code> 是 <code class="xref py py-attr docutils literal "><span class="pre">k</span></code> 张量在给定维度 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 中每行的第 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 小元素。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.le"><a class="reference internal" href="generated/torch.le.html#torch.le" title="torch.le"><code class="xref py py-obj docutils literal "><span class="pre">le</span></code></a></p></td>
<td><p>计算元素级 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>≤</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \leq \text{other}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.8623000000000001em;vertical-align:-0.19444em;" class="strut"></span><span class="mord text"><span class="mord">input</span></span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">≤</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mord text"><span class="mord">other</span></span></span></span></span> </p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.less_equal"><a class="reference internal" href="generated/torch.less_equal.html#torch.less_equal" title="torch.less_equal"><code class="xref py py-obj docutils literal "><span class="pre">less_equal</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.le()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.lt"><a class="reference internal" href="generated/torch.lt.html#torch.lt" title="torch.lt"><code class="xref py py-obj docutils literal "><span class="pre">lt</span></code></a></p></td>
<td><p>计算元素级 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>&lt;</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} &lt; \text{other}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.8623000000000001em;vertical-align:-0.19444em;" class="strut"></span><span class="mord text"><span class="mord">input</span></span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">&lt;</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mord text"><span class="mord">other</span></span></span></span></span> </p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.less"><a class="reference internal" href="generated/torch.less.html#torch.less" title="torch.less"><code class="xref py py-obj docutils literal "><span class="pre">less</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.lt()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.maximum"><a class="reference internal" href="generated/torch.maximum.html#torch.maximum" title="torch.maximum"><code class="xref py py-obj docutils literal "><span class="pre">maximum</span></code></a></p></td>
<td><p>计算元素-wise <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 的最大值。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.minimum"><a class="reference internal" href="generated/torch.minimum.html#torch.minimum" title="torch.minimum"><code class="xref py py-obj docutils literal "><span class="pre">minimum</span></code></a></p></td>
<td><p>计算元素-wise <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 的最小值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.fmax"><a class="reference internal" href="generated/torch.fmax.html#torch.fmax" title="torch.fmax"><code class="xref py py-obj docutils literal "><span class="pre">fmax</span></code></a></p></td>
<td><p>计算元素-wise <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 的最大值。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.fmin"><a class="reference internal" href="generated/torch.fmin.html#torch.fmin" title="torch.fmin"><code class="xref py py-obj docutils literal "><span class="pre">fmin</span></code></a></p></td>
<td><p>计算元素-wise <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 的最小值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.ne"><a class="reference internal" href="generated/torch.ne.html#torch.ne" title="torch.ne"><code class="xref py py-obj docutils literal "><span class="pre">ne</span></code></a></p></td>
<td><p>计算元素级 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo mathvariant="normal">≠</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \neq \text{other}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.8888799999999999em;vertical-align:-0.19444em;" class="strut"></span><span class="mord text"><span class="mord">input</span></span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span style="height:0.8888799999999999em;vertical-align:-0.19444em;" class="strut"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mord text"><span class="mord">other</span></span></span></span></span> </p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.not_equal"><a class="reference internal" href="generated/torch.not_equal.html#torch.not_equal" title="torch.not_equal"><code class="xref py py-obj docutils literal "><span class="pre">not_equal</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.ne()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sort"><a class="reference internal" href="generated/torch.sort.html#torch.sort" title="torch.sort"><code class="xref py py-obj docutils literal "><span class="pre">sort</span></code></a></p></td>
<td><p>按给定维度对 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量的元素进行升序排序。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.topk"><a class="reference internal" href="generated/torch.topk.html#torch.topk" title="torch.topk"><code class="xref py py-obj docutils literal "><span class="pre">topk</span></code></a></p></td>
<td><p>返回给定 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量沿给定维度上的 <code class="xref py py-attr docutils literal "><span class="pre">k</span></code> 最大元素。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.msort"><a class="reference internal" href="generated/torch.msort.html#torch.msort" title="torch.msort"><code class="xref py py-obj docutils literal "><span class="pre">msort</span></code></a></p></td>
<td><p>按值升序对 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 张量的第一个维度上的元素进行排序。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="spectral-ops">
<h3>光谱操作 ¶</h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.stft"><a class="reference internal" href="generated/torch.stft.html#torch.stft" title="torch.stft"><code class="xref py py-obj docutils literal "><span class="pre">stft</span></code></a></p></td>
<td><p>短时傅里叶变换（STFT）。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.istft"><a class="reference internal" href="generated/torch.istft.html#torch.istft" title="torch.istft"><code class="xref py py-obj docutils literal "><span class="pre">istft</span></code></a></p></td>
<td><p>逆短时傅里叶变换。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.bartlett_window"><a class="reference internal" href="generated/torch.bartlett_window.html#torch.bartlett_window" title="torch.bartlett_window"><code class="xref py py-obj docutils literal "><span class="pre">bartlett_window</span></code></a></p></td>
<td><p>巴特莱特窗口函数</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.blackman_window"><a class="reference internal" href="generated/torch.blackman_window.html#torch.blackman_window" title="torch.blackman_window"><code class="xref py py-obj docutils literal "><span class="pre">blackman_window</span></code></a></p></td>
<td><p>布朗曼窗口函数</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.hamming_window"><a class="reference internal" href="generated/torch.hamming_window.html#torch.hamming_window" title="torch.hamming_window"><code class="xref py py-obj docutils literal "><span class="pre">hamming_window</span></code></a></p></td>
<td><p>汉明窗口函数</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.hann_window"><a class="reference internal" href="generated/torch.hann_window.html#torch.hann_window" title="torch.hann_window"><code class="xref py py-obj docutils literal "><span class="pre">hann_window</span></code></a></p></td>
<td><p>汉宁窗口函数</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.kaiser_window"><a class="reference internal" href="generated/torch.kaiser_window.html#torch.kaiser_window" title="torch.kaiser_window"><code class="xref py py-obj docutils literal "><span class="pre">kaiser_window</span></code></a></p></td>
<td><p>计算长度为 <code class="xref py py-attr docutils literal "><span class="pre">window_length</span></code> 的 Kaiser 窗和形状参数为 <code class="xref py py-attr docutils literal "><span class="pre">beta</span></code> 的窗。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="other-operations">
<h3>其他操作 ¶</h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.atleast_1d"><a class="reference internal" href="generated/torch.atleast_1d.html#torch.atleast_1d" title="torch.atleast_1d"><code class="xref py py-obj docutils literal "><span class="pre">atleast_1d</span></code></a></p></td>
<td><p>返回每个输入张量的 1 维视图，其中没有零维。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.atleast_2d"><a class="reference internal" href="generated/torch.atleast_2d.html#torch.atleast_2d" title="torch.atleast_2d"><code class="xref py py-obj docutils literal "><span class="pre">atleast_2d</span></code></a></p></td>
<td><p>返回每个输入张量的二维视图，其中零维数为 0。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.atleast_3d"><a class="reference internal" href="generated/torch.atleast_3d.html#torch.atleast_3d" title="torch.atleast_3d"><code class="xref py py-obj docutils literal "><span class="pre">atleast_3d</span></code></a></p></td>
<td><p>返回每个输入张量的三维视图，其中零维数为 0。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.bincount"><a class="reference internal" href="generated/torch.bincount.html#torch.bincount" title="torch.bincount"><code class="xref py py-obj docutils literal "><span class="pre">bincount</span></code></a></p></td>
<td><p>计算非负整数数组中每个值的频率。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.block_diag"><a class="reference internal" href="generated/torch.block_diag.html#torch.block_diag" title="torch.block_diag"><code class="xref py py-obj docutils literal "><span class="pre">block_diag</span></code></a></p></td>
<td><p>从提供的张量创建块对角矩阵。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.broadcast_tensors"><a class="reference internal" href="generated/torch.broadcast_tensors.html#torch.broadcast_tensors" title="torch.broadcast_tensors"><code class="xref py py-obj docutils literal "><span class="pre">broadcast_tensors</span></code></a></p></td>
<td><p>根据广播语义广播给定的张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.broadcast_to"><a class="reference internal" href="generated/torch.broadcast_to.html#torch.broadcast_to" title="torch.broadcast_to"><code class="xref py py-obj docutils literal "><span class="pre">broadcast_to</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 广播到 <code class="xref py py-attr docutils literal "><span class="pre">shape</span></code> 的形状。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.broadcast_shapes"><a class="reference internal" href="generated/torch.broadcast_shapes.html#torch.broadcast_shapes" title="torch.broadcast_shapes"><code class="xref py py-obj docutils literal "><span class="pre">broadcast_shapes</span></code></a></p></td>
<td><p>与 <code class="xref py py-func docutils literal "><span class="pre">broadcast_tensors()</span></code> 类似，但用于形状。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.bucketize"><a class="reference internal" href="generated/torch.bucketize.html#torch.bucketize" title="torch.bucketize"><code class="xref py py-obj docutils literal "><span class="pre">bucketize</span></code></a></p></td>
<td><p>返回每个值所属的桶的索引，其中桶的边界由 <code class="xref py py-attr docutils literal "><span class="pre">boundaries</span></code> 设置。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.cartesian_prod"><a class="reference internal" href="generated/torch.cartesian_prod.html#torch.cartesian_prod" title="torch.cartesian_prod"><code class="xref py py-obj docutils literal "><span class="pre">cartesian_prod</span></code></a></p></td>
<td><p>对给定的张量序列进行笛卡尔积。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.cdist"><a class="reference internal" href="generated/torch.cdist.html#torch.cdist" title="torch.cdist"><code class="xref py py-obj docutils literal "><span class="pre">cdist</span></code></a></p></td>
<td><p>计算两个行向量集合中每对之间的批处理 p-范数距离。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.clone"><a class="reference internal" href="generated/torch.clone.html#torch.clone" title="torch.clone"><code class="xref py py-obj docutils literal "><span class="pre">clone</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的副本。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.combinations"><a class="reference internal" href="generated/torch.combinations.html#torch.combinations" title="torch.combinations"><code class="xref py py-obj docutils literal "><span class="pre">combinations</span></code></a></p></td>
<td><p>计算给定张量的长度为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.43056em;vertical-align:0em;" class="strut"></span><span style="margin-right:0.02778em;" class="mord mathnormal">r</span></span></span></span> 的组合。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.corrcoef"><a class="reference internal" href="generated/torch.corrcoef.html#torch.corrcoef" title="torch.corrcoef"><code class="xref py py-obj docutils literal "><span class="pre">corrcoef</span></code></a></p></td>
<td><p>估计由 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 矩阵给出的变量的皮尔逊积矩相关系数矩阵，其中行是变量，列是观测值。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.cov"><a class="reference internal" href="generated/torch.cov.html#torch.cov" title="torch.cov"><code class="xref py py-obj docutils literal "><span class="pre">cov</span></code></a></p></td>
<td><p>估计由 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 矩阵给出的变量的协方差矩阵，其中行是变量，列是观测值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.cross"><a class="reference internal" href="generated/torch.cross.html#torch.cross" title="torch.cross"><code class="xref py py-obj docutils literal "><span class="pre">cross</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 维度中向量的叉积。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.cummax"><a class="reference internal" href="generated/torch.cummax.html#torch.cummax" title="torch.cummax"><code class="xref py py-obj docutils literal "><span class="pre">cummax</span></code></a></p></td>
<td><p>返回一个 namedtuple <code class="docutils literal "><span class="pre">(values,</span> <span class="pre">indices)</span></code> ，其中 <code class="docutils literal "><span class="pre">values</span></code> 是 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 在 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 维度的元素累积最大值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.cummin"><a class="reference internal" href="generated/torch.cummin.html#torch.cummin" title="torch.cummin"><code class="xref py py-obj docutils literal "><span class="pre">cummin</span></code></a></p></td>
<td><p>返回一个命名元组 <code class="docutils literal "><span class="pre">(values,</span> <span class="pre">indices)</span></code> ，其中 <code class="docutils literal "><span class="pre">values</span></code> 是 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 在维度 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 上的元素累积最小值。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.cumprod"><a class="reference internal" href="generated/torch.cumprod.html#torch.cumprod" title="torch.cumprod"><code class="xref py py-obj docutils literal "><span class="pre">cumprod</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 在维度 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 上的元素累积乘积。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.cumsum"><a class="reference internal" href="generated/torch.cumsum.html#torch.cumsum" title="torch.cumsum"><code class="xref py py-obj docutils literal "><span class="pre">cumsum</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 在维度 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 上的元素累积和。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.diag"><a class="reference internal" href="generated/torch.diag.html#torch.diag" title="torch.diag"><code class="xref py py-obj docutils literal "><span class="pre">diag</span></code></a></p></td>
<td><p></p><ul class="simple">
<li><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 是一个向量（1-D 张量），则返回一个 2-D 正方形张量。</p></li>
</ul>
<p></p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.diag_embed"><a class="reference internal" href="generated/torch.diag_embed.html#torch.diag_embed" title="torch.diag_embed"><code class="xref py py-obj docutils literal "><span class="pre">diag_embed</span></code></a></p></td>
<td><p>创建一个张量，其中某些二维平面的对角线（由 <code class="xref py py-attr docutils literal "><span class="pre">dim1</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">dim2</span></code> 指定）被 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 填充。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.diagflat"><a class="reference internal" href="generated/torch.diagflat.html#torch.diagflat" title="torch.diagflat"><code class="xref py py-obj docutils literal "><span class="pre">diagflat</span></code></a></p></td>
<td><p></p><ul class="simple">
<li><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 是一个向量（1 维张量），则返回一个 2 维正方形张量。</p></li>
</ul>
<p></p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.diagonal"><a class="reference internal" href="generated/torch.diagonal.html#torch.diagonal" title="torch.diagonal"><code class="xref py py-obj docutils literal "><span class="pre">diagonal</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的局部视图，其中相对于 <code class="xref py py-attr docutils literal "><span class="pre">dim1</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">dim2</span></code> 的对角线元素作为形状末尾的一个维度附加。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.diff"><a class="reference internal" href="generated/torch.diff.html#torch.diff" title="torch.diff"><code class="xref py py-obj docutils literal "><span class="pre">diff</span></code></a></p></td>
<td><p>计算给定维度上的 n 阶前向差分。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.einsum"><a class="reference internal" href="generated/torch.einsum.html#torch.einsum" title="torch.einsum"><code class="xref py py-obj docutils literal "><span class="pre">einsum</span></code></a></p></td>
<td><p>沿着使用爱因斯坦求和约定表示法指定的维度对输入 <code class="xref py py-attr docutils literal "><span class="pre">operands</span></code> 的元素求和。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.flatten"><a class="reference internal" href="generated/torch.flatten.html#torch.flatten" title="torch.flatten"><code class="xref py py-obj docutils literal "><span class="pre">flatten</span></code></a></p></td>
<td><p>将 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 通过重塑成一维张量来展平。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.flip"><a class="reference internal" href="generated/torch.flip.html#torch.flip" title="torch.flip"><code class="xref py py-obj docutils literal "><span class="pre">flip</span></code></a></p></td>
<td><p>沿给定轴在 dims 中对 n 维张量顺序进行反转。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.fliplr"><a class="reference internal" href="generated/torch.fliplr.html#torch.fliplr" title="torch.fliplr"><code class="xref py py-obj docutils literal "><span class="pre">fliplr</span></code></a></p></td>
<td><p>在左右方向上翻转张量，返回一个新的张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.flipud"><a class="reference internal" href="generated/torch.flipud.html#torch.flipud" title="torch.flipud"><code class="xref py py-obj docutils literal "><span class="pre">flipud</span></code></a></p></td>
<td><p>在上下方向上翻转张量，返回一个新的张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.kron"><a class="reference internal" href="generated/torch.kron.html#torch.kron" title="torch.kron"><code class="xref py py-obj docutils literal "><span class="pre">kron</span></code></a></p></td>
<td><p>计算由 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 表示的克罗内克积，记作 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding="application/x-tex">\otimes</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.66666em;vertical-align:-0.08333em;" class="strut"></span><span class="mord">⊗</span></span></span></span> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.rot90"><a class="reference internal" href="generated/torch.rot90.html#torch.rot90" title="torch.rot90"><code class="xref py py-obj docutils literal "><span class="pre">rot90</span></code></a></p></td>
<td><p>在指定维度轴的平面上将 n 维张量旋转 90 度。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.gcd"><a class="reference internal" href="generated/torch.gcd.html#torch.gcd" title="torch.gcd"><code class="xref py py-obj docutils literal "><span class="pre">gcd</span></code></a></p></td>
<td><p>计算元素-wise 的最大公约数（GCD） <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.histc"><a class="reference internal" href="generated/torch.histc.html#torch.histc" title="torch.histc"><code class="xref py py-obj docutils literal "><span class="pre">histc</span></code></a></p></td>
<td><p>计算张量的直方图。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.histogram"><a class="reference internal" href="generated/torch.histogram.html#torch.histogram" title="torch.histogram"><code class="xref py py-obj docutils literal "><span class="pre">histogram</span></code></a></p></td>
<td><p>计算张量中值的直方图。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.histogramdd"><a class="reference internal" href="generated/torch.histogramdd.html#torch.histogramdd" title="torch.histogramdd"><code class="xref py py-obj docutils literal "><span class="pre">histogramdd</span></code></a></p></td>
<td><p>计算张量中值的多元直方图。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.meshgrid"><a class="reference internal" href="generated/torch.meshgrid.html#torch.meshgrid" title="torch.meshgrid"><code class="xref py py-obj docutils literal "><span class="pre">meshgrid</span></code></a></p></td>
<td><p>根据 attr:tensors 中的 1D 输入创建坐标网格。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.lcm"><a class="reference internal" href="generated/torch.lcm.html#torch.lcm" title="torch.lcm"><code class="xref py py-obj docutils literal "><span class="pre">lcm</span></code></a></p></td>
<td><p>计算 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">other</span></code> 的逐元素最小公倍数（LCM）。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.logcumsumexp"><a class="reference internal" href="generated/torch.logcumsumexp.html#torch.logcumsumexp" title="torch.logcumsumexp"><code class="xref py py-obj docutils literal "><span class="pre">logcumsumexp</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 元素在维度 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 的指数的累积和的对数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.ravel"><a class="reference internal" href="generated/torch.ravel.html#torch.ravel" title="torch.ravel"><code class="xref py py-obj docutils literal "><span class="pre">ravel</span></code></a></p></td>
<td><p>返回一个连续的展平张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.renorm"><a class="reference internal" href="generated/torch.renorm.html#torch.renorm" title="torch.renorm"><code class="xref py py-obj docutils literal "><span class="pre">renorm</span></code></a></p></td>
<td><p>返回一个张量，其中每个沿维度 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 的子张量都被归一化，使得子张量的 p-范数低于值 <code class="xref py py-attr docutils literal "><span class="pre">maxnorm</span></code> </p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.repeat_interleave"><a class="reference internal" href="generated/torch.repeat_interleave.html#torch.repeat_interleave" title="torch.repeat_interleave"><code class="xref py py-obj docutils literal "><span class="pre">repeat_interleave</span></code></a></p></td>
<td><p>重复张量的元素。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.roll"><a class="reference internal" href="generated/torch.roll.html#torch.roll" title="torch.roll"><code class="xref py py-obj docutils literal "><span class="pre">roll</span></code></a></p></td>
<td><p>将张量 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 沿给定维度滚动。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.searchsorted"><a class="reference internal" href="generated/torch.searchsorted.html#torch.searchsorted" title="torch.searchsorted"><code class="xref py py-obj docutils literal "><span class="pre">searchsorted</span></code></a></p></td>
<td><p>从 <code class="xref py py-attr docutils literal "><span class="pre">sorted_sequence</span></code> 的最内层维度找到索引，使得如果将 <code class="xref py py-attr docutils literal "><span class="pre">values</span></code> 中的对应值插入到这些索引之前，当排序时， <code class="xref py py-attr docutils literal "><span class="pre">sorted_sequence</span></code> 中最内层维度的顺序将保持不变。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.tensordot"><a class="reference internal" href="generated/torch.tensordot.html#torch.tensordot" title="torch.tensordot"><code class="xref py py-obj docutils literal "><span class="pre">tensordot</span></code></a></p></td>
<td><p>返回 a 和 b 在多个维度上的收缩。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.trace"><a class="reference internal" href="generated/torch.trace.html#torch.trace" title="torch.trace"><code class="xref py py-obj docutils literal "><span class="pre">trace</span></code></a></p></td>
<td><p>返回输入 2-D 矩阵对角线元素的和。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.tril"><a class="reference internal" href="generated/torch.tril.html#torch.tril" title="torch.tril"><code class="xref py py-obj docutils literal "><span class="pre">tril</span></code></a></p></td>
<td><p>返回矩阵（2-D 张量）或矩阵批量的下三角部分（ <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> ），结果张量的其他元素（ <code class="xref py py-attr docutils literal "><span class="pre">out</span></code> ）设置为 0。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.tril_indices"><a class="reference internal" href="generated/torch.tril_indices.html#torch.tril_indices" title="torch.tril_indices"><code class="xref py py-obj docutils literal "><span class="pre">tril_indices</span></code></a></p></td>
<td><p>返回一个 <code class="xref py py-attr docutils literal "><span class="pre">row</span></code> -by- <code class="xref py py-attr docutils literal "><span class="pre">col</span></code> 矩阵下三角部分的索引，在一个 2-by-N 张量中，第一行包含所有索引的行坐标，第二行包含列坐标。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.triu"><a class="reference internal" href="generated/torch.triu.html#torch.triu" title="torch.triu"><code class="xref py py-obj docutils literal "><span class="pre">triu</span></code></a></p></td>
<td><p>返回矩阵（2-D 张量）或矩阵批量的上三角部分（ <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> ），结果张量的其他元素（ <code class="xref py py-attr docutils literal "><span class="pre">out</span></code> ）设置为 0。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.triu_indices"><a class="reference internal" href="generated/torch.triu_indices.html#torch.triu_indices" title="torch.triu_indices"><code class="xref py py-obj docutils literal "><span class="pre">triu_indices</span></code></a></p></td>
<td><p>返回一个 2-by-N 张量上三角部分的索引，其中第一行包含所有索引的行坐标，第二行包含列坐标。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.unflatten"><a class="reference internal" href="generated/torch.unflatten.html#torch.unflatten" title="torch.unflatten"><code class="xref py py-obj docutils literal "><span class="pre">unflatten</span></code></a></p></td>
<td><p>在多个维度上扩展输入张量的一个维度。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.vander"><a class="reference internal" href="generated/torch.vander.html#torch.vander" title="torch.vander"><code class="xref py py-obj docutils literal "><span class="pre">vander</span></code></a></p></td>
<td><p>生成 Vandermonde 矩阵。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.view_as_real"><a class="reference internal" href="generated/torch.view_as_real.html#torch.view_as_real" title="torch.view_as_real"><code class="xref py py-obj docutils literal "><span class="pre">view_as_real</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的视图作为实数张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.view_as_complex"><a class="reference internal" href="generated/torch.view_as_complex.html#torch.view_as_complex" title="torch.view_as_complex"><code class="xref py py-obj docutils literal "><span class="pre">view_as_complex</span></code></a></p></td>
<td><p>返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的视图作为复数张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.resolve_conj"><a class="reference internal" href="generated/torch.resolve_conj.html#torch.resolve_conj" title="torch.resolve_conj"><code class="xref py py-obj docutils literal "><span class="pre">resolve_conj</span></code></a></p></td>
<td><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的共轭位设置为 True，则返回一个新的具有实际共轭的张量，否则返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.resolve_neg"><a class="reference internal" href="generated/torch.resolve_neg.html#torch.resolve_neg" title="torch.resolve_neg"><code class="xref py py-obj docutils literal "><span class="pre">resolve_neg</span></code></a></p></td>
<td><p>如果 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 的负号位设置为 True，则返回一个新的具有实际取反的张量，否则返回 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="blas-and-lapack-operations">
<h3>BLAS 和 LAPACK 操作</h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.addbmm"><a class="reference internal" href="generated/torch.addbmm.html#torch.addbmm" title="torch.addbmm"><code class="xref py py-obj docutils literal "><span class="pre">addbmm</span></code></a></p></td>
<td><p>执行存储在 <code class="xref py py-attr docutils literal "><span class="pre">batch1</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">batch2</span></code> 中的矩阵的批量矩阵-矩阵乘法，具有减少的加法步骤（所有矩阵乘法都沿第一个维度累积）。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.addmm"><a class="reference internal" href="generated/torch.addmm.html#torch.addmm" title="torch.addmm"><code class="xref py py-obj docutils literal "><span class="pre">addmm</span></code></a></p></td>
<td><p>执行矩阵 <code class="xref py py-attr docutils literal "><span class="pre">mat1</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">mat2</span></code> 的矩阵乘法。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.addmv"><a class="reference internal" href="generated/torch.addmv.html#torch.addmv" title="torch.addmv"><code class="xref py py-obj docutils literal "><span class="pre">addmv</span></code></a></p></td>
<td><p>执行矩阵 <code class="xref py py-attr docutils literal "><span class="pre">mat</span></code> 和向量 <code class="xref py py-attr docutils literal "><span class="pre">vec</span></code> 的矩阵-向量乘法。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.addr"><a class="reference internal" href="generated/torch.addr.html#torch.addr" title="torch.addr"><code class="xref py py-obj docutils literal "><span class="pre">addr</span></code></a></p></td>
<td><p>执行向量 <code class="xref py py-attr docutils literal "><span class="pre">vec1</span></code> 和向量 <code class="xref py py-attr docutils literal "><span class="pre">vec2</span></code> 的外积，并将其加到矩阵 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 上。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.baddbmm"><a class="reference internal" href="generated/torch.baddbmm.html#torch.baddbmm" title="torch.baddbmm"><code class="xref py py-obj docutils literal "><span class="pre">baddbmm</span></code></a></p></td>
<td><p>执行矩阵 <code class="xref py py-attr docutils literal "><span class="pre">batch1</span></code> 和矩阵 <code class="xref py py-attr docutils literal "><span class="pre">batch2</span></code> 的批量矩阵-矩阵乘法。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.bmm"><a class="reference internal" href="generated/torch.bmm.html#torch.bmm" title="torch.bmm"><code class="xref py py-obj docutils literal "><span class="pre">bmm</span></code></a></p></td>
<td><p>执行存储在 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">mat2</span></code> 中的矩阵的批量矩阵-矩阵乘法。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.chain_matmul"><a class="reference internal" href="generated/torch.chain_matmul.html#torch.chain_matmul" title="torch.chain_matmul"><code class="xref py py-obj docutils literal "><span class="pre">chain_matmul</span></code></a></p></td>
<td><p>返回 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.68333em;vertical-align:0em;" class="strut"></span><span style="margin-right:0.10903em;" class="mord mathnormal">N</span></span></span></span> 2-D 张量的矩阵乘积。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.cholesky"><a class="reference internal" href="generated/torch.cholesky.html#torch.cholesky" title="torch.cholesky"><code class="xref py py-obj docutils literal "><span class="pre">cholesky</span></code></a></p></td>
<td><p>计算对称正定矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.68333em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">A</span></span></span></span> 或对称正定矩阵批次的 Cholesky 分解。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.cholesky_inverse"><a class="reference internal" href="generated/torch.cholesky_inverse.html#torch.cholesky_inverse" title="torch.cholesky_inverse"><code class="xref py py-obj docutils literal "><span class="pre">cholesky_inverse</span></code></a></p></td>
<td><p>计算给定其 Cholesky 分解的复 Hermitian 或实对称正定矩阵的逆。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.cholesky_solve"><a class="reference internal" href="generated/torch.cholesky_solve.html#torch.cholesky_solve" title="torch.cholesky_solve"><code class="xref py py-obj docutils literal "><span class="pre">cholesky_solve</span></code></a></p></td>
<td><p>计算给定其 Cholesky 分解的复 Hermitian 或实对称正定 lhs 的线性方程组的解。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.dot"><a class="reference internal" href="generated/torch.dot.html#torch.dot" title="torch.dot"><code class="xref py py-obj docutils literal "><span class="pre">dot</span></code></a></p></td>
<td><p>计算两个一维张量的点积。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.geqrf"><a class="reference internal" href="generated/torch.geqrf.html#torch.geqrf" title="torch.geqrf"><code class="xref py py-obj docutils literal "><span class="pre">geqrf</span></code></a></p></td>
<td><p>这是一个用于直接调用 LAPACK 的 geqrf 的低级函数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.ger"><a class="reference internal" href="generated/torch.ger.html#torch.ger" title="torch.ger"><code class="xref py py-obj docutils literal "><span class="pre">ger</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.outer()</span></code> 的别名。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.inner"><a class="reference internal" href="generated/torch.inner.html#torch.inner" title="torch.inner"><code class="xref py py-obj docutils literal "><span class="pre">inner</span></code></a></p></td>
<td><p>计算一维张量的点积。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.inverse"><a class="reference internal" href="generated/torch.inverse.html#torch.inverse" title="torch.inverse"><code class="xref py py-obj docutils literal "><span class="pre">inverse</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.linalg.inv()</span></code> 的别名。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.det"><a class="reference internal" href="generated/torch.det.html#torch.det" title="torch.det"><code class="xref py py-obj docutils literal "><span class="pre">det</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.linalg.det()</span></code> 的别名。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.logdet"><a class="reference internal" href="generated/torch.logdet.html#torch.logdet" title="torch.logdet"><code class="xref py py-obj docutils literal "><span class="pre">logdet</span></code></a></p></td>
<td><p>计算方阵或方阵批次的对数行列式。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.slogdet"><a class="reference internal" href="generated/torch.slogdet.html#torch.slogdet" title="torch.slogdet"><code class="xref py py-obj docutils literal "><span class="pre">slogdet</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.linalg.slogdet()</span></code> 的别名。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.lu"><a class="reference internal" href="generated/torch.lu.html#torch.lu" title="torch.lu"><code class="xref py py-obj docutils literal "><span class="pre">lu</span></code></a></p></td>
<td><p>计算矩阵或矩阵批次的 LU 分解。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.lu_solve"><a class="reference internal" href="generated/torch.lu_solve.html#torch.lu_solve" title="torch.lu_solve"><code class="xref py py-obj docutils literal "><span class="pre">lu_solve</span></code></a></p></td>
<td><p>返回使用 A 的带部分主元 LU 分解的线性系统 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>x</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">Ax = b</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.68333em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">x</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">b</span></span></span></span> 的 LU 求解。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.lu_unpack"><a class="reference internal" href="generated/torch.lu_unpack.html#torch.lu_unpack" title="torch.lu_unpack"><code class="xref py py-obj docutils literal "><span class="pre">lu_unpack</span></code></a></p></td>
<td><p>将由 <code class="xref py py-func docutils literal "><span class="pre">lu_factor()</span></code> 返回的 LU 分解解包到 P、L、U 矩阵中。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.matmul"><a class="reference internal" href="generated/torch.matmul.html#torch.matmul" title="torch.matmul"><code class="xref py py-obj docutils literal "><span class="pre">matmul</span></code></a></p></td>
<td><p>两个张量的矩阵乘积。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.matrix_power"><a class="reference internal" href="generated/torch.matrix_power.html#torch.matrix_power" title="torch.matrix_power"><code class="xref py py-obj docutils literal "><span class="pre">matrix_power</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.linalg.matrix_power()</span></code> 的别名。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.matrix_exp"><a class="reference internal" href="generated/torch.matrix_exp.html#torch.matrix_exp" title="torch.matrix_exp"><code class="xref py py-obj docutils literal "><span class="pre">matrix_exp</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.linalg.matrix_exp()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.mm"><a class="reference internal" href="generated/torch.mm.html#torch.mm" title="torch.mm"><code class="xref py py-obj docutils literal "><span class="pre">mm</span></code></a></p></td>
<td><p>执行矩阵 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">mat2</span></code> 的矩阵乘法。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.mv"><a class="reference internal" href="generated/torch.mv.html#torch.mv" title="torch.mv"><code class="xref py py-obj docutils literal "><span class="pre">mv</span></code></a></p></td>
<td><p>执行矩阵 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和向量 <code class="xref py py-attr docutils literal "><span class="pre">vec</span></code> 的矩阵-向量乘积。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.orgqr"><a class="reference internal" href="generated/torch.orgqr.html#torch.orgqr" title="torch.orgqr"><code class="xref py py-obj docutils literal "><span class="pre">orgqr</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.linalg.householder_product()</span></code> 的别名</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.ormqr"><a class="reference internal" href="generated/torch.ormqr.html#torch.ormqr" title="torch.ormqr"><code class="xref py py-obj docutils literal "><span class="pre">ormqr</span></code></a></p></td>
<td><p>计算 Householder 矩阵乘积与一般矩阵的矩阵乘法。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.outer"><a class="reference internal" href="generated/torch.outer.html#torch.outer" title="torch.outer"><code class="xref py py-obj docutils literal "><span class="pre">outer</span></code></a></p></td>
<td><p>外积 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">vec2</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.pinverse"><a class="reference internal" href="generated/torch.pinverse.html#torch.pinverse" title="torch.pinverse"><code class="xref py py-obj docutils literal "><span class="pre">pinverse</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.linalg.pinv()</span></code> 的别名。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.qr"><a class="reference internal" href="generated/torch.qr.html#torch.qr" title="torch.qr"><code class="xref py py-obj docutils literal "><span class="pre">qr</span></code></a></p></td>
<td><p>计算矩阵或矩阵批量的 QR 分解，并返回一个包含张量的 namedtuple（Q, R），其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>=</mo><mi>Q</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">\text{input} = Q R</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.8623000000000001em;vertical-align:-0.19444em;" class="strut"></span><span class="mord text"><span class="mord">input</span></span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.8777699999999999em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.00773em;" class="mord mathnormal">QR</span></span></span></span> 是正交矩阵或正交矩阵批， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.68333em;vertical-align:0em;" class="strut"></span><span style="margin-right:0.00773em;" class="mord mathnormal">R</span></span></span></span> 是上三角矩阵或上三角矩阵批。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.svd"><a class="reference internal" href="generated/torch.svd.html#torch.svd" title="torch.svd"><code class="xref py py-obj docutils literal "><span class="pre">svd</span></code></a></p></td>
<td><p>计算矩阵或矩阵批量的奇异值分解 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> .</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.svd_lowrank"><a class="reference internal" href="generated/torch.svd_lowrank.html#torch.svd_lowrank" title="torch.svd_lowrank"><code class="xref py py-obj docutils literal "><span class="pre">svd_lowrank</span></code></a></p></td>
<td><p>返回矩阵、矩阵批或稀疏矩阵的奇异值分解 <code class="docutils literal "><span class="pre">(U,</span> <span class="pre">S,</span> <span class="pre">V)</span></code> ，使得 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>≈</mo><mi>U</mi><mi mathvariant="normal">diag</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><msup><mi>V</mi><mtext>H</mtext></msup></mrow><annotation encoding="application/x-tex">A \approx U \operatorname{diag}(S) V^{\text{H}}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.68333em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">A</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">≈</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1.0913309999999998em;vertical-align:-0.25em;" class="strut"></span><span style="margin-right:0.10903em;" class="mord mathnormal">U</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span class="mop"><span style="margin-right:0.01389em;" class="mord mathrm">diag</span></span><span class="mopen">(</span><span style="margin-right:0.05764em;" class="mord mathnormal">S</span><span class="mclose">)</span><span class="mord"><span style="margin-right:0.22222em;" class="mord mathnormal">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height:0.8413309999999999em;" class="vlist"><span style="top:-3.063em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">H</span></span></span></span></span></span></span></span></span></span></span></span></span> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.pca_lowrank"><a class="reference internal" href="generated/torch.pca_lowrank.html#torch.pca_lowrank" title="torch.pca_lowrank"><code class="xref py py-obj docutils literal "><span class="pre">pca_lowrank</span></code></a></p></td>
<td><p>对低秩矩阵、此类矩阵的批或稀疏矩阵执行线性主成分分析（PCA）。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.lobpcg"><a class="reference internal" href="generated/torch.lobpcg.html#torch.lobpcg" title="torch.lobpcg"><code class="xref py py-obj docutils literal "><span class="pre">lobpcg</span></code></a></p></td>
<td><p>使用无矩阵的 LOBPCG 方法，找到对称正定广义特征值问题的 k 个最大（或最小）特征值及其对应的特征向量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.trapz"><a class="reference internal" href="generated/torch.trapz.html#torch.trapz" title="torch.trapz"><code class="xref py py-obj docutils literal "><span class="pre">trapz</span></code></a></p></td>
<td><p> <code class="xref py py-func docutils literal "><span class="pre">torch.trapezoid()</span></code> 的别名</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.trapezoid"><a class="reference internal" href="generated/torch.trapezoid.html#torch.trapezoid" title="torch.trapezoid"><code class="xref py py-obj docutils literal "><span class="pre">trapezoid</span></code></a></p></td>
<td><p>沿 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 计算梯形规则。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.cumulative_trapezoid"><a class="reference internal" href="generated/torch.cumulative_trapezoid.html#torch.cumulative_trapezoid" title="torch.cumulative_trapezoid"><code class="xref py py-obj docutils literal "><span class="pre">cumulative_trapezoid</span></code></a></p></td>
<td><p></p><p>累计计算沿 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 的梯形法则。</p>
<p></p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.triangular_solve"><a class="reference internal" href="generated/torch.triangular_solve.html#torch.triangular_solve" title="torch.triangular_solve"><code class="xref py py-obj docutils literal "><span class="pre">triangular_solve</span></code></a></p></td>
<td><p>解具有平方上三角或下三角可逆矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.68333em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">A</span></span></span></span> 和多个右端项 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">b</span></span></span></span> 的方程组。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.vdot"><a class="reference internal" href="generated/torch.vdot.html#torch.vdot" title="torch.vdot"><code class="xref py py-obj docutils literal "><span class="pre">vdot</span></code></a></p></td>
<td><p>沿一个维度计算两个 1D 向量的点积。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="foreach-operations">
<h3>每次操作 ¶</h3>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>此 API 处于测试阶段，未来可能会有所变动。不支持前向模式 AD。</p>
</div>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch._foreach_abs"><a class="reference internal" href="generated/torch._foreach_abs.html#torch._foreach_abs" title="torch._foreach_abs"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_abs</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.abs()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_abs_"><a class="reference internal" href="generated/torch._foreach_abs_.html#torch._foreach_abs_" title="torch._foreach_abs_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_abs_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.abs()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_acos"><a class="reference internal" href="generated/torch._foreach_acos.html#torch._foreach_acos" title="torch._foreach_acos"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_acos</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.acos()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_acos_"><a class="reference internal" href="generated/torch._foreach_acos_.html#torch._foreach_acos_" title="torch._foreach_acos_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_acos_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.acos()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_asin"><a class="reference internal" href="generated/torch._foreach_asin.html#torch._foreach_asin" title="torch._foreach_asin"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_asin</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.asin()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_asin_"><a class="reference internal" href="generated/torch._foreach_asin_.html#torch._foreach_asin_" title="torch._foreach_asin_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_asin_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.asin()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_atan"><a class="reference internal" href="generated/torch._foreach_atan.html#torch._foreach_atan" title="torch._foreach_atan"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_atan</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.atan()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_atan_"><a class="reference internal" href="generated/torch._foreach_atan_.html#torch._foreach_atan_" title="torch._foreach_atan_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_atan_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.atan()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_ceil"><a class="reference internal" href="generated/torch._foreach_ceil.html#torch._foreach_ceil" title="torch._foreach_ceil"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_ceil</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.ceil()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_ceil_"><a class="reference internal" href="generated/torch._foreach_ceil_.html#torch._foreach_ceil_" title="torch._foreach_ceil_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_ceil_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.ceil()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_cos"><a class="reference internal" href="generated/torch._foreach_cos.html#torch._foreach_cos" title="torch._foreach_cos"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_cos</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.cos()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_cos_"><a class="reference internal" href="generated/torch._foreach_cos_.html#torch._foreach_cos_" title="torch._foreach_cos_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_cos_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.cos()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_cosh"><a class="reference internal" href="generated/torch._foreach_cosh.html#torch._foreach_cosh" title="torch._foreach_cosh"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_cosh</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.cosh()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_cosh_"><a class="reference internal" href="generated/torch._foreach_cosh_.html#torch._foreach_cosh_" title="torch._foreach_cosh_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_cosh_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.cosh()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_erf"><a class="reference internal" href="generated/torch._foreach_erf.html#torch._foreach_erf" title="torch._foreach_erf"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_erf</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.erf()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_erf_"><a class="reference internal" href="generated/torch._foreach_erf_.html#torch._foreach_erf_" title="torch._foreach_erf_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_erf_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.erf()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_erfc"><a class="reference internal" href="generated/torch._foreach_erfc.html#torch._foreach_erfc" title="torch._foreach_erfc"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_erfc</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.erfc()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_erfc_"><a class="reference internal" href="generated/torch._foreach_erfc_.html#torch._foreach_erfc_" title="torch._foreach_erfc_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_erfc_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.erfc()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_exp"><a class="reference internal" href="generated/torch._foreach_exp.html#torch._foreach_exp" title="torch._foreach_exp"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_exp</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.exp()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_exp_"><a class="reference internal" href="generated/torch._foreach_exp_.html#torch._foreach_exp_" title="torch._foreach_exp_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_exp_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.exp()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_expm1"><a class="reference internal" href="generated/torch._foreach_expm1.html#torch._foreach_expm1" title="torch._foreach_expm1"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_expm1</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.expm1()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_expm1_"><a class="reference internal" href="generated/torch._foreach_expm1_.html#torch._foreach_expm1_" title="torch._foreach_expm1_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_expm1_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.expm1()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_floor"><a class="reference internal" href="generated/torch._foreach_floor.html#torch._foreach_floor" title="torch._foreach_floor"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_floor</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.floor()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_floor_"><a class="reference internal" href="generated/torch._foreach_floor_.html#torch._foreach_floor_" title="torch._foreach_floor_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_floor_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.floor()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_log"><a class="reference internal" href="generated/torch._foreach_log.html#torch._foreach_log" title="torch._foreach_log"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_log</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.log()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_log_"><a class="reference internal" href="generated/torch._foreach_log_.html#torch._foreach_log_" title="torch._foreach_log_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_log_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.log()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_log10"><a class="reference internal" href="generated/torch._foreach_log10.html#torch._foreach_log10" title="torch._foreach_log10"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_log10</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.log10()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_log10_"><a class="reference internal" href="generated/torch._foreach_log10_.html#torch._foreach_log10_" title="torch._foreach_log10_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_log10_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.log10()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_log1p"><a class="reference internal" href="generated/torch._foreach_log1p.html#torch._foreach_log1p" title="torch._foreach_log1p"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_log1p</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.log1p()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_log1p_"><a class="reference internal" href="generated/torch._foreach_log1p_.html#torch._foreach_log1p_" title="torch._foreach_log1p_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_log1p_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.log1p()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_log2"><a class="reference internal" href="generated/torch._foreach_log2.html#torch._foreach_log2" title="torch._foreach_log2"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_log2</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.log2()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_log2_"><a class="reference internal" href="generated/torch._foreach_log2_.html#torch._foreach_log2_" title="torch._foreach_log2_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_log2_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.log2()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_neg"><a class="reference internal" href="generated/torch._foreach_neg.html#torch._foreach_neg" title="torch._foreach_neg"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_neg</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.neg()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_neg_"><a class="reference internal" href="generated/torch._foreach_neg_.html#torch._foreach_neg_" title="torch._foreach_neg_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_neg_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.neg()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_tan"><a class="reference internal" href="generated/torch._foreach_tan.html#torch._foreach_tan" title="torch._foreach_tan"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_tan</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.tan()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_tan_"><a class="reference internal" href="generated/torch._foreach_tan_.html#torch._foreach_tan_" title="torch._foreach_tan_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_tan_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.tan()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_sin"><a class="reference internal" href="generated/torch._foreach_sin.html#torch._foreach_sin" title="torch._foreach_sin"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_sin</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.sin()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_sin_"><a class="reference internal" href="generated/torch._foreach_sin_.html#torch._foreach_sin_" title="torch._foreach_sin_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_sin_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.sin()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_sinh"><a class="reference internal" href="generated/torch._foreach_sinh.html#torch._foreach_sinh" title="torch._foreach_sinh"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_sinh</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.sinh()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_sinh_"><a class="reference internal" href="generated/torch._foreach_sinh_.html#torch._foreach_sinh_" title="torch._foreach_sinh_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_sinh_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.sinh()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_round"><a class="reference internal" href="generated/torch._foreach_round.html#torch._foreach_round" title="torch._foreach_round"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_round</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.round()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_round_"><a class="reference internal" href="generated/torch._foreach_round_.html#torch._foreach_round_" title="torch._foreach_round_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_round_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.round()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_sqrt"><a class="reference internal" href="generated/torch._foreach_sqrt.html#torch._foreach_sqrt" title="torch._foreach_sqrt"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_sqrt</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.sqrt()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_sqrt_"><a class="reference internal" href="generated/torch._foreach_sqrt_.html#torch._foreach_sqrt_" title="torch._foreach_sqrt_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_sqrt_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.sqrt()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_lgamma"><a class="reference internal" href="generated/torch._foreach_lgamma.html#torch._foreach_lgamma" title="torch._foreach_lgamma"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_lgamma</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.lgamma()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_lgamma_"><a class="reference internal" href="generated/torch._foreach_lgamma_.html#torch._foreach_lgamma_" title="torch._foreach_lgamma_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_lgamma_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.lgamma()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_frac"><a class="reference internal" href="generated/torch._foreach_frac.html#torch._foreach_frac" title="torch._foreach_frac"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_frac</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.frac()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_frac_"><a class="reference internal" href="generated/torch._foreach_frac_.html#torch._foreach_frac_" title="torch._foreach_frac_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_frac_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.frac()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_reciprocal"><a class="reference internal" href="generated/torch._foreach_reciprocal.html#torch._foreach_reciprocal" title="torch._foreach_reciprocal"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_reciprocal</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.reciprocal()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_reciprocal_"><a class="reference internal" href="generated/torch._foreach_reciprocal_.html#torch._foreach_reciprocal_" title="torch._foreach_reciprocal_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_reciprocal_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.reciprocal()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_sigmoid"><a class="reference internal" href="generated/torch._foreach_sigmoid.html#torch._foreach_sigmoid" title="torch._foreach_sigmoid"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_sigmoid</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.sigmoid()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_sigmoid_"><a class="reference internal" href="generated/torch._foreach_sigmoid_.html#torch._foreach_sigmoid_" title="torch._foreach_sigmoid_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_sigmoid_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.sigmoid()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_trunc"><a class="reference internal" href="generated/torch._foreach_trunc.html#torch._foreach_trunc" title="torch._foreach_trunc"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_trunc</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.trunc()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._foreach_trunc_"><a class="reference internal" href="generated/torch._foreach_trunc_.html#torch._foreach_trunc_" title="torch._foreach_trunc_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_trunc_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.trunc()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch._foreach_zero_"><a class="reference internal" href="generated/torch._foreach_zero_.html#torch._foreach_zero_" title="torch._foreach_zero_"><code class="xref py py-obj docutils literal "><span class="pre">_foreach_zero_</span></code></a></p></td>
<td><p>将 <code class="xref py py-func docutils literal "><span class="pre">torch.zero()</span></code> 应用于输入列表中的每个张量。</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="utilities">
<h2>公用事业</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.compiled_with_cxx11_abi"><a class="reference internal" href="generated/torch.compiled_with_cxx11_abi.html#torch.compiled_with_cxx11_abi" title="torch.compiled_with_cxx11_abi"><code class="xref py py-obj docutils literal "><span class="pre">compiled_with_cxx11_abi</span></code></a></p></td>
<td><p>返回 PyTorch 是否使用 _GLIBCXX_USE_CXX11_ABI=1 编译</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.result_type"><a class="reference internal" href="generated/torch.result_type.html#torch.result_type" title="torch.result_type"><code class="xref py py-obj docutils literal "><span class="pre">result_type</span></code></a></p></td>
<td><p>返回对提供的输入张量执行算术运算的结果 <code class="xref py py-class docutils literal "><span class="pre">torch.dtype</span></code> </p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.can_cast"><a class="reference internal" href="generated/torch.can_cast.html#torch.can_cast" title="torch.can_cast"><code class="xref py py-obj docutils literal "><span class="pre">can_cast</span></code></a></p></td>
<td><p>判断在 PyTorch 类型提升规则下是否允许类型转换，这些规则在类型提升文档中描述</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.promote_types"><a class="reference internal" href="generated/torch.promote_types.html#torch.promote_types" title="torch.promote_types"><code class="xref py py-obj docutils literal "><span class="pre">promote_types</span></code></a></p></td>
<td><p>返回具有最小大小和标量类型的 <code class="xref py py-class docutils literal "><span class="pre">torch.dtype</span></code> ，该类型不小于也不低于 type1 或 type2</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.use_deterministic_algorithms"><a class="reference internal" href="generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms" title="torch.use_deterministic_algorithms"><code class="xref py py-obj docutils literal "><span class="pre">use_deterministic_algorithms</span></code></a></p></td>
<td><p>设置 PyTorch 操作是否必须使用“确定性”算法。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.are_deterministic_algorithms_enabled"><a class="reference internal" href="generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled" title="torch.are_deterministic_algorithms_enabled"><code class="xref py py-obj docutils literal "><span class="pre">are_deterministic_algorithms_enabled</span></code></a></p></td>
<td><p>如果全局确定性标志已开启，则返回 True。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.is_deterministic_algorithms_warn_only_enabled"><a class="reference internal" href="generated/torch.is_deterministic_algorithms_warn_only_enabled.html#torch.is_deterministic_algorithms_warn_only_enabled" title="torch.is_deterministic_algorithms_warn_only_enabled"><code class="xref py py-obj docutils literal "><span class="pre">is_deterministic_algorithms_warn_only_enabled</span></code></a></p></td>
<td><p>如果全局确定性标志设置为仅警告，则返回 True。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.set_deterministic_debug_mode"><a class="reference internal" href="generated/torch.set_deterministic_debug_mode.html#torch.set_deterministic_debug_mode" title="torch.set_deterministic_debug_mode"><code class="xref py py-obj docutils literal "><span class="pre">set_deterministic_debug_mode</span></code></a></p></td>
<td><p>设置确定性操作的调试模式。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.get_deterministic_debug_mode"><a class="reference internal" href="generated/torch.get_deterministic_debug_mode.html#torch.get_deterministic_debug_mode" title="torch.get_deterministic_debug_mode"><code class="xref py py-obj docutils literal "><span class="pre">get_deterministic_debug_mode</span></code></a></p></td>
<td><p>返回确定性操作的调试模式当前值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.set_float32_matmul_precision"><a class="reference internal" href="generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision" title="torch.set_float32_matmul_precision"><code class="xref py py-obj docutils literal "><span class="pre">set_float32_matmul_precision</span></code></a></p></td>
<td><p>设置 float32 矩阵乘法的内部精度。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.get_float32_matmul_precision"><a class="reference internal" href="generated/torch.get_float32_matmul_precision.html#torch.get_float32_matmul_precision" title="torch.get_float32_matmul_precision"><code class="xref py py-obj docutils literal "><span class="pre">get_float32_matmul_precision</span></code></a></p></td>
<td><p>返回 float32 矩阵乘法精度的当前值。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.set_warn_always"><a class="reference internal" href="generated/torch.set_warn_always.html#torch.set_warn_always" title="torch.set_warn_always"><code class="xref py py-obj docutils literal "><span class="pre">set_warn_always</span></code></a></p></td>
<td><p>当此标志为 False（默认）时，一些 PyTorch 警告可能只在每个进程中出现一次。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.get_device_module"><a class="reference internal" href="generated/torch.get_device_module.html#torch.get_device_module" title="torch.get_device_module"><code class="xref py py-obj docutils literal "><span class="pre">get_device_module</span></code></a></p></td>
<td><p>返回与给定设备关联的模块（例如，torch.device('cuda')，"mtia:0"，"xpu"，...）。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.is_warn_always_enabled"><a class="reference internal" href="generated/torch.is_warn_always_enabled.html#torch.is_warn_always_enabled" title="torch.is_warn_always_enabled"><code class="xref py py-obj docutils literal "><span class="pre">is_warn_always_enabled</span></code></a></p></td>
<td><p>如果全局 warn_always 标志已开启，则返回 True。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.vmap"><a class="reference internal" href="generated/torch.vmap.html#torch.vmap" title="torch.vmap"><code class="xref py py-obj docutils literal "><span class="pre">vmap</span></code></a></p></td>
<td><p>vmap 是向量化映射； <code class="docutils literal "><span class="pre">vmap(func)</span></code> 返回一个对输入的某个维度应用 <code class="docutils literal "><span class="pre">func</span></code> 的新函数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch._assert"><a class="reference internal" href="generated/torch._assert.html#torch._assert" title="torch._assert"><code class="xref py py-obj docutils literal "><span class="pre">_assert</span></code></a></p></td>
<td><p>Python 的 assert 的包装器，可进行符号跟踪。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="symbolic-numbers">
<h2>符号数 ¶</h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.SymInt">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">SymInt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">node</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#SymInt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/__init__.py#L412"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymInt" title="Permalink to this definition">¶</a></dt>
<dd><p>类似于 int（包括魔法方法），但将所有操作重定向到包装的节点。这特别用于在符号形状工作流程中符号化记录操作。</p>
<dl class="py method">
<dt class="sig sig-object py" id="torch.SymInt.as_integer_ratio">
<span class="sig-name descname"><span class="pre">as_integer_ratio</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#SymInt.as_integer_ratio"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/__init__.py#L594"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymInt.as_integer_ratio" title="Permalink to this definition">¶</a></dt>
<dd><p>将此整数表示为精确的整数比</p>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[torch.SymInt, int]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.SymFloat">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch.</span></span><span class="sig-name descname"><span class="pre">SymFloat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">node</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#SymFloat"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/__init__.py#L609"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymFloat" title="Permalink to this definition">¶</a></dt>
<dd><p>类似于浮点数（包括魔术方法），但将所有操作重定向到包装的节点。这特别用于在符号形状工作流程中记录操作。</p>
<dl class="py method">
<dt class="sig sig-object py" id="torch.SymFloat.as_integer_ratio">
<span class="sig-name descname"><span class="pre">as_integer_ratio</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch.html#SymFloat.as_integer_ratio"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/__init__.py#L706"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.SymFloat.as_integer_ratio" title="Permalink to this definition">¶</a></dt>
<dd><p>将这个浮点数表示为精确的整数比</p>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p>元组[int, int]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.SymFloat.conjugate">
conjugate()[来源][来源] ¶</dt>
<dd><p>返回浮点数的复共轭。</p>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torch.SymFloat" title="torch.SymFloat"><em>SymFloat</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.SymFloat.hex">
hex()[来源][来源] ¶</dt>
<dd><p>返回浮点数的十六进制表示。</p>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torch.SymFloat.is_integer">
is_integer()[来源][来源] ¶</dt>
<dd><p>判断浮点数是否为整数。</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torch.SymBool">
class torch.SymBool(node)[source][source]</dt>
<dd><p>类似于 bool（包括魔术方法），但将所有对包装节点的操作重定向。这特别用于在符号形状工作流程中符号记录操作。</p>
<p>与常规 bool 不同，常规布尔运算将强制执行额外的守卫而不是符号评估。请使用位运算符来处理此情况。</p>
</dd></dl>

<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.sym_float"><a class="reference internal" href="generated/torch.sym_float.html#torch.sym_float" title="torch.sym_float"><code class="xref py py-obj docutils literal "><span class="pre">sym_float</span></code></a></p></td>
<td><p>具有符号整数感知的浮点数转换实用工具。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sym_fresh_size"><a class="reference internal" href="generated/torch.sym_fresh_size.html#torch.sym_fresh_size" title="torch.sym_fresh_size"><code class="xref py py-obj docutils literal "><span class="pre">sym_fresh_size</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.sym_int"><a class="reference internal" href="generated/torch.sym_int.html#torch.sym_int" title="torch.sym_int"><code class="xref py py-obj docutils literal "><span class="pre">sym_int</span></code></a></p></td>
<td><p>具有符号整数感知的整数转换实用工具。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sym_max"><a class="reference internal" href="generated/torch.sym_max.html#torch.sym_max" title="torch.sym_max"><code class="xref py py-obj docutils literal "><span class="pre">sym_max</span></code></a></p></td>
<td><p>具有符号整数感知的 max 函数，避免在 a &lt; b 时分支。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.sym_min"><a class="reference internal" href="generated/torch.sym_min.html#torch.sym_min" title="torch.sym_min"><code class="xref py py-obj docutils literal "><span class="pre">sym_min</span></code></a></p></td>
<td><p>具有符号整数感知的 min()函数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sym_not"><a class="reference internal" href="generated/torch.sym_not.html#torch.sym_not" title="torch.sym_not"><code class="xref py py-obj docutils literal "><span class="pre">sym_not</span></code></a></p></td>
<td><p>基于符号整数感知的逻辑否定实用工具。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.sym_ite"><a class="reference internal" href="generated/torch.sym_ite.html#torch.sym_ite" title="torch.sym_ite"><code class="xref py py-obj docutils literal "><span class="pre">sym_ite</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.sym_sum"><a class="reference internal" href="generated/torch.sym_sum.html#torch.sym_sum" title="torch.sym_sum"><code class="xref py py-obj docutils literal "><span class="pre">sym_sum</span></code></a></p></td>
<td><p>多元加法，对于长列表来说比迭代二进制加法计算更快。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="export-path">
<h2>导出路径 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>此功能为原型，未来可能存在兼容性破坏的更改。</p>
<p>导出生成的/exportdb/index</p>
</div>
</section>
<section id="control-flow">
<h2>控制流 ¶</h2>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>该功能是一个原型，未来可能会有不兼容的更改。</p>
</div>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.cond"><a class="reference internal" href="generated/torch.cond.html#torch.cond" title="torch.cond"><code class="xref py py-obj docutils literal "><span class="pre">cond</span></code></a></p></td>
<td><p>条件性地应用 true_fn 或 false_fn。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="optimizations">
<h2>优化 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.compile"><a class="reference internal" href="generated/torch.compile.html#torch.compile" title="torch.compile"><code class="xref py py-obj docutils literal "><span class="pre">compile</span></code></a></p></td>
<td><p>使用 TorchDynamo 和指定后端优化给定的模型/函数。</p></td>
</tr>
</tbody>
</table>
<p><a class="reference external" href="https://pytorch.org/docs/main/torch.compiler.html">torch.compile 文档</a></p>
</section>
<section id="operator-tags">
<h2>运算符标签 ¶</h2>
<dl class="py class">
<dt class="sig sig-object py" id="torch.Tag">
类 torch.Tag ¶</dt>
<dd><p>成员：</p>
<p>核心</p>
<p>数据依赖输出</p>
<p>动态输出形状</p>
<p>灵活布局</p>
<p>生成</p>
<p>内置视图</p>
<p>可能是别名或修改</p>
<p>需要修正步长顺序</p>
<p>非确定性的位运算</p>
<p>非确定性的随机种子</p>
<p>矢量运算</p>
<p>pt2 兼容标签</p>
<p>查看副本</p>
<dl class="py property">
<dt class="sig sig-object py" id="torch.Tag.name">
属性名称¶</dt>
<dd></dd></dl>

</dd></dl>

<span class="target" id="module-torch.contrib"></span><span class="target" id="module-torch.utils.backcompat"></span><span class="target" id="module-torch.utils.hipify"></span><span class="target" id="module-torch.utils.model_dump"></span><span class="target" id="module-torch.utils.viz"></span><span class="target" id="module-torch.functional"></span><span class="target" id="module-torch.quasirandom"></span><span class="target" id="module-torch.return_types"></span><span class="target" id="module-torch.serialization"></span><span class="target" id="module-torch.signal.windows.windows"></span><span class="target" id="module-torch.sparse.semi_structured"></span><span class="target" id="module-torch.storage"></span><span class="target" id="module-torch.torch_version"></span><span class="target" id="module-torch.types"></span><span class="target" id="module-torch.version"></span></section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        下一页 <img height="16" width="16" class="next-page" src="_static/images/chevron-right-orange.svg"> <img height="16" width="16" class="previous-page" src="_static/images/chevron-right-orange.svg"> 上一页
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>© 版权所有 PyTorch 贡献者。</p>
  </div>
    
      <div>使用 Sphinx 构建，主题由 Read the Docs 提供。</div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch</a><ul>
<li><a class="reference internal" href="#tensors">张量</a><ul>
<li><a class="reference internal" href="#creation-ops">创建操作</a></li>
<li><a class="reference internal" href="#indexing-slicing-joining-mutating-ops">索引、切片、连接、修改操作</a></li>
</ul>
</li>
<li><a class="reference internal" href="#accelerators">加速器</a></li>
<li><a class="reference internal" href="#generators">生成器</a></li>
<li><a class="reference internal" href="#random-sampling">随机抽样</a><ul>
<li><a class="reference internal" href="#in-place-random-sampling">原地随机抽样</a></li>
<li><a class="reference internal" href="#quasi-random-sampling">准随机抽样</a></li>
</ul>
</li>
<li><a class="reference internal" href="#serialization">序列化</a></li>
<li><a class="reference internal" href="#parallelism">并行</a></li>
<li><a class="reference internal" href="#locally-disabling-gradient-computation">局部禁用梯度计算</a></li>
<li><a class="reference internal" href="#math-operations">数学运算</a><ul>
<li><a class="reference internal" href="#constants">常量</a></li>
<li><a class="reference internal" href="#pointwise-ops">点运算</a></li>
<li><a class="reference internal" href="#reduction-ops">聚合运算</a></li>
<li><a class="reference internal" href="#comparison-ops">比较操作</a></li>
<li><a class="reference internal" href="#spectral-ops">光谱操作</a></li>
<li><a class="reference internal" href="#other-operations">其他操作</a></li>
<li><a class="reference internal" href="#blas-and-lapack-operations">BLAS 和 LAPACK 操作</a></li>
<li><a class="reference internal" href="#foreach-operations">每次操作</a></li>
</ul>
</li>
<li><a class="reference internal" href="#utilities">工具</a></li>
<li><a class="reference internal" href="#symbolic-numbers">符号数</a></li>
<li><a class="reference internal" href="#export-path">导出路径</a></li>
<li><a class="reference internal" href="#control-flow">控制流程</a></li>
<li><a class="reference internal" href="#optimizations">优化</a></li>
<li><a class="reference internal" href="#operator-tags">操作标签</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script="" type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0">


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>文档</h2>
          <p>查看 PyTorch 的全面开发者文档</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">查看文档</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>教程</h2>
          <p>深入了解初学者和高级开发者的教程</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">查看教程</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>资源</h2>
          <p>查找开发资源并获得您的疑问解答</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">查看资源</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">开始使用</a></li>
            <li><a href="https://pytorch.org/features">功能</a></li>
            <li><a href="https://pytorch.org/ecosystem">生态系统</a></li>
            <li><a href="https://pytorch.org/blog/">博客</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">贡献</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">资源</a></li>
            <li><a href="https://pytorch.org/tutorials">教程</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">文档</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">讨论</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub 问题和任务</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">品牌指南</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">保持更新</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">推特</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">领英</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch 播客</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">苹果</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">谷歌</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">亚马逊</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">条款</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">隐私</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© 版权所有 Linux 基金会。PyTorch 基金会是 Linux 基金会的一个项目。有关本网站的使用条款、商标政策以及其他适用于 PyTorch 基金会的政策，请参阅 www.linuxfoundation.org/policies/。PyTorch 基金会支持 PyTorch 开源项目，该项目已被确立为 LF Projects, LLC 的 PyTorch 项目系列。有关适用于 PyTorch 项目系列 LF Projects, LLC 的政策，请参阅 www.lfprojects.org/policies/。</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">为了分析流量并优化您的体验，我们在本网站上提供 cookie。通过点击或导航，您同意允许我们使用 cookie。作为本网站的当前维护者，Facebook 的 cookie 政策适用。了解更多信息，包括可用的控制选项：cookie 政策。</p>
    <img class="close-button" src="_static/images/pytorch-x.svg" width="16" height="16">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>学习</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">开始使用</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">教程</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">学习基础知识</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch 菜谱</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">PyTorch 入门 - YouTube 系列</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>生态系统</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">工具</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">社区</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">论坛</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">开发者资源</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">贡献者奖项 - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">关于 PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">执行火炬</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch 文档</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>文档</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch 领域</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>博客 &amp; 新闻</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch 博客</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">社区博客</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">视频</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">社区故事</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">活动</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">通讯</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>关于</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch 基金会</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">治理委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">云信用计划</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">技术顾问委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">员工</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">联系我们</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

</body></html>