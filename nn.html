<!DOCTYPE html>
<html lang="zh_CN">
<head>
  <meta charset="UTF-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn — PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/nn.html">
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css">
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css">
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css">
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css">
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css">
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css">
    <link rel="index" title="Index" href="genindex.html">
    <link rel="search" title="Search" href="search.html">
    <link rel="next" title="Buffer" href="generated/torch.nn.parameter.Buffer.html">
    <link rel="prev" title="torch.compile" href="generated/torch.compile.html">

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head><body class="pytorch-body"><div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">学习</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class="dropdown-title">开始使用</span>
                  <p>在本地运行 PyTorch 或快速开始使用支持的云平台之一</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">教程</span><p></p>
                  <p>PyTorch 教程中的新内容</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">学习基础知识</span><p></p>
                  <p>熟悉 PyTorch 的概念和模块</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch 食谱</span><p></p>
                  <p>精简版、可直接部署的 PyTorch 代码示例</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">PyTorch 入门 - YouTube 系列</span><p></p>
                  <p>通过我们引人入胜的 YouTube 教程系列掌握 PyTorch 基础知识</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">生态系统</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">工具</span><p></p>
                  <p>了解 PyTorch 生态系统中的工具和框架</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">社区</span>
                  <p>加入 PyTorch 开发者社区，贡献、学习并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">论坛</span>
                  <p>讨论 PyTorch 代码、问题、安装、研究的地方</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">开发者资源</span>
                  <p>查找资源并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">贡献者奖项 - 2024</span><p></p>
                  <p>本届 PyTorch 会议揭晓获奖者</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">关于 PyTorch Edge</span><p></p>
                  <p>为边缘设备构建创新和隐私感知的 AI 体验</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span><p></p>
                  <p>基于移动和边缘设备的端到端推理能力解决方案</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch 文档</span><p></p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">文档</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span><p></p>
                  <p>探索文档以获取全面指导，了解如何使用 PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch 领域</span><p></p>
                  <p>阅读 PyTorch 领域的文档，了解更多关于特定领域库的信息</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">博客与新闻</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch 博客</span><p></p>
                  <p>捕捉最新的技术新闻和事件</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">社区博客</span><p></p>
                  <p>PyTorch 生态系统故事</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">视频</span><p></p>
                  <p>了解最新的 PyTorch 教程、新内容等</p>
                </a><a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">社区故事</span><p></p>
                  <p>学习如何我们的社区使用 PyTorch 解决真实、日常的机器学习问题</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">活动</span><p></p>
                  <p>查找活动、网络研讨会和播客</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">通讯</span><p></p>
                  <p>跟踪最新更新</p>
                </a>
            </div>
          </div></li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">关于</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch 基金会</span><p></p>
                  <p>了解更多关于 PyTorch 基金会的信息</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">管理委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">云信用计划</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">技术顾问委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">员工</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">联系我们</span><p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">成为会员</a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>



   

    

    <div class="table-of-contents-link-wrapper">
      <span>目录</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="https://pytorch.org/docs/versions.html">主程序 (2.7.0+cpu ) ▼</a>
    </div>
    <div id="searchBox">
    <div class="searchbox" id="googleSearchBox">
      <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
      <div class="gcse-search"></div>
    </div>
    <div id="sphinxSearchBox" style="display: none;">
      <div role="search">
        <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
          <input type="text" name="q" placeholder="Search Docs">
          <input type="hidden" name="check_keywords" value="yes">
          <input type="hidden" name="area" value="default">
        </form>
      </div>
    </div>
  </div>
  <form id="searchForm">
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="google" checked="">谷歌搜索</label>
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="sphinx">经典搜索</label>
  </form>

  <script>
     document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
  </script>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/nn.html">您正在查看不稳定开发者预览文档。请点击此处查看最新稳定版本的文档。</a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">社区</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch 治理 | 构建 + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch 贡献指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch 设计哲学</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch 治理 | 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch 治理 | 维护者</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者笔记</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">自动混合精度示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">广播语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU 多线程和 TorchScript 推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA 语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/custom_operators.html">PyTorch 自定义算子页面</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">分布式数据并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">扩展 PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">使用 autograd.Function 扩展 torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">常见问题解答</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/fsdp.html">FSDP 笔记</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/get_start_xpu.html">在 Intel GPU 上入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm)语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">大规模部署功能</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/libtorch_stable_abi.html">LibTorch 稳定 ABI</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">模块</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS 后端</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">多进程最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">数值精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">可重现性</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">序列化语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows 常见问题解答</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语言绑定</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">张量属性</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">张量视图</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="accelerator.html">torch.accelerator</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">理解 CUDA 内存使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">生成快照</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">使用可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">快照 API 参考</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.memory.html">torch.mtia.memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">元设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.html">torch.distributed.tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.fsdp.fully_shard.html">torch.distributed.fsdp.fully_shard</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">复数</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP 通信钩子</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">分布式 RPC 框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1"><a class="reference internal" href="nested.html">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">类型信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">命名张量</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">命名张量操作覆盖率</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_environment_variables.html">火炬环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">库</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">火炬推荐</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch 在 XLA 设备上</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/ao">torchao</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        文档 &gt;</li>

        
      <li>torch.nn</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/nn.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg" width="16" height="16"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">快捷键</div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="module-torch.nn">
<span id="torch-nn"></span><h1>torch.nn ¬</h1>
<span class="target" id="module-torch.nn.modules"></span><p>这些是图的基礎構建塊：</p>
<nav class="contents local" id="id1">
<p class="topic-title">torch.nn</p>
<ul class="simple">
<li><p><a class="reference internal" href="#containers" id="id2">容器</a></p></li>
<li><p><a class="reference internal" href="#convolution-layers" id="id3">卷积层</a></p></li>
<li><p><a class="reference internal" href="#pooling-layers" id="id4">池化层</a></p></li>
<li><p><a class="reference internal" href="#padding-layers" id="id5">填充层</a></p></li>
<li><p><a class="reference internal" href="#non-linear-activations-weighted-sum-nonlinearity" id="id6">非线性激活（加权求和，非线性）</a></p></li>
<li><p><a class="reference internal" href="#non-linear-activations-other" id="id7">非线性激活（其他）</a></p></li>
<li><p><a class="reference internal" href="#normalization-layers" id="id8">归一化层</a></p></li>
<li><p><a class="reference internal" href="#recurrent-layers" id="id9">循环层</a></p></li>
<li><p><a class="reference internal" href="#transformer-layers" id="id10">Transformer 层</a></p></li>
<li><p><a class="reference internal" href="#linear-layers" id="id11">线性层</a></p></li>
<li><p><a class="reference internal" href="#dropout-layers" id="id12">Dropout 层</a></p></li>
<li><p><a class="reference internal" href="#sparse-layers" id="id13">稀疏层</a></p></li>
<li><p><a class="reference internal" href="#distance-functions" id="id14">距离函数</a></p></li>
<li><p><a class="reference internal" href="#loss-functions" id="id15">损失函数</a></p></li>
<li><p><a class="reference internal" href="#vision-layers" id="id16">视觉层</a></p></li>
<li><p><a class="reference internal" href="#shuffle-layers" id="id17">打乱层</a></p></li>
<li><p><a class="reference internal" href="#module-torch.nn.parallel" id="id18">数据并行层（多 GPU，分布式）</a></p></li>
<li><p><a class="reference internal" href="#module-torch.nn.utils" id="id19">工具</a></p></li>
<li><p><a class="reference internal" href="#quantized-functions" id="id20">量化函数</a></p></li>
<li><p><a class="reference internal" href="#lazy-modules-initialization" id="id21">懒加载模块初始化</a></p>
<ul>
<li><p><a class="reference internal" href="#aliases" id="id22">别名</a></p></li>
</ul>
</li>
</ul>
</nav>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.parameter.Buffer"><a class="reference internal" href="generated/torch.nn.parameter.Buffer.html#torch.nn.parameter.Buffer" title="torch.nn.parameter.Buffer"><code class="xref py py-obj docutils literal "><span class="pre">Buffer</span></code></a></p></td>
<td><p>一种不应被视为模型参数的张量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.parameter.Parameter"><a class="reference internal" href="generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter"><code class="xref py py-obj docutils literal "><span class="pre">Parameter</span></code></a></p></td>
<td><p>一种应被视为模块参数的张量。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.parameter.UninitializedParameter"><a class="reference internal" href="generated/torch.nn.parameter.UninitializedParameter.html#torch.nn.parameter.UninitializedParameter" title="torch.nn.parameter.UninitializedParameter"><code class="xref py py-obj docutils literal "><span class="pre">UninitializedParameter</span></code></a></p></td>
<td><p>未初始化的参数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.parameter.UninitializedBuffer"><a class="reference internal" href="generated/torch.nn.parameter.UninitializedBuffer.html#torch.nn.parameter.UninitializedBuffer" title="torch.nn.parameter.UninitializedBuffer"><code class="xref py py-obj docutils literal "><span class="pre">UninitializedBuffer</span></code></a></p></td>
<td><p>未初始化的缓冲区。</p></td>
</tr>
</tbody>
</table>
<section id="containers">
<h2>容器 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.Module"><a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-obj docutils literal "><span class="pre">Module</span></code></a></p></td>
<td><p>所有神经网络模块的基类。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.Sequential"><a class="reference internal" href="generated/torch.nn.Sequential.html#torch.nn.Sequential" title="torch.nn.Sequential"><code class="xref py py-obj docutils literal "><span class="pre">Sequential</span></code></a></p></td>
<td><p>顺序容器。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.ModuleList"><a class="reference internal" href="generated/torch.nn.ModuleList.html#torch.nn.ModuleList" title="torch.nn.ModuleList"><code class="xref py py-obj docutils literal "><span class="pre">ModuleList</span></code></a></p></td>
<td><p>在列表中包含子模块。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.ModuleDict"><a class="reference internal" href="generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict" title="torch.nn.ModuleDict"><code class="xref py py-obj docutils literal "><span class="pre">ModuleDict</span></code></a></p></td>
<td><p>在字典中包含子模块。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.ParameterList"><a class="reference internal" href="generated/torch.nn.ParameterList.html#torch.nn.ParameterList" title="torch.nn.ParameterList"><code class="xref py py-obj docutils literal "><span class="pre">ParameterList</span></code></a></p></td>
<td><p>在列表中包含参数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.ParameterDict"><a class="reference internal" href="generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict" title="torch.nn.ParameterDict"><code class="xref py py-obj docutils literal "><span class="pre">ParameterDict</span></code></a></p></td>
<td><p>在字典中包含参数。</p></td>
</tr>
</tbody>
</table>
<p>模块的全局钩子</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.modules.module.register_module_forward_pre_hook"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_forward_pre_hook.html#torch.nn.modules.module.register_module_forward_pre_hook" title="torch.nn.modules.module.register_module_forward_pre_hook"><code class="xref py py-obj docutils literal "><span class="pre">register_module_forward_pre_hook</span></code></a></p></td>
<td><p>注册对所有模块通用的前向预钩子</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.modules.module.register_module_forward_hook"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_forward_hook.html#torch.nn.modules.module.register_module_forward_hook" title="torch.nn.modules.module.register_module_forward_hook"><code class="xref py py-obj docutils literal "><span class="pre">register_module_forward_hook</span></code></a></p></td>
<td><p>注册对所有模块的全局前向钩子</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.modules.module.register_module_backward_hook"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_backward_hook.html#torch.nn.modules.module.register_module_backward_hook" title="torch.nn.modules.module.register_module_backward_hook"><code class="xref py py-obj docutils literal "><span class="pre">register_module_backward_hook</span></code></a></p></td>
<td><p>注册对所有模块通用的后向钩子</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.modules.module.register_module_full_backward_pre_hook"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_full_backward_pre_hook.html#torch.nn.modules.module.register_module_full_backward_pre_hook" title="torch.nn.modules.module.register_module_full_backward_pre_hook"><code class="xref py py-obj docutils literal "><span class="pre">register_module_full_backward_pre_hook</span></code></a></p></td>
<td><p>注册所有模块通用的反向前钩子。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.modules.module.register_module_full_backward_hook"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_full_backward_hook.html#torch.nn.modules.module.register_module_full_backward_hook" title="torch.nn.modules.module.register_module_full_backward_hook"><code class="xref py py-obj docutils literal "><span class="pre">register_module_full_backward_hook</span></code></a></p></td>
<td><p>注册所有模块通用的反向钩子。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.modules.module.register_module_buffer_registration_hook"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_buffer_registration_hook.html#torch.nn.modules.module.register_module_buffer_registration_hook" title="torch.nn.modules.module.register_module_buffer_registration_hook"><code class="xref py py-obj docutils literal "><span class="pre">register_module_buffer_registration_hook</span></code></a></p></td>
<td><p>注册所有模块通用的缓冲区注册钩子。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.modules.module.register_module_module_registration_hook"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_module_registration_hook.html#torch.nn.modules.module.register_module_module_registration_hook" title="torch.nn.modules.module.register_module_module_registration_hook"><code class="xref py py-obj docutils literal "><span class="pre">register_module_module_registration_hook</span></code></a></p></td>
<td><p>注册所有模块通用的模块注册钩子。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.modules.module.register_module_parameter_registration_hook"><a class="reference internal" href="generated/torch.nn.modules.module.register_module_parameter_registration_hook.html#torch.nn.modules.module.register_module_parameter_registration_hook" title="torch.nn.modules.module.register_module_parameter_registration_hook"><code class="xref py py-obj docutils literal "><span class="pre">register_module_parameter_registration_hook</span></code></a></p></td>
<td><p>注册适用于所有模块的通用参数注册钩子。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="convolution-layers">
<h2>卷积层 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Conv1d.html#torch.nn.Conv1d" title="torch.nn.Conv1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.Conv1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 1D 卷积。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Conv2d.html#torch.nn.Conv2d" title="torch.nn.Conv2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.Conv2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 2D 卷积。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Conv3d.html#torch.nn.Conv3d" title="torch.nn.Conv3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.Conv3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 3D 卷积。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.ConvTranspose1d.html#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ConvTranspose1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入图像应用 1D 转置卷积算子。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ConvTranspose2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入图像应用 2D 转置卷积算子。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.ConvTranspose3d.html#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ConvTranspose3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入图像应用 3D 转置卷积算子。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.LazyConv1d.html#torch.nn.LazyConv1d" title="torch.nn.LazyConv1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LazyConv1d</span></code></a></p></td>
<td><p>一个具有懒加载 <code class="docutils literal "><span class="pre">in_channels</span></code> 参数的 <code class="xref py py-class docutils literal "><span class="pre">torch.nn.Conv1d</span></code> 模块。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LazyConv2d.html#torch.nn.LazyConv2d" title="torch.nn.LazyConv2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LazyConv2d</span></code></a></p></td>
<td><p>一个具有懒加载 <code class="docutils literal "><span class="pre">in_channels</span></code> 参数的 <code class="xref py py-class docutils literal "><span class="pre">torch.nn.Conv2d</span></code> 模块。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.LazyConv3d.html#torch.nn.LazyConv3d" title="torch.nn.LazyConv3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LazyConv3d</span></code></a></p></td>
<td><p>一个具有懒加载 <code class="docutils literal "><span class="pre">in_channels</span></code> 参数的 <code class="xref py py-class docutils literal "><span class="pre">torch.nn.Conv3d</span></code> 模块。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LazyConvTranspose1d.html#torch.nn.LazyConvTranspose1d" title="torch.nn.LazyConvTranspose1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LazyConvTranspose1d</span></code></a></p></td>
<td><p>一个具有懒加载 <code class="docutils literal "><span class="pre">in_channels</span></code> 参数的 <code class="xref py py-class docutils literal "><span class="pre">torch.nn.ConvTranspose1d</span></code> 模块。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.LazyConvTranspose2d.html#torch.nn.LazyConvTranspose2d" title="torch.nn.LazyConvTranspose2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LazyConvTranspose2d</span></code></a></p></td>
<td><p>一个具有懒加载 <code class="docutils literal "><span class="pre">in_channels</span></code> 参数的 <code class="xref py py-class docutils literal "><span class="pre">torch.nn.ConvTranspose2d</span></code> 模块。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LazyConvTranspose3d.html#torch.nn.LazyConvTranspose3d" title="torch.nn.LazyConvTranspose3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LazyConvTranspose3d</span></code></a></p></td>
<td><p>具有懒加载参数的模块。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Unfold.html#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-obj docutils literal "><span class="pre">nn.Unfold</span></code></a></p></td>
<td><p>从批处理的输入张量中提取滑动局部块。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Fold.html#torch.nn.Fold" title="torch.nn.Fold"><code class="xref py py-obj docutils literal "><span class="pre">nn.Fold</span></code></a></p></td>
<td><p>将滑动局部块数组合并成一个大张量。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="pooling-layers">
<h2>池化层 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.MaxPool1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 1D 最大池化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.MaxPool2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 2D 最大池化。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.MaxPool3d.html#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.MaxPool3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 3D 最大池化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.MaxUnpool1d.html#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.MaxUnpool1d</span></code></a></p></td>
<td><p>计算部分逆元 <code class="xref py py-class docutils literal "><span class="pre">MaxPool1d</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.MaxUnpool2d</span></code></a></p></td>
<td><p>计算部分逆元 <code class="xref py py-class docutils literal "><span class="pre">MaxPool2d</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.MaxUnpool3d.html#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.MaxUnpool3d</span></code></a></p></td>
<td><p>计算部分逆元 <code class="xref py py-class docutils literal "><span class="pre">MaxPool3d</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.AvgPool1d.html#torch.nn.AvgPool1d" title="torch.nn.AvgPool1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.AvgPool1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 1D 平均池化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d" title="torch.nn.AvgPool2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.AvgPool2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 2D 平均池化。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.AvgPool3d.html#torch.nn.AvgPool3d" title="torch.nn.AvgPool3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.AvgPool3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 3D 平均池化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.FractionalMaxPool2d.html#torch.nn.FractionalMaxPool2d" title="torch.nn.FractionalMaxPool2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.FractionalMaxPool2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 2D 分数最大池化。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.FractionalMaxPool3d.html#torch.nn.FractionalMaxPool3d" title="torch.nn.FractionalMaxPool3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.FractionalMaxPool3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 3D 分数最大池化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LPPool1d.html#torch.nn.LPPool1d" title="torch.nn.LPPool1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LPPool1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 1D 幂平均池化。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.LPPool2d.html#torch.nn.LPPool2d" title="torch.nn.LPPool2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LPPool2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 2D 功率平均池化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LPPool3d.html#torch.nn.LPPool3d" title="torch.nn.LPPool3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LPPool3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 3D 功率平均池化。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool1d.html#torch.nn.AdaptiveMaxPool1d" title="torch.nn.AdaptiveMaxPool1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.AdaptiveMaxPool1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 1D 自适应最大池化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d" title="torch.nn.AdaptiveMaxPool2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.AdaptiveMaxPool2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 2D 自适应最大池化。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.AdaptiveMaxPool3d.html#torch.nn.AdaptiveMaxPool3d" title="torch.nn.AdaptiveMaxPool3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.AdaptiveMaxPool3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 3D 自适应最大池化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool1d.html#torch.nn.AdaptiveAvgPool1d" title="torch.nn.AdaptiveAvgPool1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.AdaptiveAvgPool1d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 1D 自适应平均池化。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d" title="torch.nn.AdaptiveAvgPool2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.AdaptiveAvgPool2d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 2D 自适应平均池化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.AdaptiveAvgPool3d.html#torch.nn.AdaptiveAvgPool3d" title="torch.nn.AdaptiveAvgPool3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.AdaptiveAvgPool3d</span></code></a></p></td>
<td><p>对由多个输入平面组成的输入信号应用 3D 自适应平均池化。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="padding-layers">
<h2>填充层</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.ReflectionPad1d.html#torch.nn.ReflectionPad1d" title="torch.nn.ReflectionPad1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ReflectionPad1d</span></code></a></p></td>
<td><p>使用输入边界的反射对输入张量进行填充。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.ReflectionPad2d.html#torch.nn.ReflectionPad2d" title="torch.nn.ReflectionPad2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ReflectionPad2d</span></code></a></p></td>
<td><p>使用输入边界的反射对输入张量进行填充。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.ReflectionPad3d.html#torch.nn.ReflectionPad3d" title="torch.nn.ReflectionPad3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ReflectionPad3d</span></code></a></p></td>
<td><p>使用输入边界的反射对输入张量进行填充。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.ReplicationPad1d.html#torch.nn.ReplicationPad1d" title="torch.nn.ReplicationPad1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ReplicationPad1d</span></code></a></p></td>
<td><p>使用输入边界的复制对输入张量进行填充。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.ReplicationPad2d.html#torch.nn.ReplicationPad2d" title="torch.nn.ReplicationPad2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ReplicationPad2d</span></code></a></p></td>
<td><p>使用输入边界的复制对输入张量进行填充。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.ReplicationPad3d.html#torch.nn.ReplicationPad3d" title="torch.nn.ReplicationPad3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ReplicationPad3d</span></code></a></p></td>
<td><p>使用输入边界的复制对输入张量进行填充。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.ZeroPad1d.html#torch.nn.ZeroPad1d" title="torch.nn.ZeroPad1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ZeroPad1d</span></code></a></p></td>
<td><p>使用零填充输入张量边界。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.ZeroPad2d.html#torch.nn.ZeroPad2d" title="torch.nn.ZeroPad2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ZeroPad2d</span></code></a></p></td>
<td><p>使用零填充输入张量的边界。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.ZeroPad3d.html#torch.nn.ZeroPad3d" title="torch.nn.ZeroPad3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ZeroPad3d</span></code></a></p></td>
<td><p>使用零填充输入张量的边界。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.ConstantPad1d.html#torch.nn.ConstantPad1d" title="torch.nn.ConstantPad1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ConstantPad1d</span></code></a></p></td>
<td><p>使用常数填充输入张量的边界。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.ConstantPad2d.html#torch.nn.ConstantPad2d" title="torch.nn.ConstantPad2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ConstantPad2d</span></code></a></p></td>
<td><p>使用常数填充输入张量的边界。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.ConstantPad3d.html#torch.nn.ConstantPad3d" title="torch.nn.ConstantPad3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.ConstantPad3d</span></code></a></p></td>
<td><p>使用常数填充输入张量的边界。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.CircularPad1d.html#torch.nn.CircularPad1d" title="torch.nn.CircularPad1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.CircularPad1d</span></code></a></p></td>
<td><p>使用输入边界的循环填充来填充输入张量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.CircularPad2d.html#torch.nn.CircularPad2d" title="torch.nn.CircularPad2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.CircularPad2d</span></code></a></p></td>
<td><p>使用输入边界的循环填充来填充输入张量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.CircularPad3d.html#torch.nn.CircularPad3d" title="torch.nn.CircularPad3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.CircularPad3d</span></code></a></p></td>
<td><p>使用输入边界的循环填充来填充输入张量。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="non-linear-activations-weighted-sum-nonlinearity">
<h2>非线性激活（加权求和，非线性）</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.ELU.html#torch.nn.ELU" title="torch.nn.ELU"><code class="xref py py-obj docutils literal "><span class="pre">nn.ELU</span></code></a></p></td>
<td><p>应用指数线性单元（ELU）函数，逐元素。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Hardshrink.html#torch.nn.Hardshrink" title="torch.nn.Hardshrink"><code class="xref py py-obj docutils literal "><span class="pre">nn.Hardshrink</span></code></a></p></td>
<td><p>应用硬收缩（Hardshrink）函数，逐元素。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Hardsigmoid.html#torch.nn.Hardsigmoid" title="torch.nn.Hardsigmoid"><code class="xref py py-obj docutils literal "><span class="pre">nn.Hardsigmoid</span></code></a></p></td>
<td><p>应用硬 Sigmoid 函数，逐元素。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Hardtanh.html#torch.nn.Hardtanh" title="torch.nn.Hardtanh"><code class="xref py py-obj docutils literal "><span class="pre">nn.Hardtanh</span></code></a></p></td>
<td><p>逐元素应用 HardTanh 函数。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Hardswish.html#torch.nn.Hardswish" title="torch.nn.Hardswish"><code class="xref py py-obj docutils literal "><span class="pre">nn.Hardswish</span></code></a></p></td>
<td><p>逐元素应用 Hardswish 函数。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU" title="torch.nn.LeakyReLU"><code class="xref py py-obj docutils literal "><span class="pre">nn.LeakyReLU</span></code></a></p></td>
<td><p>逐元素应用 LeakyReLU 函数。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.LogSigmoid.html#torch.nn.LogSigmoid" title="torch.nn.LogSigmoid"><code class="xref py py-obj docutils literal "><span class="pre">nn.LogSigmoid</span></code></a></p></td>
<td><p>逐元素应用 Logsigmoid 函数。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention" title="torch.nn.MultiheadAttention"><code class="xref py py-obj docutils literal "><span class="pre">nn.MultiheadAttention</span></code></a></p></td>
<td><p>允许模型联合关注来自不同表示子空间的信息。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.PReLU.html#torch.nn.PReLU" title="torch.nn.PReLU"><code class="xref py py-obj docutils literal "><span class="pre">nn.PReLU</span></code></a></p></td>
<td><p>应用逐元素 PReLU 函数。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.ReLU.html#torch.nn.ReLU" title="torch.nn.ReLU"><code class="xref py py-obj docutils literal "><span class="pre">nn.ReLU</span></code></a></p></td>
<td><p>逐元素应用 ReLU 线性单元函数。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.ReLU6.html#torch.nn.ReLU6" title="torch.nn.ReLU6"><code class="xref py py-obj docutils literal "><span class="pre">nn.ReLU6</span></code></a></p></td>
<td><p>逐元素应用 ReLU6 函数。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.RReLU.html#torch.nn.RReLU" title="torch.nn.RReLU"><code class="xref py py-obj docutils literal "><span class="pre">nn.RReLU</span></code></a></p></td>
<td><p>应用随机化的泄漏整流线性单元函数，逐元素。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.SELU.html#torch.nn.SELU" title="torch.nn.SELU"><code class="xref py py-obj docutils literal "><span class="pre">nn.SELU</span></code></a></p></td>
<td><p>应用 SELU 函数，逐元素。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.CELU.html#torch.nn.CELU" title="torch.nn.CELU"><code class="xref py py-obj docutils literal "><span class="pre">nn.CELU</span></code></a></p></td>
<td><p>应用 CELU 函数，逐元素。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.GELU.html#torch.nn.GELU" title="torch.nn.GELU"><code class="xref py py-obj docutils literal "><span class="pre">nn.GELU</span></code></a></p></td>
<td><p>应用高斯误差线性单元函数。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid" title="torch.nn.Sigmoid"><code class="xref py py-obj docutils literal "><span class="pre">nn.Sigmoid</span></code></a></p></td>
<td><p>逐元素应用 Sigmoid 函数。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.SiLU.html#torch.nn.SiLU" title="torch.nn.SiLU"><code class="xref py py-obj docutils literal "><span class="pre">nn.SiLU</span></code></a></p></td>
<td><p>逐元素应用 Sigmoid 线性单元（SiLU）函数。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Mish.html#torch.nn.Mish" title="torch.nn.Mish"><code class="xref py py-obj docutils literal "><span class="pre">nn.Mish</span></code></a></p></td>
<td><p>逐元素应用 Mish 函数。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Softplus.html#torch.nn.Softplus" title="torch.nn.Softplus"><code class="xref py py-obj docutils literal "><span class="pre">nn.Softplus</span></code></a></p></td>
<td><p>逐元素应用 Softplus 函数。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Softshrink.html#torch.nn.Softshrink" title="torch.nn.Softshrink"><code class="xref py py-obj docutils literal "><span class="pre">nn.Softshrink</span></code></a></p></td>
<td><p>逐元素应用软收缩函数。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Softsign.html#torch.nn.Softsign" title="torch.nn.Softsign"><code class="xref py py-obj docutils literal "><span class="pre">nn.Softsign</span></code></a></p></td>
<td><p>逐元素应用软符号函数。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Tanh.html#torch.nn.Tanh" title="torch.nn.Tanh"><code class="xref py py-obj docutils literal "><span class="pre">nn.Tanh</span></code></a></p></td>
<td><p>逐元素应用双曲正切（Tanh）函数。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Tanhshrink.html#torch.nn.Tanhshrink" title="torch.nn.Tanhshrink"><code class="xref py py-obj docutils literal "><span class="pre">nn.Tanhshrink</span></code></a></p></td>
<td><p>逐元素应用双曲正切收缩函数。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Threshold.html#torch.nn.Threshold" title="torch.nn.Threshold"><code class="xref py py-obj docutils literal "><span class="pre">nn.Threshold</span></code></a></p></td>
<td><p>阈值化输入张量的每个元素。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.GLU.html#torch.nn.GLU" title="torch.nn.GLU"><code class="xref py py-obj docutils literal "><span class="pre">nn.GLU</span></code></a></p></td>
<td><p>应用门控线性单元函数。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="non-linear-activations-other">
<h2>非线性激活（其他）¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Softmin.html#torch.nn.Softmin" title="torch.nn.Softmin"><code class="xref py py-obj docutils literal "><span class="pre">nn.Softmin</span></code></a></p></td>
<td><p>将 Softmin 函数应用于 n 维输入张量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Softmax.html#torch.nn.Softmax" title="torch.nn.Softmax"><code class="xref py py-obj docutils literal "><span class="pre">nn.Softmax</span></code></a></p></td>
<td><p>将 Softmax 函数应用于 n 维输入张量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Softmax2d.html#torch.nn.Softmax2d" title="torch.nn.Softmax2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.Softmax2d</span></code></a></p></td>
<td><p>对每个空间位置的特征应用 SoftMax。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax" title="torch.nn.LogSoftmax"><code class="xref py py-obj docutils literal "><span class="pre">nn.LogSoftmax</span></code></a></p></td>
<td><p>将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mtext>Softmax</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log(\text{Softmax}(x))</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord text"><span class="mord">Softmax</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span> 函数应用于 n 维输入张量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.AdaptiveLogSoftmaxWithLoss.html#torch.nn.AdaptiveLogSoftmaxWithLoss" title="torch.nn.AdaptiveLogSoftmaxWithLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.AdaptiveLogSoftmaxWithLoss</span></code></a></p></td>
<td><p>高效的 softmax 近似。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="normalization-layers">
<h2>标准化层</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d" title="torch.nn.BatchNorm1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.BatchNorm1d</span></code></a></p></td>
<td><p>对 2D 或 3D 输入应用批量归一化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.BatchNorm2d</span></code></a></p></td>
<td><p>对 4D 输入应用批量归一化。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d" title="torch.nn.BatchNorm3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.BatchNorm3d</span></code></a></p></td>
<td><p>对 5D 输入应用批量归一化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LazyBatchNorm1d.html#torch.nn.LazyBatchNorm1d" title="torch.nn.LazyBatchNorm1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LazyBatchNorm1d</span></code></a></p></td>
<td><p>带有懒加载初始化的模块。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.LazyBatchNorm2d.html#torch.nn.LazyBatchNorm2d" title="torch.nn.LazyBatchNorm2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LazyBatchNorm2d</span></code></a></p></td>
<td><p>带有懒加载初始化的模块。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LazyBatchNorm3d.html#torch.nn.LazyBatchNorm3d" title="torch.nn.LazyBatchNorm3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LazyBatchNorm3d</span></code></a></p></td>
<td><p>带有懒加载初始化的模块。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm" title="torch.nn.GroupNorm"><code class="xref py py-obj docutils literal "><span class="pre">nn.GroupNorm</span></code></a></p></td>
<td><p>在输入的小批量上应用组归一化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm" title="torch.nn.SyncBatchNorm"><code class="xref py py-obj docutils literal "><span class="pre">nn.SyncBatchNorm</span></code></a></p></td>
<td><p>在 N 维输入上应用批量归一化。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.InstanceNorm1d</span></code></a></p></td>
<td><p>应用实例归一化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.InstanceNorm2d</span></code></a></p></td>
<td><p>应用实例归一化。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.InstanceNorm3d</span></code></a></p></td>
<td><p>应用实例归一化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm1d.html#torch.nn.LazyInstanceNorm1d" title="torch.nn.LazyInstanceNorm1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LazyInstanceNorm1d</span></code></a></p></td>
<td><p>一个具有懒加载 <code class="docutils literal "><span class="pre">num_features</span></code> 参数的 <code class="xref py py-class docutils literal "><span class="pre">torch.nn.InstanceNorm1d</span></code> 模块。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm2d.html#torch.nn.LazyInstanceNorm2d" title="torch.nn.LazyInstanceNorm2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LazyInstanceNorm2d</span></code></a></p></td>
<td><p>一个具有懒加载 <code class="docutils literal "><span class="pre">num_features</span></code> 参数的 <code class="xref py py-class docutils literal "><span class="pre">torch.nn.InstanceNorm2d</span></code> 模块。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LazyInstanceNorm3d.html#torch.nn.LazyInstanceNorm3d" title="torch.nn.LazyInstanceNorm3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.LazyInstanceNorm3d</span></code></a></p></td>
<td><p>一个具有懒加载 <code class="docutils literal "><span class="pre">num_features</span></code> 参数的 <code class="xref py py-class docutils literal "><span class="pre">torch.nn.InstanceNorm3d</span></code> 模块。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-obj docutils literal "><span class="pre">nn.LayerNorm</span></code></a></p></td>
<td><p>在输入的小批量上应用层归一化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LocalResponseNorm.html#torch.nn.LocalResponseNorm" title="torch.nn.LocalResponseNorm"><code class="xref py py-obj docutils literal "><span class="pre">nn.LocalResponseNorm</span></code></a></p></td>
<td><p>对输入信号应用本地响应归一化。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm" title="torch.nn.RMSNorm"><code class="xref py py-obj docutils literal "><span class="pre">nn.RMSNorm</span></code></a></p></td>
<td><p>对输入的小批量应用均方根层归一化。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="recurrent-layers">
<h2>循环层</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.RNNBase.html#torch.nn.RNNBase" title="torch.nn.RNNBase"><code class="xref py py-obj docutils literal "><span class="pre">nn.RNNBase</span></code></a></p></td>
<td><p>RNN 模块（RNN、LSTM、GRU）的基类。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.RNN.html#torch.nn.RNN" title="torch.nn.RNN"><code class="xref py py-obj docutils literal "><span class="pre">nn.RNN</span></code></a></p></td>
<td><p>将多层 Elman RNN 应用于输入序列，并使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>tanh</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\tanh</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mop">tanh</span></span></span></span> 或 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ReLU</mtext></mrow><annotation encoding="application/x-tex">\text{ReLU}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.68333em;vertical-align:0em;" class="strut"></span><span class="mord text"><span class="mord">ReLU</span></span></span></span></span> 非线性。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.LSTM.html#torch.nn.LSTM" title="torch.nn.LSTM"><code class="xref py py-obj docutils literal "><span class="pre">nn.LSTM</span></code></a></p></td>
<td><p>将多层长短期记忆（LSTM）RNN 应用于输入序列。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.GRU.html#torch.nn.GRU" title="torch.nn.GRU"><code class="xref py py-obj docutils literal "><span class="pre">nn.GRU</span></code></a></p></td>
<td><p>将多层门控循环单元（GRU）RNN 应用于输入序列。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.RNNCell.html#torch.nn.RNNCell" title="torch.nn.RNNCell"><code class="xref py py-obj docutils literal "><span class="pre">nn.RNNCell</span></code></a></p></td>
<td><p>带有 tanh 或 ReLU 非线性函数的 Elman RNN 单元。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell" title="torch.nn.LSTMCell"><code class="xref py py-obj docutils literal "><span class="pre">nn.LSTMCell</span></code></a></p></td>
<td><p>长短期记忆（LSTM）单元。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.GRUCell.html#torch.nn.GRUCell" title="torch.nn.GRUCell"><code class="xref py py-obj docutils literal "><span class="pre">nn.GRUCell</span></code></a></p></td>
<td><p>门控循环单元（GRU）单元。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="transformer-layers">
<h2>变换层 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Transformer.html#torch.nn.Transformer" title="torch.nn.Transformer"><code class="xref py py-obj docutils literal "><span class="pre">nn.Transformer</span></code></a></p></td>
<td><p>变换模型。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder" title="torch.nn.TransformerEncoder"><code class="xref py py-obj docutils literal "><span class="pre">nn.TransformerEncoder</span></code></a></p></td>
<td><p>TransformerEncoder 是由 N 个编码器层堆叠而成。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder" title="torch.nn.TransformerDecoder"><code class="xref py py-obj docutils literal "><span class="pre">nn.TransformerDecoder</span></code></a></p></td>
<td><p>TransformerDecoder 是由 N 个解码器层堆叠而成。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer" title="torch.nn.TransformerEncoderLayer"><code class="xref py py-obj docutils literal "><span class="pre">nn.TransformerEncoderLayer</span></code></a></p></td>
<td><p>TransformerEncoderLayer 由自注意力机制和前馈网络组成。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer" title="torch.nn.TransformerDecoderLayer"><code class="xref py py-obj docutils literal "><span class="pre">nn.TransformerDecoderLayer</span></code></a></p></td>
<td><p>TransformerDecoderLayer 由自注意力机制、多头注意力机制和前馈网络组成。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="linear-layers">
<h2>线性层</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Identity.html#torch.nn.Identity" title="torch.nn.Identity"><code class="xref py py-obj docutils literal "><span class="pre">nn.Identity</span></code></a></p></td>
<td><p>一个对参数不敏感的占位符身份运算符。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear"><code class="xref py py-obj docutils literal "><span class="pre">nn.Linear</span></code></a></p></td>
<td><p>对传入数据进行仿射线性变换： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = xA^T + b</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.924661em;vertical-align:-0.08333em;" class="strut"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height:0.8413309999999999em;" class="vlist"><span style="top:-3.063em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span style="margin-right:0.13889em;" class="mord mathnormal mtight">T</span></span></span></span></span></span></span></span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">+</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">b</span></span></span></span> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Bilinear.html#torch.nn.Bilinear" title="torch.nn.Bilinear"><code class="xref py py-obj docutils literal "><span class="pre">nn.Bilinear</span></code></a></p></td>
<td><p>对传入数据进行双线性变换： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msubsup><mi>x</mi><mn>1</mn><mi>T</mi></msubsup><mi>A</mi><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = x_1^T A x_2 + b</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">=</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1.0894389999999998em;vertical-align:-0.24810799999999997em;" class="strut"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.8413309999999999em;" class="vlist"><span style="top:-2.4518920000000004em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span style="margin-right:0.13889em;" class="mord mathnormal mtight">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.24810799999999997em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.30110799999999993em;" class="vlist"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.15em;" class="vlist"><span></span></span></span></span></span></span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">+</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.69444em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">b</span></span></span></span> 。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear" title="torch.nn.LazyLinear"><code class="xref py py-obj docutils literal "><span class="pre">nn.LazyLinear</span></code></a></p></td>
<td><p>一个自动推断 in_features 的模块。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="dropout-layers">
<h2>Dropout 层 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Dropout.html#torch.nn.Dropout" title="torch.nn.Dropout"><code class="xref py py-obj docutils literal "><span class="pre">nn.Dropout</span></code></a></p></td>
<td><p>在训练过程中，以概率 <code class="xref py py-attr docutils literal "><span class="pre">p</span></code> 随机将输入张量的一些元素置为零。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Dropout1d.html#torch.nn.Dropout1d" title="torch.nn.Dropout1d"><code class="xref py py-obj docutils literal "><span class="pre">nn.Dropout1d</span></code></a></p></td>
<td><p>随机将整个通道置为零。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d" title="torch.nn.Dropout2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.Dropout2d</span></code></a></p></td>
<td><p>随机将整个通道置零</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Dropout3d.html#torch.nn.Dropout3d" title="torch.nn.Dropout3d"><code class="xref py py-obj docutils literal "><span class="pre">nn.Dropout3d</span></code></a></p></td>
<td><p>随机将整个通道置零</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.AlphaDropout.html#torch.nn.AlphaDropout" title="torch.nn.AlphaDropout"><code class="xref py py-obj docutils literal "><span class="pre">nn.AlphaDropout</span></code></a></p></td>
<td><p>在输入上应用 Alpha Dropout</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.FeatureAlphaDropout.html#torch.nn.FeatureAlphaDropout" title="torch.nn.FeatureAlphaDropout"><code class="xref py py-obj docutils literal "><span class="pre">nn.FeatureAlphaDropout</span></code></a></p></td>
<td><p>随机屏蔽整个通道</p></td>
</tr>
</tbody>
</table>
</section>
<section id="sparse-layers">
<h2>稀疏层 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Embedding.html#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-obj docutils literal "><span class="pre">nn.Embedding</span></code></a></p></td>
<td><p>一个简单的查找表，存储固定字典和大小的嵌入。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code class="xref py py-obj docutils literal "><span class="pre">nn.EmbeddingBag</span></code></a></p></td>
<td><p>计算嵌入“包”的总和或平均值，而不实例化中间嵌入。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="distance-functions">
<h2>距离函数 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.CosineSimilarity.html#torch.nn.CosineSimilarity" title="torch.nn.CosineSimilarity"><code class="xref py py-obj docutils literal "><span class="pre">nn.CosineSimilarity</span></code></a></p></td>
<td><p>在 dim 维度上计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">x_1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.58056em;vertical-align:-0.15em;" class="strut"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.30110799999999993em;" class="vlist"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">x_2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.58056em;vertical-align:-0.15em;" class="strut"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.30110799999999993em;" class="vlist"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> 之间的余弦相似度。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance" title="torch.nn.PairwiseDistance"><code class="xref py py-obj docutils literal "><span class="pre">nn.PairwiseDistance</span></code></a></p></td>
<td><p>计算输入向量的成对距离，或输入矩阵列之间的距离。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="loss-functions">
<h2>损失函数</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.L1Loss.html#torch.nn.L1Loss" title="torch.nn.L1Loss"><code class="xref py py-obj docutils literal "><span class="pre">nn.L1Loss</span></code></a></p></td>
<td><p>创建一个标准，用于衡量输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.43056em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">x</span></span></span></span> 中的每个元素与目标 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span></span></span></span> 之间的平均绝对误差（MAE）。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.MSELoss.html#torch.nn.MSELoss" title="torch.nn.MSELoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.MSELoss</span></code></a></p></td>
<td><p>创建一个标准，用于衡量输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.43056em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">x</span></span></span></span> 和目标 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span></span></span></span> 之间每个元素的均方误差（平方 L2 范数）。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" title="torch.nn.CrossEntropyLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.CrossEntropyLoss</span></code></a></p></td>
<td><p>此标准计算输入 logits 和目标之间的交叉熵损失。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.CTCLoss.html#torch.nn.CTCLoss" title="torch.nn.CTCLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.CTCLoss</span></code></a></p></td>
<td><p>连接主义时序分类损失。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.NLLLoss</span></code></a></p></td>
<td><p>负对数似然损失。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.PoissonNLLLoss.html#torch.nn.PoissonNLLLoss" title="torch.nn.PoissonNLLLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.PoissonNLLLoss</span></code></a></p></td>
<td><p>目标泊松分布的负对数似然损失。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.GaussianNLLLoss.html#torch.nn.GaussianNLLLoss" title="torch.nn.GaussianNLLLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.GaussianNLLLoss</span></code></a></p></td>
<td><p>高斯负对数似然损失。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss" title="torch.nn.KLDivLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.KLDivLoss</span></code></a></p></td>
<td><p>库尔巴克-莱布勒散度损失。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.BCELoss.html#torch.nn.BCELoss" title="torch.nn.BCELoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.BCELoss</span></code></a></p></td>
<td><p>创建一个测量目标与输入概率之间二元交叉熵的准则。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss" title="torch.nn.BCEWithLogitsLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.BCEWithLogitsLoss</span></code></a></p></td>
<td><p>该损失将 Sigmoid 层和 BCELoss 合并为一个单一类。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.MarginRankingLoss.html#torch.nn.MarginRankingLoss" title="torch.nn.MarginRankingLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.MarginRankingLoss</span></code></a></p></td>
<td><p>创建一个准则，用于衡量输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">x1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.64444em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">x</span><span class="mord">1</span></span></span></span> 、 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">x2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.64444em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">x</span><span class="mord">2</span></span></span></span> （两个 1D mini-batch 或 0D 张量）和标签 1D mini-batch 或 0D 张量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span></span></span></span> （包含 1 或-1）给出的损失。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.HingeEmbeddingLoss.html#torch.nn.HingeEmbeddingLoss" title="torch.nn.HingeEmbeddingLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.HingeEmbeddingLoss</span></code></a></p></td>
<td><p>根据输入张量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.43056em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">x</span></span></span></span> 和标签张量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span></span></span></span> （包含 1 或-1）计算损失。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.MultiLabelMarginLoss.html#torch.nn.MultiLabelMarginLoss" title="torch.nn.MultiLabelMarginLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.MultiLabelMarginLoss</span></code></a></p></td>
<td><p>创建一个准则，用于优化输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.43056em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">x</span></span></span></span> （一个 2D mini-batch 张量）和输出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span></span></span></span> （是一个 2D 张量，包含目标类索引）之间的多类多分类 hinge 损失（基于边界的损失）。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss" title="torch.nn.HuberLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.HuberLoss</span></code></a></p></td>
<td><p>创建一个标准，当元素-wise 绝对误差低于 delta 时使用平方项，否则使用 delta 缩放的 L1 项。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss" title="torch.nn.SmoothL1Loss"><code class="xref py py-obj docutils literal "><span class="pre">nn.SmoothL1Loss</span></code></a></p></td>
<td><p>创建一个标准，当元素-wise 绝对误差低于 beta 时使用平方项，否则使用 L1 项。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.SoftMarginLoss.html#torch.nn.SoftMarginLoss" title="torch.nn.SoftMarginLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.SoftMarginLoss</span></code></a></p></td>
<td><p>创建一个标准，优化输入张量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.43056em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">x</span></span></span></span> 和目标张量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span></span></span></span> （包含 1 或-1）之间的二分类逻辑损失。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.MultiLabelSoftMarginLoss.html#torch.nn.MultiLabelSoftMarginLoss" title="torch.nn.MultiLabelSoftMarginLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.MultiLabelSoftMarginLoss</span></code></a></p></td>
<td><p>创建一个标准，基于最大熵，在输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.43056em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">x</span></span></span></span> 和目标 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span></span></span></span> （大小为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mopen">(</span><span style="margin-right:0.10903em;" class="mord mathnormal">N</span><span class="mpunct">,</span><span style="margin-right:0.16666666666666666em;" class="mspace"></span><span style="margin-right:0.07153em;" class="mord mathnormal">C</span><span class="mclose">)</span></span></span></span> ）之间优化多标签一对多损失。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.CosineEmbeddingLoss.html#torch.nn.CosineEmbeddingLoss" title="torch.nn.CosineEmbeddingLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.CosineEmbeddingLoss</span></code></a></p></td>
<td><p>创建一个标准，用于衡量输入张量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">x_1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.58056em;vertical-align:-0.15em;" class="strut"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.30110799999999993em;" class="vlist"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> 、 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">x_2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.58056em;vertical-align:-0.15em;" class="strut"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height:0.30110799999999993em;" class="vlist"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span style="height:2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height:0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> 和标签张量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span></span></span></span> （值为 1 或-1）的损失。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.MultiMarginLoss.html#torch.nn.MultiMarginLoss" title="torch.nn.MultiMarginLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.MultiMarginLoss</span></code></a></p></td>
<td><p>创建一个标准，用于优化输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.43056em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">x</span></span></span></span> （一个 2D 小批量张量）和输出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span></span></span></span> （是一个 1D 的目标类别索引张量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>≤</mo><mi>y</mi><mo>≤</mo><mtext>x.size</mtext><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0 \leq y \leq \text{x.size}(1)-1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.78041em;vertical-align:-0.13597em;" class="strut"></span><span class="mord">0</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">≤</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:0.8304100000000001em;vertical-align:-0.19444em;" class="strut"></span><span style="margin-right:0.03588em;" class="mord mathnormal">y</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span><span class="mrel">≤</span><span style="margin-right:0.2777777777777778em;" class="mspace"></span></span><span class="base"><span style="height:1em;vertical-align:-0.25em;" class="strut"></span><span class="mord text"><span class="mord">x.size</span></span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span><span class="mbin">−</span><span style="margin-right:0.2222222222222222em;" class="mspace"></span></span><span class="base"><span style="height:0.64444em;vertical-align:0em;" class="strut"></span><span class="mord">1</span></span></span></span> ）之间的多类分类 Hinge 损失（基于边界的损失）。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.TripletMarginLoss.html#torch.nn.TripletMarginLoss" title="torch.nn.TripletMarginLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.TripletMarginLoss</span></code></a></p></td>
<td><p>创建一个标准，用于衡量给定输入张量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">x1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.64444em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">x</span><span class="mord">1</span></span></span></span> 、 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">x2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.64444em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">x</span><span class="mord">2</span></span></span></span> 、 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">x3</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.64444em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">x</span><span class="mord">3</span></span></span></span> （分别代表锚、正例和负例）以及一个大于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.64444em;vertical-align:0em;" class="strut"></span><span class="mord">0</span></span></span></span> 的边界的三元组损失。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss" title="torch.nn.TripletMarginWithDistanceLoss"><code class="xref py py-obj docutils literal "><span class="pre">nn.TripletMarginWithDistanceLoss</span></code></a></p></td>
<td><p>创建一个标准，用于衡量给定输入张量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.43056em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">a</span></span></span></span> 、 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.625em;vertical-align:-0.19444em;" class="strut"></span><span class="mord mathnormal">p</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span style="height:0.43056em;vertical-align:0em;" class="strut"></span><span class="mord mathnormal">n</span></span></span></span> （分别代表锚、正例和负例）的三元组损失，以及一个非负实值函数（“距离函数”），用于计算锚和正例之间的“正距离”以及锚和负例之间的“负距离”。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="vision-layers">
<h2>视觉层 ¶</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle" title="torch.nn.PixelShuffle"><code class="xref py py-obj docutils literal "><span class="pre">nn.PixelShuffle</span></code></a></p></td>
<td><p>根据上采样因子对张量中的元素进行重新排列。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.PixelUnshuffle.html#torch.nn.PixelUnshuffle" title="torch.nn.PixelUnshuffle"><code class="xref py py-obj docutils literal "><span class="pre">nn.PixelUnshuffle</span></code></a></p></td>
<td><p>反向执行 PixelShuffle 操作。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Upsample.html#torch.nn.Upsample" title="torch.nn.Upsample"><code class="xref py py-obj docutils literal "><span class="pre">nn.Upsample</span></code></a></p></td>
<td><p>对给定的多通道 1D（时间），2D（空间）或 3D（体积）数据进行上采样。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.UpsamplingNearest2d.html#torch.nn.UpsamplingNearest2d" title="torch.nn.UpsamplingNearest2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.UpsamplingNearest2d</span></code></a></p></td>
<td><p>对由多个输入通道组成的输入信号应用 2D 最近邻上采样。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.UpsamplingBilinear2d.html#torch.nn.UpsamplingBilinear2d" title="torch.nn.UpsamplingBilinear2d"><code class="xref py py-obj docutils literal "><span class="pre">nn.UpsamplingBilinear2d</span></code></a></p></td>
<td><p>对由多个输入通道组成的输入信号应用 2D 双线性上采样。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="shuffle-layers">
<h2>混洗层</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.ChannelShuffle.html#torch.nn.ChannelShuffle" title="torch.nn.ChannelShuffle"><code class="xref py py-obj docutils literal "><span class="pre">nn.ChannelShuffle</span></code></a></p></td>
<td><p>将张量中的通道分割并重新排列。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="module-torch.nn.parallel">
<span id="dataparallel-layers-multi-gpu-distributed"></span><h2>数据并行层（多 GPU，分布式）</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.DataParallel.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-obj docutils literal "><span class="pre">nn.DataParallel</span></code></a></p></td>
<td><p>在模块级别实现数据并行性。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code class="xref py py-obj docutils literal "><span class="pre">nn.parallel.DistributedDataParallel</span></code></a></p></td>
<td><p>基于模块级别实现基于 <code class="docutils literal "><span class="pre">torch.distributed</span></code> 的分布式数据并行性。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="module-torch.nn.utils">
<span id="utilities"></span><h2>工具程序 §</h2>
<p>来自 <code class="docutils literal "><span class="pre">torch.nn.utils</span></code> 模块：</p>
<p>参数梯度裁剪的实用函数。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.utils.clip_grad_norm_"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_" title="torch.nn.utils.clip_grad_norm_"><code class="xref py py-obj docutils literal "><span class="pre">clip_grad_norm_</span></code></a></p></td>
<td><p>裁剪参数迭代器的梯度范数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.utils.clip_grad_norm"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_norm.html#torch.nn.utils.clip_grad_norm" title="torch.nn.utils.clip_grad_norm"><code class="xref py py-obj docutils literal "><span class="pre">clip_grad_norm</span></code></a></p></td>
<td><p>裁剪参数迭代器的梯度范数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.utils.clip_grad_value_"><a class="reference internal" href="generated/torch.nn.utils.clip_grad_value_.html#torch.nn.utils.clip_grad_value_" title="torch.nn.utils.clip_grad_value_"><code class="xref py py-obj docutils literal "><span class="pre">clip_grad_value_</span></code></a></p></td>
<td><p>在指定值处裁剪参数迭代器的梯度。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.utils.get_total_norm"><a class="reference internal" href="generated/torch.nn.utils.get_total_norm.html#torch.nn.utils.get_total_norm" title="torch.nn.utils.get_total_norm"><code class="xref py py-obj docutils literal "><span class="pre">get_total_norm</span></code></a></p></td>
<td><p>计算张量序列的范数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.utils.clip_grads_with_norm_"><a class="reference internal" href="generated/torch.nn.utils.clip_grads_with_norm_.html#torch.nn.utils.clip_grads_with_norm_" title="torch.nn.utils.clip_grads_with_norm_"><code class="xref py py-obj docutils literal "><span class="pre">clip_grads_with_norm_</span></code></a></p></td>
<td><p>根据预计算的范数和期望的最大范数，缩放参数序列的梯度。</p></td>
</tr>
</tbody>
</table>
<p>提供将模块参数从单个向量到单个向量以及从单个向量到张量序列的展开和还原的实用函数。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.utils.parameters_to_vector"><a class="reference internal" href="generated/torch.nn.utils.parameters_to_vector.html#torch.nn.utils.parameters_to_vector" title="torch.nn.utils.parameters_to_vector"><code class="xref py py-obj docutils literal "><span class="pre">parameters_to_vector</span></code></a></p></td>
<td><p>将参数序列展开成一个单一向量。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.utils.vector_to_parameters"><a class="reference internal" href="generated/torch.nn.utils.vector_to_parameters.html#torch.nn.utils.vector_to_parameters" title="torch.nn.utils.vector_to_parameters"><code class="xref py py-obj docutils literal "><span class="pre">vector_to_parameters</span></code></a></p></td>
<td><p>将向量的切片复制到参数的可迭代对象中。</p></td>
</tr>
</tbody>
</table>
<p>用于将模块与 BatchNorm 模块融合的实用函数。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.utils.fuse_conv_bn_eval"><a class="reference internal" href="generated/torch.nn.utils.fuse_conv_bn_eval.html#torch.nn.utils.fuse_conv_bn_eval" title="torch.nn.utils.fuse_conv_bn_eval"><code class="xref py py-obj docutils literal "><span class="pre">fuse_conv_bn_eval</span></code></a></p></td>
<td><p>将卷积模块和 BatchNorm 模块融合成一个全新的卷积模块。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.utils.fuse_conv_bn_weights"><a class="reference internal" href="generated/torch.nn.utils.fuse_conv_bn_weights.html#torch.nn.utils.fuse_conv_bn_weights" title="torch.nn.utils.fuse_conv_bn_weights"><code class="xref py py-obj docutils literal "><span class="pre">fuse_conv_bn_weights</span></code></a></p></td>
<td><p>将卷积模块参数和 BatchNorm 模块参数融合成新的卷积模块参数。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.utils.fuse_linear_bn_eval"><a class="reference internal" href="generated/torch.nn.utils.fuse_linear_bn_eval.html#torch.nn.utils.fuse_linear_bn_eval" title="torch.nn.utils.fuse_linear_bn_eval"><code class="xref py py-obj docutils literal "><span class="pre">fuse_linear_bn_eval</span></code></a></p></td>
<td><p>将线性模块和 BatchNorm 模块融合成一个全新的线性模块。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.utils.fuse_linear_bn_weights"><a class="reference internal" href="generated/torch.nn.utils.fuse_linear_bn_weights.html#torch.nn.utils.fuse_linear_bn_weights" title="torch.nn.utils.fuse_linear_bn_weights"><code class="xref py py-obj docutils literal "><span class="pre">fuse_linear_bn_weights</span></code></a></p></td>
<td><p>将线性模块参数和 BatchNorm 模块参数融合成新的线性模块参数。</p></td>
</tr>
</tbody>
</table>
<p>用于转换模块参数内存格式的实用函数。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.utils.convert_conv2d_weight_memory_format"><a class="reference internal" href="generated/torch.nn.utils.convert_conv2d_weight_memory_format.html#torch.nn.utils.convert_conv2d_weight_memory_format" title="torch.nn.utils.convert_conv2d_weight_memory_format"><code class="xref py py-obj docutils literal "><span class="pre">convert_conv2d_weight_memory_format</span></code></a></p></td>
<td><p>将 <code class="docutils literal "><span class="pre">nn.Conv2d.weight</span></code> 的 <code class="docutils literal "><span class="pre">memory_format</span></code> 转换为 <code class="docutils literal "><span class="pre">memory_format</span></code> 。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.utils.convert_conv3d_weight_memory_format"><a class="reference internal" href="generated/torch.nn.utils.convert_conv3d_weight_memory_format.html#torch.nn.utils.convert_conv3d_weight_memory_format" title="torch.nn.utils.convert_conv3d_weight_memory_format"><code class="xref py py-obj docutils literal "><span class="pre">convert_conv3d_weight_memory_format</span></code></a></p></td>
<td><p>将 <code class="docutils literal "><span class="pre">memory_format</span></code> 的 <code class="docutils literal "><span class="pre">nn.Conv3d.weight</span></code> 转换为 <code class="docutils literal "><span class="pre">memory_format</span></code> ，转换递归应用于嵌套的 <code class="docutils literal "><span class="pre">nn.Module</span></code> ，包括 <code class="docutils literal "><span class="pre">module</span></code> 。</p></td>
</tr>
</tbody>
</table>
<p>用于应用和移除模块参数权重归一化的实用函数。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.utils.weight_norm"><a class="reference internal" href="generated/torch.nn.utils.weight_norm.html#torch.nn.utils.weight_norm" title="torch.nn.utils.weight_norm"><code class="xref py py-obj docutils literal "><span class="pre">weight_norm</span></code></a></p></td>
<td><p>在给定的模块中应用参数的权重归一化。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.utils.remove_weight_norm"><a class="reference internal" href="generated/torch.nn.utils.remove_weight_norm.html#torch.nn.utils.remove_weight_norm" title="torch.nn.utils.remove_weight_norm"><code class="xref py py-obj docutils literal "><span class="pre">remove_weight_norm</span></code></a></p></td>
<td><p>从模块中移除权重归一化的重新参数化。</p></td>
</tr>
<tr class="row-odd"><td><p></p><p id="torch.nn.utils.spectral_norm"><a class="reference internal" href="generated/torch.nn.utils.spectral_norm.html#torch.nn.utils.spectral_norm" title="torch.nn.utils.spectral_norm"><code class="xref py py-obj docutils literal "><span class="pre">spectral_norm</span></code></a></p></td>
<td><p>将频谱归一化应用于给定模块中的参数。</p></td>
</tr>
<tr class="row-even"><td><p></p><p id="torch.nn.utils.remove_spectral_norm"><a class="reference internal" href="generated/torch.nn.utils.remove_spectral_norm.html#torch.nn.utils.remove_spectral_norm" title="torch.nn.utils.remove_spectral_norm"><code class="xref py py-obj docutils literal "><span class="pre">remove_spectral_norm</span></code></a></p></td>
<td><p>从模块中移除频谱归一化的重新参数化。</p></td>
</tr>
</tbody>
</table>
<p>初始化 Module 参数的实用函数。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p></p><p id="torch.nn.utils.skip_init"><a class="reference internal" href="generated/torch.nn.utils.skip_init.html#torch.nn.utils.skip_init" title="torch.nn.utils.skip_init"><code class="xref py py-obj docutils literal "><span class="pre">skip_init</span></code></a></p></td>
<td><p>给定模块类对象和 args/kwargs，实例化模块而不初始化参数/缓冲区。</p></td>
</tr>
</tbody>
</table>
<p>用于修剪模块参数的实用类和函数。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.BasePruningMethod.html#torch.nn.utils.prune.BasePruningMethod" title="torch.nn.utils.prune.BasePruningMethod"><code class="xref py py-obj docutils literal "><span class="pre">prune.BasePruningMethod</span></code></a></p></td>
<td><p>用于创建新修剪技术的抽象基类。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.PruningContainer.html#torch.nn.utils.prune.PruningContainer" title="torch.nn.utils.prune.PruningContainer"><code class="xref py py-obj docutils literal "><span class="pre">prune.PruningContainer</span></code></a></p></td>
<td><p>包含一系列修剪方法的容器，用于迭代修剪。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.Identity.html#torch.nn.utils.prune.Identity" title="torch.nn.utils.prune.Identity"><code class="xref py py-obj docutils literal "><span class="pre">prune.Identity</span></code></a></p></td>
<td><p>不修剪任何单元的实用修剪方法，但生成由全 1 组成的修剪参数化掩码。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.RandomUnstructured.html#torch.nn.utils.prune.RandomUnstructured" title="torch.nn.utils.prune.RandomUnstructured"><code class="xref py py-obj docutils literal "><span class="pre">prune.RandomUnstructured</span></code></a></p></td>
<td><p>随机剪枝张量中的（当前未剪枝）单元。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.L1Unstructured.html#torch.nn.utils.prune.L1Unstructured" title="torch.nn.utils.prune.L1Unstructured"><code class="xref py py-obj docutils literal "><span class="pre">prune.L1Unstructured</span></code></a></p></td>
<td><p>通过将具有最低 L1 范数的单元置零来剪枝张量中的（当前未剪枝）单元。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.RandomStructured.html#torch.nn.utils.prune.RandomStructured" title="torch.nn.utils.prune.RandomStructured"><code class="xref py py-obj docutils literal "><span class="pre">prune.RandomStructured</span></code></a></p></td>
<td><p>随机剪枝张量中的（当前未剪枝）整个通道。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.LnStructured.html#torch.nn.utils.prune.LnStructured" title="torch.nn.utils.prune.LnStructured"><code class="xref py py-obj docutils literal "><span class="pre">prune.LnStructured</span></code></a></p></td>
<td><p>根据它们的 L <code class="docutils literal "><span class="pre">n</span></code> -范数剪枝张量中的（当前未剪枝）整个通道。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.CustomFromMask.html#torch.nn.utils.prune.CustomFromMask" title="torch.nn.utils.prune.CustomFromMask"><code class="xref py py-obj docutils literal "><span class="pre">prune.CustomFromMask</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.identity.html#torch.nn.utils.prune.identity" title="torch.nn.utils.prune.identity"><code class="xref py py-obj docutils literal "><span class="pre">prune.identity</span></code></a></p></td>
<td><p>无需剪枝任何单元，应用剪枝重参数化。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.random_unstructured.html#torch.nn.utils.prune.random_unstructured" title="torch.nn.utils.prune.random_unstructured"><code class="xref py py-obj docutils literal "><span class="pre">prune.random_unstructured</span></code></a></p></td>
<td><p>通过移除随机（当前未剪枝）单元来剪枝张量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.l1_unstructured.html#torch.nn.utils.prune.l1_unstructured" title="torch.nn.utils.prune.l1_unstructured"><code class="xref py py-obj docutils literal "><span class="pre">prune.l1_unstructured</span></code></a></p></td>
<td><p>通过移除具有最低 L1 范数的单元来剪枝张量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.random_structured.html#torch.nn.utils.prune.random_structured" title="torch.nn.utils.prune.random_structured"><code class="xref py py-obj docutils literal "><span class="pre">prune.random_structured</span></code></a></p></td>
<td><p>沿指定维度移除随机通道来剪枝张量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.ln_structured.html#torch.nn.utils.prune.ln_structured" title="torch.nn.utils.prune.ln_structured"><code class="xref py py-obj docutils literal "><span class="pre">prune.ln_structured</span></code></a></p></td>
<td><p>通过移除指定维度上 L <code class="docutils literal "><span class="pre">n</span></code> -范数最低的通道来修剪张量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.global_unstructured.html#torch.nn.utils.prune.global_unstructured" title="torch.nn.utils.prune.global_unstructured"><code class="xref py py-obj docutils literal "><span class="pre">prune.global_unstructured</span></code></a></p></td>
<td><p>对 <code class="docutils literal "><span class="pre">parameters</span></code> 中所有参数对应的张量进行全局修剪，应用指定的 <code class="docutils literal "><span class="pre">pruning_method</span></code> 。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.custom_from_mask.html#torch.nn.utils.prune.custom_from_mask" title="torch.nn.utils.prune.custom_from_mask"><code class="xref py py-obj docutils literal "><span class="pre">prune.custom_from_mask</span></code></a></p></td>
<td><p>通过应用预计算的掩码 <code class="docutils literal "><span class="pre">mask</span></code> ，修剪 <code class="docutils literal "><span class="pre">module</span></code> 中名为 <code class="docutils literal "><span class="pre">name</span></code> 的参数对应的张量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.remove.html#torch.nn.utils.prune.remove" title="torch.nn.utils.prune.remove"><code class="xref py py-obj docutils literal "><span class="pre">prune.remove</span></code></a></p></td>
<td><p>从模块中移除修剪重参数化，从前向钩子中移除修剪方法。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.prune.is_pruned.html#torch.nn.utils.prune.is_pruned" title="torch.nn.utils.prune.is_pruned"><code class="xref py py-obj docutils literal "><span class="pre">prune.is_pruned</span></code></a></p></td>
<td><p>检查模块是否被剪枝，可以通过查找剪枝前钩子来实现。</p></td>
</tr>
</tbody>
</table>
<p>使用新参数化功能在 <code class="xref py py-func docutils literal "><span class="pre">torch.nn.utils.parameterize.register_parametrization()</span></code> 中实现参数化。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.parametrizations.orthogonal.html#torch.nn.utils.parametrizations.orthogonal" title="torch.nn.utils.parametrizations.orthogonal"><code class="xref py py-obj docutils literal "><span class="pre">parametrizations.orthogonal</span></code></a></p></td>
<td><p>将正交或单位参数化应用于矩阵或矩阵批。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.parametrizations.weight_norm.html#torch.nn.utils.parametrizations.weight_norm" title="torch.nn.utils.parametrizations.weight_norm"><code class="xref py py-obj docutils literal "><span class="pre">parametrizations.weight_norm</span></code></a></p></td>
<td><p>将权重归一化应用于给定模块中的参数。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.parametrizations.spectral_norm.html#torch.nn.utils.parametrizations.spectral_norm" title="torch.nn.utils.parametrizations.spectral_norm"><code class="xref py py-obj docutils literal "><span class="pre">parametrizations.spectral_norm</span></code></a></p></td>
<td><p>将频谱归一化应用于给定模块中的参数。</p></td>
</tr>
</tbody>
</table>
<p>用于在现有模块上参数化张量的实用函数。请注意，这些函数可以用于在给定的从输入空间映射到参数化空间的特定函数的情况下，参数化给定的参数或缓冲区。它们不是将对象转换为参数的参数化。有关如何实现自己的参数化的更多信息，请参阅参数化教程。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.parametrize.register_parametrization.html#torch.nn.utils.parametrize.register_parametrization" title="torch.nn.utils.parametrize.register_parametrization"><code class="xref py py-obj docutils literal "><span class="pre">parametrize.register_parametrization</span></code></a></p></td>
<td><p>将参数化注册到模块中的张量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.parametrize.remove_parametrizations.html#torch.nn.utils.parametrize.remove_parametrizations" title="torch.nn.utils.parametrize.remove_parametrizations"><code class="xref py py-obj docutils literal "><span class="pre">parametrize.remove_parametrizations</span></code></a></p></td>
<td><p>删除模块中张量的参数化。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.parametrize.cached.html#torch.nn.utils.parametrize.cached" title="torch.nn.utils.parametrize.cached"><code class="xref py py-obj docutils literal "><span class="pre">parametrize.cached</span></code></a></p></td>
<td><p>允许在注册的参数化中启用缓存系统的上下文管理器。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.parametrize.is_parametrized.html#torch.nn.utils.parametrize.is_parametrized" title="torch.nn.utils.parametrize.is_parametrized"><code class="xref py py-obj docutils literal "><span class="pre">parametrize.is_parametrized</span></code></a></p></td>
<td><p>判断一个模块是否有参数化。</p></td>
</tr>
</tbody>
</table>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.parametrize.ParametrizationList.html#torch.nn.utils.parametrize.ParametrizationList" title="torch.nn.utils.parametrize.ParametrizationList"><code class="xref py py-obj docutils literal "><span class="pre">parametrize.ParametrizationList</span></code></a></p></td>
<td><p>一个顺序容器，用于持有和管理参数化的原始参数或缓冲区。</p></td>
</tr>
</tbody>
</table>
<p>以无状态方式调用给定模块的实用函数。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.stateless.functional_call.html#torch.nn.utils.stateless.functional_call" title="torch.nn.utils.stateless.functional_call"><code class="xref py py-obj docutils literal "><span class="pre">stateless.functional_call</span></code></a></p></td>
<td><p>通过替换模块参数和缓冲区，对模块进行功能调用。</p></td>
</tr>
</tbody>
</table>
<p>其他模块中的实用函数。</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-obj docutils literal "><span class="pre">nn.utils.rnn.PackedSequence</span></code></a></p></td>
<td><p>存储打包序列的数据和 <code class="xref py py-attr docutils literal "><span class="pre">batch_sizes</span></code> 的列表。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-obj docutils literal "><span class="pre">nn.utils.rnn.pack_padded_sequence</span></code></a></p></td>
<td><p>打包包含可变长度填充序列的张量。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence" title="torch.nn.utils.rnn.pad_packed_sequence"><code class="xref py py-obj docutils literal "><span class="pre">nn.utils.rnn.pad_packed_sequence</span></code></a></p></td>
<td><p>补齐一串长度可变的序列。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.rnn.pad_sequence.html#torch.nn.utils.rnn.pad_sequence" title="torch.nn.utils.rnn.pad_sequence"><code class="xref py py-obj docutils literal "><span class="pre">nn.utils.rnn.pad_sequence</span></code></a></p></td>
<td><p>使用 <code class="xref py py-attr docutils literal "><span class="pre">padding_value</span></code> 补齐长度可变的张量列表。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence" title="torch.nn.utils.rnn.pack_sequence"><code class="xref py py-obj docutils literal "><span class="pre">nn.utils.rnn.pack_sequence</span></code></a></p></td>
<td><p>打包长度可变的张量列表。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.utils.rnn.unpack_sequence.html#torch.nn.utils.rnn.unpack_sequence" title="torch.nn.utils.rnn.unpack_sequence"><code class="xref py py-obj docutils literal "><span class="pre">nn.utils.rnn.unpack_sequence</span></code></a></p></td>
<td><p>将打包的 PackedSequence 解包成长度可变的张量列表。</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.utils.rnn.unpad_sequence.html#torch.nn.utils.rnn.unpad_sequence" title="torch.nn.utils.rnn.unpad_sequence"><code class="xref py py-obj docutils literal "><span class="pre">nn.utils.rnn.unpad_sequence</span></code></a></p></td>
<td><p>将填充的 Tensor 填充成一个变长 Tensor 的列表。</p></td>
</tr>
</tbody>
</table>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.Flatten.html#torch.nn.Flatten" title="torch.nn.Flatten"><code class="xref py py-obj docutils literal "><span class="pre">nn.Flatten</span></code></a></p></td>
<td><p>将连续维度的范围展平成一个张量。</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torch.nn.Unflatten.html#torch.nn.Unflatten" title="torch.nn.Unflatten"><code class="xref py py-obj docutils literal "><span class="pre">nn.Unflatten</span></code></a></p></td>
<td><p>将张量维度展开到期望的形状。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="quantized-functions">
<h2>量化函数 ¶</h2>
<p>量化是指执行计算和存储张量时使用低于浮点精度位宽的技术。PyTorch 支持按张量和对称线性量化。要了解如何在 PyTorch 中使用量化函数，请参阅量化文档。</p>
</section>
<section id="lazy-modules-initialization">
<h2>懒加载模块初始化</h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.modules.lazy.LazyModuleMixin.html#torch.nn.modules.lazy.LazyModuleMixin" title="torch.nn.modules.lazy.LazyModuleMixin"><code class="xref py py-obj docutils literal "><span class="pre">nn.modules.lazy.LazyModuleMixin</span></code></a></p></td>
<td><p>用于懒加载参数的模块混入，也称为“懒加载模块”。</p></td>
</tr>
</tbody>
</table>
<section id="aliases">
<h3>别名</h3>
<p>以下是对 <code class="docutils literal "><span class="pre">torch.nn</span></code> 中对应项的别名：</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torch.nn.RMSNorm.html#torch.nn.RMSNorm" title="torch.nn.modules.normalization.RMSNorm"><code class="xref py py-obj docutils literal "><span class="pre">nn.modules.normalization.RMSNorm</span></code></a></p></td>
<td><p>对输入的小批量应用均方根层归一化。</p></td>
</tr>
</tbody>
</table>
<span class="target" id="module-torch.nn.backends"></span><span class="target" id="module-torch.nn.utils.stateless"></span><span class="target" id="module-torch.nn.backends.thnn"></span><span class="target" id="module-torch.nn.common_types"></span><span class="target" id="module-torch.nn.cpp"></span><span class="target" id="module-torch.nn.functional"></span><span class="target" id="module-torch.nn.grad"></span><span class="target" id="module-torch.nn.init"></span><span class="target" id="module-torch.nn.modules.activation"></span><span class="target" id="module-torch.nn.modules.adaptive"></span><span class="target" id="module-torch.nn.modules.batchnorm"></span><span class="target" id="module-torch.nn.modules.channelshuffle"></span><span class="target" id="module-torch.nn.modules.container"></span><span class="target" id="module-torch.nn.modules.conv"></span><span class="target" id="module-torch.nn.modules.distance"></span><span class="target" id="module-torch.nn.modules.dropout"></span><span class="target" id="module-torch.nn.modules.flatten"></span><span class="target" id="module-torch.nn.modules.fold"></span><span class="target" id="module-torch.nn.modules.instancenorm"></span><span class="target" id="module-torch.nn.modules.lazy"></span><span class="target" id="module-torch.nn.modules.linear"></span><span class="target" id="module-torch.nn.modules.loss"></span><span class="target" id="module-torch.nn.modules.module"></span><span class="target" id="module-torch.nn.modules.normalization"></span><span class="target" id="module-torch.nn.modules.padding"></span><span class="target" id="module-torch.nn.modules.pixelshuffle"></span><span class="target" id="module-torch.nn.modules.pooling"></span><span class="target" id="module-torch.nn.modules.rnn"></span><span class="target" id="module-torch.nn.modules.sparse"></span><span class="target" id="module-torch.nn.modules.transformer"></span><span class="target" id="module-torch.nn.modules.upsampling"></span><span class="target" id="module-torch.nn.modules.utils"></span><span class="target" id="module-torch.nn.parallel.comm"></span><span class="target" id="module-torch.nn.parallel.distributed"></span><span class="target" id="module-torch.nn.parallel.parallel_apply"></span><span class="target" id="module-torch.nn.parallel.replicate"></span><span class="target" id="module-torch.nn.parallel.scatter_gather"></span><span class="target" id="module-torch.nn.parameter"></span><span class="target" id="module-torch.nn.utils.clip_grad"></span><span class="target" id="module-torch.nn.utils.convert_parameters"></span><span class="target" id="module-torch.nn.utils.fusion"></span><span class="target" id="module-torch.nn.utils.init"></span><span class="target" id="module-torch.nn.utils.memory_format"></span><span class="target" id="module-torch.nn.utils.parametrizations"></span><span class="target" id="module-torch.nn.utils.parametrize"></span><span class="target" id="module-torch.nn.utils.prune"></span><span class="target" id="module-torch.nn.utils.rnn"></span></section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        下一个 <img height="16" width="16" class="next-page" src="_static/images/chevron-right-orange.svg"> <img height="16" width="16" class="previous-page" src="_static/images/chevron-right-orange.svg"> 上一个
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>© 版权所有 PyTorch 贡献者。</p>
  </div>
    
      <div>使用 Sphinx 构建，主题由 Read the Docs 提供。</div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.nn</a><ul>
<li><a class="reference internal" href="#containers">容器</a><ul>
</ul>
</li>
<li><a class="reference internal" href="#convolution-layers">卷积层</a></li>
<li><a class="reference internal" href="#pooling-layers">池化层</a></li>
<li><a class="reference internal" href="#padding-layers">填充层</a></li>
<li><a class="reference internal" href="#non-linear-activations-weighted-sum-nonlinearity">非线性激活（加权求和，非线性）</a></li>
<li><a class="reference internal" href="#non-linear-activations-other">非线性激活（其他）</a></li>
<li><a class="reference internal" href="#normalization-layers">正则化层</a></li>
<li><a class="reference internal" href="#recurrent-layers">循环层</a></li>
<li><a class="reference internal" href="#transformer-layers">传输层</a></li>
<li><a class="reference internal" href="#linear-layers">线性层</a></li>
<li><a class="reference internal" href="#dropout-layers">Dropout 层</a></li>
<li><a class="reference internal" href="#sparse-layers">稀疏层</a></li>
<li><a class="reference internal" href="#distance-functions">距离函数</a></li>
<li><a class="reference internal" href="#loss-functions">损失函数</a></li>
<li><a class="reference internal" href="#vision-layers">视觉层</a></li>
<li><a class="reference internal" href="#shuffle-layers">洗牌层</a></li>
<li><a class="reference internal" href="#module-torch.nn.parallel">DataParallel 层（多 GPU，分布式）</a></li>
<li><a class="reference internal" href="#module-torch.nn.utils">公用工具</a><ul>
</ul>
</li>
<li><a class="reference internal" href="#quantized-functions">量化函数</a></li>
<li><a class="reference internal" href="#lazy-modules-initialization">懒加载模块初始化</a><ul>
<li><a class="reference internal" href="#aliases">别名</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script="" type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0">


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>文档</h2>
          <p>PyTorch 开发者文档全面访问</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">查看文档</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>教程</h2>
          <p>获取初学者和高级开发者的深入教程</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">查看教程</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>资源</h2>
          <p>查找开发资源并获得您的疑问解答</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">查看资源</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">开始使用</a></li>
            <li><a href="https://pytorch.org/features">功能</a></li>
            <li><a href="https://pytorch.org/ecosystem">生态系统</a></li>
            <li><a href="https://pytorch.org/blog/">博客</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">贡献</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">资源</a></li>
            <li><a href="https://pytorch.org/tutorials">教程</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">文档</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">讨论</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub 问题和任务</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">品牌指南</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">保持更新</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">推特</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">领英</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch 播客</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">苹果</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">谷歌</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">亚马逊</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">条款</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">隐私</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© 版权所有 Linux 基金会。PyTorch 基金会是 Linux 基金会的一个项目。有关本网站的使用条款、商标政策以及其他适用于 PyTorch 基金会的政策，请参阅 www.linuxfoundation.org/policies/。PyTorch 基金会支持 PyTorch 开源项目，该项目已被确立为 LF Projects, LLC 的 PyTorch 项目系列。有关适用于 PyTorch 项目系列 LF Projects, LLC 的政策，请参阅 www.lfprojects.org/policies/。</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">为分析流量并优化您的体验，我们在本网站上提供 cookies。通过点击或导航，您同意允许我们使用 cookies。作为本站点的当前维护者，Facebook 的 Cookies 政策适用。了解更多信息，包括可用的控制选项：Cookies 政策。</p>
    <img class="close-button" src="_static/images/pytorch-x.svg" width="16" height="16">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>学习</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">开始学习</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">教程</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">学习基础知识</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch 菜谱</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">PyTorch 入门 - YouTube 系列</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>生态系统</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">工具</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">社区</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">论坛</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">开发者资源</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">贡献者奖项 - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">关于 PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch 文档</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>文档</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch 领域</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>博客 &amp; 新闻</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch 博客</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">社区博客</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">视频</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">社区故事</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">活动</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">通讯</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>关于</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch 基金会</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">治理委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">云信用计划</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">技术顾问委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">员工</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">联系我们</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

</body></html>