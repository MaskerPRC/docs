<!DOCTYPE html>
<html lang="zh_CN">
<head>
  <meta charset="UTF-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nested — PyTorch main documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/nested.html">
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css">
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css">
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" type="text/css">
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css">
  <link rel="stylesheet" href="_static/sphinx-dropdown.css" type="text/css">
  <link rel="stylesheet" href="_static/panels-bootstrap.min.css" type="text/css">
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css">
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css">
    <link rel="index" title="Index" href="genindex.html">
    <link rel="search" title="Search" href="search.html">
    <link rel="next" title="torch.Size" href="size.html">
    <link rel="prev" title="torch.masked" href="masked.html">

<!--
  Search engines should not index the main version of documentation.
  Stable documentation are built without release == 'main'.
-->
<meta name="robots" content="noindex">


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head><body class="pytorch-body"><div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">学习</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class="dropdown-title">开始使用</span>
                  <p>在本地运行 PyTorch 或快速开始使用支持的云平台之一</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">教程</span><p></p>
                  <p>PyTorch 教程中的新内容</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">学习基础知识</span><p></p>
                  <p>熟悉 PyTorch 的概念和模块</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch 食谱</span><p></p>
                  <p>精简版、可直接部署的 PyTorch 代码示例</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">PyTorch 入门 - YouTube 系列</span><p></p>
                  <p>通过我们引人入胜的 YouTube 教程系列掌握 PyTorch 基础知识</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">生态系统</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">工具</span><p></p>
                  <p>了解 PyTorch 生态系统中的工具和框架</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">社区</span>
                  <p>加入 PyTorch 开发者社区，贡献、学习并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">论坛</span>
                  <p>讨论 PyTorch 代码、问题、安装、研究的地方</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">开发者资源</span>
                  <p>查找资源并获得问题解答</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">贡献者奖项 - 2024</span><p></p>
                  <p>本届 PyTorch 会议揭晓获奖者</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">关于 PyTorch Edge</span><p></p>
                  <p>为边缘设备构建创新和隐私感知的 AI 体验</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span><p></p>
                  <p>基于移动和边缘设备的端到端推理能力解决方案</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch 文档</span><p></p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">文档</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span><p></p>
                  <p>探索文档以获取全面指导，了解如何使用 PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch 领域</span><p></p>
                  <p>阅读 PyTorch 领域的文档，了解更多关于特定领域库的信息</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">博客与新闻</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch 博客</span><p></p>
                  <p>捕捉最新的技术新闻和事件</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">社区博客</span><p></p>
                  <p>PyTorch 生态系统故事</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">视频</span><p></p>
                  <p>了解最新的 PyTorch 教程、新内容等</p>
                </a><a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">社区故事</span><p></p>
                  <p>学习如何我们的社区使用 PyTorch 解决真实、日常的机器学习问题</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">活动</span><p></p>
                  <p>查找活动、网络研讨会和播客</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">通讯</span><p></p>
                  <p>跟踪最新更新</p>
                </a>
            </div>
          </div></li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">关于</a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch 基金会</span><p></p>
                  <p>了解更多关于 PyTorch 基金会的信息</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">管理委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">云信用计划</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">技术顾问委员会</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">员工</span><p></p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">联系我们</span><p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">成为会员</a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>



   

    

    <div class="table-of-contents-link-wrapper">
      <span>目录</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="https://pytorch.org/docs/versions.html">主程序 (2.7.0+cpu ) ▼</a>
    </div>
    <div id="searchBox">
    <div class="searchbox" id="googleSearchBox">
      <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>
      <div class="gcse-search"></div>
    </div>
    <div id="sphinxSearchBox" style="display: none;">
      <div role="search">
        <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
          <input type="text" name="q" placeholder="Search Docs">
          <input type="hidden" name="check_keywords" value="yes">
          <input type="hidden" name="area" value="default">
        </form>
      </div>
    </div>
  </div>
  <form id="searchForm">
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="google" checked="">谷歌搜索</label>
    <label style="margin-bottom: 1rem">
      <input type="radio" name="searchType" value="sphinx">经典搜索</label>
  </form>

  <script>
     document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
  </script>

          </div>

          

<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/nested.html">您正在查看不稳定开发者预览文档。请点击此处查看最新稳定版本的文档。</a>
</div>


            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">社区</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/build_ci_governance.html">PyTorch 治理 | 构建 + CI</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch 贡献指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/design.html">PyTorch 设计哲学</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch 治理 | 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch 治理 | 维护者</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者笔记</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">自动混合精度示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">广播语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU 多线程和 TorchScript 推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA 语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/custom_operators.html">PyTorch 自定义算子页面</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">分布式数据并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">扩展 PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.func.html">使用 autograd.Function 扩展 torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">常见问题解答</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/fsdp.html">FSDP 笔记</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/get_start_xpu.html">在 Intel GPU 上入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/gradcheck.html">Gradcheck 机制</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/hip.html">HIP (ROCm)语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">大规模部署功能</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/libtorch_stable_abi.html">LibTorch 稳定 ABI</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/modules.html">模块</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/mps.html">MPS 后端</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">多进程最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/numerical_accuracy.html">数值精度</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">可重现性</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">序列化语义</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows 常见问题解答</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语言绑定</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy.html">torch::deploy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">张量属性</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">张量视图</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="library.html">torch.library</a></li>
<li class="toctree-l1"><a class="reference internal" href="accelerator.html">torch.accelerator</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpu.html">torch.cpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html">理解 CUDA 内存使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">生成快照</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">使用可视化工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">快照 API 参考</a></li>
<li class="toctree-l1"><a class="reference internal" href="mps.html">torch.mps</a></li>
<li class="toctree-l1"><a class="reference internal" href="xpu.html">torch.xpu</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.html">torch.mtia</a></li>
<li class="toctree-l1"><a class="reference internal" href="mtia.memory.html">torch.mtia.memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta.html">元设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">torch.export</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.html">torch.distributed.tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.algorithms.join.html">torch.distributed.algorithms.join</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.elastic.html">torch.distributed.elastic</a></li>
<li class="toctree-l1"><a class="reference internal" href="fsdp.html">torch.distributed.fsdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.fsdp.fully_shard.html">torch.distributed.fsdp.fully_shard</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.tensor.parallel.html">torch.distributed.tensor.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.optim.html">torch.distributed.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.pipelining.html">torch.distributed.pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.checkpoint.html">torch.distributed.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.compiler.html">torch.compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="func.html">torch.func</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.html">torch.fx</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx.experimental.html">torch.fx.experimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitor.html">torch.monitor</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal.html">torch.signal</a></li>
<li class="toctree-l1"><a class="reference internal" href="special.html">torch.special</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch.overrides.html">torch.overrides</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">torch.package</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler.html">torch.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.attention.html">torch.nn.attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">复数</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_comm_hooks.html">DDP 通信钩子</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">量化</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">分布式 RPC 框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="masked.html">torch.masked</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.nested</a></li>
<li class="toctree-l1"><a class="reference internal" href="size.html">torch.Size</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">torch.testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">torch.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark_utils.html">torch.utils.benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="deterministic.html">torch.utils.deterministic</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_utils.html">torch.utils.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_tracker.html">torch.utils.module_tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">类型信息</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">命名张量</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">命名张量操作覆盖率</a></li>
<li class="toctree-l1"><a class="reference internal" href="config_mod.html">torch.__config__</a></li>
<li class="toctree-l1"><a class="reference internal" href="future_mod.html">torch.__future__</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">torch._logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_environment_variables.html">火炬环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">库</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">火炬推荐</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch 在 XLA 设备上</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/ao">torchao</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        文档 &gt;</li>

        
      <li>torch.nested</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/nested.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg" width="16" height="16"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">快捷键</div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="module-torch.nested">
<span id="torch-nested"></span><h1>torch.nested<a class="headerlink" href="#module-torch.nested" title="Permalink to this heading">¶</a></h1>
<section id="introduction">
<h2>简介</h2>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>PyTorch 嵌套张量的 API 目前处于原型阶段，未来将进行更改。</p>
</div>
<p>嵌套张量允许将不规则形状的数据包含在其中，并作为一个单独的张量进行操作。此类数据以高效的打包表示形式存储在下面，同时暴露标准的 PyTorch 张量接口以应用操作。</p>
<p>嵌套张量的一种常见应用是表达存在于各个领域的可变长度序列数据，例如变化的句子长度、图像大小以及音频/视频剪辑长度。传统上，此类数据通过填充序列到批次中的最大长度来处理，在填充形式上执行计算，然后通过掩码来去除填充。这种方法效率低下且容易出错，嵌套张量正是为了解决这些问题而存在的。</p>
<p>调用嵌套张量上操作的 API 与常规的 <code class="docutils literal "><span class="pre">torch.Tensor</span></code> 没有区别，允许与现有模型无缝集成，主要区别在于输入的构建。</p>
<p>由于这是一个原型功能，支持的运算集有限，但正在增长。我们欢迎提出问题、功能请求和贡献。有关贡献的更多信息，请参阅此 Readme。</p>
</section>
<section id="construction">
<span id="id1"></span><h2>构造 ¶</h2>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>PyTorch 中存在两种嵌套张量形式，由构造时指定的布局来区分。布局可以是 <code class="docutils literal "><span class="pre">torch.strided</span></code> 或 <code class="docutils literal "><span class="pre">torch.jagged</span></code> 。我们建议尽可能使用 <code class="docutils literal "><span class="pre">torch.jagged</span></code> 布局。虽然它目前只支持单个稀疏维度，但它具有更好的操作覆盖范围，正在积极开发中，并且与 <code class="docutils literal "><span class="pre">torch.compile</span></code> 很好地集成。这些文档遵循此建议，并将具有 <code class="docutils literal "><span class="pre">torch.jagged</span></code> 布局的嵌套张量简称为“NJTs”。</p>
</div>
<p>构造过程简单，涉及将张量列表传递给 <code class="docutils literal "><span class="pre">torch.nested.nested_tensor</span></code> 构造函数。具有 <code class="docutils literal "><span class="pre">torch.jagged</span></code> 布局的嵌套张量（又称“NJT”）支持单个稀疏维度。此构造函数将根据下文数据布局部分中描述的布局将输入张量复制到连续的内存块中。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([0, 1, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([3, 4, 5, 6, 7])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">([</span><span class="n">component</span> <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="n">nt</span><span class="p">])</span>
<span class="go">[tensor([0, 1, 2]), tensor([3, 4, 5, 6, 7])]</span>
</pre></div>
</div>
<p>列表中的每个张量必须具有相同的维度数，但形状可以在单个维度上有所不同。如果输入组件的维度不匹配，构造函数将抛出错误。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="c1"># 2D tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="c1"># 3D tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go">RuntimeError: When constructing a nested tensor, all tensors in list must have the same dim</span>
</pre></div>
</div>
<p>在构造过程中，可以通过通常的关键字参数选择 dtype、device 以及是否需要梯度。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">([</span><span class="n">component</span> <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="n">nt</span><span class="p">])</span>
<span class="go">[tensor([0., 1., 2.], device='cuda:0',</span>
<span class="go">       grad_fn=&lt;UnbindBackwardAutogradNestedTensor0&gt;), tensor([3., 4., 5., 6., 7.], device='cuda:0',</span>
<span class="go">       grad_fn=&lt;UnbindBackwardAutogradNestedTensor0&gt;)]</span>
</pre></div>
</div>
<p>可以用于从构造函数传递给张量的张量中保留自动微分历史。当使用此构造函数时，梯度将通过嵌套张量反向流回原始组件。请注意，此构造函数仍然会将输入组件复制到一个打包的连续内存块中。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">23</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">as_nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        ...,</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        ...,</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.]])</span>
</pre></div>
</div>
<p>上述函数都创建连续的 NJT，其中分配了一块内存来存储底层组件的打包形式（有关更多详细信息，请参阅下面的数据布局部分）。</p>
<p>还可以创建一个非连续的 NJT 视图，该视图基于现有的密集张量并带有填充，从而避免内存分配和复制。 <code class="docutils literal "><span class="pre">torch.nested.narrow()</span></code> 是实现此目的的工具。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">padded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seq_lens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="n">padded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">seq_lens</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, j1, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
<span class="go">False</span>
</pre></div>
</div>
<p>请注意，嵌套张量作为原始填充密集张量的视图，引用相同的内存而不进行复制/分配。对于非连续 NJT 的操作支持相对有限，因此如果您遇到支持差距，始终可以使用 <code class="docutils literal "><span class="pre">contiguous()</span></code> 将其转换为连续的 NJT。</p>
</section>
<section id="data-layout-and-shape">
<span id="data-layout"></span><h2>数据布局和形状</h2>
<p>为了提高效率，嵌套张量通常将张量组件打包成连续的内存块，并维护额外的元数据来指定批次项边界。对于 <code class="docutils literal "><span class="pre">torch.jagged</span></code> 布局，连续的内存块存储在 <code class="docutils literal "><span class="pre">values</span></code> 组件中，而 <code class="docutils literal "><span class="pre">offsets</span></code> 组件用于界定有锯齿维度的批次项边界。</p>
<img alt="_images/njt_visual.png" src="_images/njt_visual.png">
<p>在必要时，可以直接访问底层的 NJT 组件。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="c1"># text 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="c1"># text 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># note the "packing" of the ragged dimension; no padding needed</span>
<span class="go">torch.Size([82, 128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">offsets</span><span class="p">()</span>
<span class="go">tensor([ 0, 50, 82])</span>
</pre></div>
</div>
<p>直接从锯齿状的 <code class="docutils literal "><span class="pre">values</span></code> 和 <code class="docutils literal "><span class="pre">offsets</span></code> 组成部分构建 NJT 也可能很有用； <code class="docutils literal "><span class="pre">torch.nested.nested_tensor_from_jagged()</span></code> 构造函数用于此目的。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">82</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">82</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor_from_jagged</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">,</span> <span class="n">offsets</span><span class="o">=</span><span class="n">offsets</span><span class="p">)</span>
</pre></div>
</div>
<p>NJT 具有比其组成部分高一个维度的明确形状。下面示例中的粗糙维度的底层结构由一个符号值（ <code class="docutils literal "><span class="pre">j1</span></code> ）表示。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 128])</span>
</pre></div>
</div>
<p>NJT 必须具有相同的粗糙结构才能相互兼容。例如，要运行涉及两个 NJT 的二进制运算，粗糙结构必须匹配（即它们的形状中必须具有相同的粗糙形状符号）。在细节上，每个符号对应一个确切的 <code class="docutils literal "><span class="pre">offsets</span></code> 张量，因此两个 NJT 必须具有相同的 <code class="docutils literal "><span class="pre">offsets</span></code> 张量才能相互兼容。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt1</span><span class="o">.</span><span class="n">offsets</span><span class="p">()</span> <span class="ow">is</span> <span class="n">nt2</span><span class="o">.</span><span class="n">offsets</span><span class="p">()</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt3</span> <span class="o">=</span> <span class="n">nt1</span> <span class="o">+</span> <span class="n">nt2</span>
<span class="go">RuntimeError: cannot call binary pointwise function add.Tensor with inputs of shapes (2, j2, 128) and (2, j3, 128)</span>
</pre></div>
</div>
<p>在上述示例中，尽管两个 NJT 的概念形状相同，但它们不共享对同一个 <code class="docutils literal "><span class="pre">offsets</span></code> 张量的引用，因此它们的形状不同，它们不兼容。我们认识到这种行为是不直观的，并且正在努力为嵌套张量的 beta 版本放宽这一限制。有关解决方案，请参阅本文件的故障排除部分。</p>
<p>除了 <code class="docutils literal "><span class="pre">offsets</span></code> 元数据之外，NJT 还可以计算并缓存其组件的最小和最大序列长度，这对于调用特定内核（例如 SDPA）非常有用。目前还没有公开的 API 可以访问这些信息，但在 beta 版本中将会改变。</p>
</section>
<section id="supported-operations">
<span id="id2"></span><h2>支持的操作</h2>
<p>本节包含了一组您可能会用到的嵌套张量常见操作列表。这并不是一个全面的列表，因为 PyTorch 中有大约两千个操作。虽然其中相当一部分操作目前支持嵌套张量，但全面支持仍然是一个巨大的任务。嵌套张量的理想状态是支持所有适用于非嵌套张量的 PyTorch 操作。为了帮助我们实现这一目标，请考虑：</p>
<ul class="simple">
<li><p>在此处请求您用例所需的特定操作，以帮助我们确定优先级。</p></li>
<li><p>贡献！为给定的 PyTorch 操作添加嵌套张量支持并不困难；请参阅下面的贡献部分以获取详细信息。</p></li>
</ul>
<section id="viewing-nested-tensor-constituents">
<h3>查看嵌套张量的组成部分</h3>
<p> <code class="docutils literal "><span class="pre">unbind()</span></code> 允许您获取嵌套张量的组成部分视图。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">unbind</span><span class="p">()</span>
<span class="go">(tensor([[-0.9916, -0.3363, -0.2799],</span>
<span class="go">        [-2.3520, -0.5896, -0.4374]]), tensor([[-2.0969, -1.0104,  1.4841],</span>
<span class="go">        [ 2.0952,  0.2973,  0.2516],</span>
<span class="go">        [ 0.9035,  1.3623,  0.2026]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">unbind</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">a</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">unbind</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[ 3.6858, -3.7030, -4.4525],</span>
<span class="go">        [-2.3481,  2.0236,  0.1975]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">unbind</span><span class="p">()</span>
<span class="go">(tensor([[-2.9747, -1.0089, -0.8396],</span>
<span class="go">        [-7.0561, -1.7688, -1.3122]]), tensor([[-2.0969, -1.0104,  1.4841],</span>
<span class="go">        [ 2.0952,  0.2973,  0.2516],</span>
<span class="go">        [ 0.9035,  1.3623,  0.2026]]))</span>
</pre></div>
</div>
<p>注意， <code class="docutils literal "><span class="pre">nt.unbind()[0]</span></code> 不是一个副本，而是底层内存的切片，它代表了嵌套张量的第一个条目或组成部分。</p>
</section>
<section id="conversions-to-from-padded">
<h3>到/从填充的转换</h3>
<p>将 NJT 转换为具有指定填充值的填充密集张量。稀疏维度将被填充到最大序列长度的大小。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">padded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="n">nt</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mf">4.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">padded</span>
<span class="go">tensor([[[ 1.6107,  0.5723,  0.3913],</span>
<span class="go">         [ 0.0700, -0.4954,  1.8663],</span>
<span class="go">         [ 4.2000,  4.2000,  4.2000],</span>
<span class="go">         [ 4.2000,  4.2000,  4.2000],</span>
<span class="go">         [ 4.2000,  4.2000,  4.2000],</span>
<span class="go">         [ 4.2000,  4.2000,  4.2000]],</span>
<span class="go">        [[-0.0479, -0.7610, -0.3484],</span>
<span class="go">         [ 1.1345,  1.0556,  0.3634],</span>
<span class="go">         [-1.7122, -0.5921,  0.0540],</span>
<span class="go">         [-0.5506,  0.7608,  2.0606],</span>
<span class="go">         [ 1.5658, -1.1934,  0.3041],</span>
<span class="go">         [ 0.1483, -1.1284,  0.6957]]])</span>
</pre></div>
</div>
<p>这可以作为逃生口来绕过 NJT 支持差距，但理想情况下，应尽可能避免此类转换，以实现最佳内存使用和性能，因为更有效的嵌套张量布局不会产生填充。</p>
<p>反向转换可以使用 <code class="docutils literal "><span class="pre">torch.nested.narrow()</span></code> 完成，它将给定的密集张量应用于稀疏结构以产生 NJT。请注意，默认情况下，此操作不会复制底层数据，因此输出的 NJT 通常是连续的。如果需要连续的 NJT，则可能需要显式调用 <code class="docutils literal "><span class="pre">contiguous()</span></code> 。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">padded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seq_lens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="n">padded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">seq_lens</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, j1, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">nt</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, j2, 4])</span>
</pre></div>
</div>
</section>
<section id="shape-manipulations">
<h3>形状操作 ¶</h3>
<p>嵌套张量支持广泛的形状操作，包括视图。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 6, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 2, 3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">nt</span><span class="p">,</span> <span class="n">nt</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 12])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">nt</span><span class="p">,</span> <span class="n">nt</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 2, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 6, j1])</span>
</pre></div>
</div>
</section>
<section id="attention-mechanisms">
<h3>注意力机制</h3>
<p>由于可变长序列是注意力机制的常见输入，嵌套张量支持重要的注意力算子缩放点积注意力（SDPA）和 FlexAttention。有关 NJT 与 SDPA 的使用示例，请参阅此处；有关 NJT 与 FlexAttention 的使用示例，请参阅此处。</p>
</section>
</section>
<section id="usage-with-torch-compile">
<span id="id3"></span><h2>与 torch.compile 一起使用</h2>
<p>NJTs 设计用于与 <code class="docutils literal "><span class="pre">torch.compile()</span></code> 配合使用以实现最佳性能，我们始终建议在可能的情况下使用 <code class="docutils literal "><span class="pre">torch.compile()</span></code> 与 NJT 结合。NJT 在作为编译函数或模块的输入传递时，或者在函数内联实例化时，都能即插即用且无需断开图结构。</p>
<div class="admonition note">
<p class="admonition-title">注意</p>
<p>如果您无法为您的用例使用 <code class="docutils literal "><span class="pre">torch.compile()</span></code> ，性能和内存使用可能仍然会从 NJT 的使用中受益，但这种情况并不那么明显。重要的是要确保正在操作的张量足够大，以便性能提升不会因 Python 张量子类的开销而抵消。</p>
</div>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">sin</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compiled_f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">compiled_f</span><span class="p">(</span><span class="n">nt</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">offsets</span><span class="p">):</span> <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor_from_jagged</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span> <span class="o">*</span> <span class="mf">2.</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compiled_g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output2</span> <span class="o">=</span> <span class="n">compiled_g</span><span class="p">(</span><span class="n">nt</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">nt</span><span class="o">.</span><span class="n">offsets</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output2</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 3])</span>
</pre></div>
</div>
<p>注意，NJT 支持动态形状，以避免在结构变化时进行不必要的重新编译。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">sin</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compiled_f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output1</span> <span class="o">=</span> <span class="n">compiled_f</span><span class="p">(</span><span class="n">nt1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output2</span> <span class="o">=</span> <span class="n">compiled_f</span><span class="p">(</span><span class="n">nt2</span><span class="p">)</span>  <span class="c1"># NB: No recompile needed even though ragged structure differs</span>
</pre></div>
</div>
<p>如果您在使用 NJT + <code class="docutils literal "><span class="pre">torch.compile</span></code> 时遇到问题或古怪的错误，请提交 PyTorch 问题。在 <code class="docutils literal "><span class="pre">torch.compile</span></code> 中实现完整的子类支持是一个长期目标，目前可能存在一些粗糙的边缘。</p>
</section>
<section id="troubleshooting">
<span id="id4"></span><h2>故障排除</h2>
<p>本节包含您在利用嵌套张量时可能遇到的常见错误，以及这些错误的成因和解决建议。</p>
<section id="unimplemented-ops">
<span id="unimplemented-op"></span><h3>未实现的操作符</h3>
<p>随着嵌套张量操作符支持的增长，这种错误变得越来越少，但鉴于 PyTorch 中有数千个操作符，今天仍然有可能遇到。</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="ne">NotImplementedError</span><span class="p">:</span> <span class="n">aten</span><span class="o">.</span><span class="n">view_as_real</span><span class="o">.</span><span class="n">default</span>
</pre></div>
</div>
<p>错误很简单；我们还没有来得及为这个特定的操作符添加操作符支持。如果您愿意，您可以自己贡献一个实现，或者简单地请求我们在未来的 PyTorch 版本中添加对这个操作符的支持。</p>
</section>
<section id="ragged-structure-incompatibility">
<span id="id5"></span><h3>杂乱结构不兼容 ¶</h3>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">cannot</span> <span class="n">call</span> <span class="n">binary</span> <span class="n">pointwise</span> <span class="n">function</span> <span class="n">add</span><span class="o">.</span><span class="n">Tensor</span> <span class="k">with</span> <span class="n">inputs</span> <span class="n">of</span> <span class="n">shapes</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">j2</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">j3</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
</pre></div>
</div>
<p>当调用操作符对多个不兼容杂乱结构的 NJTs 进行操作时，会发生此错误。目前，要求输入 NJTs 必须具有完全相同的 <code class="docutils literal "><span class="pre">offsets</span></code> 构成成分，才能具有相同的符号杂乱结构符号（例如， <code class="docutils literal "><span class="pre">j1</span></code> ）。</p>
<p>对于这种情况，可以直接从 <code class="docutils literal "><span class="pre">values</span></code> 和 <code class="docutils literal "><span class="pre">offsets</span></code> 组件构建 NJTs。当两个 NJTs 都引用相同的 <code class="docutils literal "><span class="pre">offsets</span></code> 组件时，它们被认为具有相同的杂乱结构，因此是兼容的。</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor_from_jagged</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">82</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">offsets</span><span class="o">=</span><span class="n">nt1</span><span class="o">.</span><span class="n">offsets</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt3</span> <span class="o">=</span> <span class="n">nt1</span> <span class="o">+</span> <span class="n">nt2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt3</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 128])</span>
</pre></div>
</div>
</section>
<section id="data-dependent-operation-within-torch-compile">
<h3>torch.compile 中的数据相关操作 ¶</h3>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">exc</span><span class="o">.</span><span class="n">Unsupported</span><span class="p">:</span> <span class="n">data</span> <span class="n">dependent</span> <span class="n">operator</span><span class="p">:</span> <span class="n">aten</span><span class="o">.</span><span class="n">_local_scalar_dense</span><span class="o">.</span><span class="n">default</span><span class="p">;</span> <span class="n">to</span> <span class="n">enable</span><span class="p">,</span> <span class="nb">set</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">capture_scalar_outputs</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<p>这种错误发生在调用在 torch.compile 中执行数据相关操作的 op 时；这通常发生在需要检查 NJT 的 <code class="docutils literal "><span class="pre">offsets</span></code> 的值以确定输出形状的 op。例如：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">nt</span><span class="p">):</span> <span class="k">return</span> <span class="n">nt</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compiled_f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">compiled_f</span><span class="p">(</span><span class="n">nt</span><span class="p">)</span>
</pre></div>
</div>
<p>在这个例子中，在 NJT 的批处理维度上调用 <code class="docutils literal "><span class="pre">chunk()</span></code> 需要检查 NJT 的 <code class="docutils literal "><span class="pre">offsets</span></code> 数据以划分打包稀疏维度的批处理项边界。作为解决方案，可以设置几个 torch.compile 标志：</p>
<div class="doctest highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">capture_dynamic_output_shape_ops</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">capture_scalar_outputs</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<p>如果设置这些后仍然看到数据相关的操作错误，请向 PyTorch 提交问题。 <code class="docutils literal "><span class="pre">torch.compile()</span></code> 的这个区域仍在积极开发中，NJT 支持的一些方面可能不完整。</p>
</section>
</section>
<section id="contributions">
<span id="id6"></span><h2>贡献说明</h2>
<p>如果你希望为嵌套张量开发做出贡献，最有效的方法之一是为当前不支持 PyTorch 操作添加嵌套张量支持。这个过程通常包括几个简单的步骤：</p>
<ol class="arabic simple">
<li><p>确定要添加的操作名称；这应该类似于 <code class="docutils literal "><span class="pre">aten.view_as_real.default</span></code> 。该操作的签名可以在 <code class="docutils literal "><span class="pre">aten/src/ATen/native/native_functions.yaml</span></code> 中找到。</p></li>
<li><p>在 <code class="docutils literal "><span class="pre">torch/nested/_internal/ops.py</span></code> 中注册操作实现，遵循那里为其他操作建立的模式。使用 <code class="docutils literal "><span class="pre">native_functions.yaml</span></code> 中的签名进行模式验证。</p></li>
</ol>
<p>实现操作最常见的方法是将 NJT 拆解为其组成部分，在底层的 <code class="docutils literal "><span class="pre">values</span></code> 缓冲区上重新调度操作，并将相关的 NJT 元数据（包括 <code class="docutils literal "><span class="pre">offsets</span></code> ）传播到新的输出 NJT。如果操作的输出预期与输入具有不同的形状，则必须计算新的 <code class="docutils literal "><span class="pre">offsets</span></code> 等元数据。</p>
<p>当操作应用于批量或稀疏维度时，以下技巧可以帮助快速获得一个可工作的实现：</p>
<ul class="simple">
<li><p>对于非批量操作，应使用基于 <code class="docutils literal "><span class="pre">unbind()</span></code> 的回退方案。</p></li>
<li><p>对于对稀疏维度的操作，考虑将数据转换为带有适当选择的填充值的填充密集格式，运行操作，然后再转换回 NJT。在 <code class="docutils literal "><span class="pre">torch.compile</span></code> 中，这些转换可以融合以避免产生填充的中间结果。</p></li>
</ul>
</section>
<section id="detailed-docs-for-construction-and-conversion-functions">
<span id="construction-and-conversion"></span><h2>构造和转换函数的详细文档</h2>
<dl class="py function">
<dt class="sig sig-object py" id="torch.nested.nested_tensor">
<span class="sig-prename descclassname"><span class="pre">torch.nested.</span></span><span class="sig-name descname"><span class="pre">nested_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor_list</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nested.html#nested_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/nested/__init__.py#L210"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.nested.nested_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>从一个张量列表构建一个没有自动微分历史记录的嵌套张量（也称为“叶张量”，参见自动微分机制）。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>tensor_list (List[array_like]) – 张量列表，或任何可以传递给 torch.tensor 的内容，</p></li>
<li><p>维度。（列表中的每个元素都具有相同的） –</p></li>
</ul>
</dd>
<dt class="field-even">关键字参数<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p>dtype ( <code class="xref py py-class docutils literal "><span class="pre">torch.dtype</span></code> ，可选) – 返回嵌套张量的期望类型。默认：如果为 None，则与列表中最左侧的张量相同 <code class="xref py py-class docutils literal "><span class="pre">torch.dtype</span></code> 。</p></li>
<li><p>layout ( <code class="xref py py-class docutils literal "><span class="pre">torch.layout</span></code> ，可选) – 返回嵌套张量的期望布局。仅支持 strided 和 jagged 布局。默认：如果为 None，则使用 strided 布局。</p></li>
<li><p>device ( <code class="xref py py-class docutils literal "><span class="pre">torch.device</span></code> ，可选) – 返回嵌套张量的期望设备。默认：如果为 None，则与列表中最左侧的张量相同 <code class="xref py py-class docutils literal "><span class="pre">torch.device</span></code> 。</p></li>
<li><p>requires_grad (bool，可选) – 如果 autograd 应记录对返回的嵌套张量上的操作。默认： <code class="docutils literal "><span class="pre">False</span></code> 。</p></li>
<li><p>pin_memory（布尔值，可选）- 如果设置，返回的嵌套张量将在固定内存中分配。仅适用于 CPU 张量。默认值： <code class="docutils literal "><span class="pre">False</span></code> 。</p></li>
</ul>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>张量</em></a></p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.nested.nested_tensor_from_jagged">
<span class="sig-prename descclassname"><span class="pre">torch.nested.</span></span><span class="sig-name descname"><span class="pre">nested_tensor_from_jagged</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offsets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lengths</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">jagged_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_seqlen</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_seqlen</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nested.html#nested_tensor_from_jagged"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/nested/__init__.py#L359"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.nested.nested_tensor_from_jagged" title="Permalink to this definition">¶</a></dt>
<dd><p>从给定的交错组件构建交错布局嵌套张量。交错布局由一个必需的 values 缓冲区组成，交错维度打包到一个单独的维度中。offsets / lengths 元数据确定如何将此维度分割成批处理元素，并且预期将与 values 缓冲区分配在相同的设备上。</p>
<dl class="simple">
<dt>预期元数据格式：</dt><dd><ul class="simple">
<li><p>偏移量：打包维度内的索引，将其分割成不同大小的批次元素。例如：[0, 2, 3, 6] 表示一个大小为 6 的打包交错维度在概念上应分割成长度为[2, 1, 3]的批次元素。请注意，为了内核方便，需要提供起始和结束偏移量（即形状 batch_size + 1）。</p></li>
<li><p>长度：单个批次元素的长度的列表；形状 == batch_size。例如：[2, 1, 3] 表示一个大小为 6 的打包交错维度在概念上应分割成长度为[2, 1, 3]的批次元素。</p></li>
</ul>
</dd>
</dl>
<p>提供偏移量和长度可能很有用。这描述了一个具有“空洞”的嵌套张量，其中偏移量指示每个批次项的起始位置，长度指定元素的总数（见下例）。</p>
<p>返回的交错布局嵌套张量将是输入值张量的一个视图。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>值（ <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> ）- 基础缓冲区，形状为（sum_B(*), D_1, …, D_N）。交错维度被压缩成一个单一维度，使用偏移量/长度元数据来区分批处理元素。</p></li>
<li><p>偏移量（可选 <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> ）- 进入交错维度的偏移量，形状为 B + 1。</p></li>
<li><p>长度（可选 <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> ）- 形状为 B 的批处理元素长度。</p></li>
<li><p>jagged_dim（可选 python：int）- 指示 values 中的哪个维度是压缩的交错维度。如果为 None，则设置为 dim=1（即紧接批处理维度的维度）。默认：None</p></li>
<li><p>min_seqlen（可选 python：int）- 如果设置，则使用指定的值作为返回的嵌套张量的缓存最小序列长度。这可以作为一个有用的替代方案，以避免在需要时计算此值，可能避免 GPU -&gt; CPU 同步。默认：None</p></li>
<li><p>max_seqlen（可选 python：int）- 如果设置，则使用指定的值作为返回的嵌套张量的缓存最大序列长度。这可以作为一个有用的替代方案，以避免在需要时计算此值，可能避免 GPU -&gt; CPU 同步。默认：None</p></li>
</ul>
</dd>
<dt class="field-even">返回类型<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>张量</em></a></p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">nested_tensor_from_jagged</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 3D shape with the middle dimension jagged</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([5, j2, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Length of each item in the batch:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span><span class="o">.</span><span class="n">diff</span><span class="p">()</span>
<span class="go">tensor([3, 2, 1, 4, 2])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># NT with holes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">nested_tensor_from_jagged</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">nt</span><span class="o">.</span><span class="n">unbind</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Batch item 1 consists of indices [0, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Batch item 2 consists of indices [2, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">values</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Batch item 3 consists of indices [3, 5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">values</span><span class="p">[</span><span class="mi">3</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.nested.as_nested_tensor">
<span class="sig-prename descclassname"><span class="pre">torch.nested.</span></span><span class="sig-name descname"><span class="pre">as_nested_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ts</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nested.html#as_nested_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/nested/__init__.py#L27"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.nested.as_nested_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>从张量或张量列表/元组构建一个保留 autograd 历史的嵌套张量。</p>
<p>如果传递嵌套张量，除非设备/数据类型/布局不同，否则将直接返回。请注意，转换设备/数据类型将导致复制，而转换布局目前不支持此函数。</p>
<p>如果传递非嵌套张量，则将其视为大小一致的构成元素的批次。如果传递的设备/数据类型与输入不同，或者输入非连续，则将产生复制。否则，将直接使用输入的存储。</p>
<p>如果提供张量列表，则列表中的张量在构建嵌套张量时总是被复制。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><p>ts（Tensor 或 List[Tensor]或 Tuple[Tensor]）- 要作为嵌套张量处理的张量，或具有相同 ndim 的张量列表/元组</p>
</dd>
<dt class="field-even">关键字参数<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p>dtype ( <code class="xref py py-class docutils literal "><span class="pre">torch.dtype</span></code> ，可选) – 返回嵌套张量的期望类型。默认：如果为 None，则与列表中最左侧的张量相同 <code class="xref py py-class docutils literal "><span class="pre">torch.dtype</span></code> 。</p></li>
<li><p>device ( <code class="xref py py-class docutils literal "><span class="pre">torch.device</span></code> ，可选) – 返回嵌套张量的期望设备。默认：如果为 None，则与列表中最左侧的张量相同 <code class="xref py py-class docutils literal "><span class="pre">torch.device</span></code> 。</p></li>
<li><p>layout ( <code class="xref py py-class docutils literal "><span class="pre">torch.layout</span></code> ，可选) – 返回嵌套张量的期望布局。仅支持 strided 和 jagged 布局。默认：如果为 None，则使用 strided 布局。</p></li>
</ul>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>张量</em></a></p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">as_nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fake_grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">fake_grad</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([1., 1., 1.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([0., 0., 0., 0., 0.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">as_nested_tensor</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.nested.to_padded_tensor">
<span class="sig-prename descclassname"><span class="pre">torch.nested.</span></span><span class="sig-name descname"><span class="pre">to_padded_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><span class="pre">Tensor</span></a></span></span><a class="headerlink" href="#torch.nested.to_padded_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>通过填充 <code class="xref py py-attr docutils literal "><span class="pre">input</span></code> 嵌套张量返回一个新的（非嵌套）张量。前导条目将填充嵌套数据，而尾部条目将进行填充。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>由于嵌套张量和非嵌套张量在内存布局上不同， <code class="xref py py-func docutils literal "><span class="pre">to_padded_tensor()</span></code> 始终复制底层数据。</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><p>填充（浮点数）- 尾部条目的填充值。</p>
</dd>
<dt class="field-even">关键字参数<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p>输出大小（元组[int]）- 输出张量的大小。如果提供，它必须足够大以容纳所有嵌套数据；否则，将通过取每个嵌套子张量沿每个维度的最大大小来推断。</p></li>
<li><p>输出（张量，可选）- 输出张量。</p></li>
</ul>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))])</span>
<span class="go">nested_tensor([</span>
<span class="go">  tensor([[ 1.6862, -1.1282,  1.1031,  0.0464, -1.3276],</span>
<span class="go">          [-1.9967, -1.0054,  1.8972,  0.9174, -1.4995]]),</span>
<span class="go">  tensor([[-1.8546, -0.7194, -0.2918, -0.1846],</span>
<span class="go">          [ 0.2773,  0.8793, -0.5183, -0.6447],</span>
<span class="go">          [ 1.8009,  1.8468, -0.9832, -1.5272]])</span>
<span class="go">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt_infer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="n">nt</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
<span class="go">tensor([[[ 1.6862, -1.1282,  1.1031,  0.0464, -1.3276],</span>
<span class="go">         [-1.9967, -1.0054,  1.8972,  0.9174, -1.4995],</span>
<span class="go">         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],</span>
<span class="go">        [[-1.8546, -0.7194, -0.2918, -0.1846,  0.0000],</span>
<span class="go">         [ 0.2773,  0.8793, -0.5183, -0.6447,  0.0000],</span>
<span class="go">         [ 1.8009,  1.8468, -0.9832, -1.5272,  0.0000]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt_large</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="n">nt</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="go">tensor([[[ 1.6862, -1.1282,  1.1031,  0.0464, -1.3276,  1.0000],</span>
<span class="go">         [-1.9967, -1.0054,  1.8972,  0.9174, -1.4995,  1.0000],</span>
<span class="go">         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],</span>
<span class="go">         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]],</span>
<span class="go">        [[-1.8546, -0.7194, -0.2918, -0.1846,  1.0000,  1.0000],</span>
<span class="go">         [ 0.2773,  0.8793, -0.5183, -0.6447,  1.0000,  1.0000],</span>
<span class="go">         [ 1.8009,  1.8468, -0.9832, -1.5272,  1.0000,  1.0000],</span>
<span class="go">         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt_small</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="n">nt</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="go">RuntimeError: Value in output_size is less than NestedTensor padded size. Truncation is not supported.</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.nested.masked_select">
<span class="sig-prename descclassname"><span class="pre">torch.nested.</span></span><span class="sig-name descname"><span class="pre">masked_select</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nested.html#masked_select"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/nested/__init__.py#L468"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.nested.masked_select" title="Permalink to this definition">¶</a></dt>
<dd><p>根据给定的步长张量输入和步长掩码构建嵌套张量，生成的错落布局嵌套张量将在掩码等于 True 的位置保留值。掩码的维度被保留，并使用偏移量表示，这与 <code class="xref py py-func docutils literal "><span class="pre">masked_select()</span></code> 不同，其输出被折叠为 1D 张量。</p>
<p>参数：tensor（ <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> ）：构建错落布局嵌套张量的步长张量。mask（ <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> ）：应用于张量输入的步长掩码张量</p>
<p>示例：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, j4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Length of each item in the batch:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">offsets</span><span class="p">()</span><span class="o">.</span><span class="n">diff</span><span class="p">()</span>
<span class="go">tensor([1, 2, 1])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="kc">False</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([6, j5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Length of each item in the batch:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">offsets</span><span class="p">()</span><span class="o">.</span><span class="n">diff</span><span class="p">()</span>
<span class="go">tensor([0, 0, 0, 0, 0, 0])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>张量</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="torch.nested.narrow">
<span class="sig-prename descclassname"><span class="pre">torch.nested.</span></span><span class="sig-name descname"><span class="pre">narrow</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.strided</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nested.html#narrow"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="reference external" href="https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/nested/__init__.py#L281"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.nested.narrow" title="Permalink to this definition">¶</a></dt>
<dd><p>从 <code class="xref py py-attr docutils literal "><span class="pre">tensor</span></code> 构建一个嵌套张量（可能是一个视图），这是一个带偏移量的张量。这与 torch.Tensor.narrow 的语义相似，在 <code class="xref py py-attr docutils literal "><span class="pre">dim</span></code> 维度中，新的嵌套张量只显示[start, start+length)区间的元素。由于嵌套表示允许在该维度的每一“行”中具有不同的起始和长度，因此 <code class="xref py py-attr docutils literal "><span class="pre">start</span></code> 和 <code class="xref py py-attr docutils literal "><span class="pre">length</span></code> 也可以是形状为 tensor.shape[0]的张量。</p>
<p>根据嵌套张量的布局，会有一些差异。如果使用 strided 布局，torch.narrow 将复制缩小的数据到一个连续的 NT（嵌套张量）中，而 jagged 布局的 narrow()将创建原始带偏移量张量的非连续视图。这种特定的表示对于在 Transformer 模型中表示 kv 缓存非常有用，因为专门的 SDPA 内核可以轻松处理这种格式，从而提高性能。</p>
<dl class="field-list simple">
<dt class="field-odd">参数<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>tensor ( <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> ) – 一个带偏移量的张量，如果使用 jagged 布局，将作为嵌套张量的底层数据，如果使用 strided 布局，则将进行复制。</p></li>
<li><p>dim（int）- 应用狭窄操作的维度。对于交错布局，仅支持 dim=1，而 strided 支持所有维度</p></li>
<li><p>start（Union[int, <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> ]）- 狭窄操作的起始元素</p></li>
<li><p>length（Union[int, <code class="xref py py-class docutils literal "><span class="pre">torch.Tensor</span></code> ]）- 狭窄操作期间取出的元素数量</p></li>
</ul>
</dd>
<dt class="field-even">关键字参数<span class="colon">:</span></dt>
<dd class="field-even"><p>layout（ <code class="xref py py-class docutils literal "><span class="pre">torch.layout</span></code> ，可选）- 返回嵌套张量的期望布局。仅支持 strided 和交错布局。默认：如果为 None，则为 strided 布局</p>
</dd>
<dt class="field-odd">返回类型<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>张量</em></a></p>
</dd>
</dl>
<p>示例：</p>
<div class="highlight-default "><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">starts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">narrow_base</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt_narrowed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="n">narrow_base</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">starts</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt_narrowed</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
<span class="go">False</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        下一个 <img height="16" width="16" class="next-page" src="_static/images/chevron-right-orange.svg"> <img height="16" width="16" class="previous-page" src="_static/images/chevron-right-orange.svg"> 上一个
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>© 版权所有 PyTorch 贡献者。</p>
  </div>
    
      <div>使用 Sphinx 构建，主题由 Read the Docs 提供。</div>
     

</footer>

          </div>
<script>

var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.nested</a><ul>
<li><a class="reference internal" href="#introduction">简介</a></li>
<li><a class="reference internal" href="#construction">施工</a></li>
<li><a class="reference internal" href="#data-layout-and-shape">数据布局与形状</a></li>
<li><a class="reference internal" href="#supported-operations">支持的操作</a><ul>
<li><a class="reference internal" href="#viewing-nested-tensor-constituents">查看嵌套张量元素</a></li>
<li><a class="reference internal" href="#conversions-to-from-padded">转换为/从填充中</a></li>
<li><a class="reference internal" href="#shape-manipulations">形状操作</a></li>
<li><a class="reference internal" href="#attention-mechanisms">注意力机制</a></li>
</ul>
</li>
<li><a class="reference internal" href="#usage-with-torch-compile">使用 torch.compile</a></li>
<li><a class="reference internal" href="#troubleshooting">故障排除</a><ul>
<li><a class="reference internal" href="#unimplemented-ops">未实现的操作</a></li>
<li><a class="reference internal" href="#ragged-structure-incompatibility">Ragged 结构不兼容</a></li>
<li><a class="reference internal" href="#data-dependent-operation-within-torch-compile">PyTorch 编译中的数据依赖操作</a></li>
</ul>
</li>
<li><a class="reference internal" href="#contributions">贡献</a></li>
<li><a class="reference internal" href="#detailed-docs-for-construction-and-conversion-functions">构造和转换函数的详细文档</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script script="" type="text/javascript">
  var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0">


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>文档</h2>
          <p>PyTorch 开发者文档全面访问</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">查看文档</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>教程</h2>
          <p>获取初学者和高级开发者的深入教程</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">查看教程</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>资源</h2>
          <p>查找开发资源并获得您的疑问解答</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">查看资源</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">开始使用</a></li>
            <li><a href="https://pytorch.org/features">功能</a></li>
            <li><a href="https://pytorch.org/ecosystem">生态系统</a></li>
            <li><a href="https://pytorch.org/blog/">博客</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">贡献</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">资源</a></li>
            <li><a href="https://pytorch.org/tutorials">教程</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">文档</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">讨论</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub 问题和任务</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">品牌指南</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">保持更新</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">推特</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">领英</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch 播客</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">苹果</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">谷歌</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">亚马逊</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">条款</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">隐私</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© 版权所有 Linux 基金会。PyTorch 基金会是 Linux 基金会的一个项目。有关本网站的使用条款、商标政策以及其他适用于 PyTorch 基金会的政策，请参阅 www.linuxfoundation.org/policies/。PyTorch 基金会支持 PyTorch 开源项目，该项目已被确立为 LF Projects, LLC 的 PyTorch 项目系列。有关适用于 PyTorch 项目系列 LF Projects, LLC 的政策，请参阅 www.lfprojects.org/policies/。</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">为分析流量并优化您的体验，我们在本网站上提供 cookies。通过点击或导航，您同意允许我们使用 cookies。作为本站点的当前维护者，Facebook 的 Cookies 政策适用。了解更多信息，包括可用的控制选项：Cookies 政策。</p>
    <img class="close-button" src="_static/images/pytorch-x.svg" width="16" height="16">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>学习</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">开始学习</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">教程</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">学习基础知识</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch 菜谱</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">PyTorch 入门 - YouTube 系列</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>生态系统</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">工具</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">社区</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">论坛</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">开发者资源</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">贡献者奖项 - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">关于 PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch 文档</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>文档</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch 领域</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>博客 &amp; 新闻</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch 博客</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">社区博客</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">视频</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">社区故事</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">活动</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">通讯</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>关于</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch 基金会</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">治理委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">云信用计划</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">技术顾问委员会</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">员工</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">联系我们</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

</body></html>